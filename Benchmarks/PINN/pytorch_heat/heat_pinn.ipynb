{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extend the 1D case from https://github.com/udemirezen/PINN-1 to the 2D heat diffusion\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# diffusion parameter\n",
    "D = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Number of inputs and outputs: #inputs = 3 (x,y,t); #outputs = 1 (u)\n",
    "\n",
    "This NN has 5 hidden layer with 5 neurons each\n",
    "\"\"\"\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.hidden_layer1 = nn.Linear(3,5)\n",
    "        self.hidden_layer2 = nn.Linear(5,5)\n",
    "        self.hidden_layer3 = nn.Linear(5,5)\n",
    "        self.hidden_layer4 = nn.Linear(5,5)\n",
    "        self.hidden_layer5 = nn.Linear(5,5)\n",
    "        self.output_layer = nn.Linear(5,1)\n",
    "\n",
    "    def forward(self,x,y,t):\n",
    "        inputs = torch.cat([x,y,t],axis=1)\n",
    "        layer1_out = torch.sigmoid(self.hidden_layer1(inputs))\n",
    "        layer2_out = torch.sigmoid(self.hidden_layer2(layer1_out))\n",
    "        layer3_out = torch.sigmoid(self.hidden_layer3(layer2_out))\n",
    "        layer4_out = torch.sigmoid(self.hidden_layer4(layer3_out))\n",
    "        layer5_out = torch.sigmoid(self.hidden_layer5(layer4_out))\n",
    "        output = self.output_layer(layer5_out) ## For regression, no activation is used in output layer\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a nn model\n",
    "net = Net()\n",
    "net = net.to(device)\n",
    "mse_cost_function = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the heat equation\n",
    "def f(x,y,t,net):\n",
    "    u = net(x,y,t)\n",
    "    u_x = torch.autograd.grad(u.sum(), x, create_graph=True)[0]\n",
    "    u_xx = torch.autograd.grad(u_x.sum(), x, create_graph=True)[0]\n",
    "    u_y = torch.autograd.grad(u.sum(), y, create_graph=True)[0]\n",
    "    u_yy = torch.autograd.grad(u_y.sum(), y, create_graph=True)[0]\n",
    "    u_t = torch.autograd.grad(u.sum(), t, create_graph=True)[0]\n",
    "    pde = u_t - D*(u_xx + u_yy)\n",
    "    return pde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = 10 # number of points along each boundary\n",
    "def gen_boundarypts():\n",
    "    x_bc,y_bc,u_bc,t_bc = np.zeros((2000,1)),np.zeros((2000,1)),np.zeros((2000,1)),np.zeros((2000,1))\n",
    "    t_points = np.random.uniform(low=0.0, high=1.0, size=(50,1))\n",
    "    for i,t in enumerate(t_points):\n",
    "        t_bc[i*4*nb:(i+1)*4*nb,0] = np.full((4*nb,),t)\n",
    "        # below code assigns 40 points at each timestep\n",
    "\n",
    "        # top boundary is prescribed as a finite value\n",
    "        #x_bc[i*4*nb:(i*4+1)*nb,0] = np.random.uniform(low=-0.5, high=0.5, size=(10,)) # x coordinate\n",
    "        x_bc[i*4*nb:(i*4+1)*nb,0] = np.linspace(-0.5,0.5,nb)\n",
    "        y_bc[i*4*nb:(i*4+1)*nb,0] = np.full((nb,),0.5) # y coordinate\n",
    "        u_bc[i*4*nb:(i*4+1)*nb,0] = np.full((nb,),1.0) # fixed temperature value\n",
    "        # bottom\n",
    "        #x_bc[(i*4+1)*nb:(i*4+2)*nb,0] = np.random.uniform(low=-0.5, high=0.5, size=(10,)) # x coordinate\n",
    "        x_bc[(i*4+1)*nb:(i*4+2)*nb,0] = np.linspace(-0.5,0.5,nb)\n",
    "        y_bc[(i*4+1)*nb:(i*4+2)*nb,0] = np.full((nb,),-0.5) # y coordinate\n",
    "        # left\n",
    "        x_bc[(i*4+2)*nb:(i*4+3)*nb,0] = np.full((nb,),-0.5) # x coordinate\n",
    "        #y_bc[(i*4+2)*nb:(i*4+3)*nb,0] = np.random.uniform(low=-0.5, high=0.5, size=(10,)) # y coordinate\n",
    "        y_bc[(i*4+2)*nb:(i*4+3)*nb,0] = np.linspace(-0.5,0.5,nb)\n",
    "        # right\n",
    "        x_bc[(i*4+3)*nb:(i*4+4)*nb,0] = np.full((nb,),0.5) # x coordinate\n",
    "        #y_bc[(i*4+3)*nb:(i*4+4)*nb,0] = np.random.uniform(low=-0.5, high=0.5, size=(10,)) # y coordinate\n",
    "        y_bc[(i*4+3)*nb:(i*4+4)*nb,0] = np.linspace(-0.5,0.5,nb)\n",
    "    return x_bc,y_bc,u_bc,t_bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5       , -0.38888889, -0.27777778, -0.16666667, -0.05555556,\n",
       "        0.05555556,  0.16666667,  0.27777778,  0.38888889,  0.5       ])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(-0.5,0.5,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# obtain training data from initial boundary conditions\n",
    "# initial condition: u = 0 at t = 0\n",
    "x_ic = np.random.uniform(low=-0.5, high=0.5, size=(200,1))\n",
    "y_ic = np.random.uniform(low=-0.5, high=0.5, size=(200,1))\n",
    "t_ic = np.zeros((200,1))\n",
    "u_ic = np.zeros((200,1))\n",
    "\n",
    "# boundary condition: u_top = 1; else boundary = 0\n",
    "# time: t = 0~1\n",
    "x_bc,y_bc,u_bc,t_bc = gen_boundarypts()\n",
    "\n",
    "x_ibc = np.concatenate((x_ic,x_bc),axis=0)\n",
    "y_ibc = np.concatenate((y_ic,y_bc),axis=0)\n",
    "t_ibc = np.concatenate((t_ic,t_bc),axis=0)\n",
    "u_ibc = np.concatenate((u_ic,u_bc),axis=0)\n",
    "\n",
    "# plot boundary points\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.Figure()\n",
    "plt.scatter(x_bc,y_bc,s=0.2)\n",
    "axes=plt.gca()\n",
    "axes.set_aspect(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Training Loss: tensor(0.4561)\n",
      "1 Training Loss: tensor(0.4512)\n",
      "2 Training Loss: tensor(0.4463)\n",
      "3 Training Loss: tensor(0.4414)\n",
      "4 Training Loss: tensor(0.4367)\n",
      "5 Training Loss: tensor(0.4319)\n",
      "6 Training Loss: tensor(0.4272)\n",
      "7 Training Loss: tensor(0.4226)\n",
      "8 Training Loss: tensor(0.4180)\n",
      "9 Training Loss: tensor(0.4135)\n",
      "10 Training Loss: tensor(0.4091)\n",
      "11 Training Loss: tensor(0.4046)\n",
      "12 Training Loss: tensor(0.4003)\n",
      "13 Training Loss: tensor(0.3960)\n",
      "14 Training Loss: tensor(0.3918)\n",
      "15 Training Loss: tensor(0.3876)\n",
      "16 Training Loss: tensor(0.3835)\n",
      "17 Training Loss: tensor(0.3794)\n",
      "18 Training Loss: tensor(0.3754)\n",
      "19 Training Loss: tensor(0.3715)\n",
      "20 Training Loss: tensor(0.3676)\n",
      "21 Training Loss: tensor(0.3638)\n",
      "22 Training Loss: tensor(0.3600)\n",
      "23 Training Loss: tensor(0.3563)\n",
      "24 Training Loss: tensor(0.3527)\n",
      "25 Training Loss: tensor(0.3491)\n",
      "26 Training Loss: tensor(0.3455)\n",
      "27 Training Loss: tensor(0.3421)\n",
      "28 Training Loss: tensor(0.3386)\n",
      "29 Training Loss: tensor(0.3353)\n",
      "30 Training Loss: tensor(0.3319)\n",
      "31 Training Loss: tensor(0.3287)\n",
      "32 Training Loss: tensor(0.3255)\n",
      "33 Training Loss: tensor(0.3223)\n",
      "34 Training Loss: tensor(0.3192)\n",
      "35 Training Loss: tensor(0.3162)\n",
      "36 Training Loss: tensor(0.3132)\n",
      "37 Training Loss: tensor(0.3103)\n",
      "38 Training Loss: tensor(0.3074)\n",
      "39 Training Loss: tensor(0.3045)\n",
      "40 Training Loss: tensor(0.3017)\n",
      "41 Training Loss: tensor(0.2990)\n",
      "42 Training Loss: tensor(0.2963)\n",
      "43 Training Loss: tensor(0.2937)\n",
      "44 Training Loss: tensor(0.2911)\n",
      "45 Training Loss: tensor(0.2885)\n",
      "46 Training Loss: tensor(0.2860)\n",
      "47 Training Loss: tensor(0.2836)\n",
      "48 Training Loss: tensor(0.2811)\n",
      "49 Training Loss: tensor(0.2788)\n",
      "50 Training Loss: tensor(0.2765)\n",
      "51 Training Loss: tensor(0.2742)\n",
      "52 Training Loss: tensor(0.2719)\n",
      "53 Training Loss: tensor(0.2697)\n",
      "54 Training Loss: tensor(0.2676)\n",
      "55 Training Loss: tensor(0.2655)\n",
      "56 Training Loss: tensor(0.2634)\n",
      "57 Training Loss: tensor(0.2614)\n",
      "58 Training Loss: tensor(0.2594)\n",
      "59 Training Loss: tensor(0.2574)\n",
      "60 Training Loss: tensor(0.2555)\n",
      "61 Training Loss: tensor(0.2536)\n",
      "62 Training Loss: tensor(0.2518)\n",
      "63 Training Loss: tensor(0.2500)\n",
      "64 Training Loss: tensor(0.2482)\n",
      "65 Training Loss: tensor(0.2465)\n",
      "66 Training Loss: tensor(0.2447)\n",
      "67 Training Loss: tensor(0.2431)\n",
      "68 Training Loss: tensor(0.2414)\n",
      "69 Training Loss: tensor(0.2398)\n",
      "70 Training Loss: tensor(0.2383)\n",
      "71 Training Loss: tensor(0.2367)\n",
      "72 Training Loss: tensor(0.2352)\n",
      "73 Training Loss: tensor(0.2337)\n",
      "74 Training Loss: tensor(0.2323)\n",
      "75 Training Loss: tensor(0.2309)\n",
      "76 Training Loss: tensor(0.2295)\n",
      "77 Training Loss: tensor(0.2281)\n",
      "78 Training Loss: tensor(0.2268)\n",
      "79 Training Loss: tensor(0.2255)\n",
      "80 Training Loss: tensor(0.2242)\n",
      "81 Training Loss: tensor(0.2230)\n",
      "82 Training Loss: tensor(0.2217)\n",
      "83 Training Loss: tensor(0.2205)\n",
      "84 Training Loss: tensor(0.2194)\n",
      "85 Training Loss: tensor(0.2182)\n",
      "86 Training Loss: tensor(0.2171)\n",
      "87 Training Loss: tensor(0.2160)\n",
      "88 Training Loss: tensor(0.2149)\n",
      "89 Training Loss: tensor(0.2139)\n",
      "90 Training Loss: tensor(0.2129)\n",
      "91 Training Loss: tensor(0.2119)\n",
      "92 Training Loss: tensor(0.2109)\n",
      "93 Training Loss: tensor(0.2099)\n",
      "94 Training Loss: tensor(0.2090)\n",
      "95 Training Loss: tensor(0.2081)\n",
      "96 Training Loss: tensor(0.2072)\n",
      "97 Training Loss: tensor(0.2063)\n",
      "98 Training Loss: tensor(0.2054)\n",
      "99 Training Loss: tensor(0.2046)\n",
      "100 Training Loss: tensor(0.2038)\n",
      "101 Training Loss: tensor(0.2030)\n",
      "102 Training Loss: tensor(0.2022)\n",
      "103 Training Loss: tensor(0.2014)\n",
      "104 Training Loss: tensor(0.2007)\n",
      "105 Training Loss: tensor(0.2000)\n",
      "106 Training Loss: tensor(0.1993)\n",
      "107 Training Loss: tensor(0.1986)\n",
      "108 Training Loss: tensor(0.1979)\n",
      "109 Training Loss: tensor(0.1973)\n",
      "110 Training Loss: tensor(0.1966)\n",
      "111 Training Loss: tensor(0.1960)\n",
      "112 Training Loss: tensor(0.1954)\n",
      "113 Training Loss: tensor(0.1948)\n",
      "114 Training Loss: tensor(0.1942)\n",
      "115 Training Loss: tensor(0.1937)\n",
      "116 Training Loss: tensor(0.1931)\n",
      "117 Training Loss: tensor(0.1926)\n",
      "118 Training Loss: tensor(0.1921)\n",
      "119 Training Loss: tensor(0.1915)\n",
      "120 Training Loss: tensor(0.1911)\n",
      "121 Training Loss: tensor(0.1906)\n",
      "122 Training Loss: tensor(0.1901)\n",
      "123 Training Loss: tensor(0.1896)\n",
      "124 Training Loss: tensor(0.1892)\n",
      "125 Training Loss: tensor(0.1888)\n",
      "126 Training Loss: tensor(0.1884)\n",
      "127 Training Loss: tensor(0.1879)\n",
      "128 Training Loss: tensor(0.1875)\n",
      "129 Training Loss: tensor(0.1872)\n",
      "130 Training Loss: tensor(0.1868)\n",
      "131 Training Loss: tensor(0.1864)\n",
      "132 Training Loss: tensor(0.1861)\n",
      "133 Training Loss: tensor(0.1857)\n",
      "134 Training Loss: tensor(0.1854)\n",
      "135 Training Loss: tensor(0.1851)\n",
      "136 Training Loss: tensor(0.1847)\n",
      "137 Training Loss: tensor(0.1844)\n",
      "138 Training Loss: tensor(0.1841)\n",
      "139 Training Loss: tensor(0.1838)\n",
      "140 Training Loss: tensor(0.1836)\n",
      "141 Training Loss: tensor(0.1833)\n",
      "142 Training Loss: tensor(0.1830)\n",
      "143 Training Loss: tensor(0.1828)\n",
      "144 Training Loss: tensor(0.1825)\n",
      "145 Training Loss: tensor(0.1823)\n",
      "146 Training Loss: tensor(0.1820)\n",
      "147 Training Loss: tensor(0.1818)\n",
      "148 Training Loss: tensor(0.1816)\n",
      "149 Training Loss: tensor(0.1814)\n",
      "150 Training Loss: tensor(0.1812)\n",
      "151 Training Loss: tensor(0.1810)\n",
      "152 Training Loss: tensor(0.1808)\n",
      "153 Training Loss: tensor(0.1806)\n",
      "154 Training Loss: tensor(0.1804)\n",
      "155 Training Loss: tensor(0.1802)\n",
      "156 Training Loss: tensor(0.1801)\n",
      "157 Training Loss: tensor(0.1799)\n",
      "158 Training Loss: tensor(0.1797)\n",
      "159 Training Loss: tensor(0.1796)\n",
      "160 Training Loss: tensor(0.1794)\n",
      "161 Training Loss: tensor(0.1793)\n",
      "162 Training Loss: tensor(0.1792)\n",
      "163 Training Loss: tensor(0.1790)\n",
      "164 Training Loss: tensor(0.1789)\n",
      "165 Training Loss: tensor(0.1788)\n",
      "166 Training Loss: tensor(0.1786)\n",
      "167 Training Loss: tensor(0.1785)\n",
      "168 Training Loss: tensor(0.1784)\n",
      "169 Training Loss: tensor(0.1783)\n",
      "170 Training Loss: tensor(0.1782)\n",
      "171 Training Loss: tensor(0.1781)\n",
      "172 Training Loss: tensor(0.1780)\n",
      "173 Training Loss: tensor(0.1779)\n",
      "174 Training Loss: tensor(0.1778)\n",
      "175 Training Loss: tensor(0.1777)\n",
      "176 Training Loss: tensor(0.1776)\n",
      "177 Training Loss: tensor(0.1776)\n",
      "178 Training Loss: tensor(0.1775)\n",
      "179 Training Loss: tensor(0.1774)\n",
      "180 Training Loss: tensor(0.1773)\n",
      "181 Training Loss: tensor(0.1773)\n",
      "182 Training Loss: tensor(0.1772)\n",
      "183 Training Loss: tensor(0.1771)\n",
      "184 Training Loss: tensor(0.1771)\n",
      "185 Training Loss: tensor(0.1770)\n",
      "186 Training Loss: tensor(0.1769)\n",
      "187 Training Loss: tensor(0.1769)\n",
      "188 Training Loss: tensor(0.1768)\n",
      "189 Training Loss: tensor(0.1768)\n",
      "190 Training Loss: tensor(0.1767)\n",
      "191 Training Loss: tensor(0.1767)\n",
      "192 Training Loss: tensor(0.1766)\n",
      "193 Training Loss: tensor(0.1766)\n",
      "194 Training Loss: tensor(0.1765)\n",
      "195 Training Loss: tensor(0.1765)\n",
      "196 Training Loss: tensor(0.1765)\n",
      "197 Training Loss: tensor(0.1764)\n",
      "198 Training Loss: tensor(0.1764)\n",
      "199 Training Loss: tensor(0.1764)\n",
      "200 Training Loss: tensor(0.1763)\n",
      "201 Training Loss: tensor(0.1763)\n",
      "202 Training Loss: tensor(0.1763)\n",
      "203 Training Loss: tensor(0.1762)\n",
      "204 Training Loss: tensor(0.1762)\n",
      "205 Training Loss: tensor(0.1762)\n",
      "206 Training Loss: tensor(0.1762)\n",
      "207 Training Loss: tensor(0.1761)\n",
      "208 Training Loss: tensor(0.1761)\n",
      "209 Training Loss: tensor(0.1761)\n",
      "210 Training Loss: tensor(0.1761)\n",
      "211 Training Loss: tensor(0.1760)\n",
      "212 Training Loss: tensor(0.1760)\n",
      "213 Training Loss: tensor(0.1760)\n",
      "214 Training Loss: tensor(0.1760)\n",
      "215 Training Loss: tensor(0.1760)\n",
      "216 Training Loss: tensor(0.1759)\n",
      "217 Training Loss: tensor(0.1759)\n",
      "218 Training Loss: tensor(0.1759)\n",
      "219 Training Loss: tensor(0.1759)\n",
      "220 Training Loss: tensor(0.1759)\n",
      "221 Training Loss: tensor(0.1759)\n",
      "222 Training Loss: tensor(0.1759)\n",
      "223 Training Loss: tensor(0.1758)\n",
      "224 Training Loss: tensor(0.1758)\n",
      "225 Training Loss: tensor(0.1758)\n",
      "226 Training Loss: tensor(0.1758)\n",
      "227 Training Loss: tensor(0.1758)\n",
      "228 Training Loss: tensor(0.1758)\n",
      "229 Training Loss: tensor(0.1758)\n",
      "230 Training Loss: tensor(0.1758)\n",
      "231 Training Loss: tensor(0.1758)\n",
      "232 Training Loss: tensor(0.1758)\n",
      "233 Training Loss: tensor(0.1758)\n",
      "234 Training Loss: tensor(0.1757)\n",
      "235 Training Loss: tensor(0.1757)\n",
      "236 Training Loss: tensor(0.1757)\n",
      "237 Training Loss: tensor(0.1757)\n",
      "238 Training Loss: tensor(0.1757)\n",
      "239 Training Loss: tensor(0.1757)\n",
      "240 Training Loss: tensor(0.1757)\n",
      "241 Training Loss: tensor(0.1757)\n",
      "242 Training Loss: tensor(0.1757)\n",
      "243 Training Loss: tensor(0.1757)\n",
      "244 Training Loss: tensor(0.1757)\n",
      "245 Training Loss: tensor(0.1757)\n",
      "246 Training Loss: tensor(0.1757)\n",
      "247 Training Loss: tensor(0.1757)\n",
      "248 Training Loss: tensor(0.1757)\n",
      "249 Training Loss: tensor(0.1757)\n",
      "250 Training Loss: tensor(0.1757)\n",
      "251 Training Loss: tensor(0.1757)\n",
      "252 Training Loss: tensor(0.1757)\n",
      "253 Training Loss: tensor(0.1757)\n",
      "254 Training Loss: tensor(0.1757)\n",
      "255 Training Loss: tensor(0.1756)\n",
      "256 Training Loss: tensor(0.1756)\n",
      "257 Training Loss: tensor(0.1756)\n",
      "258 Training Loss: tensor(0.1756)\n",
      "259 Training Loss: tensor(0.1756)\n",
      "260 Training Loss: tensor(0.1756)\n",
      "261 Training Loss: tensor(0.1756)\n",
      "262 Training Loss: tensor(0.1756)\n",
      "263 Training Loss: tensor(0.1756)\n",
      "264 Training Loss: tensor(0.1756)\n",
      "265 Training Loss: tensor(0.1756)\n",
      "266 Training Loss: tensor(0.1756)\n",
      "267 Training Loss: tensor(0.1756)\n",
      "268 Training Loss: tensor(0.1756)\n",
      "269 Training Loss: tensor(0.1756)\n",
      "270 Training Loss: tensor(0.1756)\n",
      "271 Training Loss: tensor(0.1756)\n",
      "272 Training Loss: tensor(0.1756)\n",
      "273 Training Loss: tensor(0.1756)\n",
      "274 Training Loss: tensor(0.1756)\n",
      "275 Training Loss: tensor(0.1756)\n",
      "276 Training Loss: tensor(0.1756)\n",
      "277 Training Loss: tensor(0.1756)\n",
      "278 Training Loss: tensor(0.1756)\n",
      "279 Training Loss: tensor(0.1756)\n",
      "280 Training Loss: tensor(0.1756)\n",
      "281 Training Loss: tensor(0.1756)\n",
      "282 Training Loss: tensor(0.1756)\n",
      "283 Training Loss: tensor(0.1756)\n",
      "284 Training Loss: tensor(0.1756)\n",
      "285 Training Loss: tensor(0.1756)\n",
      "286 Training Loss: tensor(0.1756)\n",
      "287 Training Loss: tensor(0.1756)\n",
      "288 Training Loss: tensor(0.1756)\n",
      "289 Training Loss: tensor(0.1756)\n",
      "290 Training Loss: tensor(0.1756)\n",
      "291 Training Loss: tensor(0.1756)\n",
      "292 Training Loss: tensor(0.1756)\n",
      "293 Training Loss: tensor(0.1756)\n",
      "294 Training Loss: tensor(0.1756)\n",
      "295 Training Loss: tensor(0.1756)\n",
      "296 Training Loss: tensor(0.1756)\n",
      "297 Training Loss: tensor(0.1756)\n",
      "298 Training Loss: tensor(0.1756)\n",
      "299 Training Loss: tensor(0.1756)\n",
      "300 Training Loss: tensor(0.1756)\n",
      "301 Training Loss: tensor(0.1756)\n",
      "302 Training Loss: tensor(0.1756)\n",
      "303 Training Loss: tensor(0.1756)\n",
      "304 Training Loss: tensor(0.1756)\n",
      "305 Training Loss: tensor(0.1756)\n",
      "306 Training Loss: tensor(0.1756)\n",
      "307 Training Loss: tensor(0.1756)\n",
      "308 Training Loss: tensor(0.1756)\n",
      "309 Training Loss: tensor(0.1756)\n",
      "310 Training Loss: tensor(0.1756)\n",
      "311 Training Loss: tensor(0.1756)\n",
      "312 Training Loss: tensor(0.1756)\n",
      "313 Training Loss: tensor(0.1756)\n",
      "314 Training Loss: tensor(0.1756)\n",
      "315 Training Loss: tensor(0.1756)\n",
      "316 Training Loss: tensor(0.1756)\n",
      "317 Training Loss: tensor(0.1756)\n",
      "318 Training Loss: tensor(0.1756)\n",
      "319 Training Loss: tensor(0.1756)\n",
      "320 Training Loss: tensor(0.1756)\n",
      "321 Training Loss: tensor(0.1756)\n",
      "322 Training Loss: tensor(0.1756)\n",
      "323 Training Loss: tensor(0.1756)\n",
      "324 Training Loss: tensor(0.1756)\n",
      "325 Training Loss: tensor(0.1756)\n",
      "326 Training Loss: tensor(0.1756)\n",
      "327 Training Loss: tensor(0.1756)\n",
      "328 Training Loss: tensor(0.1756)\n",
      "329 Training Loss: tensor(0.1756)\n",
      "330 Training Loss: tensor(0.1756)\n",
      "331 Training Loss: tensor(0.1756)\n",
      "332 Training Loss: tensor(0.1756)\n",
      "333 Training Loss: tensor(0.1756)\n",
      "334 Training Loss: tensor(0.1756)\n",
      "335 Training Loss: tensor(0.1756)\n",
      "336 Training Loss: tensor(0.1756)\n",
      "337 Training Loss: tensor(0.1756)\n",
      "338 Training Loss: tensor(0.1756)\n",
      "339 Training Loss: tensor(0.1756)\n",
      "340 Training Loss: tensor(0.1756)\n",
      "341 Training Loss: tensor(0.1756)\n",
      "342 Training Loss: tensor(0.1756)\n",
      "343 Training Loss: tensor(0.1756)\n",
      "344 Training Loss: tensor(0.1756)\n",
      "345 Training Loss: tensor(0.1756)\n",
      "346 Training Loss: tensor(0.1756)\n",
      "347 Training Loss: tensor(0.1756)\n",
      "348 Training Loss: tensor(0.1756)\n",
      "349 Training Loss: tensor(0.1756)\n",
      "350 Training Loss: tensor(0.1756)\n",
      "351 Training Loss: tensor(0.1756)\n",
      "352 Training Loss: tensor(0.1756)\n",
      "353 Training Loss: tensor(0.1756)\n",
      "354 Training Loss: tensor(0.1756)\n",
      "355 Training Loss: tensor(0.1756)\n",
      "356 Training Loss: tensor(0.1756)\n",
      "357 Training Loss: tensor(0.1756)\n",
      "358 Training Loss: tensor(0.1756)\n",
      "359 Training Loss: tensor(0.1756)\n",
      "360 Training Loss: tensor(0.1756)\n",
      "361 Training Loss: tensor(0.1756)\n",
      "362 Training Loss: tensor(0.1756)\n",
      "363 Training Loss: tensor(0.1756)\n",
      "364 Training Loss: tensor(0.1756)\n",
      "365 Training Loss: tensor(0.1756)\n",
      "366 Training Loss: tensor(0.1756)\n",
      "367 Training Loss: tensor(0.1756)\n",
      "368 Training Loss: tensor(0.1756)\n",
      "369 Training Loss: tensor(0.1756)\n",
      "370 Training Loss: tensor(0.1756)\n",
      "371 Training Loss: tensor(0.1756)\n",
      "372 Training Loss: tensor(0.1756)\n",
      "373 Training Loss: tensor(0.1756)\n",
      "374 Training Loss: tensor(0.1756)\n",
      "375 Training Loss: tensor(0.1756)\n",
      "376 Training Loss: tensor(0.1756)\n",
      "377 Training Loss: tensor(0.1756)\n",
      "378 Training Loss: tensor(0.1756)\n",
      "379 Training Loss: tensor(0.1756)\n",
      "380 Training Loss: tensor(0.1756)\n",
      "381 Training Loss: tensor(0.1756)\n",
      "382 Training Loss: tensor(0.1756)\n",
      "383 Training Loss: tensor(0.1756)\n",
      "384 Training Loss: tensor(0.1756)\n",
      "385 Training Loss: tensor(0.1756)\n",
      "386 Training Loss: tensor(0.1756)\n",
      "387 Training Loss: tensor(0.1756)\n",
      "388 Training Loss: tensor(0.1756)\n",
      "389 Training Loss: tensor(0.1756)\n",
      "390 Training Loss: tensor(0.1756)\n",
      "391 Training Loss: tensor(0.1756)\n",
      "392 Training Loss: tensor(0.1756)\n",
      "393 Training Loss: tensor(0.1756)\n",
      "394 Training Loss: tensor(0.1756)\n",
      "395 Training Loss: tensor(0.1756)\n",
      "396 Training Loss: tensor(0.1756)\n",
      "397 Training Loss: tensor(0.1756)\n",
      "398 Training Loss: tensor(0.1756)\n",
      "399 Training Loss: tensor(0.1756)\n",
      "400 Training Loss: tensor(0.1756)\n",
      "401 Training Loss: tensor(0.1756)\n",
      "402 Training Loss: tensor(0.1756)\n",
      "403 Training Loss: tensor(0.1756)\n",
      "404 Training Loss: tensor(0.1756)\n",
      "405 Training Loss: tensor(0.1756)\n",
      "406 Training Loss: tensor(0.1756)\n",
      "407 Training Loss: tensor(0.1756)\n",
      "408 Training Loss: tensor(0.1756)\n",
      "409 Training Loss: tensor(0.1756)\n",
      "410 Training Loss: tensor(0.1756)\n",
      "411 Training Loss: tensor(0.1756)\n",
      "412 Training Loss: tensor(0.1756)\n",
      "413 Training Loss: tensor(0.1756)\n",
      "414 Training Loss: tensor(0.1756)\n",
      "415 Training Loss: tensor(0.1756)\n",
      "416 Training Loss: tensor(0.1756)\n",
      "417 Training Loss: tensor(0.1756)\n",
      "418 Training Loss: tensor(0.1756)\n",
      "419 Training Loss: tensor(0.1756)\n",
      "420 Training Loss: tensor(0.1756)\n",
      "421 Training Loss: tensor(0.1756)\n",
      "422 Training Loss: tensor(0.1756)\n",
      "423 Training Loss: tensor(0.1756)\n",
      "424 Training Loss: tensor(0.1756)\n",
      "425 Training Loss: tensor(0.1756)\n",
      "426 Training Loss: tensor(0.1756)\n",
      "427 Training Loss: tensor(0.1756)\n",
      "428 Training Loss: tensor(0.1756)\n",
      "429 Training Loss: tensor(0.1756)\n",
      "430 Training Loss: tensor(0.1756)\n",
      "431 Training Loss: tensor(0.1756)\n",
      "432 Training Loss: tensor(0.1756)\n",
      "433 Training Loss: tensor(0.1756)\n",
      "434 Training Loss: tensor(0.1756)\n",
      "435 Training Loss: tensor(0.1756)\n",
      "436 Training Loss: tensor(0.1756)\n",
      "437 Training Loss: tensor(0.1756)\n",
      "438 Training Loss: tensor(0.1756)\n",
      "439 Training Loss: tensor(0.1756)\n",
      "440 Training Loss: tensor(0.1756)\n",
      "441 Training Loss: tensor(0.1756)\n",
      "442 Training Loss: tensor(0.1756)\n",
      "443 Training Loss: tensor(0.1756)\n",
      "444 Training Loss: tensor(0.1756)\n",
      "445 Training Loss: tensor(0.1756)\n",
      "446 Training Loss: tensor(0.1756)\n",
      "447 Training Loss: tensor(0.1756)\n",
      "448 Training Loss: tensor(0.1756)\n",
      "449 Training Loss: tensor(0.1756)\n",
      "450 Training Loss: tensor(0.1756)\n",
      "451 Training Loss: tensor(0.1756)\n",
      "452 Training Loss: tensor(0.1756)\n",
      "453 Training Loss: tensor(0.1756)\n",
      "454 Training Loss: tensor(0.1756)\n",
      "455 Training Loss: tensor(0.1756)\n",
      "456 Training Loss: tensor(0.1756)\n",
      "457 Training Loss: tensor(0.1756)\n",
      "458 Training Loss: tensor(0.1756)\n",
      "459 Training Loss: tensor(0.1756)\n",
      "460 Training Loss: tensor(0.1756)\n",
      "461 Training Loss: tensor(0.1756)\n",
      "462 Training Loss: tensor(0.1756)\n",
      "463 Training Loss: tensor(0.1756)\n",
      "464 Training Loss: tensor(0.1756)\n",
      "465 Training Loss: tensor(0.1756)\n",
      "466 Training Loss: tensor(0.1756)\n",
      "467 Training Loss: tensor(0.1756)\n",
      "468 Training Loss: tensor(0.1756)\n",
      "469 Training Loss: tensor(0.1756)\n",
      "470 Training Loss: tensor(0.1756)\n",
      "471 Training Loss: tensor(0.1756)\n",
      "472 Training Loss: tensor(0.1756)\n",
      "473 Training Loss: tensor(0.1756)\n",
      "474 Training Loss: tensor(0.1756)\n",
      "475 Training Loss: tensor(0.1756)\n",
      "476 Training Loss: tensor(0.1756)\n",
      "477 Training Loss: tensor(0.1756)\n",
      "478 Training Loss: tensor(0.1756)\n",
      "479 Training Loss: tensor(0.1756)\n",
      "480 Training Loss: tensor(0.1756)\n",
      "481 Training Loss: tensor(0.1756)\n",
      "482 Training Loss: tensor(0.1756)\n",
      "483 Training Loss: tensor(0.1756)\n",
      "484 Training Loss: tensor(0.1756)\n",
      "485 Training Loss: tensor(0.1756)\n",
      "486 Training Loss: tensor(0.1756)\n",
      "487 Training Loss: tensor(0.1756)\n",
      "488 Training Loss: tensor(0.1756)\n",
      "489 Training Loss: tensor(0.1756)\n",
      "490 Training Loss: tensor(0.1756)\n",
      "491 Training Loss: tensor(0.1756)\n",
      "492 Training Loss: tensor(0.1756)\n",
      "493 Training Loss: tensor(0.1756)\n",
      "494 Training Loss: tensor(0.1756)\n",
      "495 Training Loss: tensor(0.1756)\n",
      "496 Training Loss: tensor(0.1756)\n",
      "497 Training Loss: tensor(0.1756)\n",
      "498 Training Loss: tensor(0.1756)\n",
      "499 Training Loss: tensor(0.1756)\n",
      "500 Training Loss: tensor(0.1756)\n",
      "501 Training Loss: tensor(0.1756)\n",
      "502 Training Loss: tensor(0.1756)\n",
      "503 Training Loss: tensor(0.1756)\n",
      "504 Training Loss: tensor(0.1756)\n",
      "505 Training Loss: tensor(0.1756)\n",
      "506 Training Loss: tensor(0.1756)\n",
      "507 Training Loss: tensor(0.1756)\n",
      "508 Training Loss: tensor(0.1756)\n",
      "509 Training Loss: tensor(0.1756)\n",
      "510 Training Loss: tensor(0.1756)\n",
      "511 Training Loss: tensor(0.1756)\n",
      "512 Training Loss: tensor(0.1756)\n",
      "513 Training Loss: tensor(0.1756)\n",
      "514 Training Loss: tensor(0.1756)\n",
      "515 Training Loss: tensor(0.1756)\n",
      "516 Training Loss: tensor(0.1756)\n",
      "517 Training Loss: tensor(0.1756)\n",
      "518 Training Loss: tensor(0.1756)\n",
      "519 Training Loss: tensor(0.1756)\n",
      "520 Training Loss: tensor(0.1756)\n",
      "521 Training Loss: tensor(0.1756)\n",
      "522 Training Loss: tensor(0.1756)\n",
      "523 Training Loss: tensor(0.1756)\n",
      "524 Training Loss: tensor(0.1755)\n",
      "525 Training Loss: tensor(0.1755)\n",
      "526 Training Loss: tensor(0.1755)\n",
      "527 Training Loss: tensor(0.1755)\n",
      "528 Training Loss: tensor(0.1755)\n",
      "529 Training Loss: tensor(0.1755)\n",
      "530 Training Loss: tensor(0.1755)\n",
      "531 Training Loss: tensor(0.1755)\n",
      "532 Training Loss: tensor(0.1755)\n",
      "533 Training Loss: tensor(0.1755)\n",
      "534 Training Loss: tensor(0.1755)\n",
      "535 Training Loss: tensor(0.1755)\n",
      "536 Training Loss: tensor(0.1755)\n",
      "537 Training Loss: tensor(0.1755)\n",
      "538 Training Loss: tensor(0.1755)\n",
      "539 Training Loss: tensor(0.1755)\n",
      "540 Training Loss: tensor(0.1755)\n",
      "541 Training Loss: tensor(0.1755)\n",
      "542 Training Loss: tensor(0.1755)\n",
      "543 Training Loss: tensor(0.1755)\n",
      "544 Training Loss: tensor(0.1755)\n",
      "545 Training Loss: tensor(0.1755)\n",
      "546 Training Loss: tensor(0.1755)\n",
      "547 Training Loss: tensor(0.1755)\n",
      "548 Training Loss: tensor(0.1755)\n",
      "549 Training Loss: tensor(0.1755)\n",
      "550 Training Loss: tensor(0.1755)\n",
      "551 Training Loss: tensor(0.1755)\n",
      "552 Training Loss: tensor(0.1755)\n",
      "553 Training Loss: tensor(0.1755)\n",
      "554 Training Loss: tensor(0.1755)\n",
      "555 Training Loss: tensor(0.1755)\n",
      "556 Training Loss: tensor(0.1755)\n",
      "557 Training Loss: tensor(0.1755)\n",
      "558 Training Loss: tensor(0.1755)\n",
      "559 Training Loss: tensor(0.1755)\n",
      "560 Training Loss: tensor(0.1755)\n",
      "561 Training Loss: tensor(0.1755)\n",
      "562 Training Loss: tensor(0.1755)\n",
      "563 Training Loss: tensor(0.1755)\n",
      "564 Training Loss: tensor(0.1755)\n",
      "565 Training Loss: tensor(0.1755)\n",
      "566 Training Loss: tensor(0.1755)\n",
      "567 Training Loss: tensor(0.1755)\n",
      "568 Training Loss: tensor(0.1755)\n",
      "569 Training Loss: tensor(0.1755)\n",
      "570 Training Loss: tensor(0.1755)\n",
      "571 Training Loss: tensor(0.1755)\n",
      "572 Training Loss: tensor(0.1755)\n",
      "573 Training Loss: tensor(0.1755)\n",
      "574 Training Loss: tensor(0.1755)\n",
      "575 Training Loss: tensor(0.1755)\n",
      "576 Training Loss: tensor(0.1755)\n",
      "577 Training Loss: tensor(0.1755)\n",
      "578 Training Loss: tensor(0.1755)\n",
      "579 Training Loss: tensor(0.1755)\n",
      "580 Training Loss: tensor(0.1755)\n",
      "581 Training Loss: tensor(0.1755)\n",
      "582 Training Loss: tensor(0.1755)\n",
      "583 Training Loss: tensor(0.1755)\n",
      "584 Training Loss: tensor(0.1755)\n",
      "585 Training Loss: tensor(0.1755)\n",
      "586 Training Loss: tensor(0.1755)\n",
      "587 Training Loss: tensor(0.1755)\n",
      "588 Training Loss: tensor(0.1755)\n",
      "589 Training Loss: tensor(0.1755)\n",
      "590 Training Loss: tensor(0.1755)\n",
      "591 Training Loss: tensor(0.1755)\n",
      "592 Training Loss: tensor(0.1755)\n",
      "593 Training Loss: tensor(0.1755)\n",
      "594 Training Loss: tensor(0.1755)\n",
      "595 Training Loss: tensor(0.1755)\n",
      "596 Training Loss: tensor(0.1755)\n",
      "597 Training Loss: tensor(0.1755)\n",
      "598 Training Loss: tensor(0.1755)\n",
      "599 Training Loss: tensor(0.1755)\n",
      "600 Training Loss: tensor(0.1755)\n",
      "601 Training Loss: tensor(0.1755)\n",
      "602 Training Loss: tensor(0.1755)\n",
      "603 Training Loss: tensor(0.1755)\n",
      "604 Training Loss: tensor(0.1755)\n",
      "605 Training Loss: tensor(0.1755)\n",
      "606 Training Loss: tensor(0.1755)\n",
      "607 Training Loss: tensor(0.1755)\n",
      "608 Training Loss: tensor(0.1755)\n",
      "609 Training Loss: tensor(0.1755)\n",
      "610 Training Loss: tensor(0.1755)\n",
      "611 Training Loss: tensor(0.1755)\n",
      "612 Training Loss: tensor(0.1755)\n",
      "613 Training Loss: tensor(0.1755)\n",
      "614 Training Loss: tensor(0.1755)\n",
      "615 Training Loss: tensor(0.1755)\n",
      "616 Training Loss: tensor(0.1755)\n",
      "617 Training Loss: tensor(0.1755)\n",
      "618 Training Loss: tensor(0.1755)\n",
      "619 Training Loss: tensor(0.1755)\n",
      "620 Training Loss: tensor(0.1755)\n",
      "621 Training Loss: tensor(0.1755)\n",
      "622 Training Loss: tensor(0.1755)\n",
      "623 Training Loss: tensor(0.1755)\n",
      "624 Training Loss: tensor(0.1755)\n",
      "625 Training Loss: tensor(0.1755)\n",
      "626 Training Loss: tensor(0.1755)\n",
      "627 Training Loss: tensor(0.1755)\n",
      "628 Training Loss: tensor(0.1755)\n",
      "629 Training Loss: tensor(0.1755)\n",
      "630 Training Loss: tensor(0.1755)\n",
      "631 Training Loss: tensor(0.1755)\n",
      "632 Training Loss: tensor(0.1755)\n",
      "633 Training Loss: tensor(0.1755)\n",
      "634 Training Loss: tensor(0.1755)\n",
      "635 Training Loss: tensor(0.1755)\n",
      "636 Training Loss: tensor(0.1755)\n",
      "637 Training Loss: tensor(0.1755)\n",
      "638 Training Loss: tensor(0.1755)\n",
      "639 Training Loss: tensor(0.1755)\n",
      "640 Training Loss: tensor(0.1755)\n",
      "641 Training Loss: tensor(0.1755)\n",
      "642 Training Loss: tensor(0.1755)\n",
      "643 Training Loss: tensor(0.1755)\n",
      "644 Training Loss: tensor(0.1755)\n",
      "645 Training Loss: tensor(0.1755)\n",
      "646 Training Loss: tensor(0.1755)\n",
      "647 Training Loss: tensor(0.1755)\n",
      "648 Training Loss: tensor(0.1755)\n",
      "649 Training Loss: tensor(0.1755)\n",
      "650 Training Loss: tensor(0.1755)\n",
      "651 Training Loss: tensor(0.1755)\n",
      "652 Training Loss: tensor(0.1755)\n",
      "653 Training Loss: tensor(0.1755)\n",
      "654 Training Loss: tensor(0.1755)\n",
      "655 Training Loss: tensor(0.1755)\n",
      "656 Training Loss: tensor(0.1755)\n",
      "657 Training Loss: tensor(0.1755)\n",
      "658 Training Loss: tensor(0.1755)\n",
      "659 Training Loss: tensor(0.1755)\n",
      "660 Training Loss: tensor(0.1755)\n",
      "661 Training Loss: tensor(0.1755)\n",
      "662 Training Loss: tensor(0.1755)\n",
      "663 Training Loss: tensor(0.1755)\n",
      "664 Training Loss: tensor(0.1755)\n",
      "665 Training Loss: tensor(0.1755)\n",
      "666 Training Loss: tensor(0.1755)\n",
      "667 Training Loss: tensor(0.1755)\n",
      "668 Training Loss: tensor(0.1755)\n",
      "669 Training Loss: tensor(0.1755)\n",
      "670 Training Loss: tensor(0.1755)\n",
      "671 Training Loss: tensor(0.1755)\n",
      "672 Training Loss: tensor(0.1755)\n",
      "673 Training Loss: tensor(0.1755)\n",
      "674 Training Loss: tensor(0.1755)\n",
      "675 Training Loss: tensor(0.1755)\n",
      "676 Training Loss: tensor(0.1755)\n",
      "677 Training Loss: tensor(0.1755)\n",
      "678 Training Loss: tensor(0.1755)\n",
      "679 Training Loss: tensor(0.1755)\n",
      "680 Training Loss: tensor(0.1755)\n",
      "681 Training Loss: tensor(0.1755)\n",
      "682 Training Loss: tensor(0.1755)\n",
      "683 Training Loss: tensor(0.1755)\n",
      "684 Training Loss: tensor(0.1755)\n",
      "685 Training Loss: tensor(0.1755)\n",
      "686 Training Loss: tensor(0.1755)\n",
      "687 Training Loss: tensor(0.1755)\n",
      "688 Training Loss: tensor(0.1755)\n",
      "689 Training Loss: tensor(0.1755)\n",
      "690 Training Loss: tensor(0.1755)\n",
      "691 Training Loss: tensor(0.1755)\n",
      "692 Training Loss: tensor(0.1754)\n",
      "693 Training Loss: tensor(0.1754)\n",
      "694 Training Loss: tensor(0.1754)\n",
      "695 Training Loss: tensor(0.1754)\n",
      "696 Training Loss: tensor(0.1754)\n",
      "697 Training Loss: tensor(0.1754)\n",
      "698 Training Loss: tensor(0.1754)\n",
      "699 Training Loss: tensor(0.1754)\n",
      "700 Training Loss: tensor(0.1754)\n",
      "701 Training Loss: tensor(0.1754)\n",
      "702 Training Loss: tensor(0.1754)\n",
      "703 Training Loss: tensor(0.1754)\n",
      "704 Training Loss: tensor(0.1754)\n",
      "705 Training Loss: tensor(0.1754)\n",
      "706 Training Loss: tensor(0.1754)\n",
      "707 Training Loss: tensor(0.1754)\n",
      "708 Training Loss: tensor(0.1754)\n",
      "709 Training Loss: tensor(0.1754)\n",
      "710 Training Loss: tensor(0.1754)\n",
      "711 Training Loss: tensor(0.1754)\n",
      "712 Training Loss: tensor(0.1754)\n",
      "713 Training Loss: tensor(0.1754)\n",
      "714 Training Loss: tensor(0.1754)\n",
      "715 Training Loss: tensor(0.1754)\n",
      "716 Training Loss: tensor(0.1754)\n",
      "717 Training Loss: tensor(0.1754)\n",
      "718 Training Loss: tensor(0.1754)\n",
      "719 Training Loss: tensor(0.1754)\n",
      "720 Training Loss: tensor(0.1754)\n",
      "721 Training Loss: tensor(0.1754)\n",
      "722 Training Loss: tensor(0.1754)\n",
      "723 Training Loss: tensor(0.1754)\n",
      "724 Training Loss: tensor(0.1754)\n",
      "725 Training Loss: tensor(0.1754)\n",
      "726 Training Loss: tensor(0.1754)\n",
      "727 Training Loss: tensor(0.1754)\n",
      "728 Training Loss: tensor(0.1754)\n",
      "729 Training Loss: tensor(0.1754)\n",
      "730 Training Loss: tensor(0.1754)\n",
      "731 Training Loss: tensor(0.1754)\n",
      "732 Training Loss: tensor(0.1754)\n",
      "733 Training Loss: tensor(0.1754)\n",
      "734 Training Loss: tensor(0.1754)\n",
      "735 Training Loss: tensor(0.1754)\n",
      "736 Training Loss: tensor(0.1754)\n",
      "737 Training Loss: tensor(0.1754)\n",
      "738 Training Loss: tensor(0.1754)\n",
      "739 Training Loss: tensor(0.1754)\n",
      "740 Training Loss: tensor(0.1754)\n",
      "741 Training Loss: tensor(0.1754)\n",
      "742 Training Loss: tensor(0.1754)\n",
      "743 Training Loss: tensor(0.1754)\n",
      "744 Training Loss: tensor(0.1754)\n",
      "745 Training Loss: tensor(0.1754)\n",
      "746 Training Loss: tensor(0.1754)\n",
      "747 Training Loss: tensor(0.1754)\n",
      "748 Training Loss: tensor(0.1754)\n",
      "749 Training Loss: tensor(0.1754)\n",
      "750 Training Loss: tensor(0.1754)\n",
      "751 Training Loss: tensor(0.1754)\n",
      "752 Training Loss: tensor(0.1754)\n",
      "753 Training Loss: tensor(0.1754)\n",
      "754 Training Loss: tensor(0.1754)\n",
      "755 Training Loss: tensor(0.1754)\n",
      "756 Training Loss: tensor(0.1754)\n",
      "757 Training Loss: tensor(0.1754)\n",
      "758 Training Loss: tensor(0.1754)\n",
      "759 Training Loss: tensor(0.1754)\n",
      "760 Training Loss: tensor(0.1754)\n",
      "761 Training Loss: tensor(0.1754)\n",
      "762 Training Loss: tensor(0.1754)\n",
      "763 Training Loss: tensor(0.1754)\n",
      "764 Training Loss: tensor(0.1754)\n",
      "765 Training Loss: tensor(0.1754)\n",
      "766 Training Loss: tensor(0.1754)\n",
      "767 Training Loss: tensor(0.1754)\n",
      "768 Training Loss: tensor(0.1754)\n",
      "769 Training Loss: tensor(0.1754)\n",
      "770 Training Loss: tensor(0.1754)\n",
      "771 Training Loss: tensor(0.1754)\n",
      "772 Training Loss: tensor(0.1754)\n",
      "773 Training Loss: tensor(0.1754)\n",
      "774 Training Loss: tensor(0.1754)\n",
      "775 Training Loss: tensor(0.1754)\n",
      "776 Training Loss: tensor(0.1753)\n",
      "777 Training Loss: tensor(0.1753)\n",
      "778 Training Loss: tensor(0.1753)\n",
      "779 Training Loss: tensor(0.1753)\n",
      "780 Training Loss: tensor(0.1753)\n",
      "781 Training Loss: tensor(0.1753)\n",
      "782 Training Loss: tensor(0.1753)\n",
      "783 Training Loss: tensor(0.1753)\n",
      "784 Training Loss: tensor(0.1753)\n",
      "785 Training Loss: tensor(0.1753)\n",
      "786 Training Loss: tensor(0.1753)\n",
      "787 Training Loss: tensor(0.1753)\n",
      "788 Training Loss: tensor(0.1753)\n",
      "789 Training Loss: tensor(0.1753)\n",
      "790 Training Loss: tensor(0.1753)\n",
      "791 Training Loss: tensor(0.1753)\n",
      "792 Training Loss: tensor(0.1753)\n",
      "793 Training Loss: tensor(0.1753)\n",
      "794 Training Loss: tensor(0.1753)\n",
      "795 Training Loss: tensor(0.1753)\n",
      "796 Training Loss: tensor(0.1753)\n",
      "797 Training Loss: tensor(0.1753)\n",
      "798 Training Loss: tensor(0.1753)\n",
      "799 Training Loss: tensor(0.1753)\n",
      "800 Training Loss: tensor(0.1753)\n",
      "801 Training Loss: tensor(0.1753)\n",
      "802 Training Loss: tensor(0.1753)\n",
      "803 Training Loss: tensor(0.1753)\n",
      "804 Training Loss: tensor(0.1753)\n",
      "805 Training Loss: tensor(0.1753)\n",
      "806 Training Loss: tensor(0.1753)\n",
      "807 Training Loss: tensor(0.1753)\n",
      "808 Training Loss: tensor(0.1753)\n",
      "809 Training Loss: tensor(0.1753)\n",
      "810 Training Loss: tensor(0.1753)\n",
      "811 Training Loss: tensor(0.1753)\n",
      "812 Training Loss: tensor(0.1753)\n",
      "813 Training Loss: tensor(0.1753)\n",
      "814 Training Loss: tensor(0.1753)\n",
      "815 Training Loss: tensor(0.1753)\n",
      "816 Training Loss: tensor(0.1753)\n",
      "817 Training Loss: tensor(0.1753)\n",
      "818 Training Loss: tensor(0.1753)\n",
      "819 Training Loss: tensor(0.1753)\n",
      "820 Training Loss: tensor(0.1753)\n",
      "821 Training Loss: tensor(0.1753)\n",
      "822 Training Loss: tensor(0.1753)\n",
      "823 Training Loss: tensor(0.1753)\n",
      "824 Training Loss: tensor(0.1753)\n",
      "825 Training Loss: tensor(0.1753)\n",
      "826 Training Loss: tensor(0.1753)\n",
      "827 Training Loss: tensor(0.1753)\n",
      "828 Training Loss: tensor(0.1753)\n",
      "829 Training Loss: tensor(0.1753)\n",
      "830 Training Loss: tensor(0.1753)\n",
      "831 Training Loss: tensor(0.1752)\n",
      "832 Training Loss: tensor(0.1752)\n",
      "833 Training Loss: tensor(0.1752)\n",
      "834 Training Loss: tensor(0.1752)\n",
      "835 Training Loss: tensor(0.1752)\n",
      "836 Training Loss: tensor(0.1752)\n",
      "837 Training Loss: tensor(0.1752)\n",
      "838 Training Loss: tensor(0.1752)\n",
      "839 Training Loss: tensor(0.1752)\n",
      "840 Training Loss: tensor(0.1752)\n",
      "841 Training Loss: tensor(0.1752)\n",
      "842 Training Loss: tensor(0.1752)\n",
      "843 Training Loss: tensor(0.1752)\n",
      "844 Training Loss: tensor(0.1752)\n",
      "845 Training Loss: tensor(0.1752)\n",
      "846 Training Loss: tensor(0.1752)\n",
      "847 Training Loss: tensor(0.1752)\n",
      "848 Training Loss: tensor(0.1752)\n",
      "849 Training Loss: tensor(0.1752)\n",
      "850 Training Loss: tensor(0.1752)\n",
      "851 Training Loss: tensor(0.1752)\n",
      "852 Training Loss: tensor(0.1752)\n",
      "853 Training Loss: tensor(0.1752)\n",
      "854 Training Loss: tensor(0.1752)\n",
      "855 Training Loss: tensor(0.1752)\n",
      "856 Training Loss: tensor(0.1752)\n",
      "857 Training Loss: tensor(0.1752)\n",
      "858 Training Loss: tensor(0.1752)\n",
      "859 Training Loss: tensor(0.1752)\n",
      "860 Training Loss: tensor(0.1752)\n",
      "861 Training Loss: tensor(0.1752)\n",
      "862 Training Loss: tensor(0.1752)\n",
      "863 Training Loss: tensor(0.1752)\n",
      "864 Training Loss: tensor(0.1752)\n",
      "865 Training Loss: tensor(0.1752)\n",
      "866 Training Loss: tensor(0.1752)\n",
      "867 Training Loss: tensor(0.1752)\n",
      "868 Training Loss: tensor(0.1752)\n",
      "869 Training Loss: tensor(0.1752)\n",
      "870 Training Loss: tensor(0.1752)\n",
      "871 Training Loss: tensor(0.1752)\n",
      "872 Training Loss: tensor(0.1752)\n",
      "873 Training Loss: tensor(0.1751)\n",
      "874 Training Loss: tensor(0.1751)\n",
      "875 Training Loss: tensor(0.1751)\n",
      "876 Training Loss: tensor(0.1751)\n",
      "877 Training Loss: tensor(0.1751)\n",
      "878 Training Loss: tensor(0.1751)\n",
      "879 Training Loss: tensor(0.1751)\n",
      "880 Training Loss: tensor(0.1751)\n",
      "881 Training Loss: tensor(0.1751)\n",
      "882 Training Loss: tensor(0.1751)\n",
      "883 Training Loss: tensor(0.1751)\n",
      "884 Training Loss: tensor(0.1751)\n",
      "885 Training Loss: tensor(0.1751)\n",
      "886 Training Loss: tensor(0.1751)\n",
      "887 Training Loss: tensor(0.1751)\n",
      "888 Training Loss: tensor(0.1751)\n",
      "889 Training Loss: tensor(0.1751)\n",
      "890 Training Loss: tensor(0.1751)\n",
      "891 Training Loss: tensor(0.1751)\n",
      "892 Training Loss: tensor(0.1751)\n",
      "893 Training Loss: tensor(0.1751)\n",
      "894 Training Loss: tensor(0.1751)\n",
      "895 Training Loss: tensor(0.1751)\n",
      "896 Training Loss: tensor(0.1751)\n",
      "897 Training Loss: tensor(0.1751)\n",
      "898 Training Loss: tensor(0.1751)\n",
      "899 Training Loss: tensor(0.1751)\n",
      "900 Training Loss: tensor(0.1751)\n",
      "901 Training Loss: tensor(0.1751)\n",
      "902 Training Loss: tensor(0.1751)\n",
      "903 Training Loss: tensor(0.1751)\n",
      "904 Training Loss: tensor(0.1751)\n",
      "905 Training Loss: tensor(0.1751)\n",
      "906 Training Loss: tensor(0.1750)\n",
      "907 Training Loss: tensor(0.1750)\n",
      "908 Training Loss: tensor(0.1750)\n",
      "909 Training Loss: tensor(0.1750)\n",
      "910 Training Loss: tensor(0.1750)\n",
      "911 Training Loss: tensor(0.1750)\n",
      "912 Training Loss: tensor(0.1750)\n",
      "913 Training Loss: tensor(0.1750)\n",
      "914 Training Loss: tensor(0.1750)\n",
      "915 Training Loss: tensor(0.1750)\n",
      "916 Training Loss: tensor(0.1750)\n",
      "917 Training Loss: tensor(0.1750)\n",
      "918 Training Loss: tensor(0.1750)\n",
      "919 Training Loss: tensor(0.1750)\n",
      "920 Training Loss: tensor(0.1750)\n",
      "921 Training Loss: tensor(0.1750)\n",
      "922 Training Loss: tensor(0.1750)\n",
      "923 Training Loss: tensor(0.1750)\n",
      "924 Training Loss: tensor(0.1750)\n",
      "925 Training Loss: tensor(0.1750)\n",
      "926 Training Loss: tensor(0.1750)\n",
      "927 Training Loss: tensor(0.1750)\n",
      "928 Training Loss: tensor(0.1750)\n",
      "929 Training Loss: tensor(0.1750)\n",
      "930 Training Loss: tensor(0.1750)\n",
      "931 Training Loss: tensor(0.1750)\n",
      "932 Training Loss: tensor(0.1750)\n",
      "933 Training Loss: tensor(0.1749)\n",
      "934 Training Loss: tensor(0.1749)\n",
      "935 Training Loss: tensor(0.1749)\n",
      "936 Training Loss: tensor(0.1749)\n",
      "937 Training Loss: tensor(0.1749)\n",
      "938 Training Loss: tensor(0.1749)\n",
      "939 Training Loss: tensor(0.1749)\n",
      "940 Training Loss: tensor(0.1749)\n",
      "941 Training Loss: tensor(0.1749)\n",
      "942 Training Loss: tensor(0.1749)\n",
      "943 Training Loss: tensor(0.1749)\n",
      "944 Training Loss: tensor(0.1749)\n",
      "945 Training Loss: tensor(0.1749)\n",
      "946 Training Loss: tensor(0.1749)\n",
      "947 Training Loss: tensor(0.1749)\n",
      "948 Training Loss: tensor(0.1749)\n",
      "949 Training Loss: tensor(0.1749)\n",
      "950 Training Loss: tensor(0.1749)\n",
      "951 Training Loss: tensor(0.1749)\n",
      "952 Training Loss: tensor(0.1749)\n",
      "953 Training Loss: tensor(0.1749)\n",
      "954 Training Loss: tensor(0.1749)\n",
      "955 Training Loss: tensor(0.1749)\n",
      "956 Training Loss: tensor(0.1749)\n",
      "957 Training Loss: tensor(0.1748)\n",
      "958 Training Loss: tensor(0.1748)\n",
      "959 Training Loss: tensor(0.1748)\n",
      "960 Training Loss: tensor(0.1748)\n",
      "961 Training Loss: tensor(0.1748)\n",
      "962 Training Loss: tensor(0.1748)\n",
      "963 Training Loss: tensor(0.1748)\n",
      "964 Training Loss: tensor(0.1748)\n",
      "965 Training Loss: tensor(0.1748)\n",
      "966 Training Loss: tensor(0.1748)\n",
      "967 Training Loss: tensor(0.1748)\n",
      "968 Training Loss: tensor(0.1748)\n",
      "969 Training Loss: tensor(0.1748)\n",
      "970 Training Loss: tensor(0.1748)\n",
      "971 Training Loss: tensor(0.1748)\n",
      "972 Training Loss: tensor(0.1748)\n",
      "973 Training Loss: tensor(0.1748)\n",
      "974 Training Loss: tensor(0.1748)\n",
      "975 Training Loss: tensor(0.1748)\n",
      "976 Training Loss: tensor(0.1748)\n",
      "977 Training Loss: tensor(0.1748)\n",
      "978 Training Loss: tensor(0.1747)\n",
      "979 Training Loss: tensor(0.1747)\n",
      "980 Training Loss: tensor(0.1747)\n",
      "981 Training Loss: tensor(0.1747)\n",
      "982 Training Loss: tensor(0.1747)\n",
      "983 Training Loss: tensor(0.1747)\n",
      "984 Training Loss: tensor(0.1747)\n",
      "985 Training Loss: tensor(0.1747)\n",
      "986 Training Loss: tensor(0.1747)\n",
      "987 Training Loss: tensor(0.1747)\n",
      "988 Training Loss: tensor(0.1747)\n",
      "989 Training Loss: tensor(0.1747)\n",
      "990 Training Loss: tensor(0.1747)\n",
      "991 Training Loss: tensor(0.1747)\n",
      "992 Training Loss: tensor(0.1747)\n",
      "993 Training Loss: tensor(0.1747)\n",
      "994 Training Loss: tensor(0.1747)\n",
      "995 Training Loss: tensor(0.1747)\n",
      "996 Training Loss: tensor(0.1746)\n",
      "997 Training Loss: tensor(0.1746)\n",
      "998 Training Loss: tensor(0.1746)\n",
      "999 Training Loss: tensor(0.1746)\n",
      "1000 Training Loss: tensor(0.1746)\n",
      "1001 Training Loss: tensor(0.1746)\n",
      "1002 Training Loss: tensor(0.1746)\n",
      "1003 Training Loss: tensor(0.1746)\n",
      "1004 Training Loss: tensor(0.1746)\n",
      "1005 Training Loss: tensor(0.1746)\n",
      "1006 Training Loss: tensor(0.1746)\n",
      "1007 Training Loss: tensor(0.1746)\n",
      "1008 Training Loss: tensor(0.1746)\n",
      "1009 Training Loss: tensor(0.1746)\n",
      "1010 Training Loss: tensor(0.1746)\n",
      "1011 Training Loss: tensor(0.1746)\n",
      "1012 Training Loss: tensor(0.1746)\n",
      "1013 Training Loss: tensor(0.1745)\n",
      "1014 Training Loss: tensor(0.1745)\n",
      "1015 Training Loss: tensor(0.1745)\n",
      "1016 Training Loss: tensor(0.1745)\n",
      "1017 Training Loss: tensor(0.1745)\n",
      "1018 Training Loss: tensor(0.1745)\n",
      "1019 Training Loss: tensor(0.1745)\n",
      "1020 Training Loss: tensor(0.1745)\n",
      "1021 Training Loss: tensor(0.1745)\n",
      "1022 Training Loss: tensor(0.1745)\n",
      "1023 Training Loss: tensor(0.1745)\n",
      "1024 Training Loss: tensor(0.1745)\n",
      "1025 Training Loss: tensor(0.1745)\n",
      "1026 Training Loss: tensor(0.1745)\n",
      "1027 Training Loss: tensor(0.1745)\n",
      "1028 Training Loss: tensor(0.1744)\n",
      "1029 Training Loss: tensor(0.1744)\n",
      "1030 Training Loss: tensor(0.1744)\n",
      "1031 Training Loss: tensor(0.1744)\n",
      "1032 Training Loss: tensor(0.1744)\n",
      "1033 Training Loss: tensor(0.1744)\n",
      "1034 Training Loss: tensor(0.1744)\n",
      "1035 Training Loss: tensor(0.1744)\n",
      "1036 Training Loss: tensor(0.1744)\n",
      "1037 Training Loss: tensor(0.1744)\n",
      "1038 Training Loss: tensor(0.1744)\n",
      "1039 Training Loss: tensor(0.1744)\n",
      "1040 Training Loss: tensor(0.1744)\n",
      "1041 Training Loss: tensor(0.1744)\n",
      "1042 Training Loss: tensor(0.1743)\n",
      "1043 Training Loss: tensor(0.1743)\n",
      "1044 Training Loss: tensor(0.1743)\n",
      "1045 Training Loss: tensor(0.1743)\n",
      "1046 Training Loss: tensor(0.1743)\n",
      "1047 Training Loss: tensor(0.1743)\n",
      "1048 Training Loss: tensor(0.1743)\n",
      "1049 Training Loss: tensor(0.1743)\n",
      "1050 Training Loss: tensor(0.1743)\n",
      "1051 Training Loss: tensor(0.1743)\n",
      "1052 Training Loss: tensor(0.1743)\n",
      "1053 Training Loss: tensor(0.1743)\n",
      "1054 Training Loss: tensor(0.1742)\n",
      "1055 Training Loss: tensor(0.1742)\n",
      "1056 Training Loss: tensor(0.1742)\n",
      "1057 Training Loss: tensor(0.1742)\n",
      "1058 Training Loss: tensor(0.1742)\n",
      "1059 Training Loss: tensor(0.1742)\n",
      "1060 Training Loss: tensor(0.1742)\n",
      "1061 Training Loss: tensor(0.1742)\n",
      "1062 Training Loss: tensor(0.1742)\n",
      "1063 Training Loss: tensor(0.1742)\n",
      "1064 Training Loss: tensor(0.1742)\n",
      "1065 Training Loss: tensor(0.1742)\n",
      "1066 Training Loss: tensor(0.1741)\n",
      "1067 Training Loss: tensor(0.1741)\n",
      "1068 Training Loss: tensor(0.1741)\n",
      "1069 Training Loss: tensor(0.1741)\n",
      "1070 Training Loss: tensor(0.1741)\n",
      "1071 Training Loss: tensor(0.1741)\n",
      "1072 Training Loss: tensor(0.1741)\n",
      "1073 Training Loss: tensor(0.1741)\n",
      "1074 Training Loss: tensor(0.1741)\n",
      "1075 Training Loss: tensor(0.1741)\n",
      "1076 Training Loss: tensor(0.1741)\n",
      "1077 Training Loss: tensor(0.1740)\n",
      "1078 Training Loss: tensor(0.1740)\n",
      "1079 Training Loss: tensor(0.1740)\n",
      "1080 Training Loss: tensor(0.1740)\n",
      "1081 Training Loss: tensor(0.1740)\n",
      "1082 Training Loss: tensor(0.1740)\n",
      "1083 Training Loss: tensor(0.1740)\n",
      "1084 Training Loss: tensor(0.1740)\n",
      "1085 Training Loss: tensor(0.1740)\n",
      "1086 Training Loss: tensor(0.1740)\n",
      "1087 Training Loss: tensor(0.1739)\n",
      "1088 Training Loss: tensor(0.1739)\n",
      "1089 Training Loss: tensor(0.1739)\n",
      "1090 Training Loss: tensor(0.1739)\n",
      "1091 Training Loss: tensor(0.1739)\n",
      "1092 Training Loss: tensor(0.1739)\n",
      "1093 Training Loss: tensor(0.1739)\n",
      "1094 Training Loss: tensor(0.1739)\n",
      "1095 Training Loss: tensor(0.1739)\n",
      "1096 Training Loss: tensor(0.1738)\n",
      "1097 Training Loss: tensor(0.1738)\n",
      "1098 Training Loss: tensor(0.1738)\n",
      "1099 Training Loss: tensor(0.1738)\n",
      "1100 Training Loss: tensor(0.1738)\n",
      "1101 Training Loss: tensor(0.1738)\n",
      "1102 Training Loss: tensor(0.1738)\n",
      "1103 Training Loss: tensor(0.1738)\n",
      "1104 Training Loss: tensor(0.1738)\n",
      "1105 Training Loss: tensor(0.1737)\n",
      "1106 Training Loss: tensor(0.1737)\n",
      "1107 Training Loss: tensor(0.1737)\n",
      "1108 Training Loss: tensor(0.1737)\n",
      "1109 Training Loss: tensor(0.1737)\n",
      "1110 Training Loss: tensor(0.1737)\n",
      "1111 Training Loss: tensor(0.1737)\n",
      "1112 Training Loss: tensor(0.1737)\n",
      "1113 Training Loss: tensor(0.1736)\n",
      "1114 Training Loss: tensor(0.1736)\n",
      "1115 Training Loss: tensor(0.1736)\n",
      "1116 Training Loss: tensor(0.1736)\n",
      "1117 Training Loss: tensor(0.1736)\n",
      "1118 Training Loss: tensor(0.1736)\n",
      "1119 Training Loss: tensor(0.1736)\n",
      "1120 Training Loss: tensor(0.1736)\n",
      "1121 Training Loss: tensor(0.1735)\n",
      "1122 Training Loss: tensor(0.1735)\n",
      "1123 Training Loss: tensor(0.1735)\n",
      "1124 Training Loss: tensor(0.1735)\n",
      "1125 Training Loss: tensor(0.1735)\n",
      "1126 Training Loss: tensor(0.1735)\n",
      "1127 Training Loss: tensor(0.1735)\n",
      "1128 Training Loss: tensor(0.1734)\n",
      "1129 Training Loss: tensor(0.1734)\n",
      "1130 Training Loss: tensor(0.1734)\n",
      "1131 Training Loss: tensor(0.1734)\n",
      "1132 Training Loss: tensor(0.1734)\n",
      "1133 Training Loss: tensor(0.1734)\n",
      "1134 Training Loss: tensor(0.1734)\n",
      "1135 Training Loss: tensor(0.1733)\n",
      "1136 Training Loss: tensor(0.1733)\n",
      "1137 Training Loss: tensor(0.1733)\n",
      "1138 Training Loss: tensor(0.1733)\n",
      "1139 Training Loss: tensor(0.1733)\n",
      "1140 Training Loss: tensor(0.1733)\n",
      "1141 Training Loss: tensor(0.1733)\n",
      "1142 Training Loss: tensor(0.1732)\n",
      "1143 Training Loss: tensor(0.1732)\n",
      "1144 Training Loss: tensor(0.1732)\n",
      "1145 Training Loss: tensor(0.1732)\n",
      "1146 Training Loss: tensor(0.1732)\n",
      "1147 Training Loss: tensor(0.1732)\n",
      "1148 Training Loss: tensor(0.1731)\n",
      "1149 Training Loss: tensor(0.1731)\n",
      "1150 Training Loss: tensor(0.1731)\n",
      "1151 Training Loss: tensor(0.1731)\n",
      "1152 Training Loss: tensor(0.1731)\n",
      "1153 Training Loss: tensor(0.1731)\n",
      "1154 Training Loss: tensor(0.1730)\n",
      "1155 Training Loss: tensor(0.1730)\n",
      "1156 Training Loss: tensor(0.1730)\n",
      "1157 Training Loss: tensor(0.1730)\n",
      "1158 Training Loss: tensor(0.1730)\n",
      "1159 Training Loss: tensor(0.1730)\n",
      "1160 Training Loss: tensor(0.1729)\n",
      "1161 Training Loss: tensor(0.1729)\n",
      "1162 Training Loss: tensor(0.1729)\n",
      "1163 Training Loss: tensor(0.1729)\n",
      "1164 Training Loss: tensor(0.1729)\n",
      "1165 Training Loss: tensor(0.1728)\n",
      "1166 Training Loss: tensor(0.1728)\n",
      "1167 Training Loss: tensor(0.1728)\n",
      "1168 Training Loss: tensor(0.1728)\n",
      "1169 Training Loss: tensor(0.1728)\n",
      "1170 Training Loss: tensor(0.1727)\n",
      "1171 Training Loss: tensor(0.1727)\n",
      "1172 Training Loss: tensor(0.1727)\n",
      "1173 Training Loss: tensor(0.1727)\n",
      "1174 Training Loss: tensor(0.1727)\n",
      "1175 Training Loss: tensor(0.1726)\n",
      "1176 Training Loss: tensor(0.1726)\n",
      "1177 Training Loss: tensor(0.1726)\n",
      "1178 Training Loss: tensor(0.1726)\n",
      "1179 Training Loss: tensor(0.1726)\n",
      "1180 Training Loss: tensor(0.1725)\n",
      "1181 Training Loss: tensor(0.1725)\n",
      "1182 Training Loss: tensor(0.1725)\n",
      "1183 Training Loss: tensor(0.1725)\n",
      "1184 Training Loss: tensor(0.1724)\n",
      "1185 Training Loss: tensor(0.1724)\n",
      "1186 Training Loss: tensor(0.1724)\n",
      "1187 Training Loss: tensor(0.1724)\n",
      "1188 Training Loss: tensor(0.1724)\n",
      "1189 Training Loss: tensor(0.1723)\n",
      "1190 Training Loss: tensor(0.1723)\n",
      "1191 Training Loss: tensor(0.1723)\n",
      "1192 Training Loss: tensor(0.1723)\n",
      "1193 Training Loss: tensor(0.1722)\n",
      "1194 Training Loss: tensor(0.1722)\n",
      "1195 Training Loss: tensor(0.1722)\n",
      "1196 Training Loss: tensor(0.1722)\n",
      "1197 Training Loss: tensor(0.1721)\n",
      "1198 Training Loss: tensor(0.1721)\n",
      "1199 Training Loss: tensor(0.1721)\n",
      "1200 Training Loss: tensor(0.1721)\n",
      "1201 Training Loss: tensor(0.1720)\n",
      "1202 Training Loss: tensor(0.1720)\n",
      "1203 Training Loss: tensor(0.1720)\n",
      "1204 Training Loss: tensor(0.1720)\n",
      "1205 Training Loss: tensor(0.1719)\n",
      "1206 Training Loss: tensor(0.1719)\n",
      "1207 Training Loss: tensor(0.1719)\n",
      "1208 Training Loss: tensor(0.1718)\n",
      "1209 Training Loss: tensor(0.1718)\n",
      "1210 Training Loss: tensor(0.1718)\n",
      "1211 Training Loss: tensor(0.1718)\n",
      "1212 Training Loss: tensor(0.1717)\n",
      "1213 Training Loss: tensor(0.1717)\n",
      "1214 Training Loss: tensor(0.1717)\n",
      "1215 Training Loss: tensor(0.1716)\n",
      "1216 Training Loss: tensor(0.1716)\n",
      "1217 Training Loss: tensor(0.1716)\n",
      "1218 Training Loss: tensor(0.1716)\n",
      "1219 Training Loss: tensor(0.1715)\n",
      "1220 Training Loss: tensor(0.1715)\n",
      "1221 Training Loss: tensor(0.1715)\n",
      "1222 Training Loss: tensor(0.1714)\n",
      "1223 Training Loss: tensor(0.1714)\n",
      "1224 Training Loss: tensor(0.1714)\n",
      "1225 Training Loss: tensor(0.1713)\n",
      "1226 Training Loss: tensor(0.1713)\n",
      "1227 Training Loss: tensor(0.1713)\n",
      "1228 Training Loss: tensor(0.1712)\n",
      "1229 Training Loss: tensor(0.1712)\n",
      "1230 Training Loss: tensor(0.1712)\n",
      "1231 Training Loss: tensor(0.1711)\n",
      "1232 Training Loss: tensor(0.1711)\n",
      "1233 Training Loss: tensor(0.1711)\n",
      "1234 Training Loss: tensor(0.1710)\n",
      "1235 Training Loss: tensor(0.1710)\n",
      "1236 Training Loss: tensor(0.1710)\n",
      "1237 Training Loss: tensor(0.1709)\n",
      "1238 Training Loss: tensor(0.1709)\n",
      "1239 Training Loss: tensor(0.1709)\n",
      "1240 Training Loss: tensor(0.1708)\n",
      "1241 Training Loss: tensor(0.1708)\n",
      "1242 Training Loss: tensor(0.1708)\n",
      "1243 Training Loss: tensor(0.1707)\n",
      "1244 Training Loss: tensor(0.1707)\n",
      "1245 Training Loss: tensor(0.1706)\n",
      "1246 Training Loss: tensor(0.1706)\n",
      "1247 Training Loss: tensor(0.1706)\n",
      "1248 Training Loss: tensor(0.1705)\n",
      "1249 Training Loss: tensor(0.1705)\n",
      "1250 Training Loss: tensor(0.1705)\n",
      "1251 Training Loss: tensor(0.1704)\n",
      "1252 Training Loss: tensor(0.1704)\n",
      "1253 Training Loss: tensor(0.1703)\n",
      "1254 Training Loss: tensor(0.1703)\n",
      "1255 Training Loss: tensor(0.1703)\n",
      "1256 Training Loss: tensor(0.1702)\n",
      "1257 Training Loss: tensor(0.1702)\n",
      "1258 Training Loss: tensor(0.1701)\n",
      "1259 Training Loss: tensor(0.1701)\n",
      "1260 Training Loss: tensor(0.1700)\n",
      "1261 Training Loss: tensor(0.1700)\n",
      "1262 Training Loss: tensor(0.1700)\n",
      "1263 Training Loss: tensor(0.1699)\n",
      "1264 Training Loss: tensor(0.1699)\n",
      "1265 Training Loss: tensor(0.1698)\n",
      "1266 Training Loss: tensor(0.1698)\n",
      "1267 Training Loss: tensor(0.1697)\n",
      "1268 Training Loss: tensor(0.1697)\n",
      "1269 Training Loss: tensor(0.1696)\n",
      "1270 Training Loss: tensor(0.1696)\n",
      "1271 Training Loss: tensor(0.1696)\n",
      "1272 Training Loss: tensor(0.1695)\n",
      "1273 Training Loss: tensor(0.1695)\n",
      "1274 Training Loss: tensor(0.1694)\n",
      "1275 Training Loss: tensor(0.1694)\n",
      "1276 Training Loss: tensor(0.1693)\n",
      "1277 Training Loss: tensor(0.1693)\n",
      "1278 Training Loss: tensor(0.1692)\n",
      "1279 Training Loss: tensor(0.1692)\n",
      "1280 Training Loss: tensor(0.1691)\n",
      "1281 Training Loss: tensor(0.1691)\n",
      "1282 Training Loss: tensor(0.1690)\n",
      "1283 Training Loss: tensor(0.1690)\n",
      "1284 Training Loss: tensor(0.1689)\n",
      "1285 Training Loss: tensor(0.1689)\n",
      "1286 Training Loss: tensor(0.1688)\n",
      "1287 Training Loss: tensor(0.1688)\n",
      "1288 Training Loss: tensor(0.1687)\n",
      "1289 Training Loss: tensor(0.1687)\n",
      "1290 Training Loss: tensor(0.1686)\n",
      "1291 Training Loss: tensor(0.1686)\n",
      "1292 Training Loss: tensor(0.1685)\n",
      "1293 Training Loss: tensor(0.1685)\n",
      "1294 Training Loss: tensor(0.1684)\n",
      "1295 Training Loss: tensor(0.1683)\n",
      "1296 Training Loss: tensor(0.1683)\n",
      "1297 Training Loss: tensor(0.1682)\n",
      "1298 Training Loss: tensor(0.1682)\n",
      "1299 Training Loss: tensor(0.1681)\n",
      "1300 Training Loss: tensor(0.1681)\n",
      "1301 Training Loss: tensor(0.1680)\n",
      "1302 Training Loss: tensor(0.1679)\n",
      "1303 Training Loss: tensor(0.1679)\n",
      "1304 Training Loss: tensor(0.1678)\n",
      "1305 Training Loss: tensor(0.1678)\n",
      "1306 Training Loss: tensor(0.1677)\n",
      "1307 Training Loss: tensor(0.1676)\n",
      "1308 Training Loss: tensor(0.1676)\n",
      "1309 Training Loss: tensor(0.1675)\n",
      "1310 Training Loss: tensor(0.1675)\n",
      "1311 Training Loss: tensor(0.1674)\n",
      "1312 Training Loss: tensor(0.1673)\n",
      "1313 Training Loss: tensor(0.1673)\n",
      "1314 Training Loss: tensor(0.1672)\n",
      "1315 Training Loss: tensor(0.1671)\n",
      "1316 Training Loss: tensor(0.1671)\n",
      "1317 Training Loss: tensor(0.1670)\n",
      "1318 Training Loss: tensor(0.1669)\n",
      "1319 Training Loss: tensor(0.1669)\n",
      "1320 Training Loss: tensor(0.1668)\n",
      "1321 Training Loss: tensor(0.1667)\n",
      "1322 Training Loss: tensor(0.1667)\n",
      "1323 Training Loss: tensor(0.1666)\n",
      "1324 Training Loss: tensor(0.1665)\n",
      "1325 Training Loss: tensor(0.1665)\n",
      "1326 Training Loss: tensor(0.1664)\n",
      "1327 Training Loss: tensor(0.1663)\n",
      "1328 Training Loss: tensor(0.1663)\n",
      "1329 Training Loss: tensor(0.1662)\n",
      "1330 Training Loss: tensor(0.1661)\n",
      "1331 Training Loss: tensor(0.1660)\n",
      "1332 Training Loss: tensor(0.1660)\n",
      "1333 Training Loss: tensor(0.1659)\n",
      "1334 Training Loss: tensor(0.1658)\n",
      "1335 Training Loss: tensor(0.1657)\n",
      "1336 Training Loss: tensor(0.1657)\n",
      "1337 Training Loss: tensor(0.1656)\n",
      "1338 Training Loss: tensor(0.1655)\n",
      "1339 Training Loss: tensor(0.1654)\n",
      "1340 Training Loss: tensor(0.1654)\n",
      "1341 Training Loss: tensor(0.1653)\n",
      "1342 Training Loss: tensor(0.1652)\n",
      "1343 Training Loss: tensor(0.1651)\n",
      "1344 Training Loss: tensor(0.1650)\n",
      "1345 Training Loss: tensor(0.1650)\n",
      "1346 Training Loss: tensor(0.1649)\n",
      "1347 Training Loss: tensor(0.1648)\n",
      "1348 Training Loss: tensor(0.1647)\n",
      "1349 Training Loss: tensor(0.1646)\n",
      "1350 Training Loss: tensor(0.1646)\n",
      "1351 Training Loss: tensor(0.1645)\n",
      "1352 Training Loss: tensor(0.1644)\n",
      "1353 Training Loss: tensor(0.1643)\n",
      "1354 Training Loss: tensor(0.1642)\n",
      "1355 Training Loss: tensor(0.1641)\n",
      "1356 Training Loss: tensor(0.1640)\n",
      "1357 Training Loss: tensor(0.1640)\n",
      "1358 Training Loss: tensor(0.1639)\n",
      "1359 Training Loss: tensor(0.1638)\n",
      "1360 Training Loss: tensor(0.1637)\n",
      "1361 Training Loss: tensor(0.1636)\n",
      "1362 Training Loss: tensor(0.1635)\n",
      "1363 Training Loss: tensor(0.1634)\n",
      "1364 Training Loss: tensor(0.1633)\n",
      "1365 Training Loss: tensor(0.1632)\n",
      "1366 Training Loss: tensor(0.1631)\n",
      "1367 Training Loss: tensor(0.1630)\n",
      "1368 Training Loss: tensor(0.1629)\n",
      "1369 Training Loss: tensor(0.1628)\n",
      "1370 Training Loss: tensor(0.1628)\n",
      "1371 Training Loss: tensor(0.1627)\n",
      "1372 Training Loss: tensor(0.1626)\n",
      "1373 Training Loss: tensor(0.1624)\n",
      "1374 Training Loss: tensor(0.1624)\n",
      "1375 Training Loss: tensor(0.1622)\n",
      "1376 Training Loss: tensor(0.1622)\n",
      "1377 Training Loss: tensor(0.1621)\n",
      "1378 Training Loss: tensor(0.1620)\n",
      "1379 Training Loss: tensor(0.1619)\n",
      "1380 Training Loss: tensor(0.1617)\n",
      "1381 Training Loss: tensor(0.1617)\n",
      "1382 Training Loss: tensor(0.1616)\n",
      "1383 Training Loss: tensor(0.1614)\n",
      "1384 Training Loss: tensor(0.1613)\n",
      "1385 Training Loss: tensor(0.1612)\n",
      "1386 Training Loss: tensor(0.1611)\n",
      "1387 Training Loss: tensor(0.1610)\n",
      "1388 Training Loss: tensor(0.1609)\n",
      "1389 Training Loss: tensor(0.1608)\n",
      "1390 Training Loss: tensor(0.1607)\n",
      "1391 Training Loss: tensor(0.1606)\n",
      "1392 Training Loss: tensor(0.1605)\n",
      "1393 Training Loss: tensor(0.1603)\n",
      "1394 Training Loss: tensor(0.1602)\n",
      "1395 Training Loss: tensor(0.1601)\n",
      "1396 Training Loss: tensor(0.1600)\n",
      "1397 Training Loss: tensor(0.1599)\n",
      "1398 Training Loss: tensor(0.1598)\n",
      "1399 Training Loss: tensor(0.1597)\n",
      "1400 Training Loss: tensor(0.1596)\n",
      "1401 Training Loss: tensor(0.1594)\n",
      "1402 Training Loss: tensor(0.1593)\n",
      "1403 Training Loss: tensor(0.1592)\n",
      "1404 Training Loss: tensor(0.1591)\n",
      "1405 Training Loss: tensor(0.1590)\n",
      "1406 Training Loss: tensor(0.1588)\n",
      "1407 Training Loss: tensor(0.1587)\n",
      "1408 Training Loss: tensor(0.1586)\n",
      "1409 Training Loss: tensor(0.1585)\n",
      "1410 Training Loss: tensor(0.1584)\n",
      "1411 Training Loss: tensor(0.1582)\n",
      "1412 Training Loss: tensor(0.1581)\n",
      "1413 Training Loss: tensor(0.1579)\n",
      "1414 Training Loss: tensor(0.1578)\n",
      "1415 Training Loss: tensor(0.1577)\n",
      "1416 Training Loss: tensor(0.1576)\n",
      "1417 Training Loss: tensor(0.1575)\n",
      "1418 Training Loss: tensor(0.1573)\n",
      "1419 Training Loss: tensor(0.1572)\n",
      "1420 Training Loss: tensor(0.1571)\n",
      "1421 Training Loss: tensor(0.1570)\n",
      "1422 Training Loss: tensor(0.1568)\n",
      "1423 Training Loss: tensor(0.1567)\n",
      "1424 Training Loss: tensor(0.1566)\n",
      "1425 Training Loss: tensor(0.1564)\n",
      "1426 Training Loss: tensor(0.1563)\n",
      "1427 Training Loss: tensor(0.1562)\n",
      "1428 Training Loss: tensor(0.1560)\n",
      "1429 Training Loss: tensor(0.1559)\n",
      "1430 Training Loss: tensor(0.1558)\n",
      "1431 Training Loss: tensor(0.1556)\n",
      "1432 Training Loss: tensor(0.1554)\n",
      "1433 Training Loss: tensor(0.1553)\n",
      "1434 Training Loss: tensor(0.1552)\n",
      "1435 Training Loss: tensor(0.1550)\n",
      "1436 Training Loss: tensor(0.1550)\n",
      "1437 Training Loss: tensor(0.1548)\n",
      "1438 Training Loss: tensor(0.1546)\n",
      "1439 Training Loss: tensor(0.1545)\n",
      "1440 Training Loss: tensor(0.1544)\n",
      "1441 Training Loss: tensor(0.1542)\n",
      "1442 Training Loss: tensor(0.1541)\n",
      "1443 Training Loss: tensor(0.1539)\n",
      "1444 Training Loss: tensor(0.1538)\n",
      "1445 Training Loss: tensor(0.1537)\n",
      "1446 Training Loss: tensor(0.1535)\n",
      "1447 Training Loss: tensor(0.1534)\n",
      "1448 Training Loss: tensor(0.1532)\n",
      "1449 Training Loss: tensor(0.1531)\n",
      "1450 Training Loss: tensor(0.1530)\n",
      "1451 Training Loss: tensor(0.1528)\n",
      "1452 Training Loss: tensor(0.1527)\n",
      "1453 Training Loss: tensor(0.1525)\n",
      "1454 Training Loss: tensor(0.1524)\n",
      "1455 Training Loss: tensor(0.1522)\n",
      "1456 Training Loss: tensor(0.1521)\n",
      "1457 Training Loss: tensor(0.1520)\n",
      "1458 Training Loss: tensor(0.1517)\n",
      "1459 Training Loss: tensor(0.1516)\n",
      "1460 Training Loss: tensor(0.1515)\n",
      "1461 Training Loss: tensor(0.1513)\n",
      "1462 Training Loss: tensor(0.1512)\n",
      "1463 Training Loss: tensor(0.1511)\n",
      "1464 Training Loss: tensor(0.1509)\n",
      "1465 Training Loss: tensor(0.1507)\n",
      "1466 Training Loss: tensor(0.1506)\n",
      "1467 Training Loss: tensor(0.1504)\n",
      "1468 Training Loss: tensor(0.1504)\n",
      "1469 Training Loss: tensor(0.1500)\n",
      "1470 Training Loss: tensor(0.1499)\n",
      "1471 Training Loss: tensor(0.1498)\n",
      "1472 Training Loss: tensor(0.1496)\n",
      "1473 Training Loss: tensor(0.1496)\n",
      "1474 Training Loss: tensor(0.1494)\n",
      "1475 Training Loss: tensor(0.1493)\n",
      "1476 Training Loss: tensor(0.1492)\n",
      "1477 Training Loss: tensor(0.1490)\n",
      "1478 Training Loss: tensor(0.1487)\n",
      "1479 Training Loss: tensor(0.1487)\n",
      "1480 Training Loss: tensor(0.1486)\n",
      "1481 Training Loss: tensor(0.1483)\n",
      "1482 Training Loss: tensor(0.1483)\n",
      "1483 Training Loss: tensor(0.1481)\n",
      "1484 Training Loss: tensor(0.1479)\n",
      "1485 Training Loss: tensor(0.1477)\n",
      "1486 Training Loss: tensor(0.1476)\n",
      "1487 Training Loss: tensor(0.1475)\n",
      "1488 Training Loss: tensor(0.1473)\n",
      "1489 Training Loss: tensor(0.1471)\n",
      "1490 Training Loss: tensor(0.1471)\n",
      "1491 Training Loss: tensor(0.1471)\n",
      "1492 Training Loss: tensor(0.1466)\n",
      "1493 Training Loss: tensor(0.1465)\n",
      "1494 Training Loss: tensor(0.1464)\n",
      "1495 Training Loss: tensor(0.1464)\n",
      "1496 Training Loss: tensor(0.1462)\n",
      "1497 Training Loss: tensor(0.1459)\n",
      "1498 Training Loss: tensor(0.1458)\n",
      "1499 Training Loss: tensor(0.1457)\n",
      "1500 Training Loss: tensor(0.1454)\n",
      "1501 Training Loss: tensor(0.1453)\n",
      "1502 Training Loss: tensor(0.1454)\n",
      "1503 Training Loss: tensor(0.1450)\n",
      "1504 Training Loss: tensor(0.1448)\n",
      "1505 Training Loss: tensor(0.1445)\n",
      "1506 Training Loss: tensor(0.1445)\n",
      "1507 Training Loss: tensor(0.1443)\n",
      "1508 Training Loss: tensor(0.1444)\n",
      "1509 Training Loss: tensor(0.1440)\n",
      "1510 Training Loss: tensor(0.1438)\n",
      "1511 Training Loss: tensor(0.1437)\n",
      "1512 Training Loss: tensor(0.1437)\n",
      "1513 Training Loss: tensor(0.1434)\n",
      "1514 Training Loss: tensor(0.1432)\n",
      "1515 Training Loss: tensor(0.1432)\n",
      "1516 Training Loss: tensor(0.1430)\n",
      "1517 Training Loss: tensor(0.1428)\n",
      "1518 Training Loss: tensor(0.1423)\n",
      "1519 Training Loss: tensor(0.1423)\n",
      "1520 Training Loss: tensor(0.1422)\n",
      "1521 Training Loss: tensor(0.1422)\n",
      "1522 Training Loss: tensor(0.1417)\n",
      "1523 Training Loss: tensor(0.1420)\n",
      "1524 Training Loss: tensor(0.1417)\n",
      "1525 Training Loss: tensor(0.1413)\n",
      "1526 Training Loss: tensor(0.1413)\n",
      "1527 Training Loss: tensor(0.1410)\n",
      "1528 Training Loss: tensor(0.1408)\n",
      "1529 Training Loss: tensor(0.1404)\n",
      "1530 Training Loss: tensor(0.1405)\n",
      "1531 Training Loss: tensor(0.1403)\n",
      "1532 Training Loss: tensor(0.1404)\n",
      "1533 Training Loss: tensor(0.1402)\n",
      "1534 Training Loss: tensor(0.1398)\n",
      "1535 Training Loss: tensor(0.1397)\n",
      "1536 Training Loss: tensor(0.1394)\n",
      "1537 Training Loss: tensor(0.1392)\n",
      "1538 Training Loss: tensor(0.1391)\n",
      "1539 Training Loss: tensor(0.1389)\n",
      "1540 Training Loss: tensor(0.1387)\n",
      "1541 Training Loss: tensor(0.1385)\n",
      "1542 Training Loss: tensor(0.1382)\n",
      "1543 Training Loss: tensor(0.1380)\n",
      "1544 Training Loss: tensor(0.1378)\n",
      "1545 Training Loss: tensor(0.1379)\n",
      "1546 Training Loss: tensor(0.1376)\n",
      "1547 Training Loss: tensor(0.1375)\n",
      "1548 Training Loss: tensor(0.1370)\n",
      "1549 Training Loss: tensor(0.1372)\n",
      "1550 Training Loss: tensor(0.1372)\n",
      "1551 Training Loss: tensor(0.1366)\n",
      "1552 Training Loss: tensor(0.1366)\n",
      "1553 Training Loss: tensor(0.1363)\n",
      "1554 Training Loss: tensor(0.1359)\n",
      "1555 Training Loss: tensor(0.1358)\n",
      "1556 Training Loss: tensor(0.1358)\n",
      "1557 Training Loss: tensor(0.1358)\n",
      "1558 Training Loss: tensor(0.1353)\n",
      "1559 Training Loss: tensor(0.1352)\n",
      "1560 Training Loss: tensor(0.1351)\n",
      "1561 Training Loss: tensor(0.1347)\n",
      "1562 Training Loss: tensor(0.1347)\n",
      "1563 Training Loss: tensor(0.1342)\n",
      "1564 Training Loss: tensor(0.1342)\n",
      "1565 Training Loss: tensor(0.1340)\n",
      "1566 Training Loss: tensor(0.1335)\n",
      "1567 Training Loss: tensor(0.1335)\n",
      "1568 Training Loss: tensor(0.1333)\n",
      "1569 Training Loss: tensor(0.1332)\n",
      "1570 Training Loss: tensor(0.1332)\n",
      "1571 Training Loss: tensor(0.1326)\n",
      "1572 Training Loss: tensor(0.1328)\n",
      "1573 Training Loss: tensor(0.1322)\n",
      "1574 Training Loss: tensor(0.1320)\n",
      "1575 Training Loss: tensor(0.1317)\n",
      "1576 Training Loss: tensor(0.1314)\n",
      "1577 Training Loss: tensor(0.1315)\n",
      "1578 Training Loss: tensor(0.1312)\n",
      "1579 Training Loss: tensor(0.1309)\n",
      "1580 Training Loss: tensor(0.1309)\n",
      "1581 Training Loss: tensor(0.1307)\n",
      "1582 Training Loss: tensor(0.1305)\n",
      "1583 Training Loss: tensor(0.1303)\n",
      "1584 Training Loss: tensor(0.1299)\n",
      "1585 Training Loss: tensor(0.1300)\n",
      "1586 Training Loss: tensor(0.1295)\n",
      "1587 Training Loss: tensor(0.1291)\n",
      "1588 Training Loss: tensor(0.1293)\n",
      "1589 Training Loss: tensor(0.1290)\n",
      "1590 Training Loss: tensor(0.1287)\n",
      "1591 Training Loss: tensor(0.1285)\n",
      "1592 Training Loss: tensor(0.1281)\n",
      "1593 Training Loss: tensor(0.1278)\n",
      "1594 Training Loss: tensor(0.1280)\n",
      "1595 Training Loss: tensor(0.1275)\n",
      "1596 Training Loss: tensor(0.1270)\n",
      "1597 Training Loss: tensor(0.1274)\n",
      "1598 Training Loss: tensor(0.1268)\n",
      "1599 Training Loss: tensor(0.1269)\n",
      "1600 Training Loss: tensor(0.1262)\n",
      "1601 Training Loss: tensor(0.1261)\n",
      "1602 Training Loss: tensor(0.1261)\n",
      "1603 Training Loss: tensor(0.1258)\n",
      "1604 Training Loss: tensor(0.1254)\n",
      "1605 Training Loss: tensor(0.1251)\n",
      "1606 Training Loss: tensor(0.1255)\n",
      "1607 Training Loss: tensor(0.1251)\n",
      "1608 Training Loss: tensor(0.1244)\n",
      "1609 Training Loss: tensor(0.1244)\n",
      "1610 Training Loss: tensor(0.1242)\n",
      "1611 Training Loss: tensor(0.1239)\n",
      "1612 Training Loss: tensor(0.1239)\n",
      "1613 Training Loss: tensor(0.1234)\n",
      "1614 Training Loss: tensor(0.1233)\n",
      "1615 Training Loss: tensor(0.1231)\n",
      "1616 Training Loss: tensor(0.1230)\n",
      "1617 Training Loss: tensor(0.1230)\n",
      "1618 Training Loss: tensor(0.1227)\n",
      "1619 Training Loss: tensor(0.1223)\n",
      "1620 Training Loss: tensor(0.1218)\n",
      "1621 Training Loss: tensor(0.1213)\n",
      "1622 Training Loss: tensor(0.1214)\n",
      "1623 Training Loss: tensor(0.1211)\n",
      "1624 Training Loss: tensor(0.1210)\n",
      "1625 Training Loss: tensor(0.1207)\n",
      "1626 Training Loss: tensor(0.1204)\n",
      "1627 Training Loss: tensor(0.1204)\n",
      "1628 Training Loss: tensor(0.1200)\n",
      "1629 Training Loss: tensor(0.1199)\n",
      "1630 Training Loss: tensor(0.1196)\n",
      "1631 Training Loss: tensor(0.1192)\n",
      "1632 Training Loss: tensor(0.1191)\n",
      "1633 Training Loss: tensor(0.1186)\n",
      "1634 Training Loss: tensor(0.1188)\n",
      "1635 Training Loss: tensor(0.1187)\n",
      "1636 Training Loss: tensor(0.1181)\n",
      "1637 Training Loss: tensor(0.1181)\n",
      "1638 Training Loss: tensor(0.1177)\n",
      "1639 Training Loss: tensor(0.1175)\n",
      "1640 Training Loss: tensor(0.1174)\n",
      "1641 Training Loss: tensor(0.1171)\n",
      "1642 Training Loss: tensor(0.1167)\n",
      "1643 Training Loss: tensor(0.1163)\n",
      "1644 Training Loss: tensor(0.1163)\n",
      "1645 Training Loss: tensor(0.1162)\n",
      "1646 Training Loss: tensor(0.1157)\n",
      "1647 Training Loss: tensor(0.1152)\n",
      "1648 Training Loss: tensor(0.1156)\n",
      "1649 Training Loss: tensor(0.1153)\n",
      "1650 Training Loss: tensor(0.1149)\n",
      "1651 Training Loss: tensor(0.1149)\n",
      "1652 Training Loss: tensor(0.1142)\n",
      "1653 Training Loss: tensor(0.1140)\n",
      "1654 Training Loss: tensor(0.1139)\n",
      "1655 Training Loss: tensor(0.1137)\n",
      "1656 Training Loss: tensor(0.1136)\n",
      "1657 Training Loss: tensor(0.1136)\n",
      "1658 Training Loss: tensor(0.1134)\n",
      "1659 Training Loss: tensor(0.1128)\n",
      "1660 Training Loss: tensor(0.1126)\n",
      "1661 Training Loss: tensor(0.1124)\n",
      "1662 Training Loss: tensor(0.1122)\n",
      "1663 Training Loss: tensor(0.1119)\n",
      "1664 Training Loss: tensor(0.1118)\n",
      "1665 Training Loss: tensor(0.1115)\n",
      "1666 Training Loss: tensor(0.1111)\n",
      "1667 Training Loss: tensor(0.1109)\n",
      "1668 Training Loss: tensor(0.1109)\n",
      "1669 Training Loss: tensor(0.1108)\n",
      "1670 Training Loss: tensor(0.1103)\n",
      "1671 Training Loss: tensor(0.1097)\n",
      "1672 Training Loss: tensor(0.1104)\n",
      "1673 Training Loss: tensor(0.1098)\n",
      "1674 Training Loss: tensor(0.1093)\n",
      "1675 Training Loss: tensor(0.1086)\n",
      "1676 Training Loss: tensor(0.1089)\n",
      "1677 Training Loss: tensor(0.1087)\n",
      "1678 Training Loss: tensor(0.1092)\n",
      "1679 Training Loss: tensor(0.1077)\n",
      "1680 Training Loss: tensor(0.1075)\n",
      "1681 Training Loss: tensor(0.1075)\n",
      "1682 Training Loss: tensor(0.1074)\n",
      "1683 Training Loss: tensor(0.1072)\n",
      "1684 Training Loss: tensor(0.1074)\n",
      "1685 Training Loss: tensor(0.1068)\n",
      "1686 Training Loss: tensor(0.1066)\n",
      "1687 Training Loss: tensor(0.1067)\n",
      "1688 Training Loss: tensor(0.1065)\n",
      "1689 Training Loss: tensor(0.1061)\n",
      "1690 Training Loss: tensor(0.1062)\n",
      "1691 Training Loss: tensor(0.1057)\n",
      "1692 Training Loss: tensor(0.1055)\n",
      "1693 Training Loss: tensor(0.1048)\n",
      "1694 Training Loss: tensor(0.1047)\n",
      "1695 Training Loss: tensor(0.1050)\n",
      "1696 Training Loss: tensor(0.1046)\n",
      "1697 Training Loss: tensor(0.1041)\n",
      "1698 Training Loss: tensor(0.1044)\n",
      "1699 Training Loss: tensor(0.1041)\n",
      "1700 Training Loss: tensor(0.1032)\n",
      "1701 Training Loss: tensor(0.1035)\n",
      "1702 Training Loss: tensor(0.1029)\n",
      "1703 Training Loss: tensor(0.1030)\n",
      "1704 Training Loss: tensor(0.1029)\n",
      "1705 Training Loss: tensor(0.1027)\n",
      "1706 Training Loss: tensor(0.1025)\n",
      "1707 Training Loss: tensor(0.1023)\n",
      "1708 Training Loss: tensor(0.1020)\n",
      "1709 Training Loss: tensor(0.1013)\n",
      "1710 Training Loss: tensor(0.1014)\n",
      "1711 Training Loss: tensor(0.1011)\n",
      "1712 Training Loss: tensor(0.1009)\n",
      "1713 Training Loss: tensor(0.1013)\n",
      "1714 Training Loss: tensor(0.1012)\n",
      "1715 Training Loss: tensor(0.1005)\n",
      "1716 Training Loss: tensor(0.1005)\n",
      "1717 Training Loss: tensor(0.1004)\n",
      "1718 Training Loss: tensor(0.0997)\n",
      "1719 Training Loss: tensor(0.1000)\n",
      "1720 Training Loss: tensor(0.0990)\n",
      "1721 Training Loss: tensor(0.0990)\n",
      "1722 Training Loss: tensor(0.0989)\n",
      "1723 Training Loss: tensor(0.0991)\n",
      "1724 Training Loss: tensor(0.0990)\n",
      "1725 Training Loss: tensor(0.0983)\n",
      "1726 Training Loss: tensor(0.0983)\n",
      "1727 Training Loss: tensor(0.0981)\n",
      "1728 Training Loss: tensor(0.0973)\n",
      "1729 Training Loss: tensor(0.0980)\n",
      "1730 Training Loss: tensor(0.0979)\n",
      "1731 Training Loss: tensor(0.0977)\n",
      "1732 Training Loss: tensor(0.0973)\n",
      "1733 Training Loss: tensor(0.0975)\n",
      "1734 Training Loss: tensor(0.0964)\n",
      "1735 Training Loss: tensor(0.0968)\n",
      "1736 Training Loss: tensor(0.0960)\n",
      "1737 Training Loss: tensor(0.0962)\n",
      "1738 Training Loss: tensor(0.0961)\n",
      "1739 Training Loss: tensor(0.0955)\n",
      "1740 Training Loss: tensor(0.0955)\n",
      "1741 Training Loss: tensor(0.0952)\n",
      "1742 Training Loss: tensor(0.0957)\n",
      "1743 Training Loss: tensor(0.0956)\n",
      "1744 Training Loss: tensor(0.0948)\n",
      "1745 Training Loss: tensor(0.0949)\n",
      "1746 Training Loss: tensor(0.0945)\n",
      "1747 Training Loss: tensor(0.0939)\n",
      "1748 Training Loss: tensor(0.0946)\n",
      "1749 Training Loss: tensor(0.0944)\n",
      "1750 Training Loss: tensor(0.0939)\n",
      "1751 Training Loss: tensor(0.0935)\n",
      "1752 Training Loss: tensor(0.0932)\n",
      "1753 Training Loss: tensor(0.0933)\n",
      "1754 Training Loss: tensor(0.0933)\n",
      "1755 Training Loss: tensor(0.0925)\n",
      "1756 Training Loss: tensor(0.0927)\n",
      "1757 Training Loss: tensor(0.0926)\n",
      "1758 Training Loss: tensor(0.0927)\n",
      "1759 Training Loss: tensor(0.0927)\n",
      "1760 Training Loss: tensor(0.0922)\n",
      "1761 Training Loss: tensor(0.0919)\n",
      "1762 Training Loss: tensor(0.0921)\n",
      "1763 Training Loss: tensor(0.0918)\n",
      "1764 Training Loss: tensor(0.0915)\n",
      "1765 Training Loss: tensor(0.0915)\n",
      "1766 Training Loss: tensor(0.0913)\n",
      "1767 Training Loss: tensor(0.0909)\n",
      "1768 Training Loss: tensor(0.0909)\n",
      "1769 Training Loss: tensor(0.0912)\n",
      "1770 Training Loss: tensor(0.0905)\n",
      "1771 Training Loss: tensor(0.0902)\n",
      "1772 Training Loss: tensor(0.0902)\n",
      "1773 Training Loss: tensor(0.0898)\n",
      "1774 Training Loss: tensor(0.0898)\n",
      "1775 Training Loss: tensor(0.0896)\n",
      "1776 Training Loss: tensor(0.0895)\n",
      "1777 Training Loss: tensor(0.0893)\n",
      "1778 Training Loss: tensor(0.0896)\n",
      "1779 Training Loss: tensor(0.0890)\n",
      "1780 Training Loss: tensor(0.0889)\n",
      "1781 Training Loss: tensor(0.0892)\n",
      "1782 Training Loss: tensor(0.0885)\n",
      "1783 Training Loss: tensor(0.0885)\n",
      "1784 Training Loss: tensor(0.0888)\n",
      "1785 Training Loss: tensor(0.0880)\n",
      "1786 Training Loss: tensor(0.0883)\n",
      "1787 Training Loss: tensor(0.0876)\n",
      "1788 Training Loss: tensor(0.0874)\n",
      "1789 Training Loss: tensor(0.0876)\n",
      "1790 Training Loss: tensor(0.0876)\n",
      "1791 Training Loss: tensor(0.0873)\n",
      "1792 Training Loss: tensor(0.0875)\n",
      "1793 Training Loss: tensor(0.0871)\n",
      "1794 Training Loss: tensor(0.0866)\n",
      "1795 Training Loss: tensor(0.0868)\n",
      "1796 Training Loss: tensor(0.0866)\n",
      "1797 Training Loss: tensor(0.0862)\n",
      "1798 Training Loss: tensor(0.0860)\n",
      "1799 Training Loss: tensor(0.0857)\n",
      "1800 Training Loss: tensor(0.0860)\n",
      "1801 Training Loss: tensor(0.0861)\n",
      "1802 Training Loss: tensor(0.0862)\n",
      "1803 Training Loss: tensor(0.0858)\n",
      "1804 Training Loss: tensor(0.0855)\n",
      "1805 Training Loss: tensor(0.0855)\n",
      "1806 Training Loss: tensor(0.0855)\n",
      "1807 Training Loss: tensor(0.0851)\n",
      "1808 Training Loss: tensor(0.0855)\n",
      "1809 Training Loss: tensor(0.0847)\n",
      "1810 Training Loss: tensor(0.0846)\n",
      "1811 Training Loss: tensor(0.0848)\n",
      "1812 Training Loss: tensor(0.0841)\n",
      "1813 Training Loss: tensor(0.0846)\n",
      "1814 Training Loss: tensor(0.0846)\n",
      "1815 Training Loss: tensor(0.0837)\n",
      "1816 Training Loss: tensor(0.0845)\n",
      "1817 Training Loss: tensor(0.0841)\n",
      "1818 Training Loss: tensor(0.0836)\n",
      "1819 Training Loss: tensor(0.0837)\n",
      "1820 Training Loss: tensor(0.0835)\n",
      "1821 Training Loss: tensor(0.0832)\n",
      "1822 Training Loss: tensor(0.0829)\n",
      "1823 Training Loss: tensor(0.0833)\n",
      "1824 Training Loss: tensor(0.0833)\n",
      "1825 Training Loss: tensor(0.0829)\n",
      "1826 Training Loss: tensor(0.0825)\n",
      "1827 Training Loss: tensor(0.0829)\n",
      "1828 Training Loss: tensor(0.0824)\n",
      "1829 Training Loss: tensor(0.0826)\n",
      "1830 Training Loss: tensor(0.0826)\n",
      "1831 Training Loss: tensor(0.0821)\n",
      "1832 Training Loss: tensor(0.0821)\n",
      "1833 Training Loss: tensor(0.0820)\n",
      "1834 Training Loss: tensor(0.0818)\n",
      "1835 Training Loss: tensor(0.0820)\n",
      "1836 Training Loss: tensor(0.0814)\n",
      "1837 Training Loss: tensor(0.0813)\n",
      "1838 Training Loss: tensor(0.0813)\n",
      "1839 Training Loss: tensor(0.0814)\n",
      "1840 Training Loss: tensor(0.0810)\n",
      "1841 Training Loss: tensor(0.0810)\n",
      "1842 Training Loss: tensor(0.0807)\n",
      "1843 Training Loss: tensor(0.0809)\n",
      "1844 Training Loss: tensor(0.0805)\n",
      "1845 Training Loss: tensor(0.0809)\n",
      "1846 Training Loss: tensor(0.0806)\n",
      "1847 Training Loss: tensor(0.0802)\n",
      "1848 Training Loss: tensor(0.0808)\n",
      "1849 Training Loss: tensor(0.0804)\n",
      "1850 Training Loss: tensor(0.0804)\n",
      "1851 Training Loss: tensor(0.0802)\n",
      "1852 Training Loss: tensor(0.0799)\n",
      "1853 Training Loss: tensor(0.0802)\n",
      "1854 Training Loss: tensor(0.0801)\n",
      "1855 Training Loss: tensor(0.0799)\n",
      "1856 Training Loss: tensor(0.0798)\n",
      "1857 Training Loss: tensor(0.0793)\n",
      "1858 Training Loss: tensor(0.0794)\n",
      "1859 Training Loss: tensor(0.0797)\n",
      "1860 Training Loss: tensor(0.0798)\n",
      "1861 Training Loss: tensor(0.0792)\n",
      "1862 Training Loss: tensor(0.0791)\n",
      "1863 Training Loss: tensor(0.0786)\n",
      "1864 Training Loss: tensor(0.0789)\n",
      "1865 Training Loss: tensor(0.0789)\n",
      "1866 Training Loss: tensor(0.0790)\n",
      "1867 Training Loss: tensor(0.0785)\n",
      "1868 Training Loss: tensor(0.0785)\n",
      "1869 Training Loss: tensor(0.0783)\n",
      "1870 Training Loss: tensor(0.0786)\n",
      "1871 Training Loss: tensor(0.0779)\n",
      "1872 Training Loss: tensor(0.0782)\n",
      "1873 Training Loss: tensor(0.0783)\n",
      "1874 Training Loss: tensor(0.0780)\n",
      "1875 Training Loss: tensor(0.0779)\n",
      "1876 Training Loss: tensor(0.0777)\n",
      "1877 Training Loss: tensor(0.0782)\n",
      "1878 Training Loss: tensor(0.0782)\n",
      "1879 Training Loss: tensor(0.0774)\n",
      "1880 Training Loss: tensor(0.0774)\n",
      "1881 Training Loss: tensor(0.0776)\n",
      "1882 Training Loss: tensor(0.0774)\n",
      "1883 Training Loss: tensor(0.0772)\n",
      "1884 Training Loss: tensor(0.0773)\n",
      "1885 Training Loss: tensor(0.0770)\n",
      "1886 Training Loss: tensor(0.0768)\n",
      "1887 Training Loss: tensor(0.0769)\n",
      "1888 Training Loss: tensor(0.0767)\n",
      "1889 Training Loss: tensor(0.0769)\n",
      "1890 Training Loss: tensor(0.0768)\n",
      "1891 Training Loss: tensor(0.0767)\n",
      "1892 Training Loss: tensor(0.0761)\n",
      "1893 Training Loss: tensor(0.0769)\n",
      "1894 Training Loss: tensor(0.0762)\n",
      "1895 Training Loss: tensor(0.0763)\n",
      "1896 Training Loss: tensor(0.0764)\n",
      "1897 Training Loss: tensor(0.0763)\n",
      "1898 Training Loss: tensor(0.0761)\n",
      "1899 Training Loss: tensor(0.0762)\n",
      "1900 Training Loss: tensor(0.0757)\n",
      "1901 Training Loss: tensor(0.0758)\n",
      "1902 Training Loss: tensor(0.0755)\n",
      "1903 Training Loss: tensor(0.0760)\n",
      "1904 Training Loss: tensor(0.0760)\n",
      "1905 Training Loss: tensor(0.0755)\n",
      "1906 Training Loss: tensor(0.0754)\n",
      "1907 Training Loss: tensor(0.0753)\n",
      "1908 Training Loss: tensor(0.0754)\n",
      "1909 Training Loss: tensor(0.0752)\n",
      "1910 Training Loss: tensor(0.0752)\n",
      "1911 Training Loss: tensor(0.0757)\n",
      "1912 Training Loss: tensor(0.0750)\n",
      "1913 Training Loss: tensor(0.0751)\n",
      "1914 Training Loss: tensor(0.0749)\n",
      "1915 Training Loss: tensor(0.0745)\n",
      "1916 Training Loss: tensor(0.0751)\n",
      "1917 Training Loss: tensor(0.0753)\n",
      "1918 Training Loss: tensor(0.0750)\n",
      "1919 Training Loss: tensor(0.0756)\n",
      "1920 Training Loss: tensor(0.0753)\n",
      "1921 Training Loss: tensor(0.0743)\n",
      "1922 Training Loss: tensor(0.0749)\n",
      "1923 Training Loss: tensor(0.0747)\n",
      "1924 Training Loss: tensor(0.0745)\n",
      "1925 Training Loss: tensor(0.0746)\n",
      "1926 Training Loss: tensor(0.0743)\n",
      "1927 Training Loss: tensor(0.0743)\n",
      "1928 Training Loss: tensor(0.0741)\n",
      "1929 Training Loss: tensor(0.0744)\n",
      "1930 Training Loss: tensor(0.0739)\n",
      "1931 Training Loss: tensor(0.0739)\n",
      "1932 Training Loss: tensor(0.0738)\n",
      "1933 Training Loss: tensor(0.0740)\n",
      "1934 Training Loss: tensor(0.0740)\n",
      "1935 Training Loss: tensor(0.0737)\n",
      "1936 Training Loss: tensor(0.0736)\n",
      "1937 Training Loss: tensor(0.0735)\n",
      "1938 Training Loss: tensor(0.0734)\n",
      "1939 Training Loss: tensor(0.0738)\n",
      "1940 Training Loss: tensor(0.0736)\n",
      "1941 Training Loss: tensor(0.0736)\n",
      "1942 Training Loss: tensor(0.0735)\n",
      "1943 Training Loss: tensor(0.0733)\n",
      "1944 Training Loss: tensor(0.0730)\n",
      "1945 Training Loss: tensor(0.0732)\n",
      "1946 Training Loss: tensor(0.0731)\n",
      "1947 Training Loss: tensor(0.0731)\n",
      "1948 Training Loss: tensor(0.0727)\n",
      "1949 Training Loss: tensor(0.0729)\n",
      "1950 Training Loss: tensor(0.0734)\n",
      "1951 Training Loss: tensor(0.0727)\n",
      "1952 Training Loss: tensor(0.0731)\n",
      "1953 Training Loss: tensor(0.0728)\n",
      "1954 Training Loss: tensor(0.0727)\n",
      "1955 Training Loss: tensor(0.0725)\n",
      "1956 Training Loss: tensor(0.0729)\n",
      "1957 Training Loss: tensor(0.0727)\n",
      "1958 Training Loss: tensor(0.0725)\n",
      "1959 Training Loss: tensor(0.0727)\n",
      "1960 Training Loss: tensor(0.0722)\n",
      "1961 Training Loss: tensor(0.0723)\n",
      "1962 Training Loss: tensor(0.0724)\n",
      "1963 Training Loss: tensor(0.0724)\n",
      "1964 Training Loss: tensor(0.0723)\n",
      "1965 Training Loss: tensor(0.0721)\n",
      "1966 Training Loss: tensor(0.0722)\n",
      "1967 Training Loss: tensor(0.0722)\n",
      "1968 Training Loss: tensor(0.0723)\n",
      "1969 Training Loss: tensor(0.0723)\n",
      "1970 Training Loss: tensor(0.0718)\n",
      "1971 Training Loss: tensor(0.0719)\n",
      "1972 Training Loss: tensor(0.0720)\n",
      "1973 Training Loss: tensor(0.0723)\n",
      "1974 Training Loss: tensor(0.0721)\n",
      "1975 Training Loss: tensor(0.0717)\n",
      "1976 Training Loss: tensor(0.0717)\n",
      "1977 Training Loss: tensor(0.0720)\n",
      "1978 Training Loss: tensor(0.0717)\n",
      "1979 Training Loss: tensor(0.0714)\n",
      "1980 Training Loss: tensor(0.0716)\n",
      "1981 Training Loss: tensor(0.0716)\n",
      "1982 Training Loss: tensor(0.0715)\n",
      "1983 Training Loss: tensor(0.0714)\n",
      "1984 Training Loss: tensor(0.0713)\n",
      "1985 Training Loss: tensor(0.0714)\n",
      "1986 Training Loss: tensor(0.0713)\n",
      "1987 Training Loss: tensor(0.0712)\n",
      "1988 Training Loss: tensor(0.0713)\n",
      "1989 Training Loss: tensor(0.0712)\n",
      "1990 Training Loss: tensor(0.0712)\n",
      "1991 Training Loss: tensor(0.0710)\n",
      "1992 Training Loss: tensor(0.0711)\n",
      "1993 Training Loss: tensor(0.0711)\n",
      "1994 Training Loss: tensor(0.0711)\n",
      "1995 Training Loss: tensor(0.0712)\n",
      "1996 Training Loss: tensor(0.0707)\n",
      "1997 Training Loss: tensor(0.0710)\n",
      "1998 Training Loss: tensor(0.0707)\n",
      "1999 Training Loss: tensor(0.0707)\n",
      "2000 Training Loss: tensor(0.0709)\n",
      "2001 Training Loss: tensor(0.0708)\n",
      "2002 Training Loss: tensor(0.0707)\n",
      "2003 Training Loss: tensor(0.0707)\n",
      "2004 Training Loss: tensor(0.0705)\n",
      "2005 Training Loss: tensor(0.0705)\n",
      "2006 Training Loss: tensor(0.0704)\n",
      "2007 Training Loss: tensor(0.0705)\n",
      "2008 Training Loss: tensor(0.0705)\n",
      "2009 Training Loss: tensor(0.0706)\n",
      "2010 Training Loss: tensor(0.0705)\n",
      "2011 Training Loss: tensor(0.0704)\n",
      "2012 Training Loss: tensor(0.0701)\n",
      "2013 Training Loss: tensor(0.0705)\n",
      "2014 Training Loss: tensor(0.0702)\n",
      "2015 Training Loss: tensor(0.0704)\n",
      "2016 Training Loss: tensor(0.0702)\n",
      "2017 Training Loss: tensor(0.0701)\n",
      "2018 Training Loss: tensor(0.0700)\n",
      "2019 Training Loss: tensor(0.0701)\n",
      "2020 Training Loss: tensor(0.0702)\n",
      "2021 Training Loss: tensor(0.0701)\n",
      "2022 Training Loss: tensor(0.0702)\n",
      "2023 Training Loss: tensor(0.0702)\n",
      "2024 Training Loss: tensor(0.0699)\n",
      "2025 Training Loss: tensor(0.0696)\n",
      "2026 Training Loss: tensor(0.0699)\n",
      "2027 Training Loss: tensor(0.0698)\n",
      "2028 Training Loss: tensor(0.0700)\n",
      "2029 Training Loss: tensor(0.0699)\n",
      "2030 Training Loss: tensor(0.0702)\n",
      "2031 Training Loss: tensor(0.0699)\n",
      "2032 Training Loss: tensor(0.0696)\n",
      "2033 Training Loss: tensor(0.0698)\n",
      "2034 Training Loss: tensor(0.0698)\n",
      "2035 Training Loss: tensor(0.0698)\n",
      "2036 Training Loss: tensor(0.0696)\n",
      "2037 Training Loss: tensor(0.0695)\n",
      "2038 Training Loss: tensor(0.0693)\n",
      "2039 Training Loss: tensor(0.0694)\n",
      "2040 Training Loss: tensor(0.0693)\n",
      "2041 Training Loss: tensor(0.0693)\n",
      "2042 Training Loss: tensor(0.0695)\n",
      "2043 Training Loss: tensor(0.0697)\n",
      "2044 Training Loss: tensor(0.0692)\n",
      "2045 Training Loss: tensor(0.0694)\n",
      "2046 Training Loss: tensor(0.0695)\n",
      "2047 Training Loss: tensor(0.0693)\n",
      "2048 Training Loss: tensor(0.0695)\n",
      "2049 Training Loss: tensor(0.0693)\n",
      "2050 Training Loss: tensor(0.0694)\n",
      "2051 Training Loss: tensor(0.0692)\n",
      "2052 Training Loss: tensor(0.0691)\n",
      "2053 Training Loss: tensor(0.0691)\n",
      "2054 Training Loss: tensor(0.0690)\n",
      "2055 Training Loss: tensor(0.0689)\n",
      "2056 Training Loss: tensor(0.0688)\n",
      "2057 Training Loss: tensor(0.0691)\n",
      "2058 Training Loss: tensor(0.0691)\n",
      "2059 Training Loss: tensor(0.0687)\n",
      "2060 Training Loss: tensor(0.0688)\n",
      "2061 Training Loss: tensor(0.0690)\n",
      "2062 Training Loss: tensor(0.0690)\n",
      "2063 Training Loss: tensor(0.0689)\n",
      "2064 Training Loss: tensor(0.0688)\n",
      "2065 Training Loss: tensor(0.0692)\n",
      "2066 Training Loss: tensor(0.0686)\n",
      "2067 Training Loss: tensor(0.0688)\n",
      "2068 Training Loss: tensor(0.0687)\n",
      "2069 Training Loss: tensor(0.0686)\n",
      "2070 Training Loss: tensor(0.0686)\n",
      "2071 Training Loss: tensor(0.0682)\n",
      "2072 Training Loss: tensor(0.0685)\n",
      "2073 Training Loss: tensor(0.0684)\n",
      "2074 Training Loss: tensor(0.0685)\n",
      "2075 Training Loss: tensor(0.0687)\n",
      "2076 Training Loss: tensor(0.0684)\n",
      "2077 Training Loss: tensor(0.0684)\n",
      "2078 Training Loss: tensor(0.0684)\n",
      "2079 Training Loss: tensor(0.0684)\n",
      "2080 Training Loss: tensor(0.0685)\n",
      "2081 Training Loss: tensor(0.0683)\n",
      "2082 Training Loss: tensor(0.0682)\n",
      "2083 Training Loss: tensor(0.0684)\n",
      "2084 Training Loss: tensor(0.0683)\n",
      "2085 Training Loss: tensor(0.0679)\n",
      "2086 Training Loss: tensor(0.0683)\n",
      "2087 Training Loss: tensor(0.0681)\n",
      "2088 Training Loss: tensor(0.0682)\n",
      "2089 Training Loss: tensor(0.0681)\n",
      "2090 Training Loss: tensor(0.0683)\n",
      "2091 Training Loss: tensor(0.0679)\n",
      "2092 Training Loss: tensor(0.0679)\n",
      "2093 Training Loss: tensor(0.0681)\n",
      "2094 Training Loss: tensor(0.0682)\n",
      "2095 Training Loss: tensor(0.0684)\n",
      "2096 Training Loss: tensor(0.0678)\n",
      "2097 Training Loss: tensor(0.0681)\n",
      "2098 Training Loss: tensor(0.0681)\n",
      "2099 Training Loss: tensor(0.0681)\n",
      "2100 Training Loss: tensor(0.0678)\n",
      "2101 Training Loss: tensor(0.0678)\n",
      "2102 Training Loss: tensor(0.0676)\n",
      "2103 Training Loss: tensor(0.0679)\n",
      "2104 Training Loss: tensor(0.0675)\n",
      "2105 Training Loss: tensor(0.0674)\n",
      "2106 Training Loss: tensor(0.0678)\n",
      "2107 Training Loss: tensor(0.0679)\n",
      "2108 Training Loss: tensor(0.0676)\n",
      "2109 Training Loss: tensor(0.0676)\n",
      "2110 Training Loss: tensor(0.0677)\n",
      "2111 Training Loss: tensor(0.0680)\n",
      "2112 Training Loss: tensor(0.0676)\n",
      "2113 Training Loss: tensor(0.0677)\n",
      "2114 Training Loss: tensor(0.0674)\n",
      "2115 Training Loss: tensor(0.0675)\n",
      "2116 Training Loss: tensor(0.0674)\n",
      "2117 Training Loss: tensor(0.0674)\n",
      "2118 Training Loss: tensor(0.0673)\n",
      "2119 Training Loss: tensor(0.0676)\n",
      "2120 Training Loss: tensor(0.0673)\n",
      "2121 Training Loss: tensor(0.0673)\n",
      "2122 Training Loss: tensor(0.0676)\n",
      "2123 Training Loss: tensor(0.0672)\n",
      "2124 Training Loss: tensor(0.0673)\n",
      "2125 Training Loss: tensor(0.0671)\n",
      "2126 Training Loss: tensor(0.0675)\n",
      "2127 Training Loss: tensor(0.0674)\n",
      "2128 Training Loss: tensor(0.0674)\n",
      "2129 Training Loss: tensor(0.0673)\n",
      "2130 Training Loss: tensor(0.0670)\n",
      "2131 Training Loss: tensor(0.0670)\n",
      "2132 Training Loss: tensor(0.0672)\n",
      "2133 Training Loss: tensor(0.0670)\n",
      "2134 Training Loss: tensor(0.0672)\n",
      "2135 Training Loss: tensor(0.0674)\n",
      "2136 Training Loss: tensor(0.0669)\n",
      "2137 Training Loss: tensor(0.0670)\n",
      "2138 Training Loss: tensor(0.0670)\n",
      "2139 Training Loss: tensor(0.0669)\n",
      "2140 Training Loss: tensor(0.0671)\n",
      "2141 Training Loss: tensor(0.0669)\n",
      "2142 Training Loss: tensor(0.0669)\n",
      "2143 Training Loss: tensor(0.0671)\n",
      "2144 Training Loss: tensor(0.0669)\n",
      "2145 Training Loss: tensor(0.0670)\n",
      "2146 Training Loss: tensor(0.0669)\n",
      "2147 Training Loss: tensor(0.0667)\n",
      "2148 Training Loss: tensor(0.0668)\n",
      "2149 Training Loss: tensor(0.0668)\n",
      "2150 Training Loss: tensor(0.0668)\n",
      "2151 Training Loss: tensor(0.0669)\n",
      "2152 Training Loss: tensor(0.0668)\n",
      "2153 Training Loss: tensor(0.0667)\n",
      "2154 Training Loss: tensor(0.0667)\n",
      "2155 Training Loss: tensor(0.0668)\n",
      "2156 Training Loss: tensor(0.0667)\n",
      "2157 Training Loss: tensor(0.0665)\n",
      "2158 Training Loss: tensor(0.0665)\n",
      "2159 Training Loss: tensor(0.0665)\n",
      "2160 Training Loss: tensor(0.0666)\n",
      "2161 Training Loss: tensor(0.0665)\n",
      "2162 Training Loss: tensor(0.0666)\n",
      "2163 Training Loss: tensor(0.0666)\n",
      "2164 Training Loss: tensor(0.0666)\n",
      "2165 Training Loss: tensor(0.0667)\n",
      "2166 Training Loss: tensor(0.0663)\n",
      "2167 Training Loss: tensor(0.0664)\n",
      "2168 Training Loss: tensor(0.0664)\n",
      "2169 Training Loss: tensor(0.0665)\n",
      "2170 Training Loss: tensor(0.0665)\n",
      "2171 Training Loss: tensor(0.0664)\n",
      "2172 Training Loss: tensor(0.0663)\n",
      "2173 Training Loss: tensor(0.0665)\n",
      "2174 Training Loss: tensor(0.0660)\n",
      "2175 Training Loss: tensor(0.0663)\n",
      "2176 Training Loss: tensor(0.0664)\n",
      "2177 Training Loss: tensor(0.0662)\n",
      "2178 Training Loss: tensor(0.0661)\n",
      "2179 Training Loss: tensor(0.0659)\n",
      "2180 Training Loss: tensor(0.0662)\n",
      "2181 Training Loss: tensor(0.0663)\n",
      "2182 Training Loss: tensor(0.0663)\n",
      "2183 Training Loss: tensor(0.0661)\n",
      "2184 Training Loss: tensor(0.0660)\n",
      "2185 Training Loss: tensor(0.0660)\n",
      "2186 Training Loss: tensor(0.0658)\n",
      "2187 Training Loss: tensor(0.0662)\n",
      "2188 Training Loss: tensor(0.0659)\n",
      "2189 Training Loss: tensor(0.0659)\n",
      "2190 Training Loss: tensor(0.0657)\n",
      "2191 Training Loss: tensor(0.0659)\n",
      "2192 Training Loss: tensor(0.0659)\n",
      "2193 Training Loss: tensor(0.0657)\n",
      "2194 Training Loss: tensor(0.0660)\n",
      "2195 Training Loss: tensor(0.0658)\n",
      "2196 Training Loss: tensor(0.0658)\n",
      "2197 Training Loss: tensor(0.0658)\n",
      "2198 Training Loss: tensor(0.0658)\n",
      "2199 Training Loss: tensor(0.0656)\n",
      "2200 Training Loss: tensor(0.0655)\n",
      "2201 Training Loss: tensor(0.0656)\n",
      "2202 Training Loss: tensor(0.0659)\n",
      "2203 Training Loss: tensor(0.0656)\n",
      "2204 Training Loss: tensor(0.0658)\n",
      "2205 Training Loss: tensor(0.0657)\n",
      "2206 Training Loss: tensor(0.0656)\n",
      "2207 Training Loss: tensor(0.0657)\n",
      "2208 Training Loss: tensor(0.0658)\n",
      "2209 Training Loss: tensor(0.0657)\n",
      "2210 Training Loss: tensor(0.0654)\n",
      "2211 Training Loss: tensor(0.0655)\n",
      "2212 Training Loss: tensor(0.0653)\n",
      "2213 Training Loss: tensor(0.0657)\n",
      "2214 Training Loss: tensor(0.0653)\n",
      "2215 Training Loss: tensor(0.0657)\n",
      "2216 Training Loss: tensor(0.0655)\n",
      "2217 Training Loss: tensor(0.0654)\n",
      "2218 Training Loss: tensor(0.0655)\n",
      "2219 Training Loss: tensor(0.0654)\n",
      "2220 Training Loss: tensor(0.0653)\n",
      "2221 Training Loss: tensor(0.0656)\n",
      "2222 Training Loss: tensor(0.0654)\n",
      "2223 Training Loss: tensor(0.0654)\n",
      "2224 Training Loss: tensor(0.0654)\n",
      "2225 Training Loss: tensor(0.0654)\n",
      "2226 Training Loss: tensor(0.0655)\n",
      "2227 Training Loss: tensor(0.0654)\n",
      "2228 Training Loss: tensor(0.0651)\n",
      "2229 Training Loss: tensor(0.0654)\n",
      "2230 Training Loss: tensor(0.0653)\n",
      "2231 Training Loss: tensor(0.0653)\n",
      "2232 Training Loss: tensor(0.0653)\n",
      "2233 Training Loss: tensor(0.0651)\n",
      "2234 Training Loss: tensor(0.0651)\n",
      "2235 Training Loss: tensor(0.0652)\n",
      "2236 Training Loss: tensor(0.0652)\n",
      "2237 Training Loss: tensor(0.0651)\n",
      "2238 Training Loss: tensor(0.0651)\n",
      "2239 Training Loss: tensor(0.0652)\n",
      "2240 Training Loss: tensor(0.0648)\n",
      "2241 Training Loss: tensor(0.0650)\n",
      "2242 Training Loss: tensor(0.0651)\n",
      "2243 Training Loss: tensor(0.0650)\n",
      "2244 Training Loss: tensor(0.0651)\n",
      "2245 Training Loss: tensor(0.0648)\n",
      "2246 Training Loss: tensor(0.0651)\n",
      "2247 Training Loss: tensor(0.0649)\n",
      "2248 Training Loss: tensor(0.0650)\n",
      "2249 Training Loss: tensor(0.0648)\n",
      "2250 Training Loss: tensor(0.0649)\n",
      "2251 Training Loss: tensor(0.0648)\n",
      "2252 Training Loss: tensor(0.0648)\n",
      "2253 Training Loss: tensor(0.0646)\n",
      "2254 Training Loss: tensor(0.0649)\n",
      "2255 Training Loss: tensor(0.0647)\n",
      "2256 Training Loss: tensor(0.0648)\n",
      "2257 Training Loss: tensor(0.0647)\n",
      "2258 Training Loss: tensor(0.0647)\n",
      "2259 Training Loss: tensor(0.0647)\n",
      "2260 Training Loss: tensor(0.0646)\n",
      "2261 Training Loss: tensor(0.0647)\n",
      "2262 Training Loss: tensor(0.0645)\n",
      "2263 Training Loss: tensor(0.0645)\n",
      "2264 Training Loss: tensor(0.0646)\n",
      "2265 Training Loss: tensor(0.0646)\n",
      "2266 Training Loss: tensor(0.0644)\n",
      "2267 Training Loss: tensor(0.0646)\n",
      "2268 Training Loss: tensor(0.0643)\n",
      "2269 Training Loss: tensor(0.0645)\n",
      "2270 Training Loss: tensor(0.0648)\n",
      "2271 Training Loss: tensor(0.0645)\n",
      "2272 Training Loss: tensor(0.0646)\n",
      "2273 Training Loss: tensor(0.0644)\n",
      "2274 Training Loss: tensor(0.0643)\n",
      "2275 Training Loss: tensor(0.0646)\n",
      "2276 Training Loss: tensor(0.0644)\n",
      "2277 Training Loss: tensor(0.0642)\n",
      "2278 Training Loss: tensor(0.0644)\n",
      "2279 Training Loss: tensor(0.0644)\n",
      "2280 Training Loss: tensor(0.0642)\n",
      "2281 Training Loss: tensor(0.0643)\n",
      "2282 Training Loss: tensor(0.0643)\n",
      "2283 Training Loss: tensor(0.0643)\n",
      "2284 Training Loss: tensor(0.0640)\n",
      "2285 Training Loss: tensor(0.0643)\n",
      "2286 Training Loss: tensor(0.0640)\n",
      "2287 Training Loss: tensor(0.0639)\n",
      "2288 Training Loss: tensor(0.0640)\n",
      "2289 Training Loss: tensor(0.0640)\n",
      "2290 Training Loss: tensor(0.0641)\n",
      "2291 Training Loss: tensor(0.0641)\n",
      "2292 Training Loss: tensor(0.0641)\n",
      "2293 Training Loss: tensor(0.0641)\n",
      "2294 Training Loss: tensor(0.0641)\n",
      "2295 Training Loss: tensor(0.0639)\n",
      "2296 Training Loss: tensor(0.0643)\n",
      "2297 Training Loss: tensor(0.0642)\n",
      "2298 Training Loss: tensor(0.0641)\n",
      "2299 Training Loss: tensor(0.0640)\n",
      "2300 Training Loss: tensor(0.0640)\n",
      "2301 Training Loss: tensor(0.0638)\n",
      "2302 Training Loss: tensor(0.0640)\n",
      "2303 Training Loss: tensor(0.0640)\n",
      "2304 Training Loss: tensor(0.0639)\n",
      "2305 Training Loss: tensor(0.0639)\n",
      "2306 Training Loss: tensor(0.0640)\n",
      "2307 Training Loss: tensor(0.0639)\n",
      "2308 Training Loss: tensor(0.0639)\n",
      "2309 Training Loss: tensor(0.0640)\n",
      "2310 Training Loss: tensor(0.0641)\n",
      "2311 Training Loss: tensor(0.0638)\n",
      "2312 Training Loss: tensor(0.0639)\n",
      "2313 Training Loss: tensor(0.0638)\n",
      "2314 Training Loss: tensor(0.0637)\n",
      "2315 Training Loss: tensor(0.0638)\n",
      "2316 Training Loss: tensor(0.0636)\n",
      "2317 Training Loss: tensor(0.0636)\n",
      "2318 Training Loss: tensor(0.0640)\n",
      "2319 Training Loss: tensor(0.0637)\n",
      "2320 Training Loss: tensor(0.0637)\n",
      "2321 Training Loss: tensor(0.0636)\n",
      "2322 Training Loss: tensor(0.0639)\n",
      "2323 Training Loss: tensor(0.0636)\n",
      "2324 Training Loss: tensor(0.0636)\n",
      "2325 Training Loss: tensor(0.0636)\n",
      "2326 Training Loss: tensor(0.0633)\n",
      "2327 Training Loss: tensor(0.0638)\n",
      "2328 Training Loss: tensor(0.0636)\n",
      "2329 Training Loss: tensor(0.0637)\n",
      "2330 Training Loss: tensor(0.0636)\n",
      "2331 Training Loss: tensor(0.0635)\n",
      "2332 Training Loss: tensor(0.0633)\n",
      "2333 Training Loss: tensor(0.0635)\n",
      "2334 Training Loss: tensor(0.0635)\n",
      "2335 Training Loss: tensor(0.0633)\n",
      "2336 Training Loss: tensor(0.0633)\n",
      "2337 Training Loss: tensor(0.0635)\n",
      "2338 Training Loss: tensor(0.0636)\n",
      "2339 Training Loss: tensor(0.0634)\n",
      "2340 Training Loss: tensor(0.0634)\n",
      "2341 Training Loss: tensor(0.0632)\n",
      "2342 Training Loss: tensor(0.0635)\n",
      "2343 Training Loss: tensor(0.0635)\n",
      "2344 Training Loss: tensor(0.0632)\n",
      "2345 Training Loss: tensor(0.0634)\n",
      "2346 Training Loss: tensor(0.0631)\n",
      "2347 Training Loss: tensor(0.0633)\n",
      "2348 Training Loss: tensor(0.0631)\n",
      "2349 Training Loss: tensor(0.0634)\n",
      "2350 Training Loss: tensor(0.0632)\n",
      "2351 Training Loss: tensor(0.0633)\n",
      "2352 Training Loss: tensor(0.0633)\n",
      "2353 Training Loss: tensor(0.0632)\n",
      "2354 Training Loss: tensor(0.0631)\n",
      "2355 Training Loss: tensor(0.0632)\n",
      "2356 Training Loss: tensor(0.0633)\n",
      "2357 Training Loss: tensor(0.0630)\n",
      "2358 Training Loss: tensor(0.0631)\n",
      "2359 Training Loss: tensor(0.0632)\n",
      "2360 Training Loss: tensor(0.0629)\n",
      "2361 Training Loss: tensor(0.0632)\n",
      "2362 Training Loss: tensor(0.0631)\n",
      "2363 Training Loss: tensor(0.0633)\n",
      "2364 Training Loss: tensor(0.0631)\n",
      "2365 Training Loss: tensor(0.0631)\n",
      "2366 Training Loss: tensor(0.0629)\n",
      "2367 Training Loss: tensor(0.0631)\n",
      "2368 Training Loss: tensor(0.0627)\n",
      "2369 Training Loss: tensor(0.0630)\n",
      "2370 Training Loss: tensor(0.0629)\n",
      "2371 Training Loss: tensor(0.0631)\n",
      "2372 Training Loss: tensor(0.0628)\n",
      "2373 Training Loss: tensor(0.0627)\n",
      "2374 Training Loss: tensor(0.0629)\n",
      "2375 Training Loss: tensor(0.0627)\n",
      "2376 Training Loss: tensor(0.0627)\n",
      "2377 Training Loss: tensor(0.0628)\n",
      "2378 Training Loss: tensor(0.0627)\n",
      "2379 Training Loss: tensor(0.0626)\n",
      "2380 Training Loss: tensor(0.0628)\n",
      "2381 Training Loss: tensor(0.0627)\n",
      "2382 Training Loss: tensor(0.0627)\n",
      "2383 Training Loss: tensor(0.0626)\n",
      "2384 Training Loss: tensor(0.0627)\n",
      "2385 Training Loss: tensor(0.0627)\n",
      "2386 Training Loss: tensor(0.0626)\n",
      "2387 Training Loss: tensor(0.0626)\n",
      "2388 Training Loss: tensor(0.0626)\n",
      "2389 Training Loss: tensor(0.0627)\n",
      "2390 Training Loss: tensor(0.0624)\n",
      "2391 Training Loss: tensor(0.0628)\n",
      "2392 Training Loss: tensor(0.0627)\n",
      "2393 Training Loss: tensor(0.0626)\n",
      "2394 Training Loss: tensor(0.0625)\n",
      "2395 Training Loss: tensor(0.0624)\n",
      "2396 Training Loss: tensor(0.0625)\n",
      "2397 Training Loss: tensor(0.0623)\n",
      "2398 Training Loss: tensor(0.0624)\n",
      "2399 Training Loss: tensor(0.0625)\n",
      "2400 Training Loss: tensor(0.0624)\n",
      "2401 Training Loss: tensor(0.0625)\n",
      "2402 Training Loss: tensor(0.0624)\n",
      "2403 Training Loss: tensor(0.0623)\n",
      "2404 Training Loss: tensor(0.0624)\n",
      "2405 Training Loss: tensor(0.0624)\n",
      "2406 Training Loss: tensor(0.0623)\n",
      "2407 Training Loss: tensor(0.0622)\n",
      "2408 Training Loss: tensor(0.0623)\n",
      "2409 Training Loss: tensor(0.0623)\n",
      "2410 Training Loss: tensor(0.0622)\n",
      "2411 Training Loss: tensor(0.0622)\n",
      "2412 Training Loss: tensor(0.0621)\n",
      "2413 Training Loss: tensor(0.0623)\n",
      "2414 Training Loss: tensor(0.0623)\n",
      "2415 Training Loss: tensor(0.0621)\n",
      "2416 Training Loss: tensor(0.0623)\n",
      "2417 Training Loss: tensor(0.0619)\n",
      "2418 Training Loss: tensor(0.0620)\n",
      "2419 Training Loss: tensor(0.0620)\n",
      "2420 Training Loss: tensor(0.0620)\n",
      "2421 Training Loss: tensor(0.0619)\n",
      "2422 Training Loss: tensor(0.0621)\n",
      "2423 Training Loss: tensor(0.0621)\n",
      "2424 Training Loss: tensor(0.0619)\n",
      "2425 Training Loss: tensor(0.0619)\n",
      "2426 Training Loss: tensor(0.0621)\n",
      "2427 Training Loss: tensor(0.0617)\n",
      "2428 Training Loss: tensor(0.0619)\n",
      "2429 Training Loss: tensor(0.0620)\n",
      "2430 Training Loss: tensor(0.0617)\n",
      "2431 Training Loss: tensor(0.0621)\n",
      "2432 Training Loss: tensor(0.0620)\n",
      "2433 Training Loss: tensor(0.0618)\n",
      "2434 Training Loss: tensor(0.0619)\n",
      "2435 Training Loss: tensor(0.0618)\n",
      "2436 Training Loss: tensor(0.0617)\n",
      "2437 Training Loss: tensor(0.0619)\n",
      "2438 Training Loss: tensor(0.0618)\n",
      "2439 Training Loss: tensor(0.0616)\n",
      "2440 Training Loss: tensor(0.0618)\n",
      "2441 Training Loss: tensor(0.0615)\n",
      "2442 Training Loss: tensor(0.0617)\n",
      "2443 Training Loss: tensor(0.0617)\n",
      "2444 Training Loss: tensor(0.0619)\n",
      "2445 Training Loss: tensor(0.0616)\n",
      "2446 Training Loss: tensor(0.0614)\n",
      "2447 Training Loss: tensor(0.0614)\n",
      "2448 Training Loss: tensor(0.0617)\n",
      "2449 Training Loss: tensor(0.0615)\n",
      "2450 Training Loss: tensor(0.0615)\n",
      "2451 Training Loss: tensor(0.0617)\n",
      "2452 Training Loss: tensor(0.0614)\n",
      "2453 Training Loss: tensor(0.0615)\n",
      "2454 Training Loss: tensor(0.0615)\n",
      "2455 Training Loss: tensor(0.0616)\n",
      "2456 Training Loss: tensor(0.0616)\n",
      "2457 Training Loss: tensor(0.0616)\n",
      "2458 Training Loss: tensor(0.0615)\n",
      "2459 Training Loss: tensor(0.0615)\n",
      "2460 Training Loss: tensor(0.0614)\n",
      "2461 Training Loss: tensor(0.0612)\n",
      "2462 Training Loss: tensor(0.0613)\n",
      "2463 Training Loss: tensor(0.0614)\n",
      "2464 Training Loss: tensor(0.0612)\n",
      "2465 Training Loss: tensor(0.0613)\n",
      "2466 Training Loss: tensor(0.0614)\n",
      "2467 Training Loss: tensor(0.0611)\n",
      "2468 Training Loss: tensor(0.0612)\n",
      "2469 Training Loss: tensor(0.0610)\n",
      "2470 Training Loss: tensor(0.0614)\n",
      "2471 Training Loss: tensor(0.0612)\n",
      "2472 Training Loss: tensor(0.0612)\n",
      "2473 Training Loss: tensor(0.0611)\n",
      "2474 Training Loss: tensor(0.0611)\n",
      "2475 Training Loss: tensor(0.0612)\n",
      "2476 Training Loss: tensor(0.0611)\n",
      "2477 Training Loss: tensor(0.0611)\n",
      "2478 Training Loss: tensor(0.0611)\n",
      "2479 Training Loss: tensor(0.0612)\n",
      "2480 Training Loss: tensor(0.0611)\n",
      "2481 Training Loss: tensor(0.0612)\n",
      "2482 Training Loss: tensor(0.0611)\n",
      "2483 Training Loss: tensor(0.0610)\n",
      "2484 Training Loss: tensor(0.0609)\n",
      "2485 Training Loss: tensor(0.0611)\n",
      "2486 Training Loss: tensor(0.0609)\n",
      "2487 Training Loss: tensor(0.0610)\n",
      "2488 Training Loss: tensor(0.0609)\n",
      "2489 Training Loss: tensor(0.0612)\n",
      "2490 Training Loss: tensor(0.0609)\n",
      "2491 Training Loss: tensor(0.0609)\n",
      "2492 Training Loss: tensor(0.0609)\n",
      "2493 Training Loss: tensor(0.0611)\n",
      "2494 Training Loss: tensor(0.0610)\n",
      "2495 Training Loss: tensor(0.0609)\n",
      "2496 Training Loss: tensor(0.0607)\n",
      "2497 Training Loss: tensor(0.0607)\n",
      "2498 Training Loss: tensor(0.0609)\n",
      "2499 Training Loss: tensor(0.0609)\n",
      "2500 Training Loss: tensor(0.0608)\n",
      "2501 Training Loss: tensor(0.0606)\n",
      "2502 Training Loss: tensor(0.0607)\n",
      "2503 Training Loss: tensor(0.0610)\n",
      "2504 Training Loss: tensor(0.0606)\n",
      "2505 Training Loss: tensor(0.0606)\n",
      "2506 Training Loss: tensor(0.0606)\n",
      "2507 Training Loss: tensor(0.0607)\n",
      "2508 Training Loss: tensor(0.0607)\n",
      "2509 Training Loss: tensor(0.0608)\n",
      "2510 Training Loss: tensor(0.0605)\n",
      "2511 Training Loss: tensor(0.0608)\n",
      "2512 Training Loss: tensor(0.0605)\n",
      "2513 Training Loss: tensor(0.0607)\n",
      "2514 Training Loss: tensor(0.0605)\n",
      "2515 Training Loss: tensor(0.0607)\n",
      "2516 Training Loss: tensor(0.0605)\n",
      "2517 Training Loss: tensor(0.0606)\n",
      "2518 Training Loss: tensor(0.0602)\n",
      "2519 Training Loss: tensor(0.0605)\n",
      "2520 Training Loss: tensor(0.0602)\n",
      "2521 Training Loss: tensor(0.0604)\n",
      "2522 Training Loss: tensor(0.0602)\n",
      "2523 Training Loss: tensor(0.0601)\n",
      "2524 Training Loss: tensor(0.0603)\n",
      "2525 Training Loss: tensor(0.0602)\n",
      "2526 Training Loss: tensor(0.0603)\n",
      "2527 Training Loss: tensor(0.0602)\n",
      "2528 Training Loss: tensor(0.0602)\n",
      "2529 Training Loss: tensor(0.0601)\n",
      "2530 Training Loss: tensor(0.0601)\n",
      "2531 Training Loss: tensor(0.0602)\n",
      "2532 Training Loss: tensor(0.0602)\n",
      "2533 Training Loss: tensor(0.0603)\n",
      "2534 Training Loss: tensor(0.0600)\n",
      "2535 Training Loss: tensor(0.0603)\n",
      "2536 Training Loss: tensor(0.0599)\n",
      "2537 Training Loss: tensor(0.0601)\n",
      "2538 Training Loss: tensor(0.0601)\n",
      "2539 Training Loss: tensor(0.0603)\n",
      "2540 Training Loss: tensor(0.0602)\n",
      "2541 Training Loss: tensor(0.0600)\n",
      "2542 Training Loss: tensor(0.0600)\n",
      "2543 Training Loss: tensor(0.0602)\n",
      "2544 Training Loss: tensor(0.0602)\n",
      "2545 Training Loss: tensor(0.0601)\n",
      "2546 Training Loss: tensor(0.0600)\n",
      "2547 Training Loss: tensor(0.0598)\n",
      "2548 Training Loss: tensor(0.0599)\n",
      "2549 Training Loss: tensor(0.0598)\n",
      "2550 Training Loss: tensor(0.0602)\n",
      "2551 Training Loss: tensor(0.0599)\n",
      "2552 Training Loss: tensor(0.0599)\n",
      "2553 Training Loss: tensor(0.0598)\n",
      "2554 Training Loss: tensor(0.0596)\n",
      "2555 Training Loss: tensor(0.0600)\n",
      "2556 Training Loss: tensor(0.0598)\n",
      "2557 Training Loss: tensor(0.0598)\n",
      "2558 Training Loss: tensor(0.0599)\n",
      "2559 Training Loss: tensor(0.0597)\n",
      "2560 Training Loss: tensor(0.0598)\n",
      "2561 Training Loss: tensor(0.0597)\n",
      "2562 Training Loss: tensor(0.0596)\n",
      "2563 Training Loss: tensor(0.0596)\n",
      "2564 Training Loss: tensor(0.0595)\n",
      "2565 Training Loss: tensor(0.0595)\n",
      "2566 Training Loss: tensor(0.0595)\n",
      "2567 Training Loss: tensor(0.0595)\n",
      "2568 Training Loss: tensor(0.0594)\n",
      "2569 Training Loss: tensor(0.0595)\n",
      "2570 Training Loss: tensor(0.0596)\n",
      "2571 Training Loss: tensor(0.0593)\n",
      "2572 Training Loss: tensor(0.0593)\n",
      "2573 Training Loss: tensor(0.0599)\n",
      "2574 Training Loss: tensor(0.0597)\n",
      "2575 Training Loss: tensor(0.0599)\n",
      "2576 Training Loss: tensor(0.0595)\n",
      "2577 Training Loss: tensor(0.0594)\n",
      "2578 Training Loss: tensor(0.0593)\n",
      "2579 Training Loss: tensor(0.0592)\n",
      "2580 Training Loss: tensor(0.0595)\n",
      "2581 Training Loss: tensor(0.0592)\n",
      "2582 Training Loss: tensor(0.0593)\n",
      "2583 Training Loss: tensor(0.0592)\n",
      "2584 Training Loss: tensor(0.0591)\n",
      "2585 Training Loss: tensor(0.0591)\n",
      "2586 Training Loss: tensor(0.0591)\n",
      "2587 Training Loss: tensor(0.0593)\n",
      "2588 Training Loss: tensor(0.0590)\n",
      "2589 Training Loss: tensor(0.0595)\n",
      "2590 Training Loss: tensor(0.0593)\n",
      "2591 Training Loss: tensor(0.0590)\n",
      "2592 Training Loss: tensor(0.0591)\n",
      "2593 Training Loss: tensor(0.0592)\n",
      "2594 Training Loss: tensor(0.0593)\n",
      "2595 Training Loss: tensor(0.0593)\n",
      "2596 Training Loss: tensor(0.0593)\n",
      "2597 Training Loss: tensor(0.0591)\n",
      "2598 Training Loss: tensor(0.0591)\n",
      "2599 Training Loss: tensor(0.0593)\n",
      "2600 Training Loss: tensor(0.0590)\n",
      "2601 Training Loss: tensor(0.0590)\n",
      "2602 Training Loss: tensor(0.0589)\n",
      "2603 Training Loss: tensor(0.0590)\n",
      "2604 Training Loss: tensor(0.0589)\n",
      "2605 Training Loss: tensor(0.0590)\n",
      "2606 Training Loss: tensor(0.0588)\n",
      "2607 Training Loss: tensor(0.0591)\n",
      "2608 Training Loss: tensor(0.0588)\n",
      "2609 Training Loss: tensor(0.0591)\n",
      "2610 Training Loss: tensor(0.0587)\n",
      "2611 Training Loss: tensor(0.0587)\n",
      "2612 Training Loss: tensor(0.0588)\n",
      "2613 Training Loss: tensor(0.0588)\n",
      "2614 Training Loss: tensor(0.0587)\n",
      "2615 Training Loss: tensor(0.0589)\n",
      "2616 Training Loss: tensor(0.0590)\n",
      "2617 Training Loss: tensor(0.0588)\n",
      "2618 Training Loss: tensor(0.0590)\n",
      "2619 Training Loss: tensor(0.0586)\n",
      "2620 Training Loss: tensor(0.0584)\n",
      "2621 Training Loss: tensor(0.0589)\n",
      "2622 Training Loss: tensor(0.0588)\n",
      "2623 Training Loss: tensor(0.0587)\n",
      "2624 Training Loss: tensor(0.0587)\n",
      "2625 Training Loss: tensor(0.0586)\n",
      "2626 Training Loss: tensor(0.0586)\n",
      "2627 Training Loss: tensor(0.0587)\n",
      "2628 Training Loss: tensor(0.0586)\n",
      "2629 Training Loss: tensor(0.0587)\n",
      "2630 Training Loss: tensor(0.0586)\n",
      "2631 Training Loss: tensor(0.0585)\n",
      "2632 Training Loss: tensor(0.0586)\n",
      "2633 Training Loss: tensor(0.0583)\n",
      "2634 Training Loss: tensor(0.0585)\n",
      "2635 Training Loss: tensor(0.0585)\n",
      "2636 Training Loss: tensor(0.0585)\n",
      "2637 Training Loss: tensor(0.0585)\n",
      "2638 Training Loss: tensor(0.0585)\n",
      "2639 Training Loss: tensor(0.0585)\n",
      "2640 Training Loss: tensor(0.0581)\n",
      "2641 Training Loss: tensor(0.0584)\n",
      "2642 Training Loss: tensor(0.0582)\n",
      "2643 Training Loss: tensor(0.0584)\n",
      "2644 Training Loss: tensor(0.0586)\n",
      "2645 Training Loss: tensor(0.0583)\n",
      "2646 Training Loss: tensor(0.0585)\n",
      "2647 Training Loss: tensor(0.0581)\n",
      "2648 Training Loss: tensor(0.0584)\n",
      "2649 Training Loss: tensor(0.0583)\n",
      "2650 Training Loss: tensor(0.0582)\n",
      "2651 Training Loss: tensor(0.0584)\n",
      "2652 Training Loss: tensor(0.0584)\n",
      "2653 Training Loss: tensor(0.0582)\n",
      "2654 Training Loss: tensor(0.0581)\n",
      "2655 Training Loss: tensor(0.0583)\n",
      "2656 Training Loss: tensor(0.0582)\n",
      "2657 Training Loss: tensor(0.0580)\n",
      "2658 Training Loss: tensor(0.0578)\n",
      "2659 Training Loss: tensor(0.0579)\n",
      "2660 Training Loss: tensor(0.0582)\n",
      "2661 Training Loss: tensor(0.0583)\n",
      "2662 Training Loss: tensor(0.0583)\n",
      "2663 Training Loss: tensor(0.0580)\n",
      "2664 Training Loss: tensor(0.0582)\n",
      "2665 Training Loss: tensor(0.0581)\n",
      "2666 Training Loss: tensor(0.0583)\n",
      "2667 Training Loss: tensor(0.0580)\n",
      "2668 Training Loss: tensor(0.0580)\n",
      "2669 Training Loss: tensor(0.0580)\n",
      "2670 Training Loss: tensor(0.0580)\n",
      "2671 Training Loss: tensor(0.0580)\n",
      "2672 Training Loss: tensor(0.0579)\n",
      "2673 Training Loss: tensor(0.0580)\n",
      "2674 Training Loss: tensor(0.0579)\n",
      "2675 Training Loss: tensor(0.0578)\n",
      "2676 Training Loss: tensor(0.0577)\n",
      "2677 Training Loss: tensor(0.0578)\n",
      "2678 Training Loss: tensor(0.0578)\n",
      "2679 Training Loss: tensor(0.0578)\n",
      "2680 Training Loss: tensor(0.0578)\n",
      "2681 Training Loss: tensor(0.0578)\n",
      "2682 Training Loss: tensor(0.0575)\n",
      "2683 Training Loss: tensor(0.0578)\n",
      "2684 Training Loss: tensor(0.0576)\n",
      "2685 Training Loss: tensor(0.0579)\n",
      "2686 Training Loss: tensor(0.0575)\n",
      "2687 Training Loss: tensor(0.0578)\n",
      "2688 Training Loss: tensor(0.0576)\n",
      "2689 Training Loss: tensor(0.0577)\n",
      "2690 Training Loss: tensor(0.0580)\n",
      "2691 Training Loss: tensor(0.0579)\n",
      "2692 Training Loss: tensor(0.0576)\n",
      "2693 Training Loss: tensor(0.0577)\n",
      "2694 Training Loss: tensor(0.0575)\n",
      "2695 Training Loss: tensor(0.0574)\n",
      "2696 Training Loss: tensor(0.0575)\n",
      "2697 Training Loss: tensor(0.0577)\n",
      "2698 Training Loss: tensor(0.0575)\n",
      "2699 Training Loss: tensor(0.0576)\n",
      "2700 Training Loss: tensor(0.0576)\n",
      "2701 Training Loss: tensor(0.0575)\n",
      "2702 Training Loss: tensor(0.0575)\n",
      "2703 Training Loss: tensor(0.0574)\n",
      "2704 Training Loss: tensor(0.0573)\n",
      "2705 Training Loss: tensor(0.0572)\n",
      "2706 Training Loss: tensor(0.0574)\n",
      "2707 Training Loss: tensor(0.0571)\n",
      "2708 Training Loss: tensor(0.0574)\n",
      "2709 Training Loss: tensor(0.0572)\n",
      "2710 Training Loss: tensor(0.0573)\n",
      "2711 Training Loss: tensor(0.0575)\n",
      "2712 Training Loss: tensor(0.0575)\n",
      "2713 Training Loss: tensor(0.0577)\n",
      "2714 Training Loss: tensor(0.0570)\n",
      "2715 Training Loss: tensor(0.0575)\n",
      "2716 Training Loss: tensor(0.0574)\n",
      "2717 Training Loss: tensor(0.0573)\n",
      "2718 Training Loss: tensor(0.0572)\n",
      "2719 Training Loss: tensor(0.0571)\n",
      "2720 Training Loss: tensor(0.0570)\n",
      "2721 Training Loss: tensor(0.0572)\n",
      "2722 Training Loss: tensor(0.0569)\n",
      "2723 Training Loss: tensor(0.0569)\n",
      "2724 Training Loss: tensor(0.0571)\n",
      "2725 Training Loss: tensor(0.0571)\n",
      "2726 Training Loss: tensor(0.0569)\n",
      "2727 Training Loss: tensor(0.0573)\n",
      "2728 Training Loss: tensor(0.0567)\n",
      "2729 Training Loss: tensor(0.0570)\n",
      "2730 Training Loss: tensor(0.0574)\n",
      "2731 Training Loss: tensor(0.0571)\n",
      "2732 Training Loss: tensor(0.0571)\n",
      "2733 Training Loss: tensor(0.0572)\n",
      "2734 Training Loss: tensor(0.0570)\n",
      "2735 Training Loss: tensor(0.0569)\n",
      "2736 Training Loss: tensor(0.0568)\n",
      "2737 Training Loss: tensor(0.0568)\n",
      "2738 Training Loss: tensor(0.0568)\n",
      "2739 Training Loss: tensor(0.0566)\n",
      "2740 Training Loss: tensor(0.0568)\n",
      "2741 Training Loss: tensor(0.0570)\n",
      "2742 Training Loss: tensor(0.0568)\n",
      "2743 Training Loss: tensor(0.0570)\n",
      "2744 Training Loss: tensor(0.0566)\n",
      "2745 Training Loss: tensor(0.0566)\n",
      "2746 Training Loss: tensor(0.0571)\n",
      "2747 Training Loss: tensor(0.0567)\n",
      "2748 Training Loss: tensor(0.0568)\n",
      "2749 Training Loss: tensor(0.0569)\n",
      "2750 Training Loss: tensor(0.0568)\n",
      "2751 Training Loss: tensor(0.0565)\n",
      "2752 Training Loss: tensor(0.0571)\n",
      "2753 Training Loss: tensor(0.0567)\n",
      "2754 Training Loss: tensor(0.0566)\n",
      "2755 Training Loss: tensor(0.0563)\n",
      "2756 Training Loss: tensor(0.0566)\n",
      "2757 Training Loss: tensor(0.0565)\n",
      "2758 Training Loss: tensor(0.0566)\n",
      "2759 Training Loss: tensor(0.0565)\n",
      "2760 Training Loss: tensor(0.0563)\n",
      "2761 Training Loss: tensor(0.0563)\n",
      "2762 Training Loss: tensor(0.0567)\n",
      "2763 Training Loss: tensor(0.0566)\n",
      "2764 Training Loss: tensor(0.0564)\n",
      "2765 Training Loss: tensor(0.0565)\n",
      "2766 Training Loss: tensor(0.0568)\n",
      "2767 Training Loss: tensor(0.0565)\n",
      "2768 Training Loss: tensor(0.0562)\n",
      "2769 Training Loss: tensor(0.0564)\n",
      "2770 Training Loss: tensor(0.0564)\n",
      "2771 Training Loss: tensor(0.0563)\n",
      "2772 Training Loss: tensor(0.0567)\n",
      "2773 Training Loss: tensor(0.0565)\n",
      "2774 Training Loss: tensor(0.0564)\n",
      "2775 Training Loss: tensor(0.0565)\n",
      "2776 Training Loss: tensor(0.0564)\n",
      "2777 Training Loss: tensor(0.0564)\n",
      "2778 Training Loss: tensor(0.0565)\n",
      "2779 Training Loss: tensor(0.0563)\n",
      "2780 Training Loss: tensor(0.0562)\n",
      "2781 Training Loss: tensor(0.0564)\n",
      "2782 Training Loss: tensor(0.0561)\n",
      "2783 Training Loss: tensor(0.0560)\n",
      "2784 Training Loss: tensor(0.0564)\n",
      "2785 Training Loss: tensor(0.0561)\n",
      "2786 Training Loss: tensor(0.0562)\n",
      "2787 Training Loss: tensor(0.0562)\n",
      "2788 Training Loss: tensor(0.0566)\n",
      "2789 Training Loss: tensor(0.0561)\n",
      "2790 Training Loss: tensor(0.0563)\n",
      "2791 Training Loss: tensor(0.0563)\n",
      "2792 Training Loss: tensor(0.0561)\n",
      "2793 Training Loss: tensor(0.0561)\n",
      "2794 Training Loss: tensor(0.0562)\n",
      "2795 Training Loss: tensor(0.0560)\n",
      "2796 Training Loss: tensor(0.0562)\n",
      "2797 Training Loss: tensor(0.0562)\n",
      "2798 Training Loss: tensor(0.0559)\n",
      "2799 Training Loss: tensor(0.0560)\n",
      "2800 Training Loss: tensor(0.0559)\n",
      "2801 Training Loss: tensor(0.0560)\n",
      "2802 Training Loss: tensor(0.0562)\n",
      "2803 Training Loss: tensor(0.0556)\n",
      "2804 Training Loss: tensor(0.0560)\n",
      "2805 Training Loss: tensor(0.0558)\n",
      "2806 Training Loss: tensor(0.0561)\n",
      "2807 Training Loss: tensor(0.0562)\n",
      "2808 Training Loss: tensor(0.0561)\n",
      "2809 Training Loss: tensor(0.0562)\n",
      "2810 Training Loss: tensor(0.0558)\n",
      "2811 Training Loss: tensor(0.0557)\n",
      "2812 Training Loss: tensor(0.0559)\n",
      "2813 Training Loss: tensor(0.0560)\n",
      "2814 Training Loss: tensor(0.0559)\n",
      "2815 Training Loss: tensor(0.0559)\n",
      "2816 Training Loss: tensor(0.0559)\n",
      "2817 Training Loss: tensor(0.0558)\n",
      "2818 Training Loss: tensor(0.0560)\n",
      "2819 Training Loss: tensor(0.0556)\n",
      "2820 Training Loss: tensor(0.0559)\n",
      "2821 Training Loss: tensor(0.0556)\n",
      "2822 Training Loss: tensor(0.0559)\n",
      "2823 Training Loss: tensor(0.0559)\n",
      "2824 Training Loss: tensor(0.0559)\n",
      "2825 Training Loss: tensor(0.0557)\n",
      "2826 Training Loss: tensor(0.0555)\n",
      "2827 Training Loss: tensor(0.0563)\n",
      "2828 Training Loss: tensor(0.0557)\n",
      "2829 Training Loss: tensor(0.0556)\n",
      "2830 Training Loss: tensor(0.0558)\n",
      "2831 Training Loss: tensor(0.0558)\n",
      "2832 Training Loss: tensor(0.0556)\n",
      "2833 Training Loss: tensor(0.0555)\n",
      "2834 Training Loss: tensor(0.0556)\n",
      "2835 Training Loss: tensor(0.0554)\n",
      "2836 Training Loss: tensor(0.0555)\n",
      "2837 Training Loss: tensor(0.0555)\n",
      "2838 Training Loss: tensor(0.0558)\n",
      "2839 Training Loss: tensor(0.0559)\n",
      "2840 Training Loss: tensor(0.0556)\n",
      "2841 Training Loss: tensor(0.0554)\n",
      "2842 Training Loss: tensor(0.0555)\n",
      "2843 Training Loss: tensor(0.0557)\n",
      "2844 Training Loss: tensor(0.0556)\n",
      "2845 Training Loss: tensor(0.0557)\n",
      "2846 Training Loss: tensor(0.0554)\n",
      "2847 Training Loss: tensor(0.0556)\n",
      "2848 Training Loss: tensor(0.0558)\n",
      "2849 Training Loss: tensor(0.0556)\n",
      "2850 Training Loss: tensor(0.0554)\n",
      "2851 Training Loss: tensor(0.0554)\n",
      "2852 Training Loss: tensor(0.0553)\n",
      "2853 Training Loss: tensor(0.0555)\n",
      "2854 Training Loss: tensor(0.0556)\n",
      "2855 Training Loss: tensor(0.0554)\n",
      "2856 Training Loss: tensor(0.0553)\n",
      "2857 Training Loss: tensor(0.0553)\n",
      "2858 Training Loss: tensor(0.0555)\n",
      "2859 Training Loss: tensor(0.0552)\n",
      "2860 Training Loss: tensor(0.0555)\n",
      "2861 Training Loss: tensor(0.0553)\n",
      "2862 Training Loss: tensor(0.0556)\n",
      "2863 Training Loss: tensor(0.0552)\n",
      "2864 Training Loss: tensor(0.0557)\n",
      "2865 Training Loss: tensor(0.0556)\n",
      "2866 Training Loss: tensor(0.0553)\n",
      "2867 Training Loss: tensor(0.0552)\n",
      "2868 Training Loss: tensor(0.0551)\n",
      "2869 Training Loss: tensor(0.0553)\n",
      "2870 Training Loss: tensor(0.0555)\n",
      "2871 Training Loss: tensor(0.0553)\n",
      "2872 Training Loss: tensor(0.0550)\n",
      "2873 Training Loss: tensor(0.0551)\n",
      "2874 Training Loss: tensor(0.0552)\n",
      "2875 Training Loss: tensor(0.0551)\n",
      "2876 Training Loss: tensor(0.0556)\n",
      "2877 Training Loss: tensor(0.0548)\n",
      "2878 Training Loss: tensor(0.0552)\n",
      "2879 Training Loss: tensor(0.0552)\n",
      "2880 Training Loss: tensor(0.0550)\n",
      "2881 Training Loss: tensor(0.0557)\n",
      "2882 Training Loss: tensor(0.0550)\n",
      "2883 Training Loss: tensor(0.0551)\n",
      "2884 Training Loss: tensor(0.0555)\n",
      "2885 Training Loss: tensor(0.0551)\n",
      "2886 Training Loss: tensor(0.0550)\n",
      "2887 Training Loss: tensor(0.0553)\n",
      "2888 Training Loss: tensor(0.0548)\n",
      "2889 Training Loss: tensor(0.0550)\n",
      "2890 Training Loss: tensor(0.0553)\n",
      "2891 Training Loss: tensor(0.0548)\n",
      "2892 Training Loss: tensor(0.0546)\n",
      "2893 Training Loss: tensor(0.0549)\n",
      "2894 Training Loss: tensor(0.0549)\n",
      "2895 Training Loss: tensor(0.0549)\n",
      "2896 Training Loss: tensor(0.0548)\n",
      "2897 Training Loss: tensor(0.0549)\n",
      "2898 Training Loss: tensor(0.0550)\n",
      "2899 Training Loss: tensor(0.0548)\n",
      "2900 Training Loss: tensor(0.0551)\n",
      "2901 Training Loss: tensor(0.0556)\n",
      "2902 Training Loss: tensor(0.0549)\n",
      "2903 Training Loss: tensor(0.0549)\n",
      "2904 Training Loss: tensor(0.0548)\n",
      "2905 Training Loss: tensor(0.0546)\n",
      "2906 Training Loss: tensor(0.0549)\n",
      "2907 Training Loss: tensor(0.0548)\n",
      "2908 Training Loss: tensor(0.0546)\n",
      "2909 Training Loss: tensor(0.0543)\n",
      "2910 Training Loss: tensor(0.0547)\n",
      "2911 Training Loss: tensor(0.0548)\n",
      "2912 Training Loss: tensor(0.0545)\n",
      "2913 Training Loss: tensor(0.0550)\n",
      "2914 Training Loss: tensor(0.0547)\n",
      "2915 Training Loss: tensor(0.0549)\n",
      "2916 Training Loss: tensor(0.0548)\n",
      "2917 Training Loss: tensor(0.0553)\n",
      "2918 Training Loss: tensor(0.0545)\n",
      "2919 Training Loss: tensor(0.0545)\n",
      "2920 Training Loss: tensor(0.0547)\n",
      "2921 Training Loss: tensor(0.0549)\n",
      "2922 Training Loss: tensor(0.0544)\n",
      "2923 Training Loss: tensor(0.0548)\n",
      "2924 Training Loss: tensor(0.0548)\n",
      "2925 Training Loss: tensor(0.0545)\n",
      "2926 Training Loss: tensor(0.0547)\n",
      "2927 Training Loss: tensor(0.0545)\n",
      "2928 Training Loss: tensor(0.0548)\n",
      "2929 Training Loss: tensor(0.0544)\n",
      "2930 Training Loss: tensor(0.0546)\n",
      "2931 Training Loss: tensor(0.0543)\n",
      "2932 Training Loss: tensor(0.0545)\n",
      "2933 Training Loss: tensor(0.0545)\n",
      "2934 Training Loss: tensor(0.0545)\n",
      "2935 Training Loss: tensor(0.0548)\n",
      "2936 Training Loss: tensor(0.0545)\n",
      "2937 Training Loss: tensor(0.0546)\n",
      "2938 Training Loss: tensor(0.0543)\n",
      "2939 Training Loss: tensor(0.0545)\n",
      "2940 Training Loss: tensor(0.0540)\n",
      "2941 Training Loss: tensor(0.0550)\n",
      "2942 Training Loss: tensor(0.0548)\n",
      "2943 Training Loss: tensor(0.0547)\n",
      "2944 Training Loss: tensor(0.0548)\n",
      "2945 Training Loss: tensor(0.0542)\n",
      "2946 Training Loss: tensor(0.0542)\n",
      "2947 Training Loss: tensor(0.0544)\n",
      "2948 Training Loss: tensor(0.0545)\n",
      "2949 Training Loss: tensor(0.0547)\n",
      "2950 Training Loss: tensor(0.0544)\n",
      "2951 Training Loss: tensor(0.0545)\n",
      "2952 Training Loss: tensor(0.0545)\n",
      "2953 Training Loss: tensor(0.0548)\n",
      "2954 Training Loss: tensor(0.0542)\n",
      "2955 Training Loss: tensor(0.0546)\n",
      "2956 Training Loss: tensor(0.0545)\n",
      "2957 Training Loss: tensor(0.0544)\n",
      "2958 Training Loss: tensor(0.0546)\n",
      "2959 Training Loss: tensor(0.0542)\n",
      "2960 Training Loss: tensor(0.0541)\n",
      "2961 Training Loss: tensor(0.0541)\n",
      "2962 Training Loss: tensor(0.0542)\n",
      "2963 Training Loss: tensor(0.0543)\n",
      "2964 Training Loss: tensor(0.0543)\n",
      "2965 Training Loss: tensor(0.0545)\n",
      "2966 Training Loss: tensor(0.0544)\n",
      "2967 Training Loss: tensor(0.0542)\n",
      "2968 Training Loss: tensor(0.0542)\n",
      "2969 Training Loss: tensor(0.0543)\n",
      "2970 Training Loss: tensor(0.0547)\n",
      "2971 Training Loss: tensor(0.0545)\n",
      "2972 Training Loss: tensor(0.0545)\n",
      "2973 Training Loss: tensor(0.0544)\n",
      "2974 Training Loss: tensor(0.0540)\n",
      "2975 Training Loss: tensor(0.0541)\n",
      "2976 Training Loss: tensor(0.0542)\n",
      "2977 Training Loss: tensor(0.0545)\n",
      "2978 Training Loss: tensor(0.0543)\n",
      "2979 Training Loss: tensor(0.0540)\n",
      "2980 Training Loss: tensor(0.0542)\n",
      "2981 Training Loss: tensor(0.0542)\n",
      "2982 Training Loss: tensor(0.0540)\n",
      "2983 Training Loss: tensor(0.0542)\n",
      "2984 Training Loss: tensor(0.0543)\n",
      "2985 Training Loss: tensor(0.0541)\n",
      "2986 Training Loss: tensor(0.0542)\n",
      "2987 Training Loss: tensor(0.0540)\n",
      "2988 Training Loss: tensor(0.0542)\n",
      "2989 Training Loss: tensor(0.0540)\n",
      "2990 Training Loss: tensor(0.0538)\n",
      "2991 Training Loss: tensor(0.0542)\n",
      "2992 Training Loss: tensor(0.0542)\n",
      "2993 Training Loss: tensor(0.0542)\n",
      "2994 Training Loss: tensor(0.0539)\n",
      "2995 Training Loss: tensor(0.0541)\n",
      "2996 Training Loss: tensor(0.0543)\n",
      "2997 Training Loss: tensor(0.0543)\n",
      "2998 Training Loss: tensor(0.0541)\n",
      "2999 Training Loss: tensor(0.0543)\n",
      "3000 Training Loss: tensor(0.0544)\n",
      "3001 Training Loss: tensor(0.0537)\n",
      "3002 Training Loss: tensor(0.0541)\n",
      "3003 Training Loss: tensor(0.0541)\n",
      "3004 Training Loss: tensor(0.0543)\n",
      "3005 Training Loss: tensor(0.0541)\n",
      "3006 Training Loss: tensor(0.0543)\n",
      "3007 Training Loss: tensor(0.0540)\n",
      "3008 Training Loss: tensor(0.0538)\n",
      "3009 Training Loss: tensor(0.0539)\n",
      "3010 Training Loss: tensor(0.0544)\n",
      "3011 Training Loss: tensor(0.0542)\n",
      "3012 Training Loss: tensor(0.0540)\n",
      "3013 Training Loss: tensor(0.0540)\n",
      "3014 Training Loss: tensor(0.0539)\n",
      "3015 Training Loss: tensor(0.0538)\n",
      "3016 Training Loss: tensor(0.0542)\n",
      "3017 Training Loss: tensor(0.0541)\n",
      "3018 Training Loss: tensor(0.0542)\n",
      "3019 Training Loss: tensor(0.0543)\n",
      "3020 Training Loss: tensor(0.0543)\n",
      "3021 Training Loss: tensor(0.0538)\n",
      "3022 Training Loss: tensor(0.0540)\n",
      "3023 Training Loss: tensor(0.0542)\n",
      "3024 Training Loss: tensor(0.0542)\n",
      "3025 Training Loss: tensor(0.0541)\n",
      "3026 Training Loss: tensor(0.0541)\n",
      "3027 Training Loss: tensor(0.0538)\n",
      "3028 Training Loss: tensor(0.0541)\n",
      "3029 Training Loss: tensor(0.0534)\n",
      "3030 Training Loss: tensor(0.0540)\n",
      "3031 Training Loss: tensor(0.0538)\n",
      "3032 Training Loss: tensor(0.0539)\n",
      "3033 Training Loss: tensor(0.0541)\n",
      "3034 Training Loss: tensor(0.0542)\n",
      "3035 Training Loss: tensor(0.0541)\n",
      "3036 Training Loss: tensor(0.0539)\n",
      "3037 Training Loss: tensor(0.0537)\n",
      "3038 Training Loss: tensor(0.0538)\n",
      "3039 Training Loss: tensor(0.0538)\n",
      "3040 Training Loss: tensor(0.0537)\n",
      "3041 Training Loss: tensor(0.0534)\n",
      "3042 Training Loss: tensor(0.0537)\n",
      "3043 Training Loss: tensor(0.0537)\n",
      "3044 Training Loss: tensor(0.0540)\n",
      "3045 Training Loss: tensor(0.0538)\n",
      "3046 Training Loss: tensor(0.0540)\n",
      "3047 Training Loss: tensor(0.0537)\n",
      "3048 Training Loss: tensor(0.0538)\n",
      "3049 Training Loss: tensor(0.0539)\n",
      "3050 Training Loss: tensor(0.0534)\n",
      "3051 Training Loss: tensor(0.0539)\n",
      "3052 Training Loss: tensor(0.0538)\n",
      "3053 Training Loss: tensor(0.0536)\n",
      "3054 Training Loss: tensor(0.0536)\n",
      "3055 Training Loss: tensor(0.0538)\n",
      "3056 Training Loss: tensor(0.0539)\n",
      "3057 Training Loss: tensor(0.0534)\n",
      "3058 Training Loss: tensor(0.0537)\n",
      "3059 Training Loss: tensor(0.0537)\n",
      "3060 Training Loss: tensor(0.0539)\n",
      "3061 Training Loss: tensor(0.0535)\n",
      "3062 Training Loss: tensor(0.0540)\n",
      "3063 Training Loss: tensor(0.0538)\n",
      "3064 Training Loss: tensor(0.0537)\n",
      "3065 Training Loss: tensor(0.0533)\n",
      "3066 Training Loss: tensor(0.0535)\n",
      "3067 Training Loss: tensor(0.0533)\n",
      "3068 Training Loss: tensor(0.0540)\n",
      "3069 Training Loss: tensor(0.0532)\n",
      "3070 Training Loss: tensor(0.0531)\n",
      "3071 Training Loss: tensor(0.0539)\n",
      "3072 Training Loss: tensor(0.0535)\n",
      "3073 Training Loss: tensor(0.0531)\n",
      "3074 Training Loss: tensor(0.0536)\n",
      "3075 Training Loss: tensor(0.0538)\n",
      "3076 Training Loss: tensor(0.0534)\n",
      "3077 Training Loss: tensor(0.0531)\n",
      "3078 Training Loss: tensor(0.0535)\n",
      "3079 Training Loss: tensor(0.0533)\n",
      "3080 Training Loss: tensor(0.0533)\n",
      "3081 Training Loss: tensor(0.0536)\n",
      "3082 Training Loss: tensor(0.0529)\n",
      "3083 Training Loss: tensor(0.0535)\n",
      "3084 Training Loss: tensor(0.0536)\n",
      "3085 Training Loss: tensor(0.0536)\n",
      "3086 Training Loss: tensor(0.0531)\n",
      "3087 Training Loss: tensor(0.0530)\n",
      "3088 Training Loss: tensor(0.0531)\n",
      "3089 Training Loss: tensor(0.0530)\n",
      "3090 Training Loss: tensor(0.0535)\n",
      "3091 Training Loss: tensor(0.0537)\n",
      "3092 Training Loss: tensor(0.0536)\n",
      "3093 Training Loss: tensor(0.0536)\n",
      "3094 Training Loss: tensor(0.0539)\n",
      "3095 Training Loss: tensor(0.0532)\n",
      "3096 Training Loss: tensor(0.0535)\n",
      "3097 Training Loss: tensor(0.0538)\n",
      "3098 Training Loss: tensor(0.0537)\n",
      "3099 Training Loss: tensor(0.0530)\n",
      "3100 Training Loss: tensor(0.0534)\n",
      "3101 Training Loss: tensor(0.0534)\n",
      "3102 Training Loss: tensor(0.0531)\n",
      "3103 Training Loss: tensor(0.0536)\n",
      "3104 Training Loss: tensor(0.0534)\n",
      "3105 Training Loss: tensor(0.0533)\n",
      "3106 Training Loss: tensor(0.0532)\n",
      "3107 Training Loss: tensor(0.0535)\n",
      "3108 Training Loss: tensor(0.0535)\n",
      "3109 Training Loss: tensor(0.0536)\n",
      "3110 Training Loss: tensor(0.0533)\n",
      "3111 Training Loss: tensor(0.0535)\n",
      "3112 Training Loss: tensor(0.0535)\n",
      "3113 Training Loss: tensor(0.0535)\n",
      "3114 Training Loss: tensor(0.0528)\n",
      "3115 Training Loss: tensor(0.0531)\n",
      "3116 Training Loss: tensor(0.0536)\n",
      "3117 Training Loss: tensor(0.0531)\n",
      "3118 Training Loss: tensor(0.0535)\n",
      "3119 Training Loss: tensor(0.0534)\n",
      "3120 Training Loss: tensor(0.0535)\n",
      "3121 Training Loss: tensor(0.0539)\n",
      "3122 Training Loss: tensor(0.0528)\n",
      "3123 Training Loss: tensor(0.0533)\n",
      "3124 Training Loss: tensor(0.0532)\n",
      "3125 Training Loss: tensor(0.0532)\n",
      "3126 Training Loss: tensor(0.0528)\n",
      "3127 Training Loss: tensor(0.0528)\n",
      "3128 Training Loss: tensor(0.0534)\n",
      "3129 Training Loss: tensor(0.0533)\n",
      "3130 Training Loss: tensor(0.0531)\n",
      "3131 Training Loss: tensor(0.0532)\n",
      "3132 Training Loss: tensor(0.0536)\n",
      "3133 Training Loss: tensor(0.0533)\n",
      "3134 Training Loss: tensor(0.0531)\n",
      "3135 Training Loss: tensor(0.0528)\n",
      "3136 Training Loss: tensor(0.0533)\n",
      "3137 Training Loss: tensor(0.0533)\n",
      "3138 Training Loss: tensor(0.0532)\n",
      "3139 Training Loss: tensor(0.0531)\n",
      "3140 Training Loss: tensor(0.0531)\n",
      "3141 Training Loss: tensor(0.0533)\n",
      "3142 Training Loss: tensor(0.0529)\n",
      "3143 Training Loss: tensor(0.0528)\n",
      "3144 Training Loss: tensor(0.0524)\n",
      "3145 Training Loss: tensor(0.0530)\n",
      "3146 Training Loss: tensor(0.0531)\n",
      "3147 Training Loss: tensor(0.0530)\n",
      "3148 Training Loss: tensor(0.0531)\n",
      "3149 Training Loss: tensor(0.0529)\n",
      "3150 Training Loss: tensor(0.0530)\n",
      "3151 Training Loss: tensor(0.0534)\n",
      "3152 Training Loss: tensor(0.0534)\n",
      "3153 Training Loss: tensor(0.0526)\n",
      "3154 Training Loss: tensor(0.0529)\n",
      "3155 Training Loss: tensor(0.0534)\n",
      "3156 Training Loss: tensor(0.0529)\n",
      "3157 Training Loss: tensor(0.0527)\n",
      "3158 Training Loss: tensor(0.0529)\n",
      "3159 Training Loss: tensor(0.0527)\n",
      "3160 Training Loss: tensor(0.0532)\n",
      "3161 Training Loss: tensor(0.0531)\n",
      "3162 Training Loss: tensor(0.0527)\n",
      "3163 Training Loss: tensor(0.0531)\n",
      "3164 Training Loss: tensor(0.0526)\n",
      "3165 Training Loss: tensor(0.0530)\n",
      "3166 Training Loss: tensor(0.0527)\n",
      "3167 Training Loss: tensor(0.0529)\n",
      "3168 Training Loss: tensor(0.0527)\n",
      "3169 Training Loss: tensor(0.0530)\n",
      "3170 Training Loss: tensor(0.0529)\n",
      "3171 Training Loss: tensor(0.0529)\n",
      "3172 Training Loss: tensor(0.0528)\n",
      "3173 Training Loss: tensor(0.0530)\n",
      "3174 Training Loss: tensor(0.0527)\n",
      "3175 Training Loss: tensor(0.0526)\n",
      "3176 Training Loss: tensor(0.0531)\n",
      "3177 Training Loss: tensor(0.0530)\n",
      "3178 Training Loss: tensor(0.0523)\n",
      "3179 Training Loss: tensor(0.0525)\n",
      "3180 Training Loss: tensor(0.0531)\n",
      "3181 Training Loss: tensor(0.0532)\n",
      "3182 Training Loss: tensor(0.0529)\n",
      "3183 Training Loss: tensor(0.0526)\n",
      "3184 Training Loss: tensor(0.0530)\n",
      "3185 Training Loss: tensor(0.0528)\n",
      "3186 Training Loss: tensor(0.0527)\n",
      "3187 Training Loss: tensor(0.0531)\n",
      "3188 Training Loss: tensor(0.0525)\n",
      "3189 Training Loss: tensor(0.0527)\n",
      "3190 Training Loss: tensor(0.0526)\n",
      "3191 Training Loss: tensor(0.0532)\n",
      "3192 Training Loss: tensor(0.0527)\n",
      "3193 Training Loss: tensor(0.0528)\n",
      "3194 Training Loss: tensor(0.0523)\n",
      "3195 Training Loss: tensor(0.0528)\n",
      "3196 Training Loss: tensor(0.0522)\n",
      "3197 Training Loss: tensor(0.0526)\n",
      "3198 Training Loss: tensor(0.0526)\n",
      "3199 Training Loss: tensor(0.0524)\n",
      "3200 Training Loss: tensor(0.0525)\n",
      "3201 Training Loss: tensor(0.0529)\n",
      "3202 Training Loss: tensor(0.0531)\n",
      "3203 Training Loss: tensor(0.0522)\n",
      "3204 Training Loss: tensor(0.0530)\n",
      "3205 Training Loss: tensor(0.0524)\n",
      "3206 Training Loss: tensor(0.0523)\n",
      "3207 Training Loss: tensor(0.0523)\n",
      "3208 Training Loss: tensor(0.0524)\n",
      "3209 Training Loss: tensor(0.0524)\n",
      "3210 Training Loss: tensor(0.0528)\n",
      "3211 Training Loss: tensor(0.0525)\n",
      "3212 Training Loss: tensor(0.0522)\n",
      "3213 Training Loss: tensor(0.0522)\n",
      "3214 Training Loss: tensor(0.0524)\n",
      "3215 Training Loss: tensor(0.0522)\n",
      "3216 Training Loss: tensor(0.0526)\n",
      "3217 Training Loss: tensor(0.0524)\n",
      "3218 Training Loss: tensor(0.0521)\n",
      "3219 Training Loss: tensor(0.0523)\n",
      "3220 Training Loss: tensor(0.0527)\n",
      "3221 Training Loss: tensor(0.0524)\n",
      "3222 Training Loss: tensor(0.0525)\n",
      "3223 Training Loss: tensor(0.0525)\n",
      "3224 Training Loss: tensor(0.0527)\n",
      "3225 Training Loss: tensor(0.0524)\n",
      "3226 Training Loss: tensor(0.0526)\n",
      "3227 Training Loss: tensor(0.0528)\n",
      "3228 Training Loss: tensor(0.0526)\n",
      "3229 Training Loss: tensor(0.0526)\n",
      "3230 Training Loss: tensor(0.0530)\n",
      "3231 Training Loss: tensor(0.0524)\n",
      "3232 Training Loss: tensor(0.0525)\n",
      "3233 Training Loss: tensor(0.0523)\n",
      "3234 Training Loss: tensor(0.0519)\n",
      "3235 Training Loss: tensor(0.0527)\n",
      "3236 Training Loss: tensor(0.0521)\n",
      "3237 Training Loss: tensor(0.0525)\n",
      "3238 Training Loss: tensor(0.0524)\n",
      "3239 Training Loss: tensor(0.0524)\n",
      "3240 Training Loss: tensor(0.0525)\n",
      "3241 Training Loss: tensor(0.0521)\n",
      "3242 Training Loss: tensor(0.0524)\n",
      "3243 Training Loss: tensor(0.0523)\n",
      "3244 Training Loss: tensor(0.0520)\n",
      "3245 Training Loss: tensor(0.0524)\n",
      "3246 Training Loss: tensor(0.0528)\n",
      "3247 Training Loss: tensor(0.0521)\n",
      "3248 Training Loss: tensor(0.0522)\n",
      "3249 Training Loss: tensor(0.0524)\n",
      "3250 Training Loss: tensor(0.0521)\n",
      "3251 Training Loss: tensor(0.0527)\n",
      "3252 Training Loss: tensor(0.0519)\n",
      "3253 Training Loss: tensor(0.0520)\n",
      "3254 Training Loss: tensor(0.0522)\n",
      "3255 Training Loss: tensor(0.0523)\n",
      "3256 Training Loss: tensor(0.0524)\n",
      "3257 Training Loss: tensor(0.0523)\n",
      "3258 Training Loss: tensor(0.0516)\n",
      "3259 Training Loss: tensor(0.0520)\n",
      "3260 Training Loss: tensor(0.0521)\n",
      "3261 Training Loss: tensor(0.0520)\n",
      "3262 Training Loss: tensor(0.0520)\n",
      "3263 Training Loss: tensor(0.0523)\n",
      "3264 Training Loss: tensor(0.0523)\n",
      "3265 Training Loss: tensor(0.0517)\n",
      "3266 Training Loss: tensor(0.0520)\n",
      "3267 Training Loss: tensor(0.0523)\n",
      "3268 Training Loss: tensor(0.0521)\n",
      "3269 Training Loss: tensor(0.0518)\n",
      "3270 Training Loss: tensor(0.0522)\n",
      "3271 Training Loss: tensor(0.0520)\n",
      "3272 Training Loss: tensor(0.0521)\n",
      "3273 Training Loss: tensor(0.0520)\n",
      "3274 Training Loss: tensor(0.0520)\n",
      "3275 Training Loss: tensor(0.0522)\n",
      "3276 Training Loss: tensor(0.0521)\n",
      "3277 Training Loss: tensor(0.0517)\n",
      "3278 Training Loss: tensor(0.0518)\n",
      "3279 Training Loss: tensor(0.0522)\n",
      "3280 Training Loss: tensor(0.0520)\n",
      "3281 Training Loss: tensor(0.0522)\n",
      "3282 Training Loss: tensor(0.0522)\n",
      "3283 Training Loss: tensor(0.0521)\n",
      "3284 Training Loss: tensor(0.0520)\n",
      "3285 Training Loss: tensor(0.0521)\n",
      "3286 Training Loss: tensor(0.0517)\n",
      "3287 Training Loss: tensor(0.0522)\n",
      "3288 Training Loss: tensor(0.0522)\n",
      "3289 Training Loss: tensor(0.0518)\n",
      "3290 Training Loss: tensor(0.0521)\n",
      "3291 Training Loss: tensor(0.0520)\n",
      "3292 Training Loss: tensor(0.0516)\n",
      "3293 Training Loss: tensor(0.0517)\n",
      "3294 Training Loss: tensor(0.0519)\n",
      "3295 Training Loss: tensor(0.0518)\n",
      "3296 Training Loss: tensor(0.0518)\n",
      "3297 Training Loss: tensor(0.0516)\n",
      "3298 Training Loss: tensor(0.0515)\n",
      "3299 Training Loss: tensor(0.0520)\n",
      "3300 Training Loss: tensor(0.0515)\n",
      "3301 Training Loss: tensor(0.0521)\n",
      "3302 Training Loss: tensor(0.0518)\n",
      "3303 Training Loss: tensor(0.0522)\n",
      "3304 Training Loss: tensor(0.0516)\n",
      "3305 Training Loss: tensor(0.0522)\n",
      "3306 Training Loss: tensor(0.0515)\n",
      "3307 Training Loss: tensor(0.0514)\n",
      "3308 Training Loss: tensor(0.0516)\n",
      "3309 Training Loss: tensor(0.0522)\n",
      "3310 Training Loss: tensor(0.0518)\n",
      "3311 Training Loss: tensor(0.0513)\n",
      "3312 Training Loss: tensor(0.0514)\n",
      "3313 Training Loss: tensor(0.0522)\n",
      "3314 Training Loss: tensor(0.0522)\n",
      "3315 Training Loss: tensor(0.0521)\n",
      "3316 Training Loss: tensor(0.0516)\n",
      "3317 Training Loss: tensor(0.0521)\n",
      "3318 Training Loss: tensor(0.0515)\n",
      "3319 Training Loss: tensor(0.0518)\n",
      "3320 Training Loss: tensor(0.0516)\n",
      "3321 Training Loss: tensor(0.0515)\n",
      "3322 Training Loss: tensor(0.0513)\n",
      "3323 Training Loss: tensor(0.0518)\n",
      "3324 Training Loss: tensor(0.0516)\n",
      "3325 Training Loss: tensor(0.0511)\n",
      "3326 Training Loss: tensor(0.0516)\n",
      "3327 Training Loss: tensor(0.0519)\n",
      "3328 Training Loss: tensor(0.0516)\n",
      "3329 Training Loss: tensor(0.0517)\n",
      "3330 Training Loss: tensor(0.0515)\n",
      "3331 Training Loss: tensor(0.0514)\n",
      "3332 Training Loss: tensor(0.0517)\n",
      "3333 Training Loss: tensor(0.0517)\n",
      "3334 Training Loss: tensor(0.0519)\n",
      "3335 Training Loss: tensor(0.0513)\n",
      "3336 Training Loss: tensor(0.0514)\n",
      "3337 Training Loss: tensor(0.0512)\n",
      "3338 Training Loss: tensor(0.0515)\n",
      "3339 Training Loss: tensor(0.0515)\n",
      "3340 Training Loss: tensor(0.0514)\n",
      "3341 Training Loss: tensor(0.0518)\n",
      "3342 Training Loss: tensor(0.0518)\n",
      "3343 Training Loss: tensor(0.0515)\n",
      "3344 Training Loss: tensor(0.0515)\n",
      "3345 Training Loss: tensor(0.0513)\n",
      "3346 Training Loss: tensor(0.0517)\n",
      "3347 Training Loss: tensor(0.0515)\n",
      "3348 Training Loss: tensor(0.0513)\n",
      "3349 Training Loss: tensor(0.0516)\n",
      "3350 Training Loss: tensor(0.0514)\n",
      "3351 Training Loss: tensor(0.0513)\n",
      "3352 Training Loss: tensor(0.0519)\n",
      "3353 Training Loss: tensor(0.0513)\n",
      "3354 Training Loss: tensor(0.0513)\n",
      "3355 Training Loss: tensor(0.0514)\n",
      "3356 Training Loss: tensor(0.0515)\n",
      "3357 Training Loss: tensor(0.0516)\n",
      "3358 Training Loss: tensor(0.0511)\n",
      "3359 Training Loss: tensor(0.0517)\n",
      "3360 Training Loss: tensor(0.0512)\n",
      "3361 Training Loss: tensor(0.0518)\n",
      "3362 Training Loss: tensor(0.0514)\n",
      "3363 Training Loss: tensor(0.0514)\n",
      "3364 Training Loss: tensor(0.0512)\n",
      "3365 Training Loss: tensor(0.0513)\n",
      "3366 Training Loss: tensor(0.0510)\n",
      "3367 Training Loss: tensor(0.0513)\n",
      "3368 Training Loss: tensor(0.0512)\n",
      "3369 Training Loss: tensor(0.0516)\n",
      "3370 Training Loss: tensor(0.0515)\n",
      "3371 Training Loss: tensor(0.0513)\n",
      "3372 Training Loss: tensor(0.0510)\n",
      "3373 Training Loss: tensor(0.0515)\n",
      "3374 Training Loss: tensor(0.0513)\n",
      "3375 Training Loss: tensor(0.0512)\n",
      "3376 Training Loss: tensor(0.0511)\n",
      "3377 Training Loss: tensor(0.0511)\n",
      "3378 Training Loss: tensor(0.0512)\n",
      "3379 Training Loss: tensor(0.0513)\n",
      "3380 Training Loss: tensor(0.0514)\n",
      "3381 Training Loss: tensor(0.0510)\n",
      "3382 Training Loss: tensor(0.0510)\n",
      "3383 Training Loss: tensor(0.0513)\n",
      "3384 Training Loss: tensor(0.0509)\n",
      "3385 Training Loss: tensor(0.0511)\n",
      "3386 Training Loss: tensor(0.0513)\n",
      "3387 Training Loss: tensor(0.0515)\n",
      "3388 Training Loss: tensor(0.0515)\n",
      "3389 Training Loss: tensor(0.0511)\n",
      "3390 Training Loss: tensor(0.0514)\n",
      "3391 Training Loss: tensor(0.0510)\n",
      "3392 Training Loss: tensor(0.0509)\n",
      "3393 Training Loss: tensor(0.0514)\n",
      "3394 Training Loss: tensor(0.0515)\n",
      "3395 Training Loss: tensor(0.0512)\n",
      "3396 Training Loss: tensor(0.0514)\n",
      "3397 Training Loss: tensor(0.0508)\n",
      "3398 Training Loss: tensor(0.0515)\n",
      "3399 Training Loss: tensor(0.0511)\n",
      "3400 Training Loss: tensor(0.0513)\n",
      "3401 Training Loss: tensor(0.0507)\n",
      "3402 Training Loss: tensor(0.0512)\n",
      "3403 Training Loss: tensor(0.0512)\n",
      "3404 Training Loss: tensor(0.0511)\n",
      "3405 Training Loss: tensor(0.0508)\n",
      "3406 Training Loss: tensor(0.0509)\n",
      "3407 Training Loss: tensor(0.0508)\n",
      "3408 Training Loss: tensor(0.0512)\n",
      "3409 Training Loss: tensor(0.0509)\n",
      "3410 Training Loss: tensor(0.0514)\n",
      "3411 Training Loss: tensor(0.0513)\n",
      "3412 Training Loss: tensor(0.0508)\n",
      "3413 Training Loss: tensor(0.0510)\n",
      "3414 Training Loss: tensor(0.0507)\n",
      "3415 Training Loss: tensor(0.0511)\n",
      "3416 Training Loss: tensor(0.0510)\n",
      "3417 Training Loss: tensor(0.0510)\n",
      "3418 Training Loss: tensor(0.0510)\n",
      "3419 Training Loss: tensor(0.0511)\n",
      "3420 Training Loss: tensor(0.0509)\n",
      "3421 Training Loss: tensor(0.0505)\n",
      "3422 Training Loss: tensor(0.0510)\n",
      "3423 Training Loss: tensor(0.0511)\n",
      "3424 Training Loss: tensor(0.0509)\n",
      "3425 Training Loss: tensor(0.0508)\n",
      "3426 Training Loss: tensor(0.0510)\n",
      "3427 Training Loss: tensor(0.0512)\n",
      "3428 Training Loss: tensor(0.0509)\n",
      "3429 Training Loss: tensor(0.0508)\n",
      "3430 Training Loss: tensor(0.0509)\n",
      "3431 Training Loss: tensor(0.0508)\n",
      "3432 Training Loss: tensor(0.0506)\n",
      "3433 Training Loss: tensor(0.0508)\n",
      "3434 Training Loss: tensor(0.0511)\n",
      "3435 Training Loss: tensor(0.0509)\n",
      "3436 Training Loss: tensor(0.0507)\n",
      "3437 Training Loss: tensor(0.0503)\n",
      "3438 Training Loss: tensor(0.0508)\n",
      "3439 Training Loss: tensor(0.0512)\n",
      "3440 Training Loss: tensor(0.0503)\n",
      "3441 Training Loss: tensor(0.0506)\n",
      "3442 Training Loss: tensor(0.0508)\n",
      "3443 Training Loss: tensor(0.0510)\n",
      "3444 Training Loss: tensor(0.0507)\n",
      "3445 Training Loss: tensor(0.0503)\n",
      "3446 Training Loss: tensor(0.0506)\n",
      "3447 Training Loss: tensor(0.0503)\n",
      "3448 Training Loss: tensor(0.0505)\n",
      "3449 Training Loss: tensor(0.0510)\n",
      "3450 Training Loss: tensor(0.0505)\n",
      "3451 Training Loss: tensor(0.0503)\n",
      "3452 Training Loss: tensor(0.0507)\n",
      "3453 Training Loss: tensor(0.0504)\n",
      "3454 Training Loss: tensor(0.0502)\n",
      "3455 Training Loss: tensor(0.0504)\n",
      "3456 Training Loss: tensor(0.0512)\n",
      "3457 Training Loss: tensor(0.0505)\n",
      "3458 Training Loss: tensor(0.0508)\n",
      "3459 Training Loss: tensor(0.0506)\n",
      "3460 Training Loss: tensor(0.0504)\n",
      "3461 Training Loss: tensor(0.0508)\n",
      "3462 Training Loss: tensor(0.0504)\n",
      "3463 Training Loss: tensor(0.0511)\n",
      "3464 Training Loss: tensor(0.0504)\n",
      "3465 Training Loss: tensor(0.0510)\n",
      "3466 Training Loss: tensor(0.0506)\n",
      "3467 Training Loss: tensor(0.0509)\n",
      "3468 Training Loss: tensor(0.0507)\n",
      "3469 Training Loss: tensor(0.0505)\n",
      "3470 Training Loss: tensor(0.0505)\n",
      "3471 Training Loss: tensor(0.0506)\n",
      "3472 Training Loss: tensor(0.0507)\n",
      "3473 Training Loss: tensor(0.0502)\n",
      "3474 Training Loss: tensor(0.0506)\n",
      "3475 Training Loss: tensor(0.0505)\n",
      "3476 Training Loss: tensor(0.0503)\n",
      "3477 Training Loss: tensor(0.0502)\n",
      "3478 Training Loss: tensor(0.0505)\n",
      "3479 Training Loss: tensor(0.0505)\n",
      "3480 Training Loss: tensor(0.0504)\n",
      "3481 Training Loss: tensor(0.0505)\n",
      "3482 Training Loss: tensor(0.0504)\n",
      "3483 Training Loss: tensor(0.0505)\n",
      "3484 Training Loss: tensor(0.0504)\n",
      "3485 Training Loss: tensor(0.0503)\n",
      "3486 Training Loss: tensor(0.0505)\n",
      "3487 Training Loss: tensor(0.0503)\n",
      "3488 Training Loss: tensor(0.0500)\n",
      "3489 Training Loss: tensor(0.0501)\n",
      "3490 Training Loss: tensor(0.0502)\n",
      "3491 Training Loss: tensor(0.0501)\n",
      "3492 Training Loss: tensor(0.0506)\n",
      "3493 Training Loss: tensor(0.0512)\n",
      "3494 Training Loss: tensor(0.0505)\n",
      "3495 Training Loss: tensor(0.0505)\n",
      "3496 Training Loss: tensor(0.0507)\n",
      "3497 Training Loss: tensor(0.0507)\n",
      "3498 Training Loss: tensor(0.0502)\n",
      "3499 Training Loss: tensor(0.0507)\n",
      "3500 Training Loss: tensor(0.0503)\n",
      "3501 Training Loss: tensor(0.0506)\n",
      "3502 Training Loss: tensor(0.0501)\n",
      "3503 Training Loss: tensor(0.0500)\n",
      "3504 Training Loss: tensor(0.0502)\n",
      "3505 Training Loss: tensor(0.0503)\n",
      "3506 Training Loss: tensor(0.0501)\n",
      "3507 Training Loss: tensor(0.0506)\n",
      "3508 Training Loss: tensor(0.0501)\n",
      "3509 Training Loss: tensor(0.0501)\n",
      "3510 Training Loss: tensor(0.0502)\n",
      "3511 Training Loss: tensor(0.0504)\n",
      "3512 Training Loss: tensor(0.0501)\n",
      "3513 Training Loss: tensor(0.0504)\n",
      "3514 Training Loss: tensor(0.0501)\n",
      "3515 Training Loss: tensor(0.0501)\n",
      "3516 Training Loss: tensor(0.0500)\n",
      "3517 Training Loss: tensor(0.0501)\n",
      "3518 Training Loss: tensor(0.0502)\n",
      "3519 Training Loss: tensor(0.0507)\n",
      "3520 Training Loss: tensor(0.0507)\n",
      "3521 Training Loss: tensor(0.0503)\n",
      "3522 Training Loss: tensor(0.0500)\n",
      "3523 Training Loss: tensor(0.0503)\n",
      "3524 Training Loss: tensor(0.0505)\n",
      "3525 Training Loss: tensor(0.0500)\n",
      "3526 Training Loss: tensor(0.0499)\n",
      "3527 Training Loss: tensor(0.0504)\n",
      "3528 Training Loss: tensor(0.0499)\n",
      "3529 Training Loss: tensor(0.0500)\n",
      "3530 Training Loss: tensor(0.0504)\n",
      "3531 Training Loss: tensor(0.0498)\n",
      "3532 Training Loss: tensor(0.0504)\n",
      "3533 Training Loss: tensor(0.0503)\n",
      "3534 Training Loss: tensor(0.0501)\n",
      "3535 Training Loss: tensor(0.0507)\n",
      "3536 Training Loss: tensor(0.0500)\n",
      "3537 Training Loss: tensor(0.0500)\n",
      "3538 Training Loss: tensor(0.0501)\n",
      "3539 Training Loss: tensor(0.0498)\n",
      "3540 Training Loss: tensor(0.0501)\n",
      "3541 Training Loss: tensor(0.0499)\n",
      "3542 Training Loss: tensor(0.0500)\n",
      "3543 Training Loss: tensor(0.0503)\n",
      "3544 Training Loss: tensor(0.0505)\n",
      "3545 Training Loss: tensor(0.0501)\n",
      "3546 Training Loss: tensor(0.0500)\n",
      "3547 Training Loss: tensor(0.0499)\n",
      "3548 Training Loss: tensor(0.0502)\n",
      "3549 Training Loss: tensor(0.0503)\n",
      "3550 Training Loss: tensor(0.0499)\n",
      "3551 Training Loss: tensor(0.0503)\n",
      "3552 Training Loss: tensor(0.0502)\n",
      "3553 Training Loss: tensor(0.0498)\n",
      "3554 Training Loss: tensor(0.0500)\n",
      "3555 Training Loss: tensor(0.0496)\n",
      "3556 Training Loss: tensor(0.0503)\n",
      "3557 Training Loss: tensor(0.0498)\n",
      "3558 Training Loss: tensor(0.0499)\n",
      "3559 Training Loss: tensor(0.0501)\n",
      "3560 Training Loss: tensor(0.0501)\n",
      "3561 Training Loss: tensor(0.0497)\n",
      "3562 Training Loss: tensor(0.0502)\n",
      "3563 Training Loss: tensor(0.0501)\n",
      "3564 Training Loss: tensor(0.0498)\n",
      "3565 Training Loss: tensor(0.0498)\n",
      "3566 Training Loss: tensor(0.0498)\n",
      "3567 Training Loss: tensor(0.0503)\n",
      "3568 Training Loss: tensor(0.0502)\n",
      "3569 Training Loss: tensor(0.0498)\n",
      "3570 Training Loss: tensor(0.0498)\n",
      "3571 Training Loss: tensor(0.0497)\n",
      "3572 Training Loss: tensor(0.0500)\n",
      "3573 Training Loss: tensor(0.0500)\n",
      "3574 Training Loss: tensor(0.0501)\n",
      "3575 Training Loss: tensor(0.0498)\n",
      "3576 Training Loss: tensor(0.0497)\n",
      "3577 Training Loss: tensor(0.0496)\n",
      "3578 Training Loss: tensor(0.0498)\n",
      "3579 Training Loss: tensor(0.0496)\n",
      "3580 Training Loss: tensor(0.0497)\n",
      "3581 Training Loss: tensor(0.0497)\n",
      "3582 Training Loss: tensor(0.0500)\n",
      "3583 Training Loss: tensor(0.0499)\n",
      "3584 Training Loss: tensor(0.0496)\n",
      "3585 Training Loss: tensor(0.0499)\n",
      "3586 Training Loss: tensor(0.0497)\n",
      "3587 Training Loss: tensor(0.0498)\n",
      "3588 Training Loss: tensor(0.0502)\n",
      "3589 Training Loss: tensor(0.0496)\n",
      "3590 Training Loss: tensor(0.0496)\n",
      "3591 Training Loss: tensor(0.0497)\n",
      "3592 Training Loss: tensor(0.0502)\n",
      "3593 Training Loss: tensor(0.0498)\n",
      "3594 Training Loss: tensor(0.0497)\n",
      "3595 Training Loss: tensor(0.0496)\n",
      "3596 Training Loss: tensor(0.0499)\n",
      "3597 Training Loss: tensor(0.0496)\n",
      "3598 Training Loss: tensor(0.0497)\n",
      "3599 Training Loss: tensor(0.0496)\n",
      "3600 Training Loss: tensor(0.0496)\n",
      "3601 Training Loss: tensor(0.0501)\n",
      "3602 Training Loss: tensor(0.0497)\n",
      "3603 Training Loss: tensor(0.0497)\n",
      "3604 Training Loss: tensor(0.0497)\n",
      "3605 Training Loss: tensor(0.0495)\n",
      "3606 Training Loss: tensor(0.0501)\n",
      "3607 Training Loss: tensor(0.0494)\n",
      "3608 Training Loss: tensor(0.0498)\n",
      "3609 Training Loss: tensor(0.0496)\n",
      "3610 Training Loss: tensor(0.0498)\n",
      "3611 Training Loss: tensor(0.0498)\n",
      "3612 Training Loss: tensor(0.0498)\n",
      "3613 Training Loss: tensor(0.0498)\n",
      "3614 Training Loss: tensor(0.0498)\n",
      "3615 Training Loss: tensor(0.0499)\n",
      "3616 Training Loss: tensor(0.0497)\n",
      "3617 Training Loss: tensor(0.0495)\n",
      "3618 Training Loss: tensor(0.0496)\n",
      "3619 Training Loss: tensor(0.0498)\n",
      "3620 Training Loss: tensor(0.0501)\n",
      "3621 Training Loss: tensor(0.0496)\n",
      "3622 Training Loss: tensor(0.0498)\n",
      "3623 Training Loss: tensor(0.0495)\n",
      "3624 Training Loss: tensor(0.0495)\n",
      "3625 Training Loss: tensor(0.0494)\n",
      "3626 Training Loss: tensor(0.0494)\n",
      "3627 Training Loss: tensor(0.0499)\n",
      "3628 Training Loss: tensor(0.0499)\n",
      "3629 Training Loss: tensor(0.0493)\n",
      "3630 Training Loss: tensor(0.0495)\n",
      "3631 Training Loss: tensor(0.0495)\n",
      "3632 Training Loss: tensor(0.0497)\n",
      "3633 Training Loss: tensor(0.0496)\n",
      "3634 Training Loss: tensor(0.0498)\n",
      "3635 Training Loss: tensor(0.0494)\n",
      "3636 Training Loss: tensor(0.0499)\n",
      "3637 Training Loss: tensor(0.0497)\n",
      "3638 Training Loss: tensor(0.0496)\n",
      "3639 Training Loss: tensor(0.0497)\n",
      "3640 Training Loss: tensor(0.0495)\n",
      "3641 Training Loss: tensor(0.0497)\n",
      "3642 Training Loss: tensor(0.0494)\n",
      "3643 Training Loss: tensor(0.0494)\n",
      "3644 Training Loss: tensor(0.0497)\n",
      "3645 Training Loss: tensor(0.0501)\n",
      "3646 Training Loss: tensor(0.0493)\n",
      "3647 Training Loss: tensor(0.0493)\n",
      "3648 Training Loss: tensor(0.0498)\n",
      "3649 Training Loss: tensor(0.0495)\n",
      "3650 Training Loss: tensor(0.0498)\n",
      "3651 Training Loss: tensor(0.0494)\n",
      "3652 Training Loss: tensor(0.0497)\n",
      "3653 Training Loss: tensor(0.0497)\n",
      "3654 Training Loss: tensor(0.0492)\n",
      "3655 Training Loss: tensor(0.0496)\n",
      "3656 Training Loss: tensor(0.0493)\n",
      "3657 Training Loss: tensor(0.0495)\n",
      "3658 Training Loss: tensor(0.0494)\n",
      "3659 Training Loss: tensor(0.0496)\n",
      "3660 Training Loss: tensor(0.0496)\n",
      "3661 Training Loss: tensor(0.0494)\n",
      "3662 Training Loss: tensor(0.0495)\n",
      "3663 Training Loss: tensor(0.0495)\n",
      "3664 Training Loss: tensor(0.0493)\n",
      "3665 Training Loss: tensor(0.0494)\n",
      "3666 Training Loss: tensor(0.0495)\n",
      "3667 Training Loss: tensor(0.0492)\n",
      "3668 Training Loss: tensor(0.0494)\n",
      "3669 Training Loss: tensor(0.0491)\n",
      "3670 Training Loss: tensor(0.0494)\n",
      "3671 Training Loss: tensor(0.0492)\n",
      "3672 Training Loss: tensor(0.0493)\n",
      "3673 Training Loss: tensor(0.0491)\n",
      "3674 Training Loss: tensor(0.0498)\n",
      "3675 Training Loss: tensor(0.0492)\n",
      "3676 Training Loss: tensor(0.0499)\n",
      "3677 Training Loss: tensor(0.0491)\n",
      "3678 Training Loss: tensor(0.0495)\n",
      "3679 Training Loss: tensor(0.0495)\n",
      "3680 Training Loss: tensor(0.0491)\n",
      "3681 Training Loss: tensor(0.0493)\n",
      "3682 Training Loss: tensor(0.0490)\n",
      "3683 Training Loss: tensor(0.0491)\n",
      "3684 Training Loss: tensor(0.0494)\n",
      "3685 Training Loss: tensor(0.0494)\n",
      "3686 Training Loss: tensor(0.0494)\n",
      "3687 Training Loss: tensor(0.0493)\n",
      "3688 Training Loss: tensor(0.0493)\n",
      "3689 Training Loss: tensor(0.0495)\n",
      "3690 Training Loss: tensor(0.0492)\n",
      "3691 Training Loss: tensor(0.0492)\n",
      "3692 Training Loss: tensor(0.0493)\n",
      "3693 Training Loss: tensor(0.0489)\n",
      "3694 Training Loss: tensor(0.0495)\n",
      "3695 Training Loss: tensor(0.0488)\n",
      "3696 Training Loss: tensor(0.0495)\n",
      "3697 Training Loss: tensor(0.0492)\n",
      "3698 Training Loss: tensor(0.0491)\n",
      "3699 Training Loss: tensor(0.0491)\n",
      "3700 Training Loss: tensor(0.0493)\n",
      "3701 Training Loss: tensor(0.0492)\n",
      "3702 Training Loss: tensor(0.0493)\n",
      "3703 Training Loss: tensor(0.0495)\n",
      "3704 Training Loss: tensor(0.0493)\n",
      "3705 Training Loss: tensor(0.0491)\n",
      "3706 Training Loss: tensor(0.0494)\n",
      "3707 Training Loss: tensor(0.0490)\n",
      "3708 Training Loss: tensor(0.0493)\n",
      "3709 Training Loss: tensor(0.0492)\n",
      "3710 Training Loss: tensor(0.0494)\n",
      "3711 Training Loss: tensor(0.0494)\n",
      "3712 Training Loss: tensor(0.0492)\n",
      "3713 Training Loss: tensor(0.0494)\n",
      "3714 Training Loss: tensor(0.0493)\n",
      "3715 Training Loss: tensor(0.0492)\n",
      "3716 Training Loss: tensor(0.0490)\n",
      "3717 Training Loss: tensor(0.0494)\n",
      "3718 Training Loss: tensor(0.0492)\n",
      "3719 Training Loss: tensor(0.0492)\n",
      "3720 Training Loss: tensor(0.0492)\n",
      "3721 Training Loss: tensor(0.0493)\n",
      "3722 Training Loss: tensor(0.0494)\n",
      "3723 Training Loss: tensor(0.0491)\n",
      "3724 Training Loss: tensor(0.0494)\n",
      "3725 Training Loss: tensor(0.0492)\n",
      "3726 Training Loss: tensor(0.0493)\n",
      "3727 Training Loss: tensor(0.0493)\n",
      "3728 Training Loss: tensor(0.0491)\n",
      "3729 Training Loss: tensor(0.0491)\n",
      "3730 Training Loss: tensor(0.0491)\n",
      "3731 Training Loss: tensor(0.0494)\n",
      "3732 Training Loss: tensor(0.0492)\n",
      "3733 Training Loss: tensor(0.0491)\n",
      "3734 Training Loss: tensor(0.0495)\n",
      "3735 Training Loss: tensor(0.0488)\n",
      "3736 Training Loss: tensor(0.0497)\n",
      "3737 Training Loss: tensor(0.0493)\n",
      "3738 Training Loss: tensor(0.0491)\n",
      "3739 Training Loss: tensor(0.0489)\n",
      "3740 Training Loss: tensor(0.0496)\n",
      "3741 Training Loss: tensor(0.0495)\n",
      "3742 Training Loss: tensor(0.0489)\n",
      "3743 Training Loss: tensor(0.0495)\n",
      "3744 Training Loss: tensor(0.0489)\n",
      "3745 Training Loss: tensor(0.0491)\n",
      "3746 Training Loss: tensor(0.0492)\n",
      "3747 Training Loss: tensor(0.0492)\n",
      "3748 Training Loss: tensor(0.0492)\n",
      "3749 Training Loss: tensor(0.0488)\n",
      "3750 Training Loss: tensor(0.0489)\n",
      "3751 Training Loss: tensor(0.0492)\n",
      "3752 Training Loss: tensor(0.0490)\n",
      "3753 Training Loss: tensor(0.0488)\n",
      "3754 Training Loss: tensor(0.0493)\n",
      "3755 Training Loss: tensor(0.0491)\n",
      "3756 Training Loss: tensor(0.0491)\n",
      "3757 Training Loss: tensor(0.0489)\n",
      "3758 Training Loss: tensor(0.0490)\n",
      "3759 Training Loss: tensor(0.0491)\n",
      "3760 Training Loss: tensor(0.0484)\n",
      "3761 Training Loss: tensor(0.0491)\n",
      "3762 Training Loss: tensor(0.0494)\n",
      "3763 Training Loss: tensor(0.0491)\n",
      "3764 Training Loss: tensor(0.0488)\n",
      "3765 Training Loss: tensor(0.0491)\n",
      "3766 Training Loss: tensor(0.0491)\n",
      "3767 Training Loss: tensor(0.0492)\n",
      "3768 Training Loss: tensor(0.0490)\n",
      "3769 Training Loss: tensor(0.0491)\n",
      "3770 Training Loss: tensor(0.0489)\n",
      "3771 Training Loss: tensor(0.0490)\n",
      "3772 Training Loss: tensor(0.0487)\n",
      "3773 Training Loss: tensor(0.0491)\n",
      "3774 Training Loss: tensor(0.0487)\n",
      "3775 Training Loss: tensor(0.0487)\n",
      "3776 Training Loss: tensor(0.0490)\n",
      "3777 Training Loss: tensor(0.0488)\n",
      "3778 Training Loss: tensor(0.0494)\n",
      "3779 Training Loss: tensor(0.0490)\n",
      "3780 Training Loss: tensor(0.0489)\n",
      "3781 Training Loss: tensor(0.0490)\n",
      "3782 Training Loss: tensor(0.0491)\n",
      "3783 Training Loss: tensor(0.0490)\n",
      "3784 Training Loss: tensor(0.0492)\n",
      "3785 Training Loss: tensor(0.0488)\n",
      "3786 Training Loss: tensor(0.0493)\n",
      "3787 Training Loss: tensor(0.0493)\n",
      "3788 Training Loss: tensor(0.0489)\n",
      "3789 Training Loss: tensor(0.0490)\n",
      "3790 Training Loss: tensor(0.0489)\n",
      "3791 Training Loss: tensor(0.0490)\n",
      "3792 Training Loss: tensor(0.0491)\n",
      "3793 Training Loss: tensor(0.0489)\n",
      "3794 Training Loss: tensor(0.0493)\n",
      "3795 Training Loss: tensor(0.0487)\n",
      "3796 Training Loss: tensor(0.0488)\n",
      "3797 Training Loss: tensor(0.0490)\n",
      "3798 Training Loss: tensor(0.0490)\n",
      "3799 Training Loss: tensor(0.0490)\n",
      "3800 Training Loss: tensor(0.0494)\n",
      "3801 Training Loss: tensor(0.0490)\n",
      "3802 Training Loss: tensor(0.0489)\n",
      "3803 Training Loss: tensor(0.0487)\n",
      "3804 Training Loss: tensor(0.0491)\n",
      "3805 Training Loss: tensor(0.0488)\n",
      "3806 Training Loss: tensor(0.0490)\n",
      "3807 Training Loss: tensor(0.0491)\n",
      "3808 Training Loss: tensor(0.0495)\n",
      "3809 Training Loss: tensor(0.0490)\n",
      "3810 Training Loss: tensor(0.0487)\n",
      "3811 Training Loss: tensor(0.0490)\n",
      "3812 Training Loss: tensor(0.0486)\n",
      "3813 Training Loss: tensor(0.0489)\n",
      "3814 Training Loss: tensor(0.0494)\n",
      "3815 Training Loss: tensor(0.0492)\n",
      "3816 Training Loss: tensor(0.0485)\n",
      "3817 Training Loss: tensor(0.0488)\n",
      "3818 Training Loss: tensor(0.0492)\n",
      "3819 Training Loss: tensor(0.0488)\n",
      "3820 Training Loss: tensor(0.0485)\n",
      "3821 Training Loss: tensor(0.0489)\n",
      "3822 Training Loss: tensor(0.0490)\n",
      "3823 Training Loss: tensor(0.0489)\n",
      "3824 Training Loss: tensor(0.0489)\n",
      "3825 Training Loss: tensor(0.0487)\n",
      "3826 Training Loss: tensor(0.0484)\n",
      "3827 Training Loss: tensor(0.0489)\n",
      "3828 Training Loss: tensor(0.0485)\n",
      "3829 Training Loss: tensor(0.0486)\n",
      "3830 Training Loss: tensor(0.0485)\n",
      "3831 Training Loss: tensor(0.0490)\n",
      "3832 Training Loss: tensor(0.0489)\n",
      "3833 Training Loss: tensor(0.0488)\n",
      "3834 Training Loss: tensor(0.0491)\n",
      "3835 Training Loss: tensor(0.0488)\n",
      "3836 Training Loss: tensor(0.0485)\n",
      "3837 Training Loss: tensor(0.0487)\n",
      "3838 Training Loss: tensor(0.0491)\n",
      "3839 Training Loss: tensor(0.0490)\n",
      "3840 Training Loss: tensor(0.0489)\n",
      "3841 Training Loss: tensor(0.0490)\n",
      "3842 Training Loss: tensor(0.0490)\n",
      "3843 Training Loss: tensor(0.0489)\n",
      "3844 Training Loss: tensor(0.0488)\n",
      "3845 Training Loss: tensor(0.0489)\n",
      "3846 Training Loss: tensor(0.0492)\n",
      "3847 Training Loss: tensor(0.0487)\n",
      "3848 Training Loss: tensor(0.0487)\n",
      "3849 Training Loss: tensor(0.0488)\n",
      "3850 Training Loss: tensor(0.0486)\n",
      "3851 Training Loss: tensor(0.0490)\n",
      "3852 Training Loss: tensor(0.0486)\n",
      "3853 Training Loss: tensor(0.0491)\n",
      "3854 Training Loss: tensor(0.0485)\n",
      "3855 Training Loss: tensor(0.0488)\n",
      "3856 Training Loss: tensor(0.0492)\n",
      "3857 Training Loss: tensor(0.0487)\n",
      "3858 Training Loss: tensor(0.0487)\n",
      "3859 Training Loss: tensor(0.0487)\n",
      "3860 Training Loss: tensor(0.0489)\n",
      "3861 Training Loss: tensor(0.0487)\n",
      "3862 Training Loss: tensor(0.0485)\n",
      "3863 Training Loss: tensor(0.0490)\n",
      "3864 Training Loss: tensor(0.0486)\n",
      "3865 Training Loss: tensor(0.0486)\n",
      "3866 Training Loss: tensor(0.0486)\n",
      "3867 Training Loss: tensor(0.0489)\n",
      "3868 Training Loss: tensor(0.0489)\n",
      "3869 Training Loss: tensor(0.0490)\n",
      "3870 Training Loss: tensor(0.0490)\n",
      "3871 Training Loss: tensor(0.0488)\n",
      "3872 Training Loss: tensor(0.0486)\n",
      "3873 Training Loss: tensor(0.0487)\n",
      "3874 Training Loss: tensor(0.0486)\n",
      "3875 Training Loss: tensor(0.0488)\n",
      "3876 Training Loss: tensor(0.0486)\n",
      "3877 Training Loss: tensor(0.0485)\n",
      "3878 Training Loss: tensor(0.0487)\n",
      "3879 Training Loss: tensor(0.0482)\n",
      "3880 Training Loss: tensor(0.0486)\n",
      "3881 Training Loss: tensor(0.0484)\n",
      "3882 Training Loss: tensor(0.0485)\n",
      "3883 Training Loss: tensor(0.0484)\n",
      "3884 Training Loss: tensor(0.0488)\n",
      "3885 Training Loss: tensor(0.0488)\n",
      "3886 Training Loss: tensor(0.0489)\n",
      "3887 Training Loss: tensor(0.0489)\n",
      "3888 Training Loss: tensor(0.0487)\n",
      "3889 Training Loss: tensor(0.0490)\n",
      "3890 Training Loss: tensor(0.0491)\n",
      "3891 Training Loss: tensor(0.0488)\n",
      "3892 Training Loss: tensor(0.0486)\n",
      "3893 Training Loss: tensor(0.0487)\n",
      "3894 Training Loss: tensor(0.0488)\n",
      "3895 Training Loss: tensor(0.0490)\n",
      "3896 Training Loss: tensor(0.0486)\n",
      "3897 Training Loss: tensor(0.0486)\n",
      "3898 Training Loss: tensor(0.0490)\n",
      "3899 Training Loss: tensor(0.0490)\n",
      "3900 Training Loss: tensor(0.0489)\n",
      "3901 Training Loss: tensor(0.0484)\n",
      "3902 Training Loss: tensor(0.0486)\n",
      "3903 Training Loss: tensor(0.0489)\n",
      "3904 Training Loss: tensor(0.0486)\n",
      "3905 Training Loss: tensor(0.0491)\n",
      "3906 Training Loss: tensor(0.0486)\n",
      "3907 Training Loss: tensor(0.0488)\n",
      "3908 Training Loss: tensor(0.0490)\n",
      "3909 Training Loss: tensor(0.0488)\n",
      "3910 Training Loss: tensor(0.0483)\n",
      "3911 Training Loss: tensor(0.0486)\n",
      "3912 Training Loss: tensor(0.0486)\n",
      "3913 Training Loss: tensor(0.0486)\n",
      "3914 Training Loss: tensor(0.0487)\n",
      "3915 Training Loss: tensor(0.0489)\n",
      "3916 Training Loss: tensor(0.0488)\n",
      "3917 Training Loss: tensor(0.0484)\n",
      "3918 Training Loss: tensor(0.0486)\n",
      "3919 Training Loss: tensor(0.0483)\n",
      "3920 Training Loss: tensor(0.0487)\n",
      "3921 Training Loss: tensor(0.0488)\n",
      "3922 Training Loss: tensor(0.0487)\n",
      "3923 Training Loss: tensor(0.0488)\n",
      "3924 Training Loss: tensor(0.0483)\n",
      "3925 Training Loss: tensor(0.0487)\n",
      "3926 Training Loss: tensor(0.0485)\n",
      "3927 Training Loss: tensor(0.0488)\n",
      "3928 Training Loss: tensor(0.0485)\n",
      "3929 Training Loss: tensor(0.0483)\n",
      "3930 Training Loss: tensor(0.0491)\n",
      "3931 Training Loss: tensor(0.0485)\n",
      "3932 Training Loss: tensor(0.0490)\n",
      "3933 Training Loss: tensor(0.0486)\n",
      "3934 Training Loss: tensor(0.0488)\n",
      "3935 Training Loss: tensor(0.0484)\n",
      "3936 Training Loss: tensor(0.0485)\n",
      "3937 Training Loss: tensor(0.0486)\n",
      "3938 Training Loss: tensor(0.0486)\n",
      "3939 Training Loss: tensor(0.0487)\n",
      "3940 Training Loss: tensor(0.0484)\n",
      "3941 Training Loss: tensor(0.0483)\n",
      "3942 Training Loss: tensor(0.0484)\n",
      "3943 Training Loss: tensor(0.0487)\n",
      "3944 Training Loss: tensor(0.0485)\n",
      "3945 Training Loss: tensor(0.0483)\n",
      "3946 Training Loss: tensor(0.0489)\n",
      "3947 Training Loss: tensor(0.0485)\n",
      "3948 Training Loss: tensor(0.0482)\n",
      "3949 Training Loss: tensor(0.0485)\n",
      "3950 Training Loss: tensor(0.0484)\n",
      "3951 Training Loss: tensor(0.0488)\n",
      "3952 Training Loss: tensor(0.0490)\n",
      "3953 Training Loss: tensor(0.0486)\n",
      "3954 Training Loss: tensor(0.0486)\n",
      "3955 Training Loss: tensor(0.0484)\n",
      "3956 Training Loss: tensor(0.0483)\n",
      "3957 Training Loss: tensor(0.0485)\n",
      "3958 Training Loss: tensor(0.0485)\n",
      "3959 Training Loss: tensor(0.0485)\n",
      "3960 Training Loss: tensor(0.0482)\n",
      "3961 Training Loss: tensor(0.0486)\n",
      "3962 Training Loss: tensor(0.0485)\n",
      "3963 Training Loss: tensor(0.0484)\n",
      "3964 Training Loss: tensor(0.0485)\n",
      "3965 Training Loss: tensor(0.0485)\n",
      "3966 Training Loss: tensor(0.0486)\n",
      "3967 Training Loss: tensor(0.0487)\n",
      "3968 Training Loss: tensor(0.0484)\n",
      "3969 Training Loss: tensor(0.0483)\n",
      "3970 Training Loss: tensor(0.0482)\n",
      "3971 Training Loss: tensor(0.0490)\n",
      "3972 Training Loss: tensor(0.0485)\n",
      "3973 Training Loss: tensor(0.0487)\n",
      "3974 Training Loss: tensor(0.0483)\n",
      "3975 Training Loss: tensor(0.0483)\n",
      "3976 Training Loss: tensor(0.0486)\n",
      "3977 Training Loss: tensor(0.0484)\n",
      "3978 Training Loss: tensor(0.0486)\n",
      "3979 Training Loss: tensor(0.0483)\n",
      "3980 Training Loss: tensor(0.0483)\n",
      "3981 Training Loss: tensor(0.0484)\n",
      "3982 Training Loss: tensor(0.0485)\n",
      "3983 Training Loss: tensor(0.0485)\n",
      "3984 Training Loss: tensor(0.0485)\n",
      "3985 Training Loss: tensor(0.0485)\n",
      "3986 Training Loss: tensor(0.0482)\n",
      "3987 Training Loss: tensor(0.0486)\n",
      "3988 Training Loss: tensor(0.0486)\n",
      "3989 Training Loss: tensor(0.0489)\n",
      "3990 Training Loss: tensor(0.0486)\n",
      "3991 Training Loss: tensor(0.0482)\n",
      "3992 Training Loss: tensor(0.0485)\n",
      "3993 Training Loss: tensor(0.0485)\n",
      "3994 Training Loss: tensor(0.0482)\n",
      "3995 Training Loss: tensor(0.0488)\n",
      "3996 Training Loss: tensor(0.0484)\n",
      "3997 Training Loss: tensor(0.0483)\n",
      "3998 Training Loss: tensor(0.0483)\n",
      "3999 Training Loss: tensor(0.0485)\n",
      "4000 Training Loss: tensor(0.0484)\n",
      "4001 Training Loss: tensor(0.0483)\n",
      "4002 Training Loss: tensor(0.0483)\n",
      "4003 Training Loss: tensor(0.0486)\n",
      "4004 Training Loss: tensor(0.0482)\n",
      "4005 Training Loss: tensor(0.0484)\n",
      "4006 Training Loss: tensor(0.0487)\n",
      "4007 Training Loss: tensor(0.0481)\n",
      "4008 Training Loss: tensor(0.0482)\n",
      "4009 Training Loss: tensor(0.0486)\n",
      "4010 Training Loss: tensor(0.0486)\n",
      "4011 Training Loss: tensor(0.0482)\n",
      "4012 Training Loss: tensor(0.0483)\n",
      "4013 Training Loss: tensor(0.0484)\n",
      "4014 Training Loss: tensor(0.0481)\n",
      "4015 Training Loss: tensor(0.0481)\n",
      "4016 Training Loss: tensor(0.0486)\n",
      "4017 Training Loss: tensor(0.0485)\n",
      "4018 Training Loss: tensor(0.0482)\n",
      "4019 Training Loss: tensor(0.0484)\n",
      "4020 Training Loss: tensor(0.0483)\n",
      "4021 Training Loss: tensor(0.0483)\n",
      "4022 Training Loss: tensor(0.0486)\n",
      "4023 Training Loss: tensor(0.0483)\n",
      "4024 Training Loss: tensor(0.0489)\n",
      "4025 Training Loss: tensor(0.0481)\n",
      "4026 Training Loss: tensor(0.0485)\n",
      "4027 Training Loss: tensor(0.0485)\n",
      "4028 Training Loss: tensor(0.0484)\n",
      "4029 Training Loss: tensor(0.0484)\n",
      "4030 Training Loss: tensor(0.0482)\n",
      "4031 Training Loss: tensor(0.0481)\n",
      "4032 Training Loss: tensor(0.0483)\n",
      "4033 Training Loss: tensor(0.0482)\n",
      "4034 Training Loss: tensor(0.0489)\n",
      "4035 Training Loss: tensor(0.0485)\n",
      "4036 Training Loss: tensor(0.0481)\n",
      "4037 Training Loss: tensor(0.0484)\n",
      "4038 Training Loss: tensor(0.0482)\n",
      "4039 Training Loss: tensor(0.0483)\n",
      "4040 Training Loss: tensor(0.0481)\n",
      "4041 Training Loss: tensor(0.0485)\n",
      "4042 Training Loss: tensor(0.0487)\n",
      "4043 Training Loss: tensor(0.0484)\n",
      "4044 Training Loss: tensor(0.0486)\n",
      "4045 Training Loss: tensor(0.0481)\n",
      "4046 Training Loss: tensor(0.0481)\n",
      "4047 Training Loss: tensor(0.0484)\n",
      "4048 Training Loss: tensor(0.0483)\n",
      "4049 Training Loss: tensor(0.0483)\n",
      "4050 Training Loss: tensor(0.0483)\n",
      "4051 Training Loss: tensor(0.0482)\n",
      "4052 Training Loss: tensor(0.0484)\n",
      "4053 Training Loss: tensor(0.0483)\n",
      "4054 Training Loss: tensor(0.0485)\n",
      "4055 Training Loss: tensor(0.0485)\n",
      "4056 Training Loss: tensor(0.0482)\n",
      "4057 Training Loss: tensor(0.0487)\n",
      "4058 Training Loss: tensor(0.0482)\n",
      "4059 Training Loss: tensor(0.0486)\n",
      "4060 Training Loss: tensor(0.0482)\n",
      "4061 Training Loss: tensor(0.0482)\n",
      "4062 Training Loss: tensor(0.0482)\n",
      "4063 Training Loss: tensor(0.0485)\n",
      "4064 Training Loss: tensor(0.0480)\n",
      "4065 Training Loss: tensor(0.0483)\n",
      "4066 Training Loss: tensor(0.0484)\n",
      "4067 Training Loss: tensor(0.0483)\n",
      "4068 Training Loss: tensor(0.0479)\n",
      "4069 Training Loss: tensor(0.0481)\n",
      "4070 Training Loss: tensor(0.0482)\n",
      "4071 Training Loss: tensor(0.0481)\n",
      "4072 Training Loss: tensor(0.0483)\n",
      "4073 Training Loss: tensor(0.0483)\n",
      "4074 Training Loss: tensor(0.0485)\n",
      "4075 Training Loss: tensor(0.0484)\n",
      "4076 Training Loss: tensor(0.0487)\n",
      "4077 Training Loss: tensor(0.0482)\n",
      "4078 Training Loss: tensor(0.0482)\n",
      "4079 Training Loss: tensor(0.0484)\n",
      "4080 Training Loss: tensor(0.0486)\n",
      "4081 Training Loss: tensor(0.0485)\n",
      "4082 Training Loss: tensor(0.0484)\n",
      "4083 Training Loss: tensor(0.0482)\n",
      "4084 Training Loss: tensor(0.0483)\n",
      "4085 Training Loss: tensor(0.0482)\n",
      "4086 Training Loss: tensor(0.0486)\n",
      "4087 Training Loss: tensor(0.0486)\n",
      "4088 Training Loss: tensor(0.0483)\n",
      "4089 Training Loss: tensor(0.0482)\n",
      "4090 Training Loss: tensor(0.0483)\n",
      "4091 Training Loss: tensor(0.0483)\n",
      "4092 Training Loss: tensor(0.0480)\n",
      "4093 Training Loss: tensor(0.0482)\n",
      "4094 Training Loss: tensor(0.0483)\n",
      "4095 Training Loss: tensor(0.0481)\n",
      "4096 Training Loss: tensor(0.0484)\n",
      "4097 Training Loss: tensor(0.0484)\n",
      "4098 Training Loss: tensor(0.0481)\n",
      "4099 Training Loss: tensor(0.0483)\n",
      "4100 Training Loss: tensor(0.0482)\n",
      "4101 Training Loss: tensor(0.0483)\n",
      "4102 Training Loss: tensor(0.0481)\n",
      "4103 Training Loss: tensor(0.0490)\n",
      "4104 Training Loss: tensor(0.0483)\n",
      "4105 Training Loss: tensor(0.0484)\n",
      "4106 Training Loss: tensor(0.0482)\n",
      "4107 Training Loss: tensor(0.0481)\n",
      "4108 Training Loss: tensor(0.0480)\n",
      "4109 Training Loss: tensor(0.0485)\n",
      "4110 Training Loss: tensor(0.0483)\n",
      "4111 Training Loss: tensor(0.0482)\n",
      "4112 Training Loss: tensor(0.0486)\n",
      "4113 Training Loss: tensor(0.0485)\n",
      "4114 Training Loss: tensor(0.0483)\n",
      "4115 Training Loss: tensor(0.0479)\n",
      "4116 Training Loss: tensor(0.0481)\n",
      "4117 Training Loss: tensor(0.0485)\n",
      "4118 Training Loss: tensor(0.0483)\n",
      "4119 Training Loss: tensor(0.0485)\n",
      "4120 Training Loss: tensor(0.0484)\n",
      "4121 Training Loss: tensor(0.0484)\n",
      "4122 Training Loss: tensor(0.0482)\n",
      "4123 Training Loss: tensor(0.0482)\n",
      "4124 Training Loss: tensor(0.0482)\n",
      "4125 Training Loss: tensor(0.0485)\n",
      "4126 Training Loss: tensor(0.0483)\n",
      "4127 Training Loss: tensor(0.0483)\n",
      "4128 Training Loss: tensor(0.0480)\n",
      "4129 Training Loss: tensor(0.0479)\n",
      "4130 Training Loss: tensor(0.0481)\n",
      "4131 Training Loss: tensor(0.0479)\n",
      "4132 Training Loss: tensor(0.0480)\n",
      "4133 Training Loss: tensor(0.0486)\n",
      "4134 Training Loss: tensor(0.0482)\n",
      "4135 Training Loss: tensor(0.0482)\n",
      "4136 Training Loss: tensor(0.0481)\n",
      "4137 Training Loss: tensor(0.0482)\n",
      "4138 Training Loss: tensor(0.0480)\n",
      "4139 Training Loss: tensor(0.0479)\n",
      "4140 Training Loss: tensor(0.0483)\n",
      "4141 Training Loss: tensor(0.0480)\n",
      "4142 Training Loss: tensor(0.0478)\n",
      "4143 Training Loss: tensor(0.0483)\n",
      "4144 Training Loss: tensor(0.0483)\n",
      "4145 Training Loss: tensor(0.0482)\n",
      "4146 Training Loss: tensor(0.0483)\n",
      "4147 Training Loss: tensor(0.0480)\n",
      "4148 Training Loss: tensor(0.0480)\n",
      "4149 Training Loss: tensor(0.0484)\n",
      "4150 Training Loss: tensor(0.0478)\n",
      "4151 Training Loss: tensor(0.0484)\n",
      "4152 Training Loss: tensor(0.0481)\n",
      "4153 Training Loss: tensor(0.0482)\n",
      "4154 Training Loss: tensor(0.0481)\n",
      "4155 Training Loss: tensor(0.0479)\n",
      "4156 Training Loss: tensor(0.0488)\n",
      "4157 Training Loss: tensor(0.0480)\n",
      "4158 Training Loss: tensor(0.0486)\n",
      "4159 Training Loss: tensor(0.0479)\n",
      "4160 Training Loss: tensor(0.0480)\n",
      "4161 Training Loss: tensor(0.0482)\n",
      "4162 Training Loss: tensor(0.0485)\n",
      "4163 Training Loss: tensor(0.0483)\n",
      "4164 Training Loss: tensor(0.0481)\n",
      "4165 Training Loss: tensor(0.0480)\n",
      "4166 Training Loss: tensor(0.0481)\n",
      "4167 Training Loss: tensor(0.0483)\n",
      "4168 Training Loss: tensor(0.0476)\n",
      "4169 Training Loss: tensor(0.0481)\n",
      "4170 Training Loss: tensor(0.0480)\n",
      "4171 Training Loss: tensor(0.0481)\n",
      "4172 Training Loss: tensor(0.0480)\n",
      "4173 Training Loss: tensor(0.0485)\n",
      "4174 Training Loss: tensor(0.0483)\n",
      "4175 Training Loss: tensor(0.0480)\n",
      "4176 Training Loss: tensor(0.0483)\n",
      "4177 Training Loss: tensor(0.0478)\n",
      "4178 Training Loss: tensor(0.0482)\n",
      "4179 Training Loss: tensor(0.0483)\n",
      "4180 Training Loss: tensor(0.0480)\n",
      "4181 Training Loss: tensor(0.0481)\n",
      "4182 Training Loss: tensor(0.0481)\n",
      "4183 Training Loss: tensor(0.0479)\n",
      "4184 Training Loss: tensor(0.0480)\n",
      "4185 Training Loss: tensor(0.0484)\n",
      "4186 Training Loss: tensor(0.0486)\n",
      "4187 Training Loss: tensor(0.0483)\n",
      "4188 Training Loss: tensor(0.0479)\n",
      "4189 Training Loss: tensor(0.0482)\n",
      "4190 Training Loss: tensor(0.0483)\n",
      "4191 Training Loss: tensor(0.0482)\n",
      "4192 Training Loss: tensor(0.0480)\n",
      "4193 Training Loss: tensor(0.0483)\n",
      "4194 Training Loss: tensor(0.0481)\n",
      "4195 Training Loss: tensor(0.0479)\n",
      "4196 Training Loss: tensor(0.0478)\n",
      "4197 Training Loss: tensor(0.0479)\n",
      "4198 Training Loss: tensor(0.0478)\n",
      "4199 Training Loss: tensor(0.0482)\n",
      "4200 Training Loss: tensor(0.0480)\n",
      "4201 Training Loss: tensor(0.0484)\n",
      "4202 Training Loss: tensor(0.0477)\n",
      "4203 Training Loss: tensor(0.0480)\n",
      "4204 Training Loss: tensor(0.0483)\n",
      "4205 Training Loss: tensor(0.0482)\n",
      "4206 Training Loss: tensor(0.0484)\n",
      "4207 Training Loss: tensor(0.0481)\n",
      "4208 Training Loss: tensor(0.0480)\n",
      "4209 Training Loss: tensor(0.0479)\n",
      "4210 Training Loss: tensor(0.0484)\n",
      "4211 Training Loss: tensor(0.0479)\n",
      "4212 Training Loss: tensor(0.0480)\n",
      "4213 Training Loss: tensor(0.0479)\n",
      "4214 Training Loss: tensor(0.0482)\n",
      "4215 Training Loss: tensor(0.0477)\n",
      "4216 Training Loss: tensor(0.0482)\n",
      "4217 Training Loss: tensor(0.0485)\n",
      "4218 Training Loss: tensor(0.0478)\n",
      "4219 Training Loss: tensor(0.0480)\n",
      "4220 Training Loss: tensor(0.0479)\n",
      "4221 Training Loss: tensor(0.0482)\n",
      "4222 Training Loss: tensor(0.0484)\n",
      "4223 Training Loss: tensor(0.0480)\n",
      "4224 Training Loss: tensor(0.0482)\n",
      "4225 Training Loss: tensor(0.0477)\n",
      "4226 Training Loss: tensor(0.0478)\n",
      "4227 Training Loss: tensor(0.0478)\n",
      "4228 Training Loss: tensor(0.0481)\n",
      "4229 Training Loss: tensor(0.0480)\n",
      "4230 Training Loss: tensor(0.0479)\n",
      "4231 Training Loss: tensor(0.0479)\n",
      "4232 Training Loss: tensor(0.0482)\n",
      "4233 Training Loss: tensor(0.0481)\n",
      "4234 Training Loss: tensor(0.0478)\n",
      "4235 Training Loss: tensor(0.0482)\n",
      "4236 Training Loss: tensor(0.0480)\n",
      "4237 Training Loss: tensor(0.0479)\n",
      "4238 Training Loss: tensor(0.0482)\n",
      "4239 Training Loss: tensor(0.0479)\n",
      "4240 Training Loss: tensor(0.0482)\n",
      "4241 Training Loss: tensor(0.0482)\n",
      "4242 Training Loss: tensor(0.0481)\n",
      "4243 Training Loss: tensor(0.0480)\n",
      "4244 Training Loss: tensor(0.0484)\n",
      "4245 Training Loss: tensor(0.0479)\n",
      "4246 Training Loss: tensor(0.0481)\n",
      "4247 Training Loss: tensor(0.0481)\n",
      "4248 Training Loss: tensor(0.0482)\n",
      "4249 Training Loss: tensor(0.0483)\n",
      "4250 Training Loss: tensor(0.0482)\n",
      "4251 Training Loss: tensor(0.0477)\n",
      "4252 Training Loss: tensor(0.0477)\n",
      "4253 Training Loss: tensor(0.0481)\n",
      "4254 Training Loss: tensor(0.0479)\n",
      "4255 Training Loss: tensor(0.0482)\n",
      "4256 Training Loss: tensor(0.0480)\n",
      "4257 Training Loss: tensor(0.0483)\n",
      "4258 Training Loss: tensor(0.0481)\n",
      "4259 Training Loss: tensor(0.0480)\n",
      "4260 Training Loss: tensor(0.0481)\n",
      "4261 Training Loss: tensor(0.0478)\n",
      "4262 Training Loss: tensor(0.0480)\n",
      "4263 Training Loss: tensor(0.0479)\n",
      "4264 Training Loss: tensor(0.0481)\n",
      "4265 Training Loss: tensor(0.0480)\n",
      "4266 Training Loss: tensor(0.0479)\n",
      "4267 Training Loss: tensor(0.0478)\n",
      "4268 Training Loss: tensor(0.0477)\n",
      "4269 Training Loss: tensor(0.0479)\n",
      "4270 Training Loss: tensor(0.0478)\n",
      "4271 Training Loss: tensor(0.0479)\n",
      "4272 Training Loss: tensor(0.0480)\n",
      "4273 Training Loss: tensor(0.0477)\n",
      "4274 Training Loss: tensor(0.0482)\n",
      "4275 Training Loss: tensor(0.0480)\n",
      "4276 Training Loss: tensor(0.0480)\n",
      "4277 Training Loss: tensor(0.0480)\n",
      "4278 Training Loss: tensor(0.0477)\n",
      "4279 Training Loss: tensor(0.0478)\n",
      "4280 Training Loss: tensor(0.0480)\n",
      "4281 Training Loss: tensor(0.0481)\n",
      "4282 Training Loss: tensor(0.0480)\n",
      "4283 Training Loss: tensor(0.0479)\n",
      "4284 Training Loss: tensor(0.0483)\n",
      "4285 Training Loss: tensor(0.0482)\n",
      "4286 Training Loss: tensor(0.0477)\n",
      "4287 Training Loss: tensor(0.0481)\n",
      "4288 Training Loss: tensor(0.0481)\n",
      "4289 Training Loss: tensor(0.0479)\n",
      "4290 Training Loss: tensor(0.0477)\n",
      "4291 Training Loss: tensor(0.0479)\n",
      "4292 Training Loss: tensor(0.0478)\n",
      "4293 Training Loss: tensor(0.0478)\n",
      "4294 Training Loss: tensor(0.0480)\n",
      "4295 Training Loss: tensor(0.0477)\n",
      "4296 Training Loss: tensor(0.0479)\n",
      "4297 Training Loss: tensor(0.0482)\n",
      "4298 Training Loss: tensor(0.0481)\n",
      "4299 Training Loss: tensor(0.0477)\n",
      "4300 Training Loss: tensor(0.0483)\n",
      "4301 Training Loss: tensor(0.0480)\n",
      "4302 Training Loss: tensor(0.0480)\n",
      "4303 Training Loss: tensor(0.0480)\n",
      "4304 Training Loss: tensor(0.0477)\n",
      "4305 Training Loss: tensor(0.0477)\n",
      "4306 Training Loss: tensor(0.0480)\n",
      "4307 Training Loss: tensor(0.0480)\n",
      "4308 Training Loss: tensor(0.0480)\n",
      "4309 Training Loss: tensor(0.0481)\n",
      "4310 Training Loss: tensor(0.0482)\n",
      "4311 Training Loss: tensor(0.0477)\n",
      "4312 Training Loss: tensor(0.0480)\n",
      "4313 Training Loss: tensor(0.0475)\n",
      "4314 Training Loss: tensor(0.0479)\n",
      "4315 Training Loss: tensor(0.0480)\n",
      "4316 Training Loss: tensor(0.0478)\n",
      "4317 Training Loss: tensor(0.0478)\n",
      "4318 Training Loss: tensor(0.0477)\n",
      "4319 Training Loss: tensor(0.0481)\n",
      "4320 Training Loss: tensor(0.0480)\n",
      "4321 Training Loss: tensor(0.0480)\n",
      "4322 Training Loss: tensor(0.0478)\n",
      "4323 Training Loss: tensor(0.0478)\n",
      "4324 Training Loss: tensor(0.0478)\n",
      "4325 Training Loss: tensor(0.0477)\n",
      "4326 Training Loss: tensor(0.0479)\n",
      "4327 Training Loss: tensor(0.0478)\n",
      "4328 Training Loss: tensor(0.0480)\n",
      "4329 Training Loss: tensor(0.0480)\n",
      "4330 Training Loss: tensor(0.0480)\n",
      "4331 Training Loss: tensor(0.0482)\n",
      "4332 Training Loss: tensor(0.0480)\n",
      "4333 Training Loss: tensor(0.0480)\n",
      "4334 Training Loss: tensor(0.0476)\n",
      "4335 Training Loss: tensor(0.0476)\n",
      "4336 Training Loss: tensor(0.0477)\n",
      "4337 Training Loss: tensor(0.0481)\n",
      "4338 Training Loss: tensor(0.0478)\n",
      "4339 Training Loss: tensor(0.0477)\n",
      "4340 Training Loss: tensor(0.0479)\n",
      "4341 Training Loss: tensor(0.0477)\n",
      "4342 Training Loss: tensor(0.0477)\n",
      "4343 Training Loss: tensor(0.0480)\n",
      "4344 Training Loss: tensor(0.0478)\n",
      "4345 Training Loss: tensor(0.0480)\n",
      "4346 Training Loss: tensor(0.0476)\n",
      "4347 Training Loss: tensor(0.0477)\n",
      "4348 Training Loss: tensor(0.0476)\n",
      "4349 Training Loss: tensor(0.0474)\n",
      "4350 Training Loss: tensor(0.0478)\n",
      "4351 Training Loss: tensor(0.0479)\n",
      "4352 Training Loss: tensor(0.0478)\n",
      "4353 Training Loss: tensor(0.0481)\n",
      "4354 Training Loss: tensor(0.0480)\n",
      "4355 Training Loss: tensor(0.0480)\n",
      "4356 Training Loss: tensor(0.0480)\n",
      "4357 Training Loss: tensor(0.0480)\n",
      "4358 Training Loss: tensor(0.0480)\n",
      "4359 Training Loss: tensor(0.0483)\n",
      "4360 Training Loss: tensor(0.0480)\n",
      "4361 Training Loss: tensor(0.0478)\n",
      "4362 Training Loss: tensor(0.0479)\n",
      "4363 Training Loss: tensor(0.0480)\n",
      "4364 Training Loss: tensor(0.0480)\n",
      "4365 Training Loss: tensor(0.0480)\n",
      "4366 Training Loss: tensor(0.0479)\n",
      "4367 Training Loss: tensor(0.0478)\n",
      "4368 Training Loss: tensor(0.0481)\n",
      "4369 Training Loss: tensor(0.0481)\n",
      "4370 Training Loss: tensor(0.0477)\n",
      "4371 Training Loss: tensor(0.0477)\n",
      "4372 Training Loss: tensor(0.0479)\n",
      "4373 Training Loss: tensor(0.0480)\n",
      "4374 Training Loss: tensor(0.0482)\n",
      "4375 Training Loss: tensor(0.0477)\n",
      "4376 Training Loss: tensor(0.0479)\n",
      "4377 Training Loss: tensor(0.0478)\n",
      "4378 Training Loss: tensor(0.0475)\n",
      "4379 Training Loss: tensor(0.0478)\n",
      "4380 Training Loss: tensor(0.0479)\n",
      "4381 Training Loss: tensor(0.0480)\n",
      "4382 Training Loss: tensor(0.0478)\n",
      "4383 Training Loss: tensor(0.0479)\n",
      "4384 Training Loss: tensor(0.0481)\n",
      "4385 Training Loss: tensor(0.0479)\n",
      "4386 Training Loss: tensor(0.0477)\n",
      "4387 Training Loss: tensor(0.0477)\n",
      "4388 Training Loss: tensor(0.0477)\n",
      "4389 Training Loss: tensor(0.0478)\n",
      "4390 Training Loss: tensor(0.0479)\n",
      "4391 Training Loss: tensor(0.0478)\n",
      "4392 Training Loss: tensor(0.0477)\n",
      "4393 Training Loss: tensor(0.0477)\n",
      "4394 Training Loss: tensor(0.0476)\n",
      "4395 Training Loss: tensor(0.0480)\n",
      "4396 Training Loss: tensor(0.0478)\n",
      "4397 Training Loss: tensor(0.0477)\n",
      "4398 Training Loss: tensor(0.0475)\n",
      "4399 Training Loss: tensor(0.0477)\n",
      "4400 Training Loss: tensor(0.0478)\n",
      "4401 Training Loss: tensor(0.0478)\n",
      "4402 Training Loss: tensor(0.0482)\n",
      "4403 Training Loss: tensor(0.0481)\n",
      "4404 Training Loss: tensor(0.0477)\n",
      "4405 Training Loss: tensor(0.0478)\n",
      "4406 Training Loss: tensor(0.0478)\n",
      "4407 Training Loss: tensor(0.0477)\n",
      "4408 Training Loss: tensor(0.0478)\n",
      "4409 Training Loss: tensor(0.0477)\n",
      "4410 Training Loss: tensor(0.0478)\n",
      "4411 Training Loss: tensor(0.0479)\n",
      "4412 Training Loss: tensor(0.0478)\n",
      "4413 Training Loss: tensor(0.0478)\n",
      "4414 Training Loss: tensor(0.0475)\n",
      "4415 Training Loss: tensor(0.0478)\n",
      "4416 Training Loss: tensor(0.0479)\n",
      "4417 Training Loss: tensor(0.0475)\n",
      "4418 Training Loss: tensor(0.0478)\n",
      "4419 Training Loss: tensor(0.0477)\n",
      "4420 Training Loss: tensor(0.0478)\n",
      "4421 Training Loss: tensor(0.0478)\n",
      "4422 Training Loss: tensor(0.0476)\n",
      "4423 Training Loss: tensor(0.0478)\n",
      "4424 Training Loss: tensor(0.0479)\n",
      "4425 Training Loss: tensor(0.0476)\n",
      "4426 Training Loss: tensor(0.0478)\n",
      "4427 Training Loss: tensor(0.0480)\n",
      "4428 Training Loss: tensor(0.0475)\n",
      "4429 Training Loss: tensor(0.0478)\n",
      "4430 Training Loss: tensor(0.0476)\n",
      "4431 Training Loss: tensor(0.0476)\n",
      "4432 Training Loss: tensor(0.0479)\n",
      "4433 Training Loss: tensor(0.0478)\n",
      "4434 Training Loss: tensor(0.0478)\n",
      "4435 Training Loss: tensor(0.0478)\n",
      "4436 Training Loss: tensor(0.0478)\n",
      "4437 Training Loss: tensor(0.0478)\n",
      "4438 Training Loss: tensor(0.0480)\n",
      "4439 Training Loss: tensor(0.0477)\n",
      "4440 Training Loss: tensor(0.0480)\n",
      "4441 Training Loss: tensor(0.0476)\n",
      "4442 Training Loss: tensor(0.0481)\n",
      "4443 Training Loss: tensor(0.0479)\n",
      "4444 Training Loss: tensor(0.0476)\n",
      "4445 Training Loss: tensor(0.0476)\n",
      "4446 Training Loss: tensor(0.0478)\n",
      "4447 Training Loss: tensor(0.0474)\n",
      "4448 Training Loss: tensor(0.0479)\n",
      "4449 Training Loss: tensor(0.0480)\n",
      "4450 Training Loss: tensor(0.0479)\n",
      "4451 Training Loss: tensor(0.0479)\n",
      "4452 Training Loss: tensor(0.0476)\n",
      "4453 Training Loss: tensor(0.0476)\n",
      "4454 Training Loss: tensor(0.0477)\n",
      "4455 Training Loss: tensor(0.0478)\n",
      "4456 Training Loss: tensor(0.0476)\n",
      "4457 Training Loss: tensor(0.0475)\n",
      "4458 Training Loss: tensor(0.0479)\n",
      "4459 Training Loss: tensor(0.0479)\n",
      "4460 Training Loss: tensor(0.0477)\n",
      "4461 Training Loss: tensor(0.0473)\n",
      "4462 Training Loss: tensor(0.0479)\n",
      "4463 Training Loss: tensor(0.0476)\n",
      "4464 Training Loss: tensor(0.0476)\n",
      "4465 Training Loss: tensor(0.0478)\n",
      "4466 Training Loss: tensor(0.0477)\n",
      "4467 Training Loss: tensor(0.0479)\n",
      "4468 Training Loss: tensor(0.0479)\n",
      "4469 Training Loss: tensor(0.0474)\n",
      "4470 Training Loss: tensor(0.0478)\n",
      "4471 Training Loss: tensor(0.0477)\n",
      "4472 Training Loss: tensor(0.0482)\n",
      "4473 Training Loss: tensor(0.0481)\n",
      "4474 Training Loss: tensor(0.0477)\n",
      "4475 Training Loss: tensor(0.0475)\n",
      "4476 Training Loss: tensor(0.0478)\n",
      "4477 Training Loss: tensor(0.0474)\n",
      "4478 Training Loss: tensor(0.0475)\n",
      "4479 Training Loss: tensor(0.0476)\n",
      "4480 Training Loss: tensor(0.0473)\n",
      "4481 Training Loss: tensor(0.0477)\n",
      "4482 Training Loss: tensor(0.0476)\n",
      "4483 Training Loss: tensor(0.0476)\n",
      "4484 Training Loss: tensor(0.0478)\n",
      "4485 Training Loss: tensor(0.0477)\n",
      "4486 Training Loss: tensor(0.0475)\n",
      "4487 Training Loss: tensor(0.0474)\n",
      "4488 Training Loss: tensor(0.0475)\n",
      "4489 Training Loss: tensor(0.0476)\n",
      "4490 Training Loss: tensor(0.0476)\n",
      "4491 Training Loss: tensor(0.0474)\n",
      "4492 Training Loss: tensor(0.0480)\n",
      "4493 Training Loss: tensor(0.0475)\n",
      "4494 Training Loss: tensor(0.0474)\n",
      "4495 Training Loss: tensor(0.0478)\n",
      "4496 Training Loss: tensor(0.0475)\n",
      "4497 Training Loss: tensor(0.0477)\n",
      "4498 Training Loss: tensor(0.0478)\n",
      "4499 Training Loss: tensor(0.0478)\n",
      "4500 Training Loss: tensor(0.0476)\n",
      "4501 Training Loss: tensor(0.0480)\n",
      "4502 Training Loss: tensor(0.0474)\n",
      "4503 Training Loss: tensor(0.0479)\n",
      "4504 Training Loss: tensor(0.0482)\n",
      "4505 Training Loss: tensor(0.0477)\n",
      "4506 Training Loss: tensor(0.0480)\n",
      "4507 Training Loss: tensor(0.0478)\n",
      "4508 Training Loss: tensor(0.0476)\n",
      "4509 Training Loss: tensor(0.0477)\n",
      "4510 Training Loss: tensor(0.0475)\n",
      "4511 Training Loss: tensor(0.0477)\n",
      "4512 Training Loss: tensor(0.0479)\n",
      "4513 Training Loss: tensor(0.0476)\n",
      "4514 Training Loss: tensor(0.0473)\n",
      "4515 Training Loss: tensor(0.0477)\n",
      "4516 Training Loss: tensor(0.0478)\n",
      "4517 Training Loss: tensor(0.0476)\n",
      "4518 Training Loss: tensor(0.0475)\n",
      "4519 Training Loss: tensor(0.0475)\n",
      "4520 Training Loss: tensor(0.0473)\n",
      "4521 Training Loss: tensor(0.0474)\n",
      "4522 Training Loss: tensor(0.0475)\n",
      "4523 Training Loss: tensor(0.0477)\n",
      "4524 Training Loss: tensor(0.0474)\n",
      "4525 Training Loss: tensor(0.0476)\n",
      "4526 Training Loss: tensor(0.0473)\n",
      "4527 Training Loss: tensor(0.0476)\n",
      "4528 Training Loss: tensor(0.0478)\n",
      "4529 Training Loss: tensor(0.0479)\n",
      "4530 Training Loss: tensor(0.0477)\n",
      "4531 Training Loss: tensor(0.0472)\n",
      "4532 Training Loss: tensor(0.0476)\n",
      "4533 Training Loss: tensor(0.0474)\n",
      "4534 Training Loss: tensor(0.0479)\n",
      "4535 Training Loss: tensor(0.0478)\n",
      "4536 Training Loss: tensor(0.0474)\n",
      "4537 Training Loss: tensor(0.0480)\n",
      "4538 Training Loss: tensor(0.0477)\n",
      "4539 Training Loss: tensor(0.0475)\n",
      "4540 Training Loss: tensor(0.0474)\n",
      "4541 Training Loss: tensor(0.0476)\n",
      "4542 Training Loss: tensor(0.0477)\n",
      "4543 Training Loss: tensor(0.0475)\n",
      "4544 Training Loss: tensor(0.0477)\n",
      "4545 Training Loss: tensor(0.0477)\n",
      "4546 Training Loss: tensor(0.0476)\n",
      "4547 Training Loss: tensor(0.0477)\n",
      "4548 Training Loss: tensor(0.0473)\n",
      "4549 Training Loss: tensor(0.0475)\n",
      "4550 Training Loss: tensor(0.0474)\n",
      "4551 Training Loss: tensor(0.0478)\n",
      "4552 Training Loss: tensor(0.0477)\n",
      "4553 Training Loss: tensor(0.0477)\n",
      "4554 Training Loss: tensor(0.0476)\n",
      "4555 Training Loss: tensor(0.0477)\n",
      "4556 Training Loss: tensor(0.0478)\n",
      "4557 Training Loss: tensor(0.0475)\n",
      "4558 Training Loss: tensor(0.0479)\n",
      "4559 Training Loss: tensor(0.0476)\n",
      "4560 Training Loss: tensor(0.0478)\n",
      "4561 Training Loss: tensor(0.0478)\n",
      "4562 Training Loss: tensor(0.0478)\n",
      "4563 Training Loss: tensor(0.0476)\n",
      "4564 Training Loss: tensor(0.0477)\n",
      "4565 Training Loss: tensor(0.0473)\n",
      "4566 Training Loss: tensor(0.0477)\n",
      "4567 Training Loss: tensor(0.0475)\n",
      "4568 Training Loss: tensor(0.0476)\n",
      "4569 Training Loss: tensor(0.0473)\n",
      "4570 Training Loss: tensor(0.0475)\n",
      "4571 Training Loss: tensor(0.0477)\n",
      "4572 Training Loss: tensor(0.0477)\n",
      "4573 Training Loss: tensor(0.0476)\n",
      "4574 Training Loss: tensor(0.0477)\n",
      "4575 Training Loss: tensor(0.0475)\n",
      "4576 Training Loss: tensor(0.0477)\n",
      "4577 Training Loss: tensor(0.0474)\n",
      "4578 Training Loss: tensor(0.0477)\n",
      "4579 Training Loss: tensor(0.0477)\n",
      "4580 Training Loss: tensor(0.0478)\n",
      "4581 Training Loss: tensor(0.0474)\n",
      "4582 Training Loss: tensor(0.0475)\n",
      "4583 Training Loss: tensor(0.0473)\n",
      "4584 Training Loss: tensor(0.0481)\n",
      "4585 Training Loss: tensor(0.0474)\n",
      "4586 Training Loss: tensor(0.0476)\n",
      "4587 Training Loss: tensor(0.0475)\n",
      "4588 Training Loss: tensor(0.0475)\n",
      "4589 Training Loss: tensor(0.0474)\n",
      "4590 Training Loss: tensor(0.0476)\n",
      "4591 Training Loss: tensor(0.0474)\n",
      "4592 Training Loss: tensor(0.0477)\n",
      "4593 Training Loss: tensor(0.0475)\n",
      "4594 Training Loss: tensor(0.0474)\n",
      "4595 Training Loss: tensor(0.0478)\n",
      "4596 Training Loss: tensor(0.0477)\n",
      "4597 Training Loss: tensor(0.0474)\n",
      "4598 Training Loss: tensor(0.0474)\n",
      "4599 Training Loss: tensor(0.0478)\n",
      "4600 Training Loss: tensor(0.0477)\n",
      "4601 Training Loss: tensor(0.0473)\n",
      "4602 Training Loss: tensor(0.0477)\n",
      "4603 Training Loss: tensor(0.0476)\n",
      "4604 Training Loss: tensor(0.0475)\n",
      "4605 Training Loss: tensor(0.0472)\n",
      "4606 Training Loss: tensor(0.0475)\n",
      "4607 Training Loss: tensor(0.0477)\n",
      "4608 Training Loss: tensor(0.0478)\n",
      "4609 Training Loss: tensor(0.0475)\n",
      "4610 Training Loss: tensor(0.0474)\n",
      "4611 Training Loss: tensor(0.0476)\n",
      "4612 Training Loss: tensor(0.0475)\n",
      "4613 Training Loss: tensor(0.0474)\n",
      "4614 Training Loss: tensor(0.0477)\n",
      "4615 Training Loss: tensor(0.0476)\n",
      "4616 Training Loss: tensor(0.0476)\n",
      "4617 Training Loss: tensor(0.0474)\n",
      "4618 Training Loss: tensor(0.0475)\n",
      "4619 Training Loss: tensor(0.0475)\n",
      "4620 Training Loss: tensor(0.0478)\n",
      "4621 Training Loss: tensor(0.0478)\n",
      "4622 Training Loss: tensor(0.0474)\n",
      "4623 Training Loss: tensor(0.0480)\n",
      "4624 Training Loss: tensor(0.0477)\n",
      "4625 Training Loss: tensor(0.0475)\n",
      "4626 Training Loss: tensor(0.0476)\n",
      "4627 Training Loss: tensor(0.0477)\n",
      "4628 Training Loss: tensor(0.0474)\n",
      "4629 Training Loss: tensor(0.0474)\n",
      "4630 Training Loss: tensor(0.0474)\n",
      "4631 Training Loss: tensor(0.0475)\n",
      "4632 Training Loss: tensor(0.0475)\n",
      "4633 Training Loss: tensor(0.0477)\n",
      "4634 Training Loss: tensor(0.0474)\n",
      "4635 Training Loss: tensor(0.0476)\n",
      "4636 Training Loss: tensor(0.0474)\n",
      "4637 Training Loss: tensor(0.0476)\n",
      "4638 Training Loss: tensor(0.0475)\n",
      "4639 Training Loss: tensor(0.0473)\n",
      "4640 Training Loss: tensor(0.0475)\n",
      "4641 Training Loss: tensor(0.0478)\n",
      "4642 Training Loss: tensor(0.0477)\n",
      "4643 Training Loss: tensor(0.0474)\n",
      "4644 Training Loss: tensor(0.0477)\n",
      "4645 Training Loss: tensor(0.0476)\n",
      "4646 Training Loss: tensor(0.0473)\n",
      "4647 Training Loss: tensor(0.0476)\n",
      "4648 Training Loss: tensor(0.0474)\n",
      "4649 Training Loss: tensor(0.0475)\n",
      "4650 Training Loss: tensor(0.0472)\n",
      "4651 Training Loss: tensor(0.0475)\n",
      "4652 Training Loss: tensor(0.0475)\n",
      "4653 Training Loss: tensor(0.0475)\n",
      "4654 Training Loss: tensor(0.0477)\n",
      "4655 Training Loss: tensor(0.0477)\n",
      "4656 Training Loss: tensor(0.0478)\n",
      "4657 Training Loss: tensor(0.0475)\n",
      "4658 Training Loss: tensor(0.0476)\n",
      "4659 Training Loss: tensor(0.0475)\n",
      "4660 Training Loss: tensor(0.0473)\n",
      "4661 Training Loss: tensor(0.0471)\n",
      "4662 Training Loss: tensor(0.0475)\n",
      "4663 Training Loss: tensor(0.0473)\n",
      "4664 Training Loss: tensor(0.0473)\n",
      "4665 Training Loss: tensor(0.0474)\n",
      "4666 Training Loss: tensor(0.0476)\n",
      "4667 Training Loss: tensor(0.0472)\n",
      "4668 Training Loss: tensor(0.0474)\n",
      "4669 Training Loss: tensor(0.0473)\n",
      "4670 Training Loss: tensor(0.0473)\n",
      "4671 Training Loss: tensor(0.0472)\n",
      "4672 Training Loss: tensor(0.0470)\n",
      "4673 Training Loss: tensor(0.0475)\n",
      "4674 Training Loss: tensor(0.0474)\n",
      "4675 Training Loss: tensor(0.0474)\n",
      "4676 Training Loss: tensor(0.0476)\n",
      "4677 Training Loss: tensor(0.0475)\n",
      "4678 Training Loss: tensor(0.0477)\n",
      "4679 Training Loss: tensor(0.0477)\n",
      "4680 Training Loss: tensor(0.0475)\n",
      "4681 Training Loss: tensor(0.0476)\n",
      "4682 Training Loss: tensor(0.0474)\n",
      "4683 Training Loss: tensor(0.0474)\n",
      "4684 Training Loss: tensor(0.0475)\n",
      "4685 Training Loss: tensor(0.0476)\n",
      "4686 Training Loss: tensor(0.0475)\n",
      "4687 Training Loss: tensor(0.0472)\n",
      "4688 Training Loss: tensor(0.0475)\n",
      "4689 Training Loss: tensor(0.0474)\n",
      "4690 Training Loss: tensor(0.0476)\n",
      "4691 Training Loss: tensor(0.0475)\n",
      "4692 Training Loss: tensor(0.0474)\n",
      "4693 Training Loss: tensor(0.0477)\n",
      "4694 Training Loss: tensor(0.0476)\n",
      "4695 Training Loss: tensor(0.0473)\n",
      "4696 Training Loss: tensor(0.0476)\n",
      "4697 Training Loss: tensor(0.0474)\n",
      "4698 Training Loss: tensor(0.0476)\n",
      "4699 Training Loss: tensor(0.0474)\n",
      "4700 Training Loss: tensor(0.0475)\n",
      "4701 Training Loss: tensor(0.0473)\n",
      "4702 Training Loss: tensor(0.0474)\n",
      "4703 Training Loss: tensor(0.0476)\n",
      "4704 Training Loss: tensor(0.0476)\n",
      "4705 Training Loss: tensor(0.0475)\n",
      "4706 Training Loss: tensor(0.0475)\n",
      "4707 Training Loss: tensor(0.0472)\n",
      "4708 Training Loss: tensor(0.0475)\n",
      "4709 Training Loss: tensor(0.0474)\n",
      "4710 Training Loss: tensor(0.0473)\n",
      "4711 Training Loss: tensor(0.0475)\n",
      "4712 Training Loss: tensor(0.0475)\n",
      "4713 Training Loss: tensor(0.0476)\n",
      "4714 Training Loss: tensor(0.0474)\n",
      "4715 Training Loss: tensor(0.0473)\n",
      "4716 Training Loss: tensor(0.0475)\n",
      "4717 Training Loss: tensor(0.0474)\n",
      "4718 Training Loss: tensor(0.0475)\n",
      "4719 Training Loss: tensor(0.0474)\n",
      "4720 Training Loss: tensor(0.0472)\n",
      "4721 Training Loss: tensor(0.0475)\n",
      "4722 Training Loss: tensor(0.0473)\n",
      "4723 Training Loss: tensor(0.0476)\n",
      "4724 Training Loss: tensor(0.0475)\n",
      "4725 Training Loss: tensor(0.0478)\n",
      "4726 Training Loss: tensor(0.0473)\n",
      "4727 Training Loss: tensor(0.0471)\n",
      "4728 Training Loss: tensor(0.0473)\n",
      "4729 Training Loss: tensor(0.0475)\n",
      "4730 Training Loss: tensor(0.0472)\n",
      "4731 Training Loss: tensor(0.0473)\n",
      "4732 Training Loss: tensor(0.0473)\n",
      "4733 Training Loss: tensor(0.0474)\n",
      "4734 Training Loss: tensor(0.0474)\n",
      "4735 Training Loss: tensor(0.0472)\n",
      "4736 Training Loss: tensor(0.0474)\n",
      "4737 Training Loss: tensor(0.0471)\n",
      "4738 Training Loss: tensor(0.0473)\n",
      "4739 Training Loss: tensor(0.0474)\n",
      "4740 Training Loss: tensor(0.0473)\n",
      "4741 Training Loss: tensor(0.0477)\n",
      "4742 Training Loss: tensor(0.0475)\n",
      "4743 Training Loss: tensor(0.0473)\n",
      "4744 Training Loss: tensor(0.0477)\n",
      "4745 Training Loss: tensor(0.0473)\n",
      "4746 Training Loss: tensor(0.0475)\n",
      "4747 Training Loss: tensor(0.0476)\n",
      "4748 Training Loss: tensor(0.0476)\n",
      "4749 Training Loss: tensor(0.0471)\n",
      "4750 Training Loss: tensor(0.0473)\n",
      "4751 Training Loss: tensor(0.0479)\n",
      "4752 Training Loss: tensor(0.0474)\n",
      "4753 Training Loss: tensor(0.0473)\n",
      "4754 Training Loss: tensor(0.0476)\n",
      "4755 Training Loss: tensor(0.0474)\n",
      "4756 Training Loss: tensor(0.0471)\n",
      "4757 Training Loss: tensor(0.0475)\n",
      "4758 Training Loss: tensor(0.0474)\n",
      "4759 Training Loss: tensor(0.0473)\n",
      "4760 Training Loss: tensor(0.0477)\n",
      "4761 Training Loss: tensor(0.0472)\n",
      "4762 Training Loss: tensor(0.0474)\n",
      "4763 Training Loss: tensor(0.0474)\n",
      "4764 Training Loss: tensor(0.0472)\n",
      "4765 Training Loss: tensor(0.0472)\n",
      "4766 Training Loss: tensor(0.0470)\n",
      "4767 Training Loss: tensor(0.0476)\n",
      "4768 Training Loss: tensor(0.0473)\n",
      "4769 Training Loss: tensor(0.0473)\n",
      "4770 Training Loss: tensor(0.0474)\n",
      "4771 Training Loss: tensor(0.0478)\n",
      "4772 Training Loss: tensor(0.0472)\n",
      "4773 Training Loss: tensor(0.0476)\n",
      "4774 Training Loss: tensor(0.0472)\n",
      "4775 Training Loss: tensor(0.0471)\n",
      "4776 Training Loss: tensor(0.0476)\n",
      "4777 Training Loss: tensor(0.0476)\n",
      "4778 Training Loss: tensor(0.0475)\n",
      "4779 Training Loss: tensor(0.0472)\n",
      "4780 Training Loss: tensor(0.0476)\n",
      "4781 Training Loss: tensor(0.0474)\n",
      "4782 Training Loss: tensor(0.0473)\n",
      "4783 Training Loss: tensor(0.0475)\n",
      "4784 Training Loss: tensor(0.0475)\n",
      "4785 Training Loss: tensor(0.0474)\n",
      "4786 Training Loss: tensor(0.0473)\n",
      "4787 Training Loss: tensor(0.0474)\n",
      "4788 Training Loss: tensor(0.0472)\n",
      "4789 Training Loss: tensor(0.0472)\n",
      "4790 Training Loss: tensor(0.0478)\n",
      "4791 Training Loss: tensor(0.0474)\n",
      "4792 Training Loss: tensor(0.0473)\n",
      "4793 Training Loss: tensor(0.0471)\n",
      "4794 Training Loss: tensor(0.0475)\n",
      "4795 Training Loss: tensor(0.0472)\n",
      "4796 Training Loss: tensor(0.0473)\n",
      "4797 Training Loss: tensor(0.0472)\n",
      "4798 Training Loss: tensor(0.0475)\n",
      "4799 Training Loss: tensor(0.0475)\n",
      "4800 Training Loss: tensor(0.0474)\n",
      "4801 Training Loss: tensor(0.0475)\n",
      "4802 Training Loss: tensor(0.0475)\n",
      "4803 Training Loss: tensor(0.0471)\n",
      "4804 Training Loss: tensor(0.0472)\n",
      "4805 Training Loss: tensor(0.0473)\n",
      "4806 Training Loss: tensor(0.0474)\n",
      "4807 Training Loss: tensor(0.0474)\n",
      "4808 Training Loss: tensor(0.0476)\n",
      "4809 Training Loss: tensor(0.0472)\n",
      "4810 Training Loss: tensor(0.0475)\n",
      "4811 Training Loss: tensor(0.0475)\n",
      "4812 Training Loss: tensor(0.0476)\n",
      "4813 Training Loss: tensor(0.0472)\n",
      "4814 Training Loss: tensor(0.0472)\n",
      "4815 Training Loss: tensor(0.0473)\n",
      "4816 Training Loss: tensor(0.0473)\n",
      "4817 Training Loss: tensor(0.0476)\n",
      "4818 Training Loss: tensor(0.0476)\n",
      "4819 Training Loss: tensor(0.0476)\n",
      "4820 Training Loss: tensor(0.0474)\n",
      "4821 Training Loss: tensor(0.0473)\n",
      "4822 Training Loss: tensor(0.0474)\n",
      "4823 Training Loss: tensor(0.0475)\n",
      "4824 Training Loss: tensor(0.0474)\n",
      "4825 Training Loss: tensor(0.0471)\n",
      "4826 Training Loss: tensor(0.0473)\n",
      "4827 Training Loss: tensor(0.0473)\n",
      "4828 Training Loss: tensor(0.0471)\n",
      "4829 Training Loss: tensor(0.0477)\n",
      "4830 Training Loss: tensor(0.0476)\n",
      "4831 Training Loss: tensor(0.0473)\n",
      "4832 Training Loss: tensor(0.0470)\n",
      "4833 Training Loss: tensor(0.0474)\n",
      "4834 Training Loss: tensor(0.0472)\n",
      "4835 Training Loss: tensor(0.0473)\n",
      "4836 Training Loss: tensor(0.0470)\n",
      "4837 Training Loss: tensor(0.0473)\n",
      "4838 Training Loss: tensor(0.0474)\n",
      "4839 Training Loss: tensor(0.0471)\n",
      "4840 Training Loss: tensor(0.0474)\n",
      "4841 Training Loss: tensor(0.0470)\n",
      "4842 Training Loss: tensor(0.0471)\n",
      "4843 Training Loss: tensor(0.0473)\n",
      "4844 Training Loss: tensor(0.0469)\n",
      "4845 Training Loss: tensor(0.0472)\n",
      "4846 Training Loss: tensor(0.0473)\n",
      "4847 Training Loss: tensor(0.0469)\n",
      "4848 Training Loss: tensor(0.0475)\n",
      "4849 Training Loss: tensor(0.0470)\n",
      "4850 Training Loss: tensor(0.0478)\n",
      "4851 Training Loss: tensor(0.0473)\n",
      "4852 Training Loss: tensor(0.0474)\n",
      "4853 Training Loss: tensor(0.0472)\n",
      "4854 Training Loss: tensor(0.0472)\n",
      "4855 Training Loss: tensor(0.0475)\n",
      "4856 Training Loss: tensor(0.0473)\n",
      "4857 Training Loss: tensor(0.0471)\n",
      "4858 Training Loss: tensor(0.0471)\n",
      "4859 Training Loss: tensor(0.0476)\n",
      "4860 Training Loss: tensor(0.0473)\n",
      "4861 Training Loss: tensor(0.0470)\n",
      "4862 Training Loss: tensor(0.0472)\n",
      "4863 Training Loss: tensor(0.0474)\n",
      "4864 Training Loss: tensor(0.0471)\n",
      "4865 Training Loss: tensor(0.0474)\n",
      "4866 Training Loss: tensor(0.0474)\n",
      "4867 Training Loss: tensor(0.0474)\n",
      "4868 Training Loss: tensor(0.0471)\n",
      "4869 Training Loss: tensor(0.0471)\n",
      "4870 Training Loss: tensor(0.0471)\n",
      "4871 Training Loss: tensor(0.0471)\n",
      "4872 Training Loss: tensor(0.0473)\n",
      "4873 Training Loss: tensor(0.0473)\n",
      "4874 Training Loss: tensor(0.0472)\n",
      "4875 Training Loss: tensor(0.0473)\n",
      "4876 Training Loss: tensor(0.0474)\n",
      "4877 Training Loss: tensor(0.0468)\n",
      "4878 Training Loss: tensor(0.0473)\n",
      "4879 Training Loss: tensor(0.0470)\n",
      "4880 Training Loss: tensor(0.0471)\n",
      "4881 Training Loss: tensor(0.0472)\n",
      "4882 Training Loss: tensor(0.0474)\n",
      "4883 Training Loss: tensor(0.0471)\n",
      "4884 Training Loss: tensor(0.0473)\n",
      "4885 Training Loss: tensor(0.0474)\n",
      "4886 Training Loss: tensor(0.0473)\n",
      "4887 Training Loss: tensor(0.0472)\n",
      "4888 Training Loss: tensor(0.0474)\n",
      "4889 Training Loss: tensor(0.0472)\n",
      "4890 Training Loss: tensor(0.0472)\n",
      "4891 Training Loss: tensor(0.0470)\n",
      "4892 Training Loss: tensor(0.0471)\n",
      "4893 Training Loss: tensor(0.0472)\n",
      "4894 Training Loss: tensor(0.0471)\n",
      "4895 Training Loss: tensor(0.0475)\n",
      "4896 Training Loss: tensor(0.0473)\n",
      "4897 Training Loss: tensor(0.0474)\n",
      "4898 Training Loss: tensor(0.0471)\n",
      "4899 Training Loss: tensor(0.0473)\n",
      "4900 Training Loss: tensor(0.0473)\n",
      "4901 Training Loss: tensor(0.0472)\n",
      "4902 Training Loss: tensor(0.0473)\n",
      "4903 Training Loss: tensor(0.0472)\n",
      "4904 Training Loss: tensor(0.0471)\n",
      "4905 Training Loss: tensor(0.0472)\n",
      "4906 Training Loss: tensor(0.0471)\n",
      "4907 Training Loss: tensor(0.0472)\n",
      "4908 Training Loss: tensor(0.0474)\n",
      "4909 Training Loss: tensor(0.0470)\n",
      "4910 Training Loss: tensor(0.0473)\n",
      "4911 Training Loss: tensor(0.0474)\n",
      "4912 Training Loss: tensor(0.0470)\n",
      "4913 Training Loss: tensor(0.0473)\n",
      "4914 Training Loss: tensor(0.0472)\n",
      "4915 Training Loss: tensor(0.0473)\n",
      "4916 Training Loss: tensor(0.0473)\n",
      "4917 Training Loss: tensor(0.0473)\n",
      "4918 Training Loss: tensor(0.0471)\n",
      "4919 Training Loss: tensor(0.0473)\n",
      "4920 Training Loss: tensor(0.0476)\n",
      "4921 Training Loss: tensor(0.0473)\n",
      "4922 Training Loss: tensor(0.0471)\n",
      "4923 Training Loss: tensor(0.0471)\n",
      "4924 Training Loss: tensor(0.0471)\n",
      "4925 Training Loss: tensor(0.0473)\n",
      "4926 Training Loss: tensor(0.0471)\n",
      "4927 Training Loss: tensor(0.0473)\n",
      "4928 Training Loss: tensor(0.0471)\n",
      "4929 Training Loss: tensor(0.0473)\n",
      "4930 Training Loss: tensor(0.0472)\n",
      "4931 Training Loss: tensor(0.0473)\n",
      "4932 Training Loss: tensor(0.0472)\n",
      "4933 Training Loss: tensor(0.0470)\n",
      "4934 Training Loss: tensor(0.0475)\n",
      "4935 Training Loss: tensor(0.0471)\n",
      "4936 Training Loss: tensor(0.0473)\n",
      "4937 Training Loss: tensor(0.0469)\n",
      "4938 Training Loss: tensor(0.0470)\n",
      "4939 Training Loss: tensor(0.0471)\n",
      "4940 Training Loss: tensor(0.0472)\n",
      "4941 Training Loss: tensor(0.0470)\n",
      "4942 Training Loss: tensor(0.0467)\n",
      "4943 Training Loss: tensor(0.0472)\n",
      "4944 Training Loss: tensor(0.0471)\n",
      "4945 Training Loss: tensor(0.0471)\n",
      "4946 Training Loss: tensor(0.0471)\n",
      "4947 Training Loss: tensor(0.0472)\n",
      "4948 Training Loss: tensor(0.0473)\n",
      "4949 Training Loss: tensor(0.0472)\n",
      "4950 Training Loss: tensor(0.0473)\n",
      "4951 Training Loss: tensor(0.0472)\n",
      "4952 Training Loss: tensor(0.0471)\n",
      "4953 Training Loss: tensor(0.0469)\n",
      "4954 Training Loss: tensor(0.0470)\n",
      "4955 Training Loss: tensor(0.0473)\n",
      "4956 Training Loss: tensor(0.0472)\n",
      "4957 Training Loss: tensor(0.0472)\n",
      "4958 Training Loss: tensor(0.0472)\n",
      "4959 Training Loss: tensor(0.0471)\n",
      "4960 Training Loss: tensor(0.0471)\n",
      "4961 Training Loss: tensor(0.0480)\n",
      "4962 Training Loss: tensor(0.0475)\n",
      "4963 Training Loss: tensor(0.0476)\n",
      "4964 Training Loss: tensor(0.0470)\n",
      "4965 Training Loss: tensor(0.0471)\n",
      "4966 Training Loss: tensor(0.0471)\n",
      "4967 Training Loss: tensor(0.0473)\n",
      "4968 Training Loss: tensor(0.0471)\n",
      "4969 Training Loss: tensor(0.0473)\n",
      "4970 Training Loss: tensor(0.0471)\n",
      "4971 Training Loss: tensor(0.0471)\n",
      "4972 Training Loss: tensor(0.0468)\n",
      "4973 Training Loss: tensor(0.0473)\n",
      "4974 Training Loss: tensor(0.0470)\n",
      "4975 Training Loss: tensor(0.0471)\n",
      "4976 Training Loss: tensor(0.0473)\n",
      "4977 Training Loss: tensor(0.0473)\n",
      "4978 Training Loss: tensor(0.0470)\n",
      "4979 Training Loss: tensor(0.0471)\n",
      "4980 Training Loss: tensor(0.0474)\n",
      "4981 Training Loss: tensor(0.0471)\n",
      "4982 Training Loss: tensor(0.0472)\n",
      "4983 Training Loss: tensor(0.0474)\n",
      "4984 Training Loss: tensor(0.0471)\n",
      "4985 Training Loss: tensor(0.0472)\n",
      "4986 Training Loss: tensor(0.0473)\n",
      "4987 Training Loss: tensor(0.0470)\n",
      "4988 Training Loss: tensor(0.0471)\n",
      "4989 Training Loss: tensor(0.0472)\n",
      "4990 Training Loss: tensor(0.0470)\n",
      "4991 Training Loss: tensor(0.0473)\n",
      "4992 Training Loss: tensor(0.0474)\n",
      "4993 Training Loss: tensor(0.0471)\n",
      "4994 Training Loss: tensor(0.0472)\n",
      "4995 Training Loss: tensor(0.0474)\n",
      "4996 Training Loss: tensor(0.0474)\n",
      "4997 Training Loss: tensor(0.0474)\n",
      "4998 Training Loss: tensor(0.0470)\n",
      "4999 Training Loss: tensor(0.0471)\n",
      "5000 Training Loss: tensor(0.0470)\n",
      "5001 Training Loss: tensor(0.0471)\n",
      "5002 Training Loss: tensor(0.0470)\n",
      "5003 Training Loss: tensor(0.0474)\n",
      "5004 Training Loss: tensor(0.0475)\n",
      "5005 Training Loss: tensor(0.0472)\n",
      "5006 Training Loss: tensor(0.0472)\n",
      "5007 Training Loss: tensor(0.0472)\n",
      "5008 Training Loss: tensor(0.0473)\n",
      "5009 Training Loss: tensor(0.0471)\n",
      "5010 Training Loss: tensor(0.0469)\n",
      "5011 Training Loss: tensor(0.0468)\n",
      "5012 Training Loss: tensor(0.0473)\n",
      "5013 Training Loss: tensor(0.0472)\n",
      "5014 Training Loss: tensor(0.0472)\n",
      "5015 Training Loss: tensor(0.0475)\n",
      "5016 Training Loss: tensor(0.0469)\n",
      "5017 Training Loss: tensor(0.0474)\n",
      "5018 Training Loss: tensor(0.0472)\n",
      "5019 Training Loss: tensor(0.0470)\n",
      "5020 Training Loss: tensor(0.0472)\n",
      "5021 Training Loss: tensor(0.0469)\n",
      "5022 Training Loss: tensor(0.0472)\n",
      "5023 Training Loss: tensor(0.0470)\n",
      "5024 Training Loss: tensor(0.0473)\n",
      "5025 Training Loss: tensor(0.0468)\n",
      "5026 Training Loss: tensor(0.0474)\n",
      "5027 Training Loss: tensor(0.0472)\n",
      "5028 Training Loss: tensor(0.0471)\n",
      "5029 Training Loss: tensor(0.0466)\n",
      "5030 Training Loss: tensor(0.0470)\n",
      "5031 Training Loss: tensor(0.0473)\n",
      "5032 Training Loss: tensor(0.0469)\n",
      "5033 Training Loss: tensor(0.0474)\n",
      "5034 Training Loss: tensor(0.0471)\n",
      "5035 Training Loss: tensor(0.0476)\n",
      "5036 Training Loss: tensor(0.0471)\n",
      "5037 Training Loss: tensor(0.0470)\n",
      "5038 Training Loss: tensor(0.0472)\n",
      "5039 Training Loss: tensor(0.0474)\n",
      "5040 Training Loss: tensor(0.0471)\n",
      "5041 Training Loss: tensor(0.0471)\n",
      "5042 Training Loss: tensor(0.0468)\n",
      "5043 Training Loss: tensor(0.0474)\n",
      "5044 Training Loss: tensor(0.0469)\n",
      "5045 Training Loss: tensor(0.0472)\n",
      "5046 Training Loss: tensor(0.0471)\n",
      "5047 Training Loss: tensor(0.0471)\n",
      "5048 Training Loss: tensor(0.0470)\n",
      "5049 Training Loss: tensor(0.0470)\n",
      "5050 Training Loss: tensor(0.0471)\n",
      "5051 Training Loss: tensor(0.0472)\n",
      "5052 Training Loss: tensor(0.0474)\n",
      "5053 Training Loss: tensor(0.0469)\n",
      "5054 Training Loss: tensor(0.0470)\n",
      "5055 Training Loss: tensor(0.0469)\n",
      "5056 Training Loss: tensor(0.0472)\n",
      "5057 Training Loss: tensor(0.0470)\n",
      "5058 Training Loss: tensor(0.0474)\n",
      "5059 Training Loss: tensor(0.0468)\n",
      "5060 Training Loss: tensor(0.0469)\n",
      "5061 Training Loss: tensor(0.0472)\n",
      "5062 Training Loss: tensor(0.0473)\n",
      "5063 Training Loss: tensor(0.0471)\n",
      "5064 Training Loss: tensor(0.0468)\n",
      "5065 Training Loss: tensor(0.0472)\n",
      "5066 Training Loss: tensor(0.0471)\n",
      "5067 Training Loss: tensor(0.0470)\n",
      "5068 Training Loss: tensor(0.0471)\n",
      "5069 Training Loss: tensor(0.0472)\n",
      "5070 Training Loss: tensor(0.0468)\n",
      "5071 Training Loss: tensor(0.0472)\n",
      "5072 Training Loss: tensor(0.0469)\n",
      "5073 Training Loss: tensor(0.0473)\n",
      "5074 Training Loss: tensor(0.0473)\n",
      "5075 Training Loss: tensor(0.0471)\n",
      "5076 Training Loss: tensor(0.0470)\n",
      "5077 Training Loss: tensor(0.0472)\n",
      "5078 Training Loss: tensor(0.0470)\n",
      "5079 Training Loss: tensor(0.0468)\n",
      "5080 Training Loss: tensor(0.0473)\n",
      "5081 Training Loss: tensor(0.0472)\n",
      "5082 Training Loss: tensor(0.0472)\n",
      "5083 Training Loss: tensor(0.0468)\n",
      "5084 Training Loss: tensor(0.0469)\n",
      "5085 Training Loss: tensor(0.0471)\n",
      "5086 Training Loss: tensor(0.0472)\n",
      "5087 Training Loss: tensor(0.0470)\n",
      "5088 Training Loss: tensor(0.0471)\n",
      "5089 Training Loss: tensor(0.0468)\n",
      "5090 Training Loss: tensor(0.0469)\n",
      "5091 Training Loss: tensor(0.0472)\n",
      "5092 Training Loss: tensor(0.0472)\n",
      "5093 Training Loss: tensor(0.0473)\n",
      "5094 Training Loss: tensor(0.0472)\n",
      "5095 Training Loss: tensor(0.0470)\n",
      "5096 Training Loss: tensor(0.0472)\n",
      "5097 Training Loss: tensor(0.0470)\n",
      "5098 Training Loss: tensor(0.0470)\n",
      "5099 Training Loss: tensor(0.0470)\n",
      "5100 Training Loss: tensor(0.0471)\n",
      "5101 Training Loss: tensor(0.0473)\n",
      "5102 Training Loss: tensor(0.0469)\n",
      "5103 Training Loss: tensor(0.0466)\n",
      "5104 Training Loss: tensor(0.0471)\n",
      "5105 Training Loss: tensor(0.0469)\n",
      "5106 Training Loss: tensor(0.0471)\n",
      "5107 Training Loss: tensor(0.0469)\n",
      "5108 Training Loss: tensor(0.0471)\n",
      "5109 Training Loss: tensor(0.0467)\n",
      "5110 Training Loss: tensor(0.0471)\n",
      "5111 Training Loss: tensor(0.0468)\n",
      "5112 Training Loss: tensor(0.0472)\n",
      "5113 Training Loss: tensor(0.0472)\n",
      "5114 Training Loss: tensor(0.0468)\n",
      "5115 Training Loss: tensor(0.0470)\n",
      "5116 Training Loss: tensor(0.0473)\n",
      "5117 Training Loss: tensor(0.0470)\n",
      "5118 Training Loss: tensor(0.0470)\n",
      "5119 Training Loss: tensor(0.0469)\n",
      "5120 Training Loss: tensor(0.0471)\n",
      "5121 Training Loss: tensor(0.0471)\n",
      "5122 Training Loss: tensor(0.0471)\n",
      "5123 Training Loss: tensor(0.0468)\n",
      "5124 Training Loss: tensor(0.0471)\n",
      "5125 Training Loss: tensor(0.0472)\n",
      "5126 Training Loss: tensor(0.0469)\n",
      "5127 Training Loss: tensor(0.0471)\n",
      "5128 Training Loss: tensor(0.0469)\n",
      "5129 Training Loss: tensor(0.0470)\n",
      "5130 Training Loss: tensor(0.0468)\n",
      "5131 Training Loss: tensor(0.0469)\n",
      "5132 Training Loss: tensor(0.0473)\n",
      "5133 Training Loss: tensor(0.0470)\n",
      "5134 Training Loss: tensor(0.0471)\n",
      "5135 Training Loss: tensor(0.0472)\n",
      "5136 Training Loss: tensor(0.0472)\n",
      "5137 Training Loss: tensor(0.0467)\n",
      "5138 Training Loss: tensor(0.0470)\n",
      "5139 Training Loss: tensor(0.0471)\n",
      "5140 Training Loss: tensor(0.0468)\n",
      "5141 Training Loss: tensor(0.0470)\n",
      "5142 Training Loss: tensor(0.0470)\n",
      "5143 Training Loss: tensor(0.0469)\n",
      "5144 Training Loss: tensor(0.0476)\n",
      "5145 Training Loss: tensor(0.0468)\n",
      "5146 Training Loss: tensor(0.0468)\n",
      "5147 Training Loss: tensor(0.0469)\n",
      "5148 Training Loss: tensor(0.0468)\n",
      "5149 Training Loss: tensor(0.0471)\n",
      "5150 Training Loss: tensor(0.0471)\n",
      "5151 Training Loss: tensor(0.0471)\n",
      "5152 Training Loss: tensor(0.0470)\n",
      "5153 Training Loss: tensor(0.0471)\n",
      "5154 Training Loss: tensor(0.0470)\n",
      "5155 Training Loss: tensor(0.0470)\n",
      "5156 Training Loss: tensor(0.0469)\n",
      "5157 Training Loss: tensor(0.0469)\n",
      "5158 Training Loss: tensor(0.0470)\n",
      "5159 Training Loss: tensor(0.0470)\n",
      "5160 Training Loss: tensor(0.0472)\n",
      "5161 Training Loss: tensor(0.0470)\n",
      "5162 Training Loss: tensor(0.0472)\n",
      "5163 Training Loss: tensor(0.0474)\n",
      "5164 Training Loss: tensor(0.0472)\n",
      "5165 Training Loss: tensor(0.0468)\n",
      "5166 Training Loss: tensor(0.0471)\n",
      "5167 Training Loss: tensor(0.0473)\n",
      "5168 Training Loss: tensor(0.0473)\n",
      "5169 Training Loss: tensor(0.0472)\n",
      "5170 Training Loss: tensor(0.0471)\n",
      "5171 Training Loss: tensor(0.0468)\n",
      "5172 Training Loss: tensor(0.0469)\n",
      "5173 Training Loss: tensor(0.0472)\n",
      "5174 Training Loss: tensor(0.0466)\n",
      "5175 Training Loss: tensor(0.0471)\n",
      "5176 Training Loss: tensor(0.0469)\n",
      "5177 Training Loss: tensor(0.0468)\n",
      "5178 Training Loss: tensor(0.0471)\n",
      "5179 Training Loss: tensor(0.0470)\n",
      "5180 Training Loss: tensor(0.0471)\n",
      "5181 Training Loss: tensor(0.0471)\n",
      "5182 Training Loss: tensor(0.0471)\n",
      "5183 Training Loss: tensor(0.0468)\n",
      "5184 Training Loss: tensor(0.0469)\n",
      "5185 Training Loss: tensor(0.0469)\n",
      "5186 Training Loss: tensor(0.0468)\n",
      "5187 Training Loss: tensor(0.0470)\n",
      "5188 Training Loss: tensor(0.0470)\n",
      "5189 Training Loss: tensor(0.0472)\n",
      "5190 Training Loss: tensor(0.0469)\n",
      "5191 Training Loss: tensor(0.0469)\n",
      "5192 Training Loss: tensor(0.0473)\n",
      "5193 Training Loss: tensor(0.0469)\n",
      "5194 Training Loss: tensor(0.0472)\n",
      "5195 Training Loss: tensor(0.0469)\n",
      "5196 Training Loss: tensor(0.0468)\n",
      "5197 Training Loss: tensor(0.0473)\n",
      "5198 Training Loss: tensor(0.0471)\n",
      "5199 Training Loss: tensor(0.0469)\n",
      "5200 Training Loss: tensor(0.0475)\n",
      "5201 Training Loss: tensor(0.0471)\n",
      "5202 Training Loss: tensor(0.0472)\n",
      "5203 Training Loss: tensor(0.0469)\n",
      "5204 Training Loss: tensor(0.0469)\n",
      "5205 Training Loss: tensor(0.0470)\n",
      "5206 Training Loss: tensor(0.0471)\n",
      "5207 Training Loss: tensor(0.0469)\n",
      "5208 Training Loss: tensor(0.0469)\n",
      "5209 Training Loss: tensor(0.0468)\n",
      "5210 Training Loss: tensor(0.0468)\n",
      "5211 Training Loss: tensor(0.0475)\n",
      "5212 Training Loss: tensor(0.0468)\n",
      "5213 Training Loss: tensor(0.0468)\n",
      "5214 Training Loss: tensor(0.0473)\n",
      "5215 Training Loss: tensor(0.0468)\n",
      "5216 Training Loss: tensor(0.0471)\n",
      "5217 Training Loss: tensor(0.0468)\n",
      "5218 Training Loss: tensor(0.0468)\n",
      "5219 Training Loss: tensor(0.0466)\n",
      "5220 Training Loss: tensor(0.0469)\n",
      "5221 Training Loss: tensor(0.0470)\n",
      "5222 Training Loss: tensor(0.0469)\n",
      "5223 Training Loss: tensor(0.0470)\n",
      "5224 Training Loss: tensor(0.0469)\n",
      "5225 Training Loss: tensor(0.0466)\n",
      "5226 Training Loss: tensor(0.0470)\n",
      "5227 Training Loss: tensor(0.0465)\n",
      "5228 Training Loss: tensor(0.0469)\n",
      "5229 Training Loss: tensor(0.0469)\n",
      "5230 Training Loss: tensor(0.0469)\n",
      "5231 Training Loss: tensor(0.0469)\n",
      "5232 Training Loss: tensor(0.0473)\n",
      "5233 Training Loss: tensor(0.0473)\n",
      "5234 Training Loss: tensor(0.0473)\n",
      "5235 Training Loss: tensor(0.0468)\n",
      "5236 Training Loss: tensor(0.0469)\n",
      "5237 Training Loss: tensor(0.0469)\n",
      "5238 Training Loss: tensor(0.0469)\n",
      "5239 Training Loss: tensor(0.0469)\n",
      "5240 Training Loss: tensor(0.0468)\n",
      "5241 Training Loss: tensor(0.0468)\n",
      "5242 Training Loss: tensor(0.0468)\n",
      "5243 Training Loss: tensor(0.0470)\n",
      "5244 Training Loss: tensor(0.0469)\n",
      "5245 Training Loss: tensor(0.0472)\n",
      "5246 Training Loss: tensor(0.0468)\n",
      "5247 Training Loss: tensor(0.0468)\n",
      "5248 Training Loss: tensor(0.0468)\n",
      "5249 Training Loss: tensor(0.0471)\n",
      "5250 Training Loss: tensor(0.0468)\n",
      "5251 Training Loss: tensor(0.0468)\n",
      "5252 Training Loss: tensor(0.0472)\n",
      "5253 Training Loss: tensor(0.0467)\n",
      "5254 Training Loss: tensor(0.0468)\n",
      "5255 Training Loss: tensor(0.0470)\n",
      "5256 Training Loss: tensor(0.0471)\n",
      "5257 Training Loss: tensor(0.0470)\n",
      "5258 Training Loss: tensor(0.0468)\n",
      "5259 Training Loss: tensor(0.0470)\n",
      "5260 Training Loss: tensor(0.0473)\n",
      "5261 Training Loss: tensor(0.0469)\n",
      "5262 Training Loss: tensor(0.0468)\n",
      "5263 Training Loss: tensor(0.0474)\n",
      "5264 Training Loss: tensor(0.0472)\n",
      "5265 Training Loss: tensor(0.0470)\n",
      "5266 Training Loss: tensor(0.0469)\n",
      "5267 Training Loss: tensor(0.0470)\n",
      "5268 Training Loss: tensor(0.0468)\n",
      "5269 Training Loss: tensor(0.0466)\n",
      "5270 Training Loss: tensor(0.0469)\n",
      "5271 Training Loss: tensor(0.0467)\n",
      "5272 Training Loss: tensor(0.0469)\n",
      "5273 Training Loss: tensor(0.0469)\n",
      "5274 Training Loss: tensor(0.0469)\n",
      "5275 Training Loss: tensor(0.0466)\n",
      "5276 Training Loss: tensor(0.0470)\n",
      "5277 Training Loss: tensor(0.0467)\n",
      "5278 Training Loss: tensor(0.0468)\n",
      "5279 Training Loss: tensor(0.0468)\n",
      "5280 Training Loss: tensor(0.0471)\n",
      "5281 Training Loss: tensor(0.0471)\n",
      "5282 Training Loss: tensor(0.0469)\n",
      "5283 Training Loss: tensor(0.0470)\n",
      "5284 Training Loss: tensor(0.0470)\n",
      "5285 Training Loss: tensor(0.0472)\n",
      "5286 Training Loss: tensor(0.0470)\n",
      "5287 Training Loss: tensor(0.0470)\n",
      "5288 Training Loss: tensor(0.0469)\n",
      "5289 Training Loss: tensor(0.0467)\n",
      "5290 Training Loss: tensor(0.0467)\n",
      "5291 Training Loss: tensor(0.0471)\n",
      "5292 Training Loss: tensor(0.0471)\n",
      "5293 Training Loss: tensor(0.0466)\n",
      "5294 Training Loss: tensor(0.0471)\n",
      "5295 Training Loss: tensor(0.0467)\n",
      "5296 Training Loss: tensor(0.0471)\n",
      "5297 Training Loss: tensor(0.0469)\n",
      "5298 Training Loss: tensor(0.0468)\n",
      "5299 Training Loss: tensor(0.0470)\n",
      "5300 Training Loss: tensor(0.0466)\n",
      "5301 Training Loss: tensor(0.0470)\n",
      "5302 Training Loss: tensor(0.0466)\n",
      "5303 Training Loss: tensor(0.0468)\n",
      "5304 Training Loss: tensor(0.0468)\n",
      "5305 Training Loss: tensor(0.0470)\n",
      "5306 Training Loss: tensor(0.0468)\n",
      "5307 Training Loss: tensor(0.0469)\n",
      "5308 Training Loss: tensor(0.0468)\n",
      "5309 Training Loss: tensor(0.0470)\n",
      "5310 Training Loss: tensor(0.0467)\n",
      "5311 Training Loss: tensor(0.0471)\n",
      "5312 Training Loss: tensor(0.0471)\n",
      "5313 Training Loss: tensor(0.0473)\n",
      "5314 Training Loss: tensor(0.0468)\n",
      "5315 Training Loss: tensor(0.0471)\n",
      "5316 Training Loss: tensor(0.0470)\n",
      "5317 Training Loss: tensor(0.0471)\n",
      "5318 Training Loss: tensor(0.0468)\n",
      "5319 Training Loss: tensor(0.0468)\n",
      "5320 Training Loss: tensor(0.0469)\n",
      "5321 Training Loss: tensor(0.0467)\n",
      "5322 Training Loss: tensor(0.0470)\n",
      "5323 Training Loss: tensor(0.0470)\n",
      "5324 Training Loss: tensor(0.0467)\n",
      "5325 Training Loss: tensor(0.0466)\n",
      "5326 Training Loss: tensor(0.0469)\n",
      "5327 Training Loss: tensor(0.0469)\n",
      "5328 Training Loss: tensor(0.0468)\n",
      "5329 Training Loss: tensor(0.0469)\n",
      "5330 Training Loss: tensor(0.0468)\n",
      "5331 Training Loss: tensor(0.0468)\n",
      "5332 Training Loss: tensor(0.0467)\n",
      "5333 Training Loss: tensor(0.0467)\n",
      "5334 Training Loss: tensor(0.0469)\n",
      "5335 Training Loss: tensor(0.0468)\n",
      "5336 Training Loss: tensor(0.0465)\n",
      "5337 Training Loss: tensor(0.0466)\n",
      "5338 Training Loss: tensor(0.0472)\n",
      "5339 Training Loss: tensor(0.0467)\n",
      "5340 Training Loss: tensor(0.0465)\n",
      "5341 Training Loss: tensor(0.0467)\n",
      "5342 Training Loss: tensor(0.0466)\n",
      "5343 Training Loss: tensor(0.0470)\n",
      "5344 Training Loss: tensor(0.0466)\n",
      "5345 Training Loss: tensor(0.0465)\n",
      "5346 Training Loss: tensor(0.0469)\n",
      "5347 Training Loss: tensor(0.0471)\n",
      "5348 Training Loss: tensor(0.0472)\n",
      "5349 Training Loss: tensor(0.0469)\n",
      "5350 Training Loss: tensor(0.0469)\n",
      "5351 Training Loss: tensor(0.0467)\n",
      "5352 Training Loss: tensor(0.0470)\n",
      "5353 Training Loss: tensor(0.0469)\n",
      "5354 Training Loss: tensor(0.0467)\n",
      "5355 Training Loss: tensor(0.0466)\n",
      "5356 Training Loss: tensor(0.0467)\n",
      "5357 Training Loss: tensor(0.0466)\n",
      "5358 Training Loss: tensor(0.0468)\n",
      "5359 Training Loss: tensor(0.0466)\n",
      "5360 Training Loss: tensor(0.0468)\n",
      "5361 Training Loss: tensor(0.0467)\n",
      "5362 Training Loss: tensor(0.0467)\n",
      "5363 Training Loss: tensor(0.0468)\n",
      "5364 Training Loss: tensor(0.0470)\n",
      "5365 Training Loss: tensor(0.0468)\n",
      "5366 Training Loss: tensor(0.0467)\n",
      "5367 Training Loss: tensor(0.0472)\n",
      "5368 Training Loss: tensor(0.0465)\n",
      "5369 Training Loss: tensor(0.0472)\n",
      "5370 Training Loss: tensor(0.0469)\n",
      "5371 Training Loss: tensor(0.0468)\n",
      "5372 Training Loss: tensor(0.0469)\n",
      "5373 Training Loss: tensor(0.0467)\n",
      "5374 Training Loss: tensor(0.0473)\n",
      "5375 Training Loss: tensor(0.0467)\n",
      "5376 Training Loss: tensor(0.0470)\n",
      "5377 Training Loss: tensor(0.0470)\n",
      "5378 Training Loss: tensor(0.0472)\n",
      "5379 Training Loss: tensor(0.0467)\n",
      "5380 Training Loss: tensor(0.0467)\n",
      "5381 Training Loss: tensor(0.0470)\n",
      "5382 Training Loss: tensor(0.0467)\n",
      "5383 Training Loss: tensor(0.0469)\n",
      "5384 Training Loss: tensor(0.0469)\n",
      "5385 Training Loss: tensor(0.0466)\n",
      "5386 Training Loss: tensor(0.0470)\n",
      "5387 Training Loss: tensor(0.0468)\n",
      "5388 Training Loss: tensor(0.0467)\n",
      "5389 Training Loss: tensor(0.0469)\n",
      "5390 Training Loss: tensor(0.0470)\n",
      "5391 Training Loss: tensor(0.0466)\n",
      "5392 Training Loss: tensor(0.0468)\n",
      "5393 Training Loss: tensor(0.0466)\n",
      "5394 Training Loss: tensor(0.0469)\n",
      "5395 Training Loss: tensor(0.0472)\n",
      "5396 Training Loss: tensor(0.0468)\n",
      "5397 Training Loss: tensor(0.0468)\n",
      "5398 Training Loss: tensor(0.0469)\n",
      "5399 Training Loss: tensor(0.0467)\n",
      "5400 Training Loss: tensor(0.0465)\n",
      "5401 Training Loss: tensor(0.0472)\n",
      "5402 Training Loss: tensor(0.0465)\n",
      "5403 Training Loss: tensor(0.0470)\n",
      "5404 Training Loss: tensor(0.0467)\n",
      "5405 Training Loss: tensor(0.0468)\n",
      "5406 Training Loss: tensor(0.0469)\n",
      "5407 Training Loss: tensor(0.0468)\n",
      "5408 Training Loss: tensor(0.0470)\n",
      "5409 Training Loss: tensor(0.0468)\n",
      "5410 Training Loss: tensor(0.0470)\n",
      "5411 Training Loss: tensor(0.0470)\n",
      "5412 Training Loss: tensor(0.0469)\n",
      "5413 Training Loss: tensor(0.0467)\n",
      "5414 Training Loss: tensor(0.0465)\n",
      "5415 Training Loss: tensor(0.0466)\n",
      "5416 Training Loss: tensor(0.0469)\n",
      "5417 Training Loss: tensor(0.0467)\n",
      "5418 Training Loss: tensor(0.0466)\n",
      "5419 Training Loss: tensor(0.0470)\n",
      "5420 Training Loss: tensor(0.0471)\n",
      "5421 Training Loss: tensor(0.0466)\n",
      "5422 Training Loss: tensor(0.0469)\n",
      "5423 Training Loss: tensor(0.0467)\n",
      "5424 Training Loss: tensor(0.0472)\n",
      "5425 Training Loss: tensor(0.0467)\n",
      "5426 Training Loss: tensor(0.0469)\n",
      "5427 Training Loss: tensor(0.0469)\n",
      "5428 Training Loss: tensor(0.0469)\n",
      "5429 Training Loss: tensor(0.0469)\n",
      "5430 Training Loss: tensor(0.0468)\n",
      "5431 Training Loss: tensor(0.0467)\n",
      "5432 Training Loss: tensor(0.0471)\n",
      "5433 Training Loss: tensor(0.0468)\n",
      "5434 Training Loss: tensor(0.0467)\n",
      "5435 Training Loss: tensor(0.0466)\n",
      "5436 Training Loss: tensor(0.0467)\n",
      "5437 Training Loss: tensor(0.0465)\n",
      "5438 Training Loss: tensor(0.0470)\n",
      "5439 Training Loss: tensor(0.0467)\n",
      "5440 Training Loss: tensor(0.0466)\n",
      "5441 Training Loss: tensor(0.0468)\n",
      "5442 Training Loss: tensor(0.0468)\n",
      "5443 Training Loss: tensor(0.0466)\n",
      "5444 Training Loss: tensor(0.0465)\n",
      "5445 Training Loss: tensor(0.0468)\n",
      "5446 Training Loss: tensor(0.0465)\n",
      "5447 Training Loss: tensor(0.0469)\n",
      "5448 Training Loss: tensor(0.0467)\n",
      "5449 Training Loss: tensor(0.0469)\n",
      "5450 Training Loss: tensor(0.0468)\n",
      "5451 Training Loss: tensor(0.0467)\n",
      "5452 Training Loss: tensor(0.0466)\n",
      "5453 Training Loss: tensor(0.0468)\n",
      "5454 Training Loss: tensor(0.0466)\n",
      "5455 Training Loss: tensor(0.0466)\n",
      "5456 Training Loss: tensor(0.0468)\n",
      "5457 Training Loss: tensor(0.0465)\n",
      "5458 Training Loss: tensor(0.0467)\n",
      "5459 Training Loss: tensor(0.0465)\n",
      "5460 Training Loss: tensor(0.0467)\n",
      "5461 Training Loss: tensor(0.0468)\n",
      "5462 Training Loss: tensor(0.0464)\n",
      "5463 Training Loss: tensor(0.0467)\n",
      "5464 Training Loss: tensor(0.0468)\n",
      "5465 Training Loss: tensor(0.0469)\n",
      "5466 Training Loss: tensor(0.0471)\n",
      "5467 Training Loss: tensor(0.0467)\n",
      "5468 Training Loss: tensor(0.0468)\n",
      "5469 Training Loss: tensor(0.0465)\n",
      "5470 Training Loss: tensor(0.0467)\n",
      "5471 Training Loss: tensor(0.0468)\n",
      "5472 Training Loss: tensor(0.0466)\n",
      "5473 Training Loss: tensor(0.0466)\n",
      "5474 Training Loss: tensor(0.0464)\n",
      "5475 Training Loss: tensor(0.0466)\n",
      "5476 Training Loss: tensor(0.0469)\n",
      "5477 Training Loss: tensor(0.0469)\n",
      "5478 Training Loss: tensor(0.0466)\n",
      "5479 Training Loss: tensor(0.0468)\n",
      "5480 Training Loss: tensor(0.0469)\n",
      "5481 Training Loss: tensor(0.0468)\n",
      "5482 Training Loss: tensor(0.0466)\n",
      "5483 Training Loss: tensor(0.0467)\n",
      "5484 Training Loss: tensor(0.0468)\n",
      "5485 Training Loss: tensor(0.0465)\n",
      "5486 Training Loss: tensor(0.0468)\n",
      "5487 Training Loss: tensor(0.0467)\n",
      "5488 Training Loss: tensor(0.0468)\n",
      "5489 Training Loss: tensor(0.0467)\n",
      "5490 Training Loss: tensor(0.0467)\n",
      "5491 Training Loss: tensor(0.0468)\n",
      "5492 Training Loss: tensor(0.0469)\n",
      "5493 Training Loss: tensor(0.0466)\n",
      "5494 Training Loss: tensor(0.0471)\n",
      "5495 Training Loss: tensor(0.0468)\n",
      "5496 Training Loss: tensor(0.0468)\n",
      "5497 Training Loss: tensor(0.0465)\n",
      "5498 Training Loss: tensor(0.0466)\n",
      "5499 Training Loss: tensor(0.0465)\n",
      "5500 Training Loss: tensor(0.0464)\n",
      "5501 Training Loss: tensor(0.0469)\n",
      "5502 Training Loss: tensor(0.0470)\n",
      "5503 Training Loss: tensor(0.0468)\n",
      "5504 Training Loss: tensor(0.0466)\n",
      "5505 Training Loss: tensor(0.0468)\n",
      "5506 Training Loss: tensor(0.0467)\n",
      "5507 Training Loss: tensor(0.0465)\n",
      "5508 Training Loss: tensor(0.0466)\n",
      "5509 Training Loss: tensor(0.0467)\n",
      "5510 Training Loss: tensor(0.0467)\n",
      "5511 Training Loss: tensor(0.0468)\n",
      "5512 Training Loss: tensor(0.0467)\n",
      "5513 Training Loss: tensor(0.0466)\n",
      "5514 Training Loss: tensor(0.0468)\n",
      "5515 Training Loss: tensor(0.0467)\n",
      "5516 Training Loss: tensor(0.0468)\n",
      "5517 Training Loss: tensor(0.0467)\n",
      "5518 Training Loss: tensor(0.0465)\n",
      "5519 Training Loss: tensor(0.0466)\n",
      "5520 Training Loss: tensor(0.0466)\n",
      "5521 Training Loss: tensor(0.0466)\n",
      "5522 Training Loss: tensor(0.0469)\n",
      "5523 Training Loss: tensor(0.0466)\n",
      "5524 Training Loss: tensor(0.0468)\n",
      "5525 Training Loss: tensor(0.0467)\n",
      "5526 Training Loss: tensor(0.0466)\n",
      "5527 Training Loss: tensor(0.0469)\n",
      "5528 Training Loss: tensor(0.0470)\n",
      "5529 Training Loss: tensor(0.0465)\n",
      "5530 Training Loss: tensor(0.0466)\n",
      "5531 Training Loss: tensor(0.0464)\n",
      "5532 Training Loss: tensor(0.0469)\n",
      "5533 Training Loss: tensor(0.0466)\n",
      "5534 Training Loss: tensor(0.0466)\n",
      "5535 Training Loss: tensor(0.0467)\n",
      "5536 Training Loss: tensor(0.0466)\n",
      "5537 Training Loss: tensor(0.0465)\n",
      "5538 Training Loss: tensor(0.0466)\n",
      "5539 Training Loss: tensor(0.0466)\n",
      "5540 Training Loss: tensor(0.0468)\n",
      "5541 Training Loss: tensor(0.0467)\n",
      "5542 Training Loss: tensor(0.0468)\n",
      "5543 Training Loss: tensor(0.0466)\n",
      "5544 Training Loss: tensor(0.0466)\n",
      "5545 Training Loss: tensor(0.0467)\n",
      "5546 Training Loss: tensor(0.0467)\n",
      "5547 Training Loss: tensor(0.0466)\n",
      "5548 Training Loss: tensor(0.0465)\n",
      "5549 Training Loss: tensor(0.0463)\n",
      "5550 Training Loss: tensor(0.0467)\n",
      "5551 Training Loss: tensor(0.0466)\n",
      "5552 Training Loss: tensor(0.0471)\n",
      "5553 Training Loss: tensor(0.0466)\n",
      "5554 Training Loss: tensor(0.0465)\n",
      "5555 Training Loss: tensor(0.0466)\n",
      "5556 Training Loss: tensor(0.0465)\n",
      "5557 Training Loss: tensor(0.0468)\n",
      "5558 Training Loss: tensor(0.0469)\n",
      "5559 Training Loss: tensor(0.0468)\n",
      "5560 Training Loss: tensor(0.0469)\n",
      "5561 Training Loss: tensor(0.0465)\n",
      "5562 Training Loss: tensor(0.0466)\n",
      "5563 Training Loss: tensor(0.0466)\n",
      "5564 Training Loss: tensor(0.0470)\n",
      "5565 Training Loss: tensor(0.0467)\n",
      "5566 Training Loss: tensor(0.0465)\n",
      "5567 Training Loss: tensor(0.0467)\n",
      "5568 Training Loss: tensor(0.0465)\n",
      "5569 Training Loss: tensor(0.0467)\n",
      "5570 Training Loss: tensor(0.0467)\n",
      "5571 Training Loss: tensor(0.0468)\n",
      "5572 Training Loss: tensor(0.0466)\n",
      "5573 Training Loss: tensor(0.0463)\n",
      "5574 Training Loss: tensor(0.0467)\n",
      "5575 Training Loss: tensor(0.0466)\n",
      "5576 Training Loss: tensor(0.0467)\n",
      "5577 Training Loss: tensor(0.0466)\n",
      "5578 Training Loss: tensor(0.0465)\n",
      "5579 Training Loss: tensor(0.0464)\n",
      "5580 Training Loss: tensor(0.0466)\n",
      "5581 Training Loss: tensor(0.0466)\n",
      "5582 Training Loss: tensor(0.0467)\n",
      "5583 Training Loss: tensor(0.0465)\n",
      "5584 Training Loss: tensor(0.0466)\n",
      "5585 Training Loss: tensor(0.0464)\n",
      "5586 Training Loss: tensor(0.0467)\n",
      "5587 Training Loss: tensor(0.0468)\n",
      "5588 Training Loss: tensor(0.0468)\n",
      "5589 Training Loss: tensor(0.0465)\n",
      "5590 Training Loss: tensor(0.0464)\n",
      "5591 Training Loss: tensor(0.0467)\n",
      "5592 Training Loss: tensor(0.0468)\n",
      "5593 Training Loss: tensor(0.0465)\n",
      "5594 Training Loss: tensor(0.0465)\n",
      "5595 Training Loss: tensor(0.0467)\n",
      "5596 Training Loss: tensor(0.0469)\n",
      "5597 Training Loss: tensor(0.0466)\n",
      "5598 Training Loss: tensor(0.0467)\n",
      "5599 Training Loss: tensor(0.0468)\n",
      "5600 Training Loss: tensor(0.0467)\n",
      "5601 Training Loss: tensor(0.0468)\n",
      "5602 Training Loss: tensor(0.0463)\n",
      "5603 Training Loss: tensor(0.0466)\n",
      "5604 Training Loss: tensor(0.0467)\n",
      "5605 Training Loss: tensor(0.0466)\n",
      "5606 Training Loss: tensor(0.0467)\n",
      "5607 Training Loss: tensor(0.0464)\n",
      "5608 Training Loss: tensor(0.0466)\n",
      "5609 Training Loss: tensor(0.0465)\n",
      "5610 Training Loss: tensor(0.0467)\n",
      "5611 Training Loss: tensor(0.0467)\n",
      "5612 Training Loss: tensor(0.0464)\n",
      "5613 Training Loss: tensor(0.0465)\n",
      "5614 Training Loss: tensor(0.0464)\n",
      "5615 Training Loss: tensor(0.0469)\n",
      "5616 Training Loss: tensor(0.0466)\n",
      "5617 Training Loss: tensor(0.0467)\n",
      "5618 Training Loss: tensor(0.0465)\n",
      "5619 Training Loss: tensor(0.0464)\n",
      "5620 Training Loss: tensor(0.0469)\n",
      "5621 Training Loss: tensor(0.0469)\n",
      "5622 Training Loss: tensor(0.0467)\n",
      "5623 Training Loss: tensor(0.0467)\n",
      "5624 Training Loss: tensor(0.0467)\n",
      "5625 Training Loss: tensor(0.0465)\n",
      "5626 Training Loss: tensor(0.0466)\n",
      "5627 Training Loss: tensor(0.0465)\n",
      "5628 Training Loss: tensor(0.0469)\n",
      "5629 Training Loss: tensor(0.0466)\n",
      "5630 Training Loss: tensor(0.0466)\n",
      "5631 Training Loss: tensor(0.0465)\n",
      "5632 Training Loss: tensor(0.0467)\n",
      "5633 Training Loss: tensor(0.0463)\n",
      "5634 Training Loss: tensor(0.0467)\n",
      "5635 Training Loss: tensor(0.0464)\n",
      "5636 Training Loss: tensor(0.0466)\n",
      "5637 Training Loss: tensor(0.0464)\n",
      "5638 Training Loss: tensor(0.0468)\n",
      "5639 Training Loss: tensor(0.0465)\n",
      "5640 Training Loss: tensor(0.0465)\n",
      "5641 Training Loss: tensor(0.0464)\n",
      "5642 Training Loss: tensor(0.0467)\n",
      "5643 Training Loss: tensor(0.0465)\n",
      "5644 Training Loss: tensor(0.0467)\n",
      "5645 Training Loss: tensor(0.0468)\n",
      "5646 Training Loss: tensor(0.0463)\n",
      "5647 Training Loss: tensor(0.0464)\n",
      "5648 Training Loss: tensor(0.0464)\n",
      "5649 Training Loss: tensor(0.0467)\n",
      "5650 Training Loss: tensor(0.0467)\n",
      "5651 Training Loss: tensor(0.0467)\n",
      "5652 Training Loss: tensor(0.0466)\n",
      "5653 Training Loss: tensor(0.0467)\n",
      "5654 Training Loss: tensor(0.0468)\n",
      "5655 Training Loss: tensor(0.0466)\n",
      "5656 Training Loss: tensor(0.0466)\n",
      "5657 Training Loss: tensor(0.0464)\n",
      "5658 Training Loss: tensor(0.0464)\n",
      "5659 Training Loss: tensor(0.0467)\n",
      "5660 Training Loss: tensor(0.0469)\n",
      "5661 Training Loss: tensor(0.0468)\n",
      "5662 Training Loss: tensor(0.0466)\n",
      "5663 Training Loss: tensor(0.0466)\n",
      "5664 Training Loss: tensor(0.0465)\n",
      "5665 Training Loss: tensor(0.0464)\n",
      "5666 Training Loss: tensor(0.0464)\n",
      "5667 Training Loss: tensor(0.0464)\n",
      "5668 Training Loss: tensor(0.0467)\n",
      "5669 Training Loss: tensor(0.0466)\n",
      "5670 Training Loss: tensor(0.0462)\n",
      "5671 Training Loss: tensor(0.0467)\n",
      "5672 Training Loss: tensor(0.0467)\n",
      "5673 Training Loss: tensor(0.0466)\n",
      "5674 Training Loss: tensor(0.0465)\n",
      "5675 Training Loss: tensor(0.0464)\n",
      "5676 Training Loss: tensor(0.0465)\n",
      "5677 Training Loss: tensor(0.0466)\n",
      "5678 Training Loss: tensor(0.0470)\n",
      "5679 Training Loss: tensor(0.0468)\n",
      "5680 Training Loss: tensor(0.0463)\n",
      "5681 Training Loss: tensor(0.0463)\n",
      "5682 Training Loss: tensor(0.0466)\n",
      "5683 Training Loss: tensor(0.0466)\n",
      "5684 Training Loss: tensor(0.0465)\n",
      "5685 Training Loss: tensor(0.0465)\n",
      "5686 Training Loss: tensor(0.0464)\n",
      "5687 Training Loss: tensor(0.0465)\n",
      "5688 Training Loss: tensor(0.0467)\n",
      "5689 Training Loss: tensor(0.0467)\n",
      "5690 Training Loss: tensor(0.0469)\n",
      "5691 Training Loss: tensor(0.0466)\n",
      "5692 Training Loss: tensor(0.0463)\n",
      "5693 Training Loss: tensor(0.0465)\n",
      "5694 Training Loss: tensor(0.0464)\n",
      "5695 Training Loss: tensor(0.0465)\n",
      "5696 Training Loss: tensor(0.0464)\n",
      "5697 Training Loss: tensor(0.0465)\n",
      "5698 Training Loss: tensor(0.0464)\n",
      "5699 Training Loss: tensor(0.0463)\n",
      "5700 Training Loss: tensor(0.0466)\n",
      "5701 Training Loss: tensor(0.0465)\n",
      "5702 Training Loss: tensor(0.0465)\n",
      "5703 Training Loss: tensor(0.0466)\n",
      "5704 Training Loss: tensor(0.0464)\n",
      "5705 Training Loss: tensor(0.0466)\n",
      "5706 Training Loss: tensor(0.0465)\n",
      "5707 Training Loss: tensor(0.0468)\n",
      "5708 Training Loss: tensor(0.0466)\n",
      "5709 Training Loss: tensor(0.0468)\n",
      "5710 Training Loss: tensor(0.0466)\n",
      "5711 Training Loss: tensor(0.0463)\n",
      "5712 Training Loss: tensor(0.0465)\n",
      "5713 Training Loss: tensor(0.0463)\n",
      "5714 Training Loss: tensor(0.0466)\n",
      "5715 Training Loss: tensor(0.0462)\n",
      "5716 Training Loss: tensor(0.0469)\n",
      "5717 Training Loss: tensor(0.0462)\n",
      "5718 Training Loss: tensor(0.0463)\n",
      "5719 Training Loss: tensor(0.0465)\n",
      "5720 Training Loss: tensor(0.0466)\n",
      "5721 Training Loss: tensor(0.0464)\n",
      "5722 Training Loss: tensor(0.0463)\n",
      "5723 Training Loss: tensor(0.0463)\n",
      "5724 Training Loss: tensor(0.0465)\n",
      "5725 Training Loss: tensor(0.0464)\n",
      "5726 Training Loss: tensor(0.0466)\n",
      "5727 Training Loss: tensor(0.0465)\n",
      "5728 Training Loss: tensor(0.0464)\n",
      "5729 Training Loss: tensor(0.0464)\n",
      "5730 Training Loss: tensor(0.0465)\n",
      "5731 Training Loss: tensor(0.0467)\n",
      "5732 Training Loss: tensor(0.0465)\n",
      "5733 Training Loss: tensor(0.0465)\n",
      "5734 Training Loss: tensor(0.0465)\n",
      "5735 Training Loss: tensor(0.0467)\n",
      "5736 Training Loss: tensor(0.0463)\n",
      "5737 Training Loss: tensor(0.0465)\n",
      "5738 Training Loss: tensor(0.0464)\n",
      "5739 Training Loss: tensor(0.0464)\n",
      "5740 Training Loss: tensor(0.0465)\n",
      "5741 Training Loss: tensor(0.0465)\n",
      "5742 Training Loss: tensor(0.0464)\n",
      "5743 Training Loss: tensor(0.0466)\n",
      "5744 Training Loss: tensor(0.0466)\n",
      "5745 Training Loss: tensor(0.0465)\n",
      "5746 Training Loss: tensor(0.0463)\n",
      "5747 Training Loss: tensor(0.0462)\n",
      "5748 Training Loss: tensor(0.0468)\n",
      "5749 Training Loss: tensor(0.0463)\n",
      "5750 Training Loss: tensor(0.0465)\n",
      "5751 Training Loss: tensor(0.0465)\n",
      "5752 Training Loss: tensor(0.0468)\n",
      "5753 Training Loss: tensor(0.0464)\n",
      "5754 Training Loss: tensor(0.0465)\n",
      "5755 Training Loss: tensor(0.0464)\n",
      "5756 Training Loss: tensor(0.0463)\n",
      "5757 Training Loss: tensor(0.0466)\n",
      "5758 Training Loss: tensor(0.0464)\n",
      "5759 Training Loss: tensor(0.0465)\n",
      "5760 Training Loss: tensor(0.0467)\n",
      "5761 Training Loss: tensor(0.0469)\n",
      "5762 Training Loss: tensor(0.0463)\n",
      "5763 Training Loss: tensor(0.0463)\n",
      "5764 Training Loss: tensor(0.0466)\n",
      "5765 Training Loss: tensor(0.0463)\n",
      "5766 Training Loss: tensor(0.0464)\n",
      "5767 Training Loss: tensor(0.0464)\n",
      "5768 Training Loss: tensor(0.0466)\n",
      "5769 Training Loss: tensor(0.0460)\n",
      "5770 Training Loss: tensor(0.0463)\n",
      "5771 Training Loss: tensor(0.0466)\n",
      "5772 Training Loss: tensor(0.0465)\n",
      "5773 Training Loss: tensor(0.0466)\n",
      "5774 Training Loss: tensor(0.0467)\n",
      "5775 Training Loss: tensor(0.0464)\n",
      "5776 Training Loss: tensor(0.0462)\n",
      "5777 Training Loss: tensor(0.0464)\n",
      "5778 Training Loss: tensor(0.0464)\n",
      "5779 Training Loss: tensor(0.0463)\n",
      "5780 Training Loss: tensor(0.0466)\n",
      "5781 Training Loss: tensor(0.0465)\n",
      "5782 Training Loss: tensor(0.0463)\n",
      "5783 Training Loss: tensor(0.0463)\n",
      "5784 Training Loss: tensor(0.0466)\n",
      "5785 Training Loss: tensor(0.0465)\n",
      "5786 Training Loss: tensor(0.0463)\n",
      "5787 Training Loss: tensor(0.0464)\n",
      "5788 Training Loss: tensor(0.0465)\n",
      "5789 Training Loss: tensor(0.0466)\n",
      "5790 Training Loss: tensor(0.0466)\n",
      "5791 Training Loss: tensor(0.0467)\n",
      "5792 Training Loss: tensor(0.0464)\n",
      "5793 Training Loss: tensor(0.0464)\n",
      "5794 Training Loss: tensor(0.0464)\n",
      "5795 Training Loss: tensor(0.0465)\n",
      "5796 Training Loss: tensor(0.0467)\n",
      "5797 Training Loss: tensor(0.0464)\n",
      "5798 Training Loss: tensor(0.0466)\n",
      "5799 Training Loss: tensor(0.0463)\n",
      "5800 Training Loss: tensor(0.0463)\n",
      "5801 Training Loss: tensor(0.0466)\n",
      "5802 Training Loss: tensor(0.0462)\n",
      "5803 Training Loss: tensor(0.0465)\n",
      "5804 Training Loss: tensor(0.0467)\n",
      "5805 Training Loss: tensor(0.0465)\n",
      "5806 Training Loss: tensor(0.0465)\n",
      "5807 Training Loss: tensor(0.0461)\n",
      "5808 Training Loss: tensor(0.0464)\n",
      "5809 Training Loss: tensor(0.0464)\n",
      "5810 Training Loss: tensor(0.0464)\n",
      "5811 Training Loss: tensor(0.0463)\n",
      "5812 Training Loss: tensor(0.0463)\n",
      "5813 Training Loss: tensor(0.0463)\n",
      "5814 Training Loss: tensor(0.0466)\n",
      "5815 Training Loss: tensor(0.0465)\n",
      "5816 Training Loss: tensor(0.0465)\n",
      "5817 Training Loss: tensor(0.0467)\n",
      "5818 Training Loss: tensor(0.0462)\n",
      "5819 Training Loss: tensor(0.0464)\n",
      "5820 Training Loss: tensor(0.0464)\n",
      "5821 Training Loss: tensor(0.0463)\n",
      "5822 Training Loss: tensor(0.0467)\n",
      "5823 Training Loss: tensor(0.0465)\n",
      "5824 Training Loss: tensor(0.0465)\n",
      "5825 Training Loss: tensor(0.0463)\n",
      "5826 Training Loss: tensor(0.0463)\n",
      "5827 Training Loss: tensor(0.0466)\n",
      "5828 Training Loss: tensor(0.0466)\n",
      "5829 Training Loss: tensor(0.0465)\n",
      "5830 Training Loss: tensor(0.0464)\n",
      "5831 Training Loss: tensor(0.0463)\n",
      "5832 Training Loss: tensor(0.0469)\n",
      "5833 Training Loss: tensor(0.0464)\n",
      "5834 Training Loss: tensor(0.0463)\n",
      "5835 Training Loss: tensor(0.0467)\n",
      "5836 Training Loss: tensor(0.0459)\n",
      "5837 Training Loss: tensor(0.0461)\n",
      "5838 Training Loss: tensor(0.0471)\n",
      "5839 Training Loss: tensor(0.0467)\n",
      "5840 Training Loss: tensor(0.0465)\n",
      "5841 Training Loss: tensor(0.0463)\n",
      "5842 Training Loss: tensor(0.0465)\n",
      "5843 Training Loss: tensor(0.0465)\n",
      "5844 Training Loss: tensor(0.0466)\n",
      "5845 Training Loss: tensor(0.0465)\n",
      "5846 Training Loss: tensor(0.0463)\n",
      "5847 Training Loss: tensor(0.0463)\n",
      "5848 Training Loss: tensor(0.0464)\n",
      "5849 Training Loss: tensor(0.0467)\n",
      "5850 Training Loss: tensor(0.0464)\n",
      "5851 Training Loss: tensor(0.0463)\n",
      "5852 Training Loss: tensor(0.0463)\n",
      "5853 Training Loss: tensor(0.0464)\n",
      "5854 Training Loss: tensor(0.0462)\n",
      "5855 Training Loss: tensor(0.0464)\n",
      "5856 Training Loss: tensor(0.0463)\n",
      "5857 Training Loss: tensor(0.0463)\n",
      "5858 Training Loss: tensor(0.0464)\n",
      "5859 Training Loss: tensor(0.0465)\n",
      "5860 Training Loss: tensor(0.0461)\n",
      "5861 Training Loss: tensor(0.0464)\n",
      "5862 Training Loss: tensor(0.0463)\n",
      "5863 Training Loss: tensor(0.0463)\n",
      "5864 Training Loss: tensor(0.0467)\n",
      "5865 Training Loss: tensor(0.0463)\n",
      "5866 Training Loss: tensor(0.0465)\n",
      "5867 Training Loss: tensor(0.0464)\n",
      "5868 Training Loss: tensor(0.0463)\n",
      "5869 Training Loss: tensor(0.0463)\n",
      "5870 Training Loss: tensor(0.0464)\n",
      "5871 Training Loss: tensor(0.0462)\n",
      "5872 Training Loss: tensor(0.0465)\n",
      "5873 Training Loss: tensor(0.0463)\n",
      "5874 Training Loss: tensor(0.0463)\n",
      "5875 Training Loss: tensor(0.0466)\n",
      "5876 Training Loss: tensor(0.0463)\n",
      "5877 Training Loss: tensor(0.0464)\n",
      "5878 Training Loss: tensor(0.0464)\n",
      "5879 Training Loss: tensor(0.0461)\n",
      "5880 Training Loss: tensor(0.0462)\n",
      "5881 Training Loss: tensor(0.0464)\n",
      "5882 Training Loss: tensor(0.0463)\n",
      "5883 Training Loss: tensor(0.0464)\n",
      "5884 Training Loss: tensor(0.0463)\n",
      "5885 Training Loss: tensor(0.0465)\n",
      "5886 Training Loss: tensor(0.0463)\n",
      "5887 Training Loss: tensor(0.0465)\n",
      "5888 Training Loss: tensor(0.0462)\n",
      "5889 Training Loss: tensor(0.0464)\n",
      "5890 Training Loss: tensor(0.0464)\n",
      "5891 Training Loss: tensor(0.0463)\n",
      "5892 Training Loss: tensor(0.0463)\n",
      "5893 Training Loss: tensor(0.0464)\n",
      "5894 Training Loss: tensor(0.0462)\n",
      "5895 Training Loss: tensor(0.0469)\n",
      "5896 Training Loss: tensor(0.0464)\n",
      "5897 Training Loss: tensor(0.0463)\n",
      "5898 Training Loss: tensor(0.0462)\n",
      "5899 Training Loss: tensor(0.0465)\n",
      "5900 Training Loss: tensor(0.0464)\n",
      "5901 Training Loss: tensor(0.0463)\n",
      "5902 Training Loss: tensor(0.0463)\n",
      "5903 Training Loss: tensor(0.0465)\n",
      "5904 Training Loss: tensor(0.0465)\n",
      "5905 Training Loss: tensor(0.0463)\n",
      "5906 Training Loss: tensor(0.0464)\n",
      "5907 Training Loss: tensor(0.0463)\n",
      "5908 Training Loss: tensor(0.0461)\n",
      "5909 Training Loss: tensor(0.0466)\n",
      "5910 Training Loss: tensor(0.0462)\n",
      "5911 Training Loss: tensor(0.0460)\n",
      "5912 Training Loss: tensor(0.0463)\n",
      "5913 Training Loss: tensor(0.0465)\n",
      "5914 Training Loss: tensor(0.0466)\n",
      "5915 Training Loss: tensor(0.0463)\n",
      "5916 Training Loss: tensor(0.0462)\n",
      "5917 Training Loss: tensor(0.0463)\n",
      "5918 Training Loss: tensor(0.0465)\n",
      "5919 Training Loss: tensor(0.0464)\n",
      "5920 Training Loss: tensor(0.0463)\n",
      "5921 Training Loss: tensor(0.0461)\n",
      "5922 Training Loss: tensor(0.0464)\n",
      "5923 Training Loss: tensor(0.0465)\n",
      "5924 Training Loss: tensor(0.0467)\n",
      "5925 Training Loss: tensor(0.0464)\n",
      "5926 Training Loss: tensor(0.0463)\n",
      "5927 Training Loss: tensor(0.0464)\n",
      "5928 Training Loss: tensor(0.0465)\n",
      "5929 Training Loss: tensor(0.0463)\n",
      "5930 Training Loss: tensor(0.0464)\n",
      "5931 Training Loss: tensor(0.0462)\n",
      "5932 Training Loss: tensor(0.0463)\n",
      "5933 Training Loss: tensor(0.0465)\n",
      "5934 Training Loss: tensor(0.0463)\n",
      "5935 Training Loss: tensor(0.0464)\n",
      "5936 Training Loss: tensor(0.0465)\n",
      "5937 Training Loss: tensor(0.0463)\n",
      "5938 Training Loss: tensor(0.0460)\n",
      "5939 Training Loss: tensor(0.0466)\n",
      "5940 Training Loss: tensor(0.0463)\n",
      "5941 Training Loss: tensor(0.0463)\n",
      "5942 Training Loss: tensor(0.0463)\n",
      "5943 Training Loss: tensor(0.0465)\n",
      "5944 Training Loss: tensor(0.0462)\n",
      "5945 Training Loss: tensor(0.0459)\n",
      "5946 Training Loss: tensor(0.0462)\n",
      "5947 Training Loss: tensor(0.0463)\n",
      "5948 Training Loss: tensor(0.0466)\n",
      "5949 Training Loss: tensor(0.0465)\n",
      "5950 Training Loss: tensor(0.0463)\n",
      "5951 Training Loss: tensor(0.0461)\n",
      "5952 Training Loss: tensor(0.0461)\n",
      "5953 Training Loss: tensor(0.0463)\n",
      "5954 Training Loss: tensor(0.0463)\n",
      "5955 Training Loss: tensor(0.0463)\n",
      "5956 Training Loss: tensor(0.0467)\n",
      "5957 Training Loss: tensor(0.0464)\n",
      "5958 Training Loss: tensor(0.0460)\n",
      "5959 Training Loss: tensor(0.0462)\n",
      "5960 Training Loss: tensor(0.0465)\n",
      "5961 Training Loss: tensor(0.0464)\n",
      "5962 Training Loss: tensor(0.0465)\n",
      "5963 Training Loss: tensor(0.0466)\n",
      "5964 Training Loss: tensor(0.0465)\n",
      "5965 Training Loss: tensor(0.0463)\n",
      "5966 Training Loss: tensor(0.0465)\n",
      "5967 Training Loss: tensor(0.0462)\n",
      "5968 Training Loss: tensor(0.0465)\n",
      "5969 Training Loss: tensor(0.0463)\n",
      "5970 Training Loss: tensor(0.0462)\n",
      "5971 Training Loss: tensor(0.0463)\n",
      "5972 Training Loss: tensor(0.0461)\n",
      "5973 Training Loss: tensor(0.0462)\n",
      "5974 Training Loss: tensor(0.0461)\n",
      "5975 Training Loss: tensor(0.0462)\n",
      "5976 Training Loss: tensor(0.0463)\n",
      "5977 Training Loss: tensor(0.0462)\n",
      "5978 Training Loss: tensor(0.0463)\n",
      "5979 Training Loss: tensor(0.0464)\n",
      "5980 Training Loss: tensor(0.0462)\n",
      "5981 Training Loss: tensor(0.0462)\n",
      "5982 Training Loss: tensor(0.0463)\n",
      "5983 Training Loss: tensor(0.0463)\n",
      "5984 Training Loss: tensor(0.0460)\n",
      "5985 Training Loss: tensor(0.0462)\n",
      "5986 Training Loss: tensor(0.0462)\n",
      "5987 Training Loss: tensor(0.0463)\n",
      "5988 Training Loss: tensor(0.0464)\n",
      "5989 Training Loss: tensor(0.0465)\n",
      "5990 Training Loss: tensor(0.0462)\n",
      "5991 Training Loss: tensor(0.0462)\n",
      "5992 Training Loss: tensor(0.0460)\n",
      "5993 Training Loss: tensor(0.0462)\n",
      "5994 Training Loss: tensor(0.0465)\n",
      "5995 Training Loss: tensor(0.0464)\n",
      "5996 Training Loss: tensor(0.0464)\n",
      "5997 Training Loss: tensor(0.0465)\n",
      "5998 Training Loss: tensor(0.0466)\n",
      "5999 Training Loss: tensor(0.0465)\n",
      "6000 Training Loss: tensor(0.0464)\n",
      "6001 Training Loss: tensor(0.0465)\n",
      "6002 Training Loss: tensor(0.0465)\n",
      "6003 Training Loss: tensor(0.0463)\n",
      "6004 Training Loss: tensor(0.0464)\n",
      "6005 Training Loss: tensor(0.0463)\n",
      "6006 Training Loss: tensor(0.0465)\n",
      "6007 Training Loss: tensor(0.0465)\n",
      "6008 Training Loss: tensor(0.0464)\n",
      "6009 Training Loss: tensor(0.0463)\n",
      "6010 Training Loss: tensor(0.0462)\n",
      "6011 Training Loss: tensor(0.0463)\n",
      "6012 Training Loss: tensor(0.0465)\n",
      "6013 Training Loss: tensor(0.0459)\n",
      "6014 Training Loss: tensor(0.0464)\n",
      "6015 Training Loss: tensor(0.0463)\n",
      "6016 Training Loss: tensor(0.0463)\n",
      "6017 Training Loss: tensor(0.0463)\n",
      "6018 Training Loss: tensor(0.0466)\n",
      "6019 Training Loss: tensor(0.0463)\n",
      "6020 Training Loss: tensor(0.0465)\n",
      "6021 Training Loss: tensor(0.0463)\n",
      "6022 Training Loss: tensor(0.0461)\n",
      "6023 Training Loss: tensor(0.0464)\n",
      "6024 Training Loss: tensor(0.0463)\n",
      "6025 Training Loss: tensor(0.0462)\n",
      "6026 Training Loss: tensor(0.0464)\n",
      "6027 Training Loss: tensor(0.0466)\n",
      "6028 Training Loss: tensor(0.0463)\n",
      "6029 Training Loss: tensor(0.0463)\n",
      "6030 Training Loss: tensor(0.0460)\n",
      "6031 Training Loss: tensor(0.0465)\n",
      "6032 Training Loss: tensor(0.0463)\n",
      "6033 Training Loss: tensor(0.0462)\n",
      "6034 Training Loss: tensor(0.0461)\n",
      "6035 Training Loss: tensor(0.0461)\n",
      "6036 Training Loss: tensor(0.0464)\n",
      "6037 Training Loss: tensor(0.0463)\n",
      "6038 Training Loss: tensor(0.0464)\n",
      "6039 Training Loss: tensor(0.0461)\n",
      "6040 Training Loss: tensor(0.0461)\n",
      "6041 Training Loss: tensor(0.0464)\n",
      "6042 Training Loss: tensor(0.0463)\n",
      "6043 Training Loss: tensor(0.0464)\n",
      "6044 Training Loss: tensor(0.0463)\n",
      "6045 Training Loss: tensor(0.0462)\n",
      "6046 Training Loss: tensor(0.0464)\n",
      "6047 Training Loss: tensor(0.0461)\n",
      "6048 Training Loss: tensor(0.0460)\n",
      "6049 Training Loss: tensor(0.0467)\n",
      "6050 Training Loss: tensor(0.0463)\n",
      "6051 Training Loss: tensor(0.0462)\n",
      "6052 Training Loss: tensor(0.0460)\n",
      "6053 Training Loss: tensor(0.0463)\n",
      "6054 Training Loss: tensor(0.0462)\n",
      "6055 Training Loss: tensor(0.0464)\n",
      "6056 Training Loss: tensor(0.0464)\n",
      "6057 Training Loss: tensor(0.0461)\n",
      "6058 Training Loss: tensor(0.0462)\n",
      "6059 Training Loss: tensor(0.0463)\n",
      "6060 Training Loss: tensor(0.0463)\n",
      "6061 Training Loss: tensor(0.0459)\n",
      "6062 Training Loss: tensor(0.0460)\n",
      "6063 Training Loss: tensor(0.0460)\n",
      "6064 Training Loss: tensor(0.0464)\n",
      "6065 Training Loss: tensor(0.0462)\n",
      "6066 Training Loss: tensor(0.0465)\n",
      "6067 Training Loss: tensor(0.0460)\n",
      "6068 Training Loss: tensor(0.0463)\n",
      "6069 Training Loss: tensor(0.0461)\n",
      "6070 Training Loss: tensor(0.0460)\n",
      "6071 Training Loss: tensor(0.0463)\n",
      "6072 Training Loss: tensor(0.0467)\n",
      "6073 Training Loss: tensor(0.0463)\n",
      "6074 Training Loss: tensor(0.0463)\n",
      "6075 Training Loss: tensor(0.0460)\n",
      "6076 Training Loss: tensor(0.0462)\n",
      "6077 Training Loss: tensor(0.0459)\n",
      "6078 Training Loss: tensor(0.0461)\n",
      "6079 Training Loss: tensor(0.0463)\n",
      "6080 Training Loss: tensor(0.0460)\n",
      "6081 Training Loss: tensor(0.0464)\n",
      "6082 Training Loss: tensor(0.0463)\n",
      "6083 Training Loss: tensor(0.0462)\n",
      "6084 Training Loss: tensor(0.0463)\n",
      "6085 Training Loss: tensor(0.0460)\n",
      "6086 Training Loss: tensor(0.0463)\n",
      "6087 Training Loss: tensor(0.0459)\n",
      "6088 Training Loss: tensor(0.0461)\n",
      "6089 Training Loss: tensor(0.0464)\n",
      "6090 Training Loss: tensor(0.0460)\n",
      "6091 Training Loss: tensor(0.0461)\n",
      "6092 Training Loss: tensor(0.0461)\n",
      "6093 Training Loss: tensor(0.0463)\n",
      "6094 Training Loss: tensor(0.0460)\n",
      "6095 Training Loss: tensor(0.0460)\n",
      "6096 Training Loss: tensor(0.0462)\n",
      "6097 Training Loss: tensor(0.0460)\n",
      "6098 Training Loss: tensor(0.0458)\n",
      "6099 Training Loss: tensor(0.0462)\n",
      "6100 Training Loss: tensor(0.0463)\n",
      "6101 Training Loss: tensor(0.0463)\n",
      "6102 Training Loss: tensor(0.0459)\n",
      "6103 Training Loss: tensor(0.0463)\n",
      "6104 Training Loss: tensor(0.0458)\n",
      "6105 Training Loss: tensor(0.0463)\n",
      "6106 Training Loss: tensor(0.0462)\n",
      "6107 Training Loss: tensor(0.0463)\n",
      "6108 Training Loss: tensor(0.0461)\n",
      "6109 Training Loss: tensor(0.0462)\n",
      "6110 Training Loss: tensor(0.0464)\n",
      "6111 Training Loss: tensor(0.0464)\n",
      "6112 Training Loss: tensor(0.0461)\n",
      "6113 Training Loss: tensor(0.0463)\n",
      "6114 Training Loss: tensor(0.0462)\n",
      "6115 Training Loss: tensor(0.0461)\n",
      "6116 Training Loss: tensor(0.0462)\n",
      "6117 Training Loss: tensor(0.0461)\n",
      "6118 Training Loss: tensor(0.0462)\n",
      "6119 Training Loss: tensor(0.0460)\n",
      "6120 Training Loss: tensor(0.0462)\n",
      "6121 Training Loss: tensor(0.0461)\n",
      "6122 Training Loss: tensor(0.0459)\n",
      "6123 Training Loss: tensor(0.0461)\n",
      "6124 Training Loss: tensor(0.0463)\n",
      "6125 Training Loss: tensor(0.0462)\n",
      "6126 Training Loss: tensor(0.0460)\n",
      "6127 Training Loss: tensor(0.0460)\n",
      "6128 Training Loss: tensor(0.0461)\n",
      "6129 Training Loss: tensor(0.0463)\n",
      "6130 Training Loss: tensor(0.0461)\n",
      "6131 Training Loss: tensor(0.0464)\n",
      "6132 Training Loss: tensor(0.0463)\n",
      "6133 Training Loss: tensor(0.0462)\n",
      "6134 Training Loss: tensor(0.0460)\n",
      "6135 Training Loss: tensor(0.0463)\n",
      "6136 Training Loss: tensor(0.0464)\n",
      "6137 Training Loss: tensor(0.0459)\n",
      "6138 Training Loss: tensor(0.0461)\n",
      "6139 Training Loss: tensor(0.0461)\n",
      "6140 Training Loss: tensor(0.0463)\n",
      "6141 Training Loss: tensor(0.0459)\n",
      "6142 Training Loss: tensor(0.0463)\n",
      "6143 Training Loss: tensor(0.0463)\n",
      "6144 Training Loss: tensor(0.0460)\n",
      "6145 Training Loss: tensor(0.0459)\n",
      "6146 Training Loss: tensor(0.0462)\n",
      "6147 Training Loss: tensor(0.0460)\n",
      "6148 Training Loss: tensor(0.0464)\n",
      "6149 Training Loss: tensor(0.0462)\n",
      "6150 Training Loss: tensor(0.0459)\n",
      "6151 Training Loss: tensor(0.0462)\n",
      "6152 Training Loss: tensor(0.0463)\n",
      "6153 Training Loss: tensor(0.0462)\n",
      "6154 Training Loss: tensor(0.0462)\n",
      "6155 Training Loss: tensor(0.0461)\n",
      "6156 Training Loss: tensor(0.0465)\n",
      "6157 Training Loss: tensor(0.0464)\n",
      "6158 Training Loss: tensor(0.0463)\n",
      "6159 Training Loss: tensor(0.0461)\n",
      "6160 Training Loss: tensor(0.0461)\n",
      "6161 Training Loss: tensor(0.0460)\n",
      "6162 Training Loss: tensor(0.0462)\n",
      "6163 Training Loss: tensor(0.0461)\n",
      "6164 Training Loss: tensor(0.0461)\n",
      "6165 Training Loss: tensor(0.0462)\n",
      "6166 Training Loss: tensor(0.0462)\n",
      "6167 Training Loss: tensor(0.0461)\n",
      "6168 Training Loss: tensor(0.0463)\n",
      "6169 Training Loss: tensor(0.0460)\n",
      "6170 Training Loss: tensor(0.0462)\n",
      "6171 Training Loss: tensor(0.0459)\n",
      "6172 Training Loss: tensor(0.0461)\n",
      "6173 Training Loss: tensor(0.0462)\n",
      "6174 Training Loss: tensor(0.0463)\n",
      "6175 Training Loss: tensor(0.0459)\n",
      "6176 Training Loss: tensor(0.0458)\n",
      "6177 Training Loss: tensor(0.0462)\n",
      "6178 Training Loss: tensor(0.0460)\n",
      "6179 Training Loss: tensor(0.0463)\n",
      "6180 Training Loss: tensor(0.0462)\n",
      "6181 Training Loss: tensor(0.0464)\n",
      "6182 Training Loss: tensor(0.0464)\n",
      "6183 Training Loss: tensor(0.0461)\n",
      "6184 Training Loss: tensor(0.0464)\n",
      "6185 Training Loss: tensor(0.0461)\n",
      "6186 Training Loss: tensor(0.0464)\n",
      "6187 Training Loss: tensor(0.0464)\n",
      "6188 Training Loss: tensor(0.0462)\n",
      "6189 Training Loss: tensor(0.0464)\n",
      "6190 Training Loss: tensor(0.0460)\n",
      "6191 Training Loss: tensor(0.0464)\n",
      "6192 Training Loss: tensor(0.0460)\n",
      "6193 Training Loss: tensor(0.0463)\n",
      "6194 Training Loss: tensor(0.0461)\n",
      "6195 Training Loss: tensor(0.0461)\n",
      "6196 Training Loss: tensor(0.0461)\n",
      "6197 Training Loss: tensor(0.0461)\n",
      "6198 Training Loss: tensor(0.0462)\n",
      "6199 Training Loss: tensor(0.0463)\n",
      "6200 Training Loss: tensor(0.0461)\n",
      "6201 Training Loss: tensor(0.0462)\n",
      "6202 Training Loss: tensor(0.0459)\n",
      "6203 Training Loss: tensor(0.0460)\n",
      "6204 Training Loss: tensor(0.0459)\n",
      "6205 Training Loss: tensor(0.0460)\n",
      "6206 Training Loss: tensor(0.0460)\n",
      "6207 Training Loss: tensor(0.0462)\n",
      "6208 Training Loss: tensor(0.0463)\n",
      "6209 Training Loss: tensor(0.0460)\n",
      "6210 Training Loss: tensor(0.0461)\n",
      "6211 Training Loss: tensor(0.0461)\n",
      "6212 Training Loss: tensor(0.0460)\n",
      "6213 Training Loss: tensor(0.0463)\n",
      "6214 Training Loss: tensor(0.0461)\n",
      "6215 Training Loss: tensor(0.0460)\n",
      "6216 Training Loss: tensor(0.0460)\n",
      "6217 Training Loss: tensor(0.0462)\n",
      "6218 Training Loss: tensor(0.0462)\n",
      "6219 Training Loss: tensor(0.0461)\n",
      "6220 Training Loss: tensor(0.0460)\n",
      "6221 Training Loss: tensor(0.0459)\n",
      "6222 Training Loss: tensor(0.0462)\n",
      "6223 Training Loss: tensor(0.0460)\n",
      "6224 Training Loss: tensor(0.0464)\n",
      "6225 Training Loss: tensor(0.0460)\n",
      "6226 Training Loss: tensor(0.0458)\n",
      "6227 Training Loss: tensor(0.0460)\n",
      "6228 Training Loss: tensor(0.0461)\n",
      "6229 Training Loss: tensor(0.0464)\n",
      "6230 Training Loss: tensor(0.0457)\n",
      "6231 Training Loss: tensor(0.0460)\n",
      "6232 Training Loss: tensor(0.0462)\n",
      "6233 Training Loss: tensor(0.0463)\n",
      "6234 Training Loss: tensor(0.0461)\n",
      "6235 Training Loss: tensor(0.0461)\n",
      "6236 Training Loss: tensor(0.0463)\n",
      "6237 Training Loss: tensor(0.0460)\n",
      "6238 Training Loss: tensor(0.0461)\n",
      "6239 Training Loss: tensor(0.0461)\n",
      "6240 Training Loss: tensor(0.0466)\n",
      "6241 Training Loss: tensor(0.0461)\n",
      "6242 Training Loss: tensor(0.0460)\n",
      "6243 Training Loss: tensor(0.0459)\n",
      "6244 Training Loss: tensor(0.0460)\n",
      "6245 Training Loss: tensor(0.0461)\n",
      "6246 Training Loss: tensor(0.0463)\n",
      "6247 Training Loss: tensor(0.0461)\n",
      "6248 Training Loss: tensor(0.0461)\n",
      "6249 Training Loss: tensor(0.0462)\n",
      "6250 Training Loss: tensor(0.0465)\n",
      "6251 Training Loss: tensor(0.0459)\n",
      "6252 Training Loss: tensor(0.0463)\n",
      "6253 Training Loss: tensor(0.0461)\n",
      "6254 Training Loss: tensor(0.0462)\n",
      "6255 Training Loss: tensor(0.0464)\n",
      "6256 Training Loss: tensor(0.0458)\n",
      "6257 Training Loss: tensor(0.0460)\n",
      "6258 Training Loss: tensor(0.0463)\n",
      "6259 Training Loss: tensor(0.0460)\n",
      "6260 Training Loss: tensor(0.0463)\n",
      "6261 Training Loss: tensor(0.0461)\n",
      "6262 Training Loss: tensor(0.0462)\n",
      "6263 Training Loss: tensor(0.0457)\n",
      "6264 Training Loss: tensor(0.0462)\n",
      "6265 Training Loss: tensor(0.0463)\n",
      "6266 Training Loss: tensor(0.0458)\n",
      "6267 Training Loss: tensor(0.0464)\n",
      "6268 Training Loss: tensor(0.0461)\n",
      "6269 Training Loss: tensor(0.0458)\n",
      "6270 Training Loss: tensor(0.0459)\n",
      "6271 Training Loss: tensor(0.0461)\n",
      "6272 Training Loss: tensor(0.0461)\n",
      "6273 Training Loss: tensor(0.0460)\n",
      "6274 Training Loss: tensor(0.0460)\n",
      "6275 Training Loss: tensor(0.0461)\n",
      "6276 Training Loss: tensor(0.0460)\n",
      "6277 Training Loss: tensor(0.0461)\n",
      "6278 Training Loss: tensor(0.0457)\n",
      "6279 Training Loss: tensor(0.0466)\n",
      "6280 Training Loss: tensor(0.0460)\n",
      "6281 Training Loss: tensor(0.0461)\n",
      "6282 Training Loss: tensor(0.0463)\n",
      "6283 Training Loss: tensor(0.0460)\n",
      "6284 Training Loss: tensor(0.0465)\n",
      "6285 Training Loss: tensor(0.0458)\n",
      "6286 Training Loss: tensor(0.0461)\n",
      "6287 Training Loss: tensor(0.0461)\n",
      "6288 Training Loss: tensor(0.0460)\n",
      "6289 Training Loss: tensor(0.0459)\n",
      "6290 Training Loss: tensor(0.0461)\n",
      "6291 Training Loss: tensor(0.0459)\n",
      "6292 Training Loss: tensor(0.0459)\n",
      "6293 Training Loss: tensor(0.0461)\n",
      "6294 Training Loss: tensor(0.0460)\n",
      "6295 Training Loss: tensor(0.0462)\n",
      "6296 Training Loss: tensor(0.0463)\n",
      "6297 Training Loss: tensor(0.0461)\n",
      "6298 Training Loss: tensor(0.0457)\n",
      "6299 Training Loss: tensor(0.0459)\n",
      "6300 Training Loss: tensor(0.0460)\n",
      "6301 Training Loss: tensor(0.0459)\n",
      "6302 Training Loss: tensor(0.0461)\n",
      "6303 Training Loss: tensor(0.0462)\n",
      "6304 Training Loss: tensor(0.0460)\n",
      "6305 Training Loss: tensor(0.0466)\n",
      "6306 Training Loss: tensor(0.0458)\n",
      "6307 Training Loss: tensor(0.0461)\n",
      "6308 Training Loss: tensor(0.0463)\n",
      "6309 Training Loss: tensor(0.0461)\n",
      "6310 Training Loss: tensor(0.0461)\n",
      "6311 Training Loss: tensor(0.0459)\n",
      "6312 Training Loss: tensor(0.0458)\n",
      "6313 Training Loss: tensor(0.0460)\n",
      "6314 Training Loss: tensor(0.0463)\n",
      "6315 Training Loss: tensor(0.0462)\n",
      "6316 Training Loss: tensor(0.0460)\n",
      "6317 Training Loss: tensor(0.0459)\n",
      "6318 Training Loss: tensor(0.0463)\n",
      "6319 Training Loss: tensor(0.0459)\n",
      "6320 Training Loss: tensor(0.0458)\n",
      "6321 Training Loss: tensor(0.0459)\n",
      "6322 Training Loss: tensor(0.0457)\n",
      "6323 Training Loss: tensor(0.0460)\n",
      "6324 Training Loss: tensor(0.0458)\n",
      "6325 Training Loss: tensor(0.0461)\n",
      "6326 Training Loss: tensor(0.0459)\n",
      "6327 Training Loss: tensor(0.0459)\n",
      "6328 Training Loss: tensor(0.0462)\n",
      "6329 Training Loss: tensor(0.0460)\n",
      "6330 Training Loss: tensor(0.0460)\n",
      "6331 Training Loss: tensor(0.0459)\n",
      "6332 Training Loss: tensor(0.0461)\n",
      "6333 Training Loss: tensor(0.0462)\n",
      "6334 Training Loss: tensor(0.0460)\n",
      "6335 Training Loss: tensor(0.0459)\n",
      "6336 Training Loss: tensor(0.0459)\n",
      "6337 Training Loss: tensor(0.0457)\n",
      "6338 Training Loss: tensor(0.0461)\n",
      "6339 Training Loss: tensor(0.0462)\n",
      "6340 Training Loss: tensor(0.0461)\n",
      "6341 Training Loss: tensor(0.0460)\n",
      "6342 Training Loss: tensor(0.0462)\n",
      "6343 Training Loss: tensor(0.0461)\n",
      "6344 Training Loss: tensor(0.0460)\n",
      "6345 Training Loss: tensor(0.0459)\n",
      "6346 Training Loss: tensor(0.0463)\n",
      "6347 Training Loss: tensor(0.0458)\n",
      "6348 Training Loss: tensor(0.0459)\n",
      "6349 Training Loss: tensor(0.0458)\n",
      "6350 Training Loss: tensor(0.0462)\n",
      "6351 Training Loss: tensor(0.0458)\n",
      "6352 Training Loss: tensor(0.0460)\n",
      "6353 Training Loss: tensor(0.0459)\n",
      "6354 Training Loss: tensor(0.0461)\n",
      "6355 Training Loss: tensor(0.0462)\n",
      "6356 Training Loss: tensor(0.0460)\n",
      "6357 Training Loss: tensor(0.0462)\n",
      "6358 Training Loss: tensor(0.0461)\n",
      "6359 Training Loss: tensor(0.0461)\n",
      "6360 Training Loss: tensor(0.0461)\n",
      "6361 Training Loss: tensor(0.0459)\n",
      "6362 Training Loss: tensor(0.0460)\n",
      "6363 Training Loss: tensor(0.0457)\n",
      "6364 Training Loss: tensor(0.0459)\n",
      "6365 Training Loss: tensor(0.0460)\n",
      "6366 Training Loss: tensor(0.0458)\n",
      "6367 Training Loss: tensor(0.0459)\n",
      "6368 Training Loss: tensor(0.0461)\n",
      "6369 Training Loss: tensor(0.0458)\n",
      "6370 Training Loss: tensor(0.0461)\n",
      "6371 Training Loss: tensor(0.0460)\n",
      "6372 Training Loss: tensor(0.0460)\n",
      "6373 Training Loss: tensor(0.0459)\n",
      "6374 Training Loss: tensor(0.0459)\n",
      "6375 Training Loss: tensor(0.0459)\n",
      "6376 Training Loss: tensor(0.0459)\n",
      "6377 Training Loss: tensor(0.0459)\n",
      "6378 Training Loss: tensor(0.0458)\n",
      "6379 Training Loss: tensor(0.0461)\n",
      "6380 Training Loss: tensor(0.0457)\n",
      "6381 Training Loss: tensor(0.0460)\n",
      "6382 Training Loss: tensor(0.0463)\n",
      "6383 Training Loss: tensor(0.0457)\n",
      "6384 Training Loss: tensor(0.0459)\n",
      "6385 Training Loss: tensor(0.0458)\n",
      "6386 Training Loss: tensor(0.0462)\n",
      "6387 Training Loss: tensor(0.0460)\n",
      "6388 Training Loss: tensor(0.0457)\n",
      "6389 Training Loss: tensor(0.0460)\n",
      "6390 Training Loss: tensor(0.0459)\n",
      "6391 Training Loss: tensor(0.0461)\n",
      "6392 Training Loss: tensor(0.0460)\n",
      "6393 Training Loss: tensor(0.0457)\n",
      "6394 Training Loss: tensor(0.0458)\n",
      "6395 Training Loss: tensor(0.0460)\n",
      "6396 Training Loss: tensor(0.0460)\n",
      "6397 Training Loss: tensor(0.0463)\n",
      "6398 Training Loss: tensor(0.0458)\n",
      "6399 Training Loss: tensor(0.0460)\n",
      "6400 Training Loss: tensor(0.0460)\n",
      "6401 Training Loss: tensor(0.0458)\n",
      "6402 Training Loss: tensor(0.0459)\n",
      "6403 Training Loss: tensor(0.0459)\n",
      "6404 Training Loss: tensor(0.0460)\n",
      "6405 Training Loss: tensor(0.0460)\n",
      "6406 Training Loss: tensor(0.0460)\n",
      "6407 Training Loss: tensor(0.0461)\n",
      "6408 Training Loss: tensor(0.0458)\n",
      "6409 Training Loss: tensor(0.0458)\n",
      "6410 Training Loss: tensor(0.0458)\n",
      "6411 Training Loss: tensor(0.0460)\n",
      "6412 Training Loss: tensor(0.0458)\n",
      "6413 Training Loss: tensor(0.0459)\n",
      "6414 Training Loss: tensor(0.0459)\n",
      "6415 Training Loss: tensor(0.0456)\n",
      "6416 Training Loss: tensor(0.0460)\n",
      "6417 Training Loss: tensor(0.0457)\n",
      "6418 Training Loss: tensor(0.0459)\n",
      "6419 Training Loss: tensor(0.0458)\n",
      "6420 Training Loss: tensor(0.0459)\n",
      "6421 Training Loss: tensor(0.0463)\n",
      "6422 Training Loss: tensor(0.0458)\n",
      "6423 Training Loss: tensor(0.0456)\n",
      "6424 Training Loss: tensor(0.0460)\n",
      "6425 Training Loss: tensor(0.0461)\n",
      "6426 Training Loss: tensor(0.0457)\n",
      "6427 Training Loss: tensor(0.0462)\n",
      "6428 Training Loss: tensor(0.0462)\n",
      "6429 Training Loss: tensor(0.0456)\n",
      "6430 Training Loss: tensor(0.0458)\n",
      "6431 Training Loss: tensor(0.0462)\n",
      "6432 Training Loss: tensor(0.0459)\n",
      "6433 Training Loss: tensor(0.0458)\n",
      "6434 Training Loss: tensor(0.0460)\n",
      "6435 Training Loss: tensor(0.0459)\n",
      "6436 Training Loss: tensor(0.0457)\n",
      "6437 Training Loss: tensor(0.0464)\n",
      "6438 Training Loss: tensor(0.0461)\n",
      "6439 Training Loss: tensor(0.0460)\n",
      "6440 Training Loss: tensor(0.0460)\n",
      "6441 Training Loss: tensor(0.0460)\n",
      "6442 Training Loss: tensor(0.0460)\n",
      "6443 Training Loss: tensor(0.0459)\n",
      "6444 Training Loss: tensor(0.0457)\n",
      "6445 Training Loss: tensor(0.0459)\n",
      "6446 Training Loss: tensor(0.0458)\n",
      "6447 Training Loss: tensor(0.0460)\n",
      "6448 Training Loss: tensor(0.0458)\n",
      "6449 Training Loss: tensor(0.0457)\n",
      "6450 Training Loss: tensor(0.0457)\n",
      "6451 Training Loss: tensor(0.0458)\n",
      "6452 Training Loss: tensor(0.0460)\n",
      "6453 Training Loss: tensor(0.0458)\n",
      "6454 Training Loss: tensor(0.0457)\n",
      "6455 Training Loss: tensor(0.0457)\n",
      "6456 Training Loss: tensor(0.0457)\n",
      "6457 Training Loss: tensor(0.0457)\n",
      "6458 Training Loss: tensor(0.0464)\n",
      "6459 Training Loss: tensor(0.0457)\n",
      "6460 Training Loss: tensor(0.0459)\n",
      "6461 Training Loss: tensor(0.0461)\n",
      "6462 Training Loss: tensor(0.0460)\n",
      "6463 Training Loss: tensor(0.0460)\n",
      "6464 Training Loss: tensor(0.0461)\n",
      "6465 Training Loss: tensor(0.0461)\n",
      "6466 Training Loss: tensor(0.0459)\n",
      "6467 Training Loss: tensor(0.0461)\n",
      "6468 Training Loss: tensor(0.0460)\n",
      "6469 Training Loss: tensor(0.0460)\n",
      "6470 Training Loss: tensor(0.0461)\n",
      "6471 Training Loss: tensor(0.0459)\n",
      "6472 Training Loss: tensor(0.0458)\n",
      "6473 Training Loss: tensor(0.0457)\n",
      "6474 Training Loss: tensor(0.0460)\n",
      "6475 Training Loss: tensor(0.0457)\n",
      "6476 Training Loss: tensor(0.0459)\n",
      "6477 Training Loss: tensor(0.0457)\n",
      "6478 Training Loss: tensor(0.0455)\n",
      "6479 Training Loss: tensor(0.0458)\n",
      "6480 Training Loss: tensor(0.0460)\n",
      "6481 Training Loss: tensor(0.0460)\n",
      "6482 Training Loss: tensor(0.0456)\n",
      "6483 Training Loss: tensor(0.0455)\n",
      "6484 Training Loss: tensor(0.0457)\n",
      "6485 Training Loss: tensor(0.0458)\n",
      "6486 Training Loss: tensor(0.0456)\n",
      "6487 Training Loss: tensor(0.0460)\n",
      "6488 Training Loss: tensor(0.0458)\n",
      "6489 Training Loss: tensor(0.0460)\n",
      "6490 Training Loss: tensor(0.0457)\n",
      "6491 Training Loss: tensor(0.0459)\n",
      "6492 Training Loss: tensor(0.0461)\n",
      "6493 Training Loss: tensor(0.0460)\n",
      "6494 Training Loss: tensor(0.0458)\n",
      "6495 Training Loss: tensor(0.0460)\n",
      "6496 Training Loss: tensor(0.0456)\n",
      "6497 Training Loss: tensor(0.0456)\n",
      "6498 Training Loss: tensor(0.0458)\n",
      "6499 Training Loss: tensor(0.0457)\n",
      "6500 Training Loss: tensor(0.0457)\n",
      "6501 Training Loss: tensor(0.0461)\n",
      "6502 Training Loss: tensor(0.0457)\n",
      "6503 Training Loss: tensor(0.0458)\n",
      "6504 Training Loss: tensor(0.0457)\n",
      "6505 Training Loss: tensor(0.0455)\n",
      "6506 Training Loss: tensor(0.0459)\n",
      "6507 Training Loss: tensor(0.0460)\n",
      "6508 Training Loss: tensor(0.0457)\n",
      "6509 Training Loss: tensor(0.0459)\n",
      "6510 Training Loss: tensor(0.0458)\n",
      "6511 Training Loss: tensor(0.0456)\n",
      "6512 Training Loss: tensor(0.0458)\n",
      "6513 Training Loss: tensor(0.0456)\n",
      "6514 Training Loss: tensor(0.0458)\n",
      "6515 Training Loss: tensor(0.0459)\n",
      "6516 Training Loss: tensor(0.0460)\n",
      "6517 Training Loss: tensor(0.0458)\n",
      "6518 Training Loss: tensor(0.0455)\n",
      "6519 Training Loss: tensor(0.0460)\n",
      "6520 Training Loss: tensor(0.0460)\n",
      "6521 Training Loss: tensor(0.0460)\n",
      "6522 Training Loss: tensor(0.0460)\n",
      "6523 Training Loss: tensor(0.0455)\n",
      "6524 Training Loss: tensor(0.0457)\n",
      "6525 Training Loss: tensor(0.0459)\n",
      "6526 Training Loss: tensor(0.0457)\n",
      "6527 Training Loss: tensor(0.0458)\n",
      "6528 Training Loss: tensor(0.0456)\n",
      "6529 Training Loss: tensor(0.0460)\n",
      "6530 Training Loss: tensor(0.0458)\n",
      "6531 Training Loss: tensor(0.0460)\n",
      "6532 Training Loss: tensor(0.0456)\n",
      "6533 Training Loss: tensor(0.0457)\n",
      "6534 Training Loss: tensor(0.0458)\n",
      "6535 Training Loss: tensor(0.0459)\n",
      "6536 Training Loss: tensor(0.0457)\n",
      "6537 Training Loss: tensor(0.0460)\n",
      "6538 Training Loss: tensor(0.0457)\n",
      "6539 Training Loss: tensor(0.0459)\n",
      "6540 Training Loss: tensor(0.0457)\n",
      "6541 Training Loss: tensor(0.0458)\n",
      "6542 Training Loss: tensor(0.0458)\n",
      "6543 Training Loss: tensor(0.0459)\n",
      "6544 Training Loss: tensor(0.0458)\n",
      "6545 Training Loss: tensor(0.0456)\n",
      "6546 Training Loss: tensor(0.0455)\n",
      "6547 Training Loss: tensor(0.0462)\n",
      "6548 Training Loss: tensor(0.0456)\n",
      "6549 Training Loss: tensor(0.0457)\n",
      "6550 Training Loss: tensor(0.0456)\n",
      "6551 Training Loss: tensor(0.0457)\n",
      "6552 Training Loss: tensor(0.0461)\n",
      "6553 Training Loss: tensor(0.0457)\n",
      "6554 Training Loss: tensor(0.0457)\n",
      "6555 Training Loss: tensor(0.0458)\n",
      "6556 Training Loss: tensor(0.0455)\n",
      "6557 Training Loss: tensor(0.0459)\n",
      "6558 Training Loss: tensor(0.0457)\n",
      "6559 Training Loss: tensor(0.0462)\n",
      "6560 Training Loss: tensor(0.0456)\n",
      "6561 Training Loss: tensor(0.0457)\n",
      "6562 Training Loss: tensor(0.0458)\n",
      "6563 Training Loss: tensor(0.0457)\n",
      "6564 Training Loss: tensor(0.0460)\n",
      "6565 Training Loss: tensor(0.0460)\n",
      "6566 Training Loss: tensor(0.0460)\n",
      "6567 Training Loss: tensor(0.0455)\n",
      "6568 Training Loss: tensor(0.0459)\n",
      "6569 Training Loss: tensor(0.0457)\n",
      "6570 Training Loss: tensor(0.0457)\n",
      "6571 Training Loss: tensor(0.0457)\n",
      "6572 Training Loss: tensor(0.0459)\n",
      "6573 Training Loss: tensor(0.0456)\n",
      "6574 Training Loss: tensor(0.0457)\n",
      "6575 Training Loss: tensor(0.0459)\n",
      "6576 Training Loss: tensor(0.0459)\n",
      "6577 Training Loss: tensor(0.0455)\n",
      "6578 Training Loss: tensor(0.0460)\n",
      "6579 Training Loss: tensor(0.0457)\n",
      "6580 Training Loss: tensor(0.0459)\n",
      "6581 Training Loss: tensor(0.0458)\n",
      "6582 Training Loss: tensor(0.0458)\n",
      "6583 Training Loss: tensor(0.0458)\n",
      "6584 Training Loss: tensor(0.0456)\n",
      "6585 Training Loss: tensor(0.0457)\n",
      "6586 Training Loss: tensor(0.0457)\n",
      "6587 Training Loss: tensor(0.0455)\n",
      "6588 Training Loss: tensor(0.0460)\n",
      "6589 Training Loss: tensor(0.0457)\n",
      "6590 Training Loss: tensor(0.0461)\n",
      "6591 Training Loss: tensor(0.0457)\n",
      "6592 Training Loss: tensor(0.0458)\n",
      "6593 Training Loss: tensor(0.0456)\n",
      "6594 Training Loss: tensor(0.0460)\n",
      "6595 Training Loss: tensor(0.0460)\n",
      "6596 Training Loss: tensor(0.0463)\n",
      "6597 Training Loss: tensor(0.0458)\n",
      "6598 Training Loss: tensor(0.0459)\n",
      "6599 Training Loss: tensor(0.0456)\n",
      "6600 Training Loss: tensor(0.0459)\n",
      "6601 Training Loss: tensor(0.0459)\n",
      "6602 Training Loss: tensor(0.0458)\n",
      "6603 Training Loss: tensor(0.0454)\n",
      "6604 Training Loss: tensor(0.0457)\n",
      "6605 Training Loss: tensor(0.0459)\n",
      "6606 Training Loss: tensor(0.0457)\n",
      "6607 Training Loss: tensor(0.0455)\n",
      "6608 Training Loss: tensor(0.0457)\n",
      "6609 Training Loss: tensor(0.0458)\n",
      "6610 Training Loss: tensor(0.0455)\n",
      "6611 Training Loss: tensor(0.0458)\n",
      "6612 Training Loss: tensor(0.0456)\n",
      "6613 Training Loss: tensor(0.0456)\n",
      "6614 Training Loss: tensor(0.0457)\n",
      "6615 Training Loss: tensor(0.0457)\n",
      "6616 Training Loss: tensor(0.0456)\n",
      "6617 Training Loss: tensor(0.0456)\n",
      "6618 Training Loss: tensor(0.0458)\n",
      "6619 Training Loss: tensor(0.0458)\n",
      "6620 Training Loss: tensor(0.0457)\n",
      "6621 Training Loss: tensor(0.0459)\n",
      "6622 Training Loss: tensor(0.0459)\n",
      "6623 Training Loss: tensor(0.0459)\n",
      "6624 Training Loss: tensor(0.0458)\n",
      "6625 Training Loss: tensor(0.0457)\n",
      "6626 Training Loss: tensor(0.0457)\n",
      "6627 Training Loss: tensor(0.0456)\n",
      "6628 Training Loss: tensor(0.0453)\n",
      "6629 Training Loss: tensor(0.0460)\n",
      "6630 Training Loss: tensor(0.0456)\n",
      "6631 Training Loss: tensor(0.0457)\n",
      "6632 Training Loss: tensor(0.0456)\n",
      "6633 Training Loss: tensor(0.0457)\n",
      "6634 Training Loss: tensor(0.0455)\n",
      "6635 Training Loss: tensor(0.0457)\n",
      "6636 Training Loss: tensor(0.0455)\n",
      "6637 Training Loss: tensor(0.0457)\n",
      "6638 Training Loss: tensor(0.0458)\n",
      "6639 Training Loss: tensor(0.0461)\n",
      "6640 Training Loss: tensor(0.0455)\n",
      "6641 Training Loss: tensor(0.0458)\n",
      "6642 Training Loss: tensor(0.0456)\n",
      "6643 Training Loss: tensor(0.0456)\n",
      "6644 Training Loss: tensor(0.0455)\n",
      "6645 Training Loss: tensor(0.0458)\n",
      "6646 Training Loss: tensor(0.0458)\n",
      "6647 Training Loss: tensor(0.0455)\n",
      "6648 Training Loss: tensor(0.0456)\n",
      "6649 Training Loss: tensor(0.0458)\n",
      "6650 Training Loss: tensor(0.0457)\n",
      "6651 Training Loss: tensor(0.0458)\n",
      "6652 Training Loss: tensor(0.0460)\n",
      "6653 Training Loss: tensor(0.0458)\n",
      "6654 Training Loss: tensor(0.0455)\n",
      "6655 Training Loss: tensor(0.0457)\n",
      "6656 Training Loss: tensor(0.0456)\n",
      "6657 Training Loss: tensor(0.0459)\n",
      "6658 Training Loss: tensor(0.0456)\n",
      "6659 Training Loss: tensor(0.0457)\n",
      "6660 Training Loss: tensor(0.0455)\n",
      "6661 Training Loss: tensor(0.0455)\n",
      "6662 Training Loss: tensor(0.0455)\n",
      "6663 Training Loss: tensor(0.0455)\n",
      "6664 Training Loss: tensor(0.0458)\n",
      "6665 Training Loss: tensor(0.0459)\n",
      "6666 Training Loss: tensor(0.0457)\n",
      "6667 Training Loss: tensor(0.0455)\n",
      "6668 Training Loss: tensor(0.0456)\n",
      "6669 Training Loss: tensor(0.0456)\n",
      "6670 Training Loss: tensor(0.0456)\n",
      "6671 Training Loss: tensor(0.0456)\n",
      "6672 Training Loss: tensor(0.0455)\n",
      "6673 Training Loss: tensor(0.0454)\n",
      "6674 Training Loss: tensor(0.0456)\n",
      "6675 Training Loss: tensor(0.0455)\n",
      "6676 Training Loss: tensor(0.0460)\n",
      "6677 Training Loss: tensor(0.0457)\n",
      "6678 Training Loss: tensor(0.0459)\n",
      "6679 Training Loss: tensor(0.0460)\n",
      "6680 Training Loss: tensor(0.0454)\n",
      "6681 Training Loss: tensor(0.0457)\n",
      "6682 Training Loss: tensor(0.0463)\n",
      "6683 Training Loss: tensor(0.0457)\n",
      "6684 Training Loss: tensor(0.0456)\n",
      "6685 Training Loss: tensor(0.0454)\n",
      "6686 Training Loss: tensor(0.0456)\n",
      "6687 Training Loss: tensor(0.0458)\n",
      "6688 Training Loss: tensor(0.0455)\n",
      "6689 Training Loss: tensor(0.0455)\n",
      "6690 Training Loss: tensor(0.0458)\n",
      "6691 Training Loss: tensor(0.0455)\n",
      "6692 Training Loss: tensor(0.0459)\n",
      "6693 Training Loss: tensor(0.0457)\n",
      "6694 Training Loss: tensor(0.0459)\n",
      "6695 Training Loss: tensor(0.0458)\n",
      "6696 Training Loss: tensor(0.0459)\n",
      "6697 Training Loss: tensor(0.0459)\n",
      "6698 Training Loss: tensor(0.0455)\n",
      "6699 Training Loss: tensor(0.0460)\n",
      "6700 Training Loss: tensor(0.0460)\n",
      "6701 Training Loss: tensor(0.0456)\n",
      "6702 Training Loss: tensor(0.0456)\n",
      "6703 Training Loss: tensor(0.0456)\n",
      "6704 Training Loss: tensor(0.0458)\n",
      "6705 Training Loss: tensor(0.0456)\n",
      "6706 Training Loss: tensor(0.0457)\n",
      "6707 Training Loss: tensor(0.0455)\n",
      "6708 Training Loss: tensor(0.0458)\n",
      "6709 Training Loss: tensor(0.0458)\n",
      "6710 Training Loss: tensor(0.0459)\n",
      "6711 Training Loss: tensor(0.0457)\n",
      "6712 Training Loss: tensor(0.0457)\n",
      "6713 Training Loss: tensor(0.0456)\n",
      "6714 Training Loss: tensor(0.0457)\n",
      "6715 Training Loss: tensor(0.0455)\n",
      "6716 Training Loss: tensor(0.0457)\n",
      "6717 Training Loss: tensor(0.0455)\n",
      "6718 Training Loss: tensor(0.0454)\n",
      "6719 Training Loss: tensor(0.0455)\n",
      "6720 Training Loss: tensor(0.0458)\n",
      "6721 Training Loss: tensor(0.0454)\n",
      "6722 Training Loss: tensor(0.0456)\n",
      "6723 Training Loss: tensor(0.0456)\n",
      "6724 Training Loss: tensor(0.0458)\n",
      "6725 Training Loss: tensor(0.0457)\n",
      "6726 Training Loss: tensor(0.0455)\n",
      "6727 Training Loss: tensor(0.0457)\n",
      "6728 Training Loss: tensor(0.0453)\n",
      "6729 Training Loss: tensor(0.0456)\n",
      "6730 Training Loss: tensor(0.0455)\n",
      "6731 Training Loss: tensor(0.0457)\n",
      "6732 Training Loss: tensor(0.0456)\n",
      "6733 Training Loss: tensor(0.0455)\n",
      "6734 Training Loss: tensor(0.0458)\n",
      "6735 Training Loss: tensor(0.0456)\n",
      "6736 Training Loss: tensor(0.0453)\n",
      "6737 Training Loss: tensor(0.0459)\n",
      "6738 Training Loss: tensor(0.0456)\n",
      "6739 Training Loss: tensor(0.0456)\n",
      "6740 Training Loss: tensor(0.0456)\n",
      "6741 Training Loss: tensor(0.0456)\n",
      "6742 Training Loss: tensor(0.0455)\n",
      "6743 Training Loss: tensor(0.0458)\n",
      "6744 Training Loss: tensor(0.0458)\n",
      "6745 Training Loss: tensor(0.0460)\n",
      "6746 Training Loss: tensor(0.0457)\n",
      "6747 Training Loss: tensor(0.0457)\n",
      "6748 Training Loss: tensor(0.0457)\n",
      "6749 Training Loss: tensor(0.0457)\n",
      "6750 Training Loss: tensor(0.0457)\n",
      "6751 Training Loss: tensor(0.0454)\n",
      "6752 Training Loss: tensor(0.0455)\n",
      "6753 Training Loss: tensor(0.0454)\n",
      "6754 Training Loss: tensor(0.0456)\n",
      "6755 Training Loss: tensor(0.0457)\n",
      "6756 Training Loss: tensor(0.0457)\n",
      "6757 Training Loss: tensor(0.0455)\n",
      "6758 Training Loss: tensor(0.0457)\n",
      "6759 Training Loss: tensor(0.0459)\n",
      "6760 Training Loss: tensor(0.0456)\n",
      "6761 Training Loss: tensor(0.0459)\n",
      "6762 Training Loss: tensor(0.0459)\n",
      "6763 Training Loss: tensor(0.0456)\n",
      "6764 Training Loss: tensor(0.0458)\n",
      "6765 Training Loss: tensor(0.0455)\n",
      "6766 Training Loss: tensor(0.0456)\n",
      "6767 Training Loss: tensor(0.0454)\n",
      "6768 Training Loss: tensor(0.0459)\n",
      "6769 Training Loss: tensor(0.0455)\n",
      "6770 Training Loss: tensor(0.0456)\n",
      "6771 Training Loss: tensor(0.0457)\n",
      "6772 Training Loss: tensor(0.0457)\n",
      "6773 Training Loss: tensor(0.0456)\n",
      "6774 Training Loss: tensor(0.0455)\n",
      "6775 Training Loss: tensor(0.0453)\n",
      "6776 Training Loss: tensor(0.0456)\n",
      "6777 Training Loss: tensor(0.0456)\n",
      "6778 Training Loss: tensor(0.0453)\n",
      "6779 Training Loss: tensor(0.0456)\n",
      "6780 Training Loss: tensor(0.0456)\n",
      "6781 Training Loss: tensor(0.0452)\n",
      "6782 Training Loss: tensor(0.0455)\n",
      "6783 Training Loss: tensor(0.0456)\n",
      "6784 Training Loss: tensor(0.0453)\n",
      "6785 Training Loss: tensor(0.0455)\n",
      "6786 Training Loss: tensor(0.0457)\n",
      "6787 Training Loss: tensor(0.0458)\n",
      "6788 Training Loss: tensor(0.0456)\n",
      "6789 Training Loss: tensor(0.0458)\n",
      "6790 Training Loss: tensor(0.0454)\n",
      "6791 Training Loss: tensor(0.0453)\n",
      "6792 Training Loss: tensor(0.0459)\n",
      "6793 Training Loss: tensor(0.0454)\n",
      "6794 Training Loss: tensor(0.0456)\n",
      "6795 Training Loss: tensor(0.0455)\n",
      "6796 Training Loss: tensor(0.0456)\n",
      "6797 Training Loss: tensor(0.0455)\n",
      "6798 Training Loss: tensor(0.0456)\n",
      "6799 Training Loss: tensor(0.0455)\n",
      "6800 Training Loss: tensor(0.0459)\n",
      "6801 Training Loss: tensor(0.0454)\n",
      "6802 Training Loss: tensor(0.0455)\n",
      "6803 Training Loss: tensor(0.0455)\n",
      "6804 Training Loss: tensor(0.0456)\n",
      "6805 Training Loss: tensor(0.0458)\n",
      "6806 Training Loss: tensor(0.0455)\n",
      "6807 Training Loss: tensor(0.0455)\n",
      "6808 Training Loss: tensor(0.0456)\n",
      "6809 Training Loss: tensor(0.0453)\n",
      "6810 Training Loss: tensor(0.0456)\n",
      "6811 Training Loss: tensor(0.0456)\n",
      "6812 Training Loss: tensor(0.0456)\n",
      "6813 Training Loss: tensor(0.0454)\n",
      "6814 Training Loss: tensor(0.0456)\n",
      "6815 Training Loss: tensor(0.0457)\n",
      "6816 Training Loss: tensor(0.0453)\n",
      "6817 Training Loss: tensor(0.0454)\n",
      "6818 Training Loss: tensor(0.0457)\n",
      "6819 Training Loss: tensor(0.0457)\n",
      "6820 Training Loss: tensor(0.0455)\n",
      "6821 Training Loss: tensor(0.0455)\n",
      "6822 Training Loss: tensor(0.0459)\n",
      "6823 Training Loss: tensor(0.0456)\n",
      "6824 Training Loss: tensor(0.0454)\n",
      "6825 Training Loss: tensor(0.0457)\n",
      "6826 Training Loss: tensor(0.0459)\n",
      "6827 Training Loss: tensor(0.0456)\n",
      "6828 Training Loss: tensor(0.0458)\n",
      "6829 Training Loss: tensor(0.0453)\n",
      "6830 Training Loss: tensor(0.0457)\n",
      "6831 Training Loss: tensor(0.0458)\n",
      "6832 Training Loss: tensor(0.0456)\n",
      "6833 Training Loss: tensor(0.0455)\n",
      "6834 Training Loss: tensor(0.0458)\n",
      "6835 Training Loss: tensor(0.0454)\n",
      "6836 Training Loss: tensor(0.0459)\n",
      "6837 Training Loss: tensor(0.0456)\n",
      "6838 Training Loss: tensor(0.0455)\n",
      "6839 Training Loss: tensor(0.0457)\n",
      "6840 Training Loss: tensor(0.0453)\n",
      "6841 Training Loss: tensor(0.0455)\n",
      "6842 Training Loss: tensor(0.0457)\n",
      "6843 Training Loss: tensor(0.0456)\n",
      "6844 Training Loss: tensor(0.0455)\n",
      "6845 Training Loss: tensor(0.0456)\n",
      "6846 Training Loss: tensor(0.0455)\n",
      "6847 Training Loss: tensor(0.0455)\n",
      "6848 Training Loss: tensor(0.0457)\n",
      "6849 Training Loss: tensor(0.0451)\n",
      "6850 Training Loss: tensor(0.0454)\n",
      "6851 Training Loss: tensor(0.0456)\n",
      "6852 Training Loss: tensor(0.0457)\n",
      "6853 Training Loss: tensor(0.0456)\n",
      "6854 Training Loss: tensor(0.0457)\n",
      "6855 Training Loss: tensor(0.0453)\n",
      "6856 Training Loss: tensor(0.0455)\n",
      "6857 Training Loss: tensor(0.0457)\n",
      "6858 Training Loss: tensor(0.0453)\n",
      "6859 Training Loss: tensor(0.0454)\n",
      "6860 Training Loss: tensor(0.0454)\n",
      "6861 Training Loss: tensor(0.0457)\n",
      "6862 Training Loss: tensor(0.0455)\n",
      "6863 Training Loss: tensor(0.0455)\n",
      "6864 Training Loss: tensor(0.0453)\n",
      "6865 Training Loss: tensor(0.0455)\n",
      "6866 Training Loss: tensor(0.0459)\n",
      "6867 Training Loss: tensor(0.0454)\n",
      "6868 Training Loss: tensor(0.0454)\n",
      "6869 Training Loss: tensor(0.0452)\n",
      "6870 Training Loss: tensor(0.0458)\n",
      "6871 Training Loss: tensor(0.0455)\n",
      "6872 Training Loss: tensor(0.0455)\n",
      "6873 Training Loss: tensor(0.0457)\n",
      "6874 Training Loss: tensor(0.0454)\n",
      "6875 Training Loss: tensor(0.0455)\n",
      "6876 Training Loss: tensor(0.0456)\n",
      "6877 Training Loss: tensor(0.0458)\n",
      "6878 Training Loss: tensor(0.0456)\n",
      "6879 Training Loss: tensor(0.0456)\n",
      "6880 Training Loss: tensor(0.0452)\n",
      "6881 Training Loss: tensor(0.0456)\n",
      "6882 Training Loss: tensor(0.0454)\n",
      "6883 Training Loss: tensor(0.0456)\n",
      "6884 Training Loss: tensor(0.0454)\n",
      "6885 Training Loss: tensor(0.0455)\n",
      "6886 Training Loss: tensor(0.0455)\n",
      "6887 Training Loss: tensor(0.0458)\n",
      "6888 Training Loss: tensor(0.0456)\n",
      "6889 Training Loss: tensor(0.0457)\n",
      "6890 Training Loss: tensor(0.0454)\n",
      "6891 Training Loss: tensor(0.0454)\n",
      "6892 Training Loss: tensor(0.0456)\n",
      "6893 Training Loss: tensor(0.0454)\n",
      "6894 Training Loss: tensor(0.0452)\n",
      "6895 Training Loss: tensor(0.0454)\n",
      "6896 Training Loss: tensor(0.0454)\n",
      "6897 Training Loss: tensor(0.0459)\n",
      "6898 Training Loss: tensor(0.0452)\n",
      "6899 Training Loss: tensor(0.0454)\n",
      "6900 Training Loss: tensor(0.0453)\n",
      "6901 Training Loss: tensor(0.0457)\n",
      "6902 Training Loss: tensor(0.0456)\n",
      "6903 Training Loss: tensor(0.0454)\n",
      "6904 Training Loss: tensor(0.0454)\n",
      "6905 Training Loss: tensor(0.0456)\n",
      "6906 Training Loss: tensor(0.0453)\n",
      "6907 Training Loss: tensor(0.0453)\n",
      "6908 Training Loss: tensor(0.0453)\n",
      "6909 Training Loss: tensor(0.0453)\n",
      "6910 Training Loss: tensor(0.0452)\n",
      "6911 Training Loss: tensor(0.0455)\n",
      "6912 Training Loss: tensor(0.0454)\n",
      "6913 Training Loss: tensor(0.0451)\n",
      "6914 Training Loss: tensor(0.0456)\n",
      "6915 Training Loss: tensor(0.0457)\n",
      "6916 Training Loss: tensor(0.0454)\n",
      "6917 Training Loss: tensor(0.0453)\n",
      "6918 Training Loss: tensor(0.0455)\n",
      "6919 Training Loss: tensor(0.0454)\n",
      "6920 Training Loss: tensor(0.0453)\n",
      "6921 Training Loss: tensor(0.0456)\n",
      "6922 Training Loss: tensor(0.0453)\n",
      "6923 Training Loss: tensor(0.0457)\n",
      "6924 Training Loss: tensor(0.0454)\n",
      "6925 Training Loss: tensor(0.0452)\n",
      "6926 Training Loss: tensor(0.0457)\n",
      "6927 Training Loss: tensor(0.0455)\n",
      "6928 Training Loss: tensor(0.0453)\n",
      "6929 Training Loss: tensor(0.0454)\n",
      "6930 Training Loss: tensor(0.0456)\n",
      "6931 Training Loss: tensor(0.0454)\n",
      "6932 Training Loss: tensor(0.0453)\n",
      "6933 Training Loss: tensor(0.0457)\n",
      "6934 Training Loss: tensor(0.0454)\n",
      "6935 Training Loss: tensor(0.0457)\n",
      "6936 Training Loss: tensor(0.0454)\n",
      "6937 Training Loss: tensor(0.0454)\n",
      "6938 Training Loss: tensor(0.0453)\n",
      "6939 Training Loss: tensor(0.0457)\n",
      "6940 Training Loss: tensor(0.0455)\n",
      "6941 Training Loss: tensor(0.0453)\n",
      "6942 Training Loss: tensor(0.0455)\n",
      "6943 Training Loss: tensor(0.0455)\n",
      "6944 Training Loss: tensor(0.0455)\n",
      "6945 Training Loss: tensor(0.0454)\n",
      "6946 Training Loss: tensor(0.0453)\n",
      "6947 Training Loss: tensor(0.0453)\n",
      "6948 Training Loss: tensor(0.0453)\n",
      "6949 Training Loss: tensor(0.0455)\n",
      "6950 Training Loss: tensor(0.0454)\n",
      "6951 Training Loss: tensor(0.0456)\n",
      "6952 Training Loss: tensor(0.0453)\n",
      "6953 Training Loss: tensor(0.0454)\n",
      "6954 Training Loss: tensor(0.0455)\n",
      "6955 Training Loss: tensor(0.0457)\n",
      "6956 Training Loss: tensor(0.0453)\n",
      "6957 Training Loss: tensor(0.0453)\n",
      "6958 Training Loss: tensor(0.0455)\n",
      "6959 Training Loss: tensor(0.0456)\n",
      "6960 Training Loss: tensor(0.0456)\n",
      "6961 Training Loss: tensor(0.0455)\n",
      "6962 Training Loss: tensor(0.0454)\n",
      "6963 Training Loss: tensor(0.0453)\n",
      "6964 Training Loss: tensor(0.0454)\n",
      "6965 Training Loss: tensor(0.0452)\n",
      "6966 Training Loss: tensor(0.0451)\n",
      "6967 Training Loss: tensor(0.0454)\n",
      "6968 Training Loss: tensor(0.0455)\n",
      "6969 Training Loss: tensor(0.0453)\n",
      "6970 Training Loss: tensor(0.0453)\n",
      "6971 Training Loss: tensor(0.0457)\n",
      "6972 Training Loss: tensor(0.0452)\n",
      "6973 Training Loss: tensor(0.0453)\n",
      "6974 Training Loss: tensor(0.0451)\n",
      "6975 Training Loss: tensor(0.0454)\n",
      "6976 Training Loss: tensor(0.0454)\n",
      "6977 Training Loss: tensor(0.0452)\n",
      "6978 Training Loss: tensor(0.0454)\n",
      "6979 Training Loss: tensor(0.0454)\n",
      "6980 Training Loss: tensor(0.0453)\n",
      "6981 Training Loss: tensor(0.0453)\n",
      "6982 Training Loss: tensor(0.0458)\n",
      "6983 Training Loss: tensor(0.0455)\n",
      "6984 Training Loss: tensor(0.0452)\n",
      "6985 Training Loss: tensor(0.0453)\n",
      "6986 Training Loss: tensor(0.0456)\n",
      "6987 Training Loss: tensor(0.0456)\n",
      "6988 Training Loss: tensor(0.0454)\n",
      "6989 Training Loss: tensor(0.0453)\n",
      "6990 Training Loss: tensor(0.0454)\n",
      "6991 Training Loss: tensor(0.0453)\n",
      "6992 Training Loss: tensor(0.0454)\n",
      "6993 Training Loss: tensor(0.0454)\n",
      "6994 Training Loss: tensor(0.0454)\n",
      "6995 Training Loss: tensor(0.0454)\n",
      "6996 Training Loss: tensor(0.0456)\n",
      "6997 Training Loss: tensor(0.0456)\n",
      "6998 Training Loss: tensor(0.0455)\n",
      "6999 Training Loss: tensor(0.0455)\n",
      "7000 Training Loss: tensor(0.0455)\n",
      "7001 Training Loss: tensor(0.0456)\n",
      "7002 Training Loss: tensor(0.0455)\n",
      "7003 Training Loss: tensor(0.0455)\n",
      "7004 Training Loss: tensor(0.0452)\n",
      "7005 Training Loss: tensor(0.0455)\n",
      "7006 Training Loss: tensor(0.0455)\n",
      "7007 Training Loss: tensor(0.0453)\n",
      "7008 Training Loss: tensor(0.0453)\n",
      "7009 Training Loss: tensor(0.0453)\n",
      "7010 Training Loss: tensor(0.0453)\n",
      "7011 Training Loss: tensor(0.0454)\n",
      "7012 Training Loss: tensor(0.0454)\n",
      "7013 Training Loss: tensor(0.0453)\n",
      "7014 Training Loss: tensor(0.0452)\n",
      "7015 Training Loss: tensor(0.0455)\n",
      "7016 Training Loss: tensor(0.0454)\n",
      "7017 Training Loss: tensor(0.0451)\n",
      "7018 Training Loss: tensor(0.0456)\n",
      "7019 Training Loss: tensor(0.0451)\n",
      "7020 Training Loss: tensor(0.0454)\n",
      "7021 Training Loss: tensor(0.0454)\n",
      "7022 Training Loss: tensor(0.0451)\n",
      "7023 Training Loss: tensor(0.0455)\n",
      "7024 Training Loss: tensor(0.0455)\n",
      "7025 Training Loss: tensor(0.0455)\n",
      "7026 Training Loss: tensor(0.0458)\n",
      "7027 Training Loss: tensor(0.0452)\n",
      "7028 Training Loss: tensor(0.0451)\n",
      "7029 Training Loss: tensor(0.0455)\n",
      "7030 Training Loss: tensor(0.0455)\n",
      "7031 Training Loss: tensor(0.0454)\n",
      "7032 Training Loss: tensor(0.0452)\n",
      "7033 Training Loss: tensor(0.0454)\n",
      "7034 Training Loss: tensor(0.0451)\n",
      "7035 Training Loss: tensor(0.0456)\n",
      "7036 Training Loss: tensor(0.0453)\n",
      "7037 Training Loss: tensor(0.0452)\n",
      "7038 Training Loss: tensor(0.0456)\n",
      "7039 Training Loss: tensor(0.0453)\n",
      "7040 Training Loss: tensor(0.0453)\n",
      "7041 Training Loss: tensor(0.0452)\n",
      "7042 Training Loss: tensor(0.0455)\n",
      "7043 Training Loss: tensor(0.0455)\n",
      "7044 Training Loss: tensor(0.0455)\n",
      "7045 Training Loss: tensor(0.0454)\n",
      "7046 Training Loss: tensor(0.0453)\n",
      "7047 Training Loss: tensor(0.0454)\n",
      "7048 Training Loss: tensor(0.0454)\n",
      "7049 Training Loss: tensor(0.0453)\n",
      "7050 Training Loss: tensor(0.0453)\n",
      "7051 Training Loss: tensor(0.0452)\n",
      "7052 Training Loss: tensor(0.0453)\n",
      "7053 Training Loss: tensor(0.0456)\n",
      "7054 Training Loss: tensor(0.0455)\n",
      "7055 Training Loss: tensor(0.0452)\n",
      "7056 Training Loss: tensor(0.0455)\n",
      "7057 Training Loss: tensor(0.0452)\n",
      "7058 Training Loss: tensor(0.0454)\n",
      "7059 Training Loss: tensor(0.0452)\n",
      "7060 Training Loss: tensor(0.0453)\n",
      "7061 Training Loss: tensor(0.0452)\n",
      "7062 Training Loss: tensor(0.0455)\n",
      "7063 Training Loss: tensor(0.0454)\n",
      "7064 Training Loss: tensor(0.0453)\n",
      "7065 Training Loss: tensor(0.0452)\n",
      "7066 Training Loss: tensor(0.0453)\n",
      "7067 Training Loss: tensor(0.0452)\n",
      "7068 Training Loss: tensor(0.0453)\n",
      "7069 Training Loss: tensor(0.0453)\n",
      "7070 Training Loss: tensor(0.0454)\n",
      "7071 Training Loss: tensor(0.0455)\n",
      "7072 Training Loss: tensor(0.0451)\n",
      "7073 Training Loss: tensor(0.0455)\n",
      "7074 Training Loss: tensor(0.0451)\n",
      "7075 Training Loss: tensor(0.0451)\n",
      "7076 Training Loss: tensor(0.0454)\n",
      "7077 Training Loss: tensor(0.0453)\n",
      "7078 Training Loss: tensor(0.0453)\n",
      "7079 Training Loss: tensor(0.0453)\n",
      "7080 Training Loss: tensor(0.0455)\n",
      "7081 Training Loss: tensor(0.0451)\n",
      "7082 Training Loss: tensor(0.0452)\n",
      "7083 Training Loss: tensor(0.0453)\n",
      "7084 Training Loss: tensor(0.0453)\n",
      "7085 Training Loss: tensor(0.0453)\n",
      "7086 Training Loss: tensor(0.0455)\n",
      "7087 Training Loss: tensor(0.0453)\n",
      "7088 Training Loss: tensor(0.0453)\n",
      "7089 Training Loss: tensor(0.0451)\n",
      "7090 Training Loss: tensor(0.0455)\n",
      "7091 Training Loss: tensor(0.0453)\n",
      "7092 Training Loss: tensor(0.0453)\n",
      "7093 Training Loss: tensor(0.0453)\n",
      "7094 Training Loss: tensor(0.0453)\n",
      "7095 Training Loss: tensor(0.0452)\n",
      "7096 Training Loss: tensor(0.0453)\n",
      "7097 Training Loss: tensor(0.0451)\n",
      "7098 Training Loss: tensor(0.0456)\n",
      "7099 Training Loss: tensor(0.0453)\n",
      "7100 Training Loss: tensor(0.0454)\n",
      "7101 Training Loss: tensor(0.0452)\n",
      "7102 Training Loss: tensor(0.0454)\n",
      "7103 Training Loss: tensor(0.0451)\n",
      "7104 Training Loss: tensor(0.0451)\n",
      "7105 Training Loss: tensor(0.0452)\n",
      "7106 Training Loss: tensor(0.0452)\n",
      "7107 Training Loss: tensor(0.0453)\n",
      "7108 Training Loss: tensor(0.0451)\n",
      "7109 Training Loss: tensor(0.0451)\n",
      "7110 Training Loss: tensor(0.0453)\n",
      "7111 Training Loss: tensor(0.0452)\n",
      "7112 Training Loss: tensor(0.0454)\n",
      "7113 Training Loss: tensor(0.0455)\n",
      "7114 Training Loss: tensor(0.0452)\n",
      "7115 Training Loss: tensor(0.0453)\n",
      "7116 Training Loss: tensor(0.0452)\n",
      "7117 Training Loss: tensor(0.0451)\n",
      "7118 Training Loss: tensor(0.0452)\n",
      "7119 Training Loss: tensor(0.0452)\n",
      "7120 Training Loss: tensor(0.0454)\n",
      "7121 Training Loss: tensor(0.0453)\n",
      "7122 Training Loss: tensor(0.0452)\n",
      "7123 Training Loss: tensor(0.0454)\n",
      "7124 Training Loss: tensor(0.0456)\n",
      "7125 Training Loss: tensor(0.0452)\n",
      "7126 Training Loss: tensor(0.0453)\n",
      "7127 Training Loss: tensor(0.0451)\n",
      "7128 Training Loss: tensor(0.0453)\n",
      "7129 Training Loss: tensor(0.0451)\n",
      "7130 Training Loss: tensor(0.0452)\n",
      "7131 Training Loss: tensor(0.0453)\n",
      "7132 Training Loss: tensor(0.0457)\n",
      "7133 Training Loss: tensor(0.0453)\n",
      "7134 Training Loss: tensor(0.0451)\n",
      "7135 Training Loss: tensor(0.0452)\n",
      "7136 Training Loss: tensor(0.0454)\n",
      "7137 Training Loss: tensor(0.0453)\n",
      "7138 Training Loss: tensor(0.0450)\n",
      "7139 Training Loss: tensor(0.0453)\n",
      "7140 Training Loss: tensor(0.0455)\n",
      "7141 Training Loss: tensor(0.0453)\n",
      "7142 Training Loss: tensor(0.0454)\n",
      "7143 Training Loss: tensor(0.0451)\n",
      "7144 Training Loss: tensor(0.0455)\n",
      "7145 Training Loss: tensor(0.0452)\n",
      "7146 Training Loss: tensor(0.0451)\n",
      "7147 Training Loss: tensor(0.0453)\n",
      "7148 Training Loss: tensor(0.0454)\n",
      "7149 Training Loss: tensor(0.0451)\n",
      "7150 Training Loss: tensor(0.0451)\n",
      "7151 Training Loss: tensor(0.0454)\n",
      "7152 Training Loss: tensor(0.0455)\n",
      "7153 Training Loss: tensor(0.0452)\n",
      "7154 Training Loss: tensor(0.0452)\n",
      "7155 Training Loss: tensor(0.0450)\n",
      "7156 Training Loss: tensor(0.0450)\n",
      "7157 Training Loss: tensor(0.0454)\n",
      "7158 Training Loss: tensor(0.0454)\n",
      "7159 Training Loss: tensor(0.0455)\n",
      "7160 Training Loss: tensor(0.0453)\n",
      "7161 Training Loss: tensor(0.0451)\n",
      "7162 Training Loss: tensor(0.0452)\n",
      "7163 Training Loss: tensor(0.0452)\n",
      "7164 Training Loss: tensor(0.0452)\n",
      "7165 Training Loss: tensor(0.0451)\n",
      "7166 Training Loss: tensor(0.0452)\n",
      "7167 Training Loss: tensor(0.0454)\n",
      "7168 Training Loss: tensor(0.0450)\n",
      "7169 Training Loss: tensor(0.0452)\n",
      "7170 Training Loss: tensor(0.0451)\n",
      "7171 Training Loss: tensor(0.0453)\n",
      "7172 Training Loss: tensor(0.0452)\n",
      "7173 Training Loss: tensor(0.0453)\n",
      "7174 Training Loss: tensor(0.0453)\n",
      "7175 Training Loss: tensor(0.0450)\n",
      "7176 Training Loss: tensor(0.0452)\n",
      "7177 Training Loss: tensor(0.0449)\n",
      "7178 Training Loss: tensor(0.0455)\n",
      "7179 Training Loss: tensor(0.0455)\n",
      "7180 Training Loss: tensor(0.0454)\n",
      "7181 Training Loss: tensor(0.0452)\n",
      "7182 Training Loss: tensor(0.0453)\n",
      "7183 Training Loss: tensor(0.0454)\n",
      "7184 Training Loss: tensor(0.0452)\n",
      "7185 Training Loss: tensor(0.0451)\n",
      "7186 Training Loss: tensor(0.0452)\n",
      "7187 Training Loss: tensor(0.0450)\n",
      "7188 Training Loss: tensor(0.0453)\n",
      "7189 Training Loss: tensor(0.0454)\n",
      "7190 Training Loss: tensor(0.0452)\n",
      "7191 Training Loss: tensor(0.0451)\n",
      "7192 Training Loss: tensor(0.0452)\n",
      "7193 Training Loss: tensor(0.0451)\n",
      "7194 Training Loss: tensor(0.0453)\n",
      "7195 Training Loss: tensor(0.0452)\n",
      "7196 Training Loss: tensor(0.0451)\n",
      "7197 Training Loss: tensor(0.0453)\n",
      "7198 Training Loss: tensor(0.0449)\n",
      "7199 Training Loss: tensor(0.0452)\n",
      "7200 Training Loss: tensor(0.0454)\n",
      "7201 Training Loss: tensor(0.0453)\n",
      "7202 Training Loss: tensor(0.0453)\n",
      "7203 Training Loss: tensor(0.0456)\n",
      "7204 Training Loss: tensor(0.0454)\n",
      "7205 Training Loss: tensor(0.0453)\n",
      "7206 Training Loss: tensor(0.0451)\n",
      "7207 Training Loss: tensor(0.0452)\n",
      "7208 Training Loss: tensor(0.0454)\n",
      "7209 Training Loss: tensor(0.0452)\n",
      "7210 Training Loss: tensor(0.0453)\n",
      "7211 Training Loss: tensor(0.0450)\n",
      "7212 Training Loss: tensor(0.0451)\n",
      "7213 Training Loss: tensor(0.0452)\n",
      "7214 Training Loss: tensor(0.0454)\n",
      "7215 Training Loss: tensor(0.0451)\n",
      "7216 Training Loss: tensor(0.0451)\n",
      "7217 Training Loss: tensor(0.0454)\n",
      "7218 Training Loss: tensor(0.0452)\n",
      "7219 Training Loss: tensor(0.0450)\n",
      "7220 Training Loss: tensor(0.0451)\n",
      "7221 Training Loss: tensor(0.0453)\n",
      "7222 Training Loss: tensor(0.0453)\n",
      "7223 Training Loss: tensor(0.0450)\n",
      "7224 Training Loss: tensor(0.0449)\n",
      "7225 Training Loss: tensor(0.0452)\n",
      "7226 Training Loss: tensor(0.0451)\n",
      "7227 Training Loss: tensor(0.0454)\n",
      "7228 Training Loss: tensor(0.0449)\n",
      "7229 Training Loss: tensor(0.0453)\n",
      "7230 Training Loss: tensor(0.0450)\n",
      "7231 Training Loss: tensor(0.0453)\n",
      "7232 Training Loss: tensor(0.0450)\n",
      "7233 Training Loss: tensor(0.0452)\n",
      "7234 Training Loss: tensor(0.0451)\n",
      "7235 Training Loss: tensor(0.0451)\n",
      "7236 Training Loss: tensor(0.0454)\n",
      "7237 Training Loss: tensor(0.0452)\n",
      "7238 Training Loss: tensor(0.0451)\n",
      "7239 Training Loss: tensor(0.0450)\n",
      "7240 Training Loss: tensor(0.0453)\n",
      "7241 Training Loss: tensor(0.0454)\n",
      "7242 Training Loss: tensor(0.0454)\n",
      "7243 Training Loss: tensor(0.0449)\n",
      "7244 Training Loss: tensor(0.0452)\n",
      "7245 Training Loss: tensor(0.0451)\n",
      "7246 Training Loss: tensor(0.0451)\n",
      "7247 Training Loss: tensor(0.0451)\n",
      "7248 Training Loss: tensor(0.0448)\n",
      "7249 Training Loss: tensor(0.0452)\n",
      "7250 Training Loss: tensor(0.0452)\n",
      "7251 Training Loss: tensor(0.0451)\n",
      "7252 Training Loss: tensor(0.0451)\n",
      "7253 Training Loss: tensor(0.0450)\n",
      "7254 Training Loss: tensor(0.0454)\n",
      "7255 Training Loss: tensor(0.0452)\n",
      "7256 Training Loss: tensor(0.0449)\n",
      "7257 Training Loss: tensor(0.0450)\n",
      "7258 Training Loss: tensor(0.0453)\n",
      "7259 Training Loss: tensor(0.0451)\n",
      "7260 Training Loss: tensor(0.0448)\n",
      "7261 Training Loss: tensor(0.0452)\n",
      "7262 Training Loss: tensor(0.0449)\n",
      "7263 Training Loss: tensor(0.0452)\n",
      "7264 Training Loss: tensor(0.0457)\n",
      "7265 Training Loss: tensor(0.0452)\n",
      "7266 Training Loss: tensor(0.0452)\n",
      "7267 Training Loss: tensor(0.0452)\n",
      "7268 Training Loss: tensor(0.0450)\n",
      "7269 Training Loss: tensor(0.0452)\n",
      "7270 Training Loss: tensor(0.0452)\n",
      "7271 Training Loss: tensor(0.0450)\n",
      "7272 Training Loss: tensor(0.0454)\n",
      "7273 Training Loss: tensor(0.0454)\n",
      "7274 Training Loss: tensor(0.0453)\n",
      "7275 Training Loss: tensor(0.0452)\n",
      "7276 Training Loss: tensor(0.0454)\n",
      "7277 Training Loss: tensor(0.0451)\n",
      "7278 Training Loss: tensor(0.0450)\n",
      "7279 Training Loss: tensor(0.0450)\n",
      "7280 Training Loss: tensor(0.0451)\n",
      "7281 Training Loss: tensor(0.0451)\n",
      "7282 Training Loss: tensor(0.0452)\n",
      "7283 Training Loss: tensor(0.0451)\n",
      "7284 Training Loss: tensor(0.0450)\n",
      "7285 Training Loss: tensor(0.0451)\n",
      "7286 Training Loss: tensor(0.0450)\n",
      "7287 Training Loss: tensor(0.0450)\n",
      "7288 Training Loss: tensor(0.0452)\n",
      "7289 Training Loss: tensor(0.0451)\n",
      "7290 Training Loss: tensor(0.0451)\n",
      "7291 Training Loss: tensor(0.0449)\n",
      "7292 Training Loss: tensor(0.0453)\n",
      "7293 Training Loss: tensor(0.0452)\n",
      "7294 Training Loss: tensor(0.0453)\n",
      "7295 Training Loss: tensor(0.0453)\n",
      "7296 Training Loss: tensor(0.0452)\n",
      "7297 Training Loss: tensor(0.0452)\n",
      "7298 Training Loss: tensor(0.0452)\n",
      "7299 Training Loss: tensor(0.0449)\n",
      "7300 Training Loss: tensor(0.0452)\n",
      "7301 Training Loss: tensor(0.0450)\n",
      "7302 Training Loss: tensor(0.0450)\n",
      "7303 Training Loss: tensor(0.0451)\n",
      "7304 Training Loss: tensor(0.0449)\n",
      "7305 Training Loss: tensor(0.0452)\n",
      "7306 Training Loss: tensor(0.0457)\n",
      "7307 Training Loss: tensor(0.0449)\n",
      "7308 Training Loss: tensor(0.0452)\n",
      "7309 Training Loss: tensor(0.0449)\n",
      "7310 Training Loss: tensor(0.0449)\n",
      "7311 Training Loss: tensor(0.0452)\n",
      "7312 Training Loss: tensor(0.0449)\n",
      "7313 Training Loss: tensor(0.0452)\n",
      "7314 Training Loss: tensor(0.0450)\n",
      "7315 Training Loss: tensor(0.0451)\n",
      "7316 Training Loss: tensor(0.0453)\n",
      "7317 Training Loss: tensor(0.0451)\n",
      "7318 Training Loss: tensor(0.0450)\n",
      "7319 Training Loss: tensor(0.0452)\n",
      "7320 Training Loss: tensor(0.0452)\n",
      "7321 Training Loss: tensor(0.0451)\n",
      "7322 Training Loss: tensor(0.0452)\n",
      "7323 Training Loss: tensor(0.0451)\n",
      "7324 Training Loss: tensor(0.0452)\n",
      "7325 Training Loss: tensor(0.0451)\n",
      "7326 Training Loss: tensor(0.0450)\n",
      "7327 Training Loss: tensor(0.0451)\n",
      "7328 Training Loss: tensor(0.0452)\n",
      "7329 Training Loss: tensor(0.0451)\n",
      "7330 Training Loss: tensor(0.0446)\n",
      "7331 Training Loss: tensor(0.0449)\n",
      "7332 Training Loss: tensor(0.0451)\n",
      "7333 Training Loss: tensor(0.0450)\n",
      "7334 Training Loss: tensor(0.0453)\n",
      "7335 Training Loss: tensor(0.0450)\n",
      "7336 Training Loss: tensor(0.0451)\n",
      "7337 Training Loss: tensor(0.0450)\n",
      "7338 Training Loss: tensor(0.0450)\n",
      "7339 Training Loss: tensor(0.0451)\n",
      "7340 Training Loss: tensor(0.0448)\n",
      "7341 Training Loss: tensor(0.0454)\n",
      "7342 Training Loss: tensor(0.0453)\n",
      "7343 Training Loss: tensor(0.0448)\n",
      "7344 Training Loss: tensor(0.0451)\n",
      "7345 Training Loss: tensor(0.0452)\n",
      "7346 Training Loss: tensor(0.0448)\n",
      "7347 Training Loss: tensor(0.0451)\n",
      "7348 Training Loss: tensor(0.0450)\n",
      "7349 Training Loss: tensor(0.0450)\n",
      "7350 Training Loss: tensor(0.0451)\n",
      "7351 Training Loss: tensor(0.0449)\n",
      "7352 Training Loss: tensor(0.0450)\n",
      "7353 Training Loss: tensor(0.0449)\n",
      "7354 Training Loss: tensor(0.0449)\n",
      "7355 Training Loss: tensor(0.0449)\n",
      "7356 Training Loss: tensor(0.0451)\n",
      "7357 Training Loss: tensor(0.0450)\n",
      "7358 Training Loss: tensor(0.0450)\n",
      "7359 Training Loss: tensor(0.0451)\n",
      "7360 Training Loss: tensor(0.0449)\n",
      "7361 Training Loss: tensor(0.0450)\n",
      "7362 Training Loss: tensor(0.0451)\n",
      "7363 Training Loss: tensor(0.0450)\n",
      "7364 Training Loss: tensor(0.0450)\n",
      "7365 Training Loss: tensor(0.0450)\n",
      "7366 Training Loss: tensor(0.0452)\n",
      "7367 Training Loss: tensor(0.0452)\n",
      "7368 Training Loss: tensor(0.0449)\n",
      "7369 Training Loss: tensor(0.0448)\n",
      "7370 Training Loss: tensor(0.0451)\n",
      "7371 Training Loss: tensor(0.0450)\n",
      "7372 Training Loss: tensor(0.0449)\n",
      "7373 Training Loss: tensor(0.0451)\n",
      "7374 Training Loss: tensor(0.0448)\n",
      "7375 Training Loss: tensor(0.0454)\n",
      "7376 Training Loss: tensor(0.0450)\n",
      "7377 Training Loss: tensor(0.0451)\n",
      "7378 Training Loss: tensor(0.0450)\n",
      "7379 Training Loss: tensor(0.0449)\n",
      "7380 Training Loss: tensor(0.0452)\n",
      "7381 Training Loss: tensor(0.0451)\n",
      "7382 Training Loss: tensor(0.0449)\n",
      "7383 Training Loss: tensor(0.0451)\n",
      "7384 Training Loss: tensor(0.0450)\n",
      "7385 Training Loss: tensor(0.0450)\n",
      "7386 Training Loss: tensor(0.0448)\n",
      "7387 Training Loss: tensor(0.0450)\n",
      "7388 Training Loss: tensor(0.0448)\n",
      "7389 Training Loss: tensor(0.0452)\n",
      "7390 Training Loss: tensor(0.0450)\n",
      "7391 Training Loss: tensor(0.0451)\n",
      "7392 Training Loss: tensor(0.0448)\n",
      "7393 Training Loss: tensor(0.0451)\n",
      "7394 Training Loss: tensor(0.0451)\n",
      "7395 Training Loss: tensor(0.0448)\n",
      "7396 Training Loss: tensor(0.0450)\n",
      "7397 Training Loss: tensor(0.0449)\n",
      "7398 Training Loss: tensor(0.0451)\n",
      "7399 Training Loss: tensor(0.0449)\n",
      "7400 Training Loss: tensor(0.0450)\n",
      "7401 Training Loss: tensor(0.0449)\n",
      "7402 Training Loss: tensor(0.0451)\n",
      "7403 Training Loss: tensor(0.0448)\n",
      "7404 Training Loss: tensor(0.0449)\n",
      "7405 Training Loss: tensor(0.0452)\n",
      "7406 Training Loss: tensor(0.0450)\n",
      "7407 Training Loss: tensor(0.0450)\n",
      "7408 Training Loss: tensor(0.0449)\n",
      "7409 Training Loss: tensor(0.0448)\n",
      "7410 Training Loss: tensor(0.0450)\n",
      "7411 Training Loss: tensor(0.0450)\n",
      "7412 Training Loss: tensor(0.0450)\n",
      "7413 Training Loss: tensor(0.0448)\n",
      "7414 Training Loss: tensor(0.0450)\n",
      "7415 Training Loss: tensor(0.0451)\n",
      "7416 Training Loss: tensor(0.0450)\n",
      "7417 Training Loss: tensor(0.0451)\n",
      "7418 Training Loss: tensor(0.0449)\n",
      "7419 Training Loss: tensor(0.0453)\n",
      "7420 Training Loss: tensor(0.0449)\n",
      "7421 Training Loss: tensor(0.0449)\n",
      "7422 Training Loss: tensor(0.0450)\n",
      "7423 Training Loss: tensor(0.0452)\n",
      "7424 Training Loss: tensor(0.0447)\n",
      "7425 Training Loss: tensor(0.0448)\n",
      "7426 Training Loss: tensor(0.0449)\n",
      "7427 Training Loss: tensor(0.0449)\n",
      "7428 Training Loss: tensor(0.0450)\n",
      "7429 Training Loss: tensor(0.0451)\n",
      "7430 Training Loss: tensor(0.0450)\n",
      "7431 Training Loss: tensor(0.0449)\n",
      "7432 Training Loss: tensor(0.0451)\n",
      "7433 Training Loss: tensor(0.0450)\n",
      "7434 Training Loss: tensor(0.0449)\n",
      "7435 Training Loss: tensor(0.0448)\n",
      "7436 Training Loss: tensor(0.0448)\n",
      "7437 Training Loss: tensor(0.0449)\n",
      "7438 Training Loss: tensor(0.0449)\n",
      "7439 Training Loss: tensor(0.0449)\n",
      "7440 Training Loss: tensor(0.0450)\n",
      "7441 Training Loss: tensor(0.0449)\n",
      "7442 Training Loss: tensor(0.0448)\n",
      "7443 Training Loss: tensor(0.0452)\n",
      "7444 Training Loss: tensor(0.0448)\n",
      "7445 Training Loss: tensor(0.0450)\n",
      "7446 Training Loss: tensor(0.0451)\n",
      "7447 Training Loss: tensor(0.0447)\n",
      "7448 Training Loss: tensor(0.0451)\n",
      "7449 Training Loss: tensor(0.0450)\n",
      "7450 Training Loss: tensor(0.0453)\n",
      "7451 Training Loss: tensor(0.0451)\n",
      "7452 Training Loss: tensor(0.0450)\n",
      "7453 Training Loss: tensor(0.0452)\n",
      "7454 Training Loss: tensor(0.0453)\n",
      "7455 Training Loss: tensor(0.0450)\n",
      "7456 Training Loss: tensor(0.0448)\n",
      "7457 Training Loss: tensor(0.0449)\n",
      "7458 Training Loss: tensor(0.0448)\n",
      "7459 Training Loss: tensor(0.0451)\n",
      "7460 Training Loss: tensor(0.0448)\n",
      "7461 Training Loss: tensor(0.0449)\n",
      "7462 Training Loss: tensor(0.0451)\n",
      "7463 Training Loss: tensor(0.0449)\n",
      "7464 Training Loss: tensor(0.0449)\n",
      "7465 Training Loss: tensor(0.0448)\n",
      "7466 Training Loss: tensor(0.0447)\n",
      "7467 Training Loss: tensor(0.0450)\n",
      "7468 Training Loss: tensor(0.0450)\n",
      "7469 Training Loss: tensor(0.0449)\n",
      "7470 Training Loss: tensor(0.0448)\n",
      "7471 Training Loss: tensor(0.0449)\n",
      "7472 Training Loss: tensor(0.0450)\n",
      "7473 Training Loss: tensor(0.0447)\n",
      "7474 Training Loss: tensor(0.0448)\n",
      "7475 Training Loss: tensor(0.0449)\n",
      "7476 Training Loss: tensor(0.0447)\n",
      "7477 Training Loss: tensor(0.0449)\n",
      "7478 Training Loss: tensor(0.0453)\n",
      "7479 Training Loss: tensor(0.0449)\n",
      "7480 Training Loss: tensor(0.0449)\n",
      "7481 Training Loss: tensor(0.0450)\n",
      "7482 Training Loss: tensor(0.0450)\n",
      "7483 Training Loss: tensor(0.0448)\n",
      "7484 Training Loss: tensor(0.0452)\n",
      "7485 Training Loss: tensor(0.0448)\n",
      "7486 Training Loss: tensor(0.0451)\n",
      "7487 Training Loss: tensor(0.0449)\n",
      "7488 Training Loss: tensor(0.0452)\n",
      "7489 Training Loss: tensor(0.0449)\n",
      "7490 Training Loss: tensor(0.0449)\n",
      "7491 Training Loss: tensor(0.0451)\n",
      "7492 Training Loss: tensor(0.0449)\n",
      "7493 Training Loss: tensor(0.0448)\n",
      "7494 Training Loss: tensor(0.0448)\n",
      "7495 Training Loss: tensor(0.0449)\n",
      "7496 Training Loss: tensor(0.0450)\n",
      "7497 Training Loss: tensor(0.0447)\n",
      "7498 Training Loss: tensor(0.0451)\n",
      "7499 Training Loss: tensor(0.0450)\n",
      "7500 Training Loss: tensor(0.0450)\n",
      "7501 Training Loss: tensor(0.0451)\n",
      "7502 Training Loss: tensor(0.0453)\n",
      "7503 Training Loss: tensor(0.0450)\n",
      "7504 Training Loss: tensor(0.0452)\n",
      "7505 Training Loss: tensor(0.0448)\n",
      "7506 Training Loss: tensor(0.0451)\n",
      "7507 Training Loss: tensor(0.0447)\n",
      "7508 Training Loss: tensor(0.0451)\n",
      "7509 Training Loss: tensor(0.0449)\n",
      "7510 Training Loss: tensor(0.0449)\n",
      "7511 Training Loss: tensor(0.0447)\n",
      "7512 Training Loss: tensor(0.0448)\n",
      "7513 Training Loss: tensor(0.0449)\n",
      "7514 Training Loss: tensor(0.0448)\n",
      "7515 Training Loss: tensor(0.0448)\n",
      "7516 Training Loss: tensor(0.0453)\n",
      "7517 Training Loss: tensor(0.0449)\n",
      "7518 Training Loss: tensor(0.0449)\n",
      "7519 Training Loss: tensor(0.0449)\n",
      "7520 Training Loss: tensor(0.0450)\n",
      "7521 Training Loss: tensor(0.0450)\n",
      "7522 Training Loss: tensor(0.0451)\n",
      "7523 Training Loss: tensor(0.0448)\n",
      "7524 Training Loss: tensor(0.0450)\n",
      "7525 Training Loss: tensor(0.0448)\n",
      "7526 Training Loss: tensor(0.0448)\n",
      "7527 Training Loss: tensor(0.0448)\n",
      "7528 Training Loss: tensor(0.0449)\n",
      "7529 Training Loss: tensor(0.0448)\n",
      "7530 Training Loss: tensor(0.0447)\n",
      "7531 Training Loss: tensor(0.0450)\n",
      "7532 Training Loss: tensor(0.0446)\n",
      "7533 Training Loss: tensor(0.0448)\n",
      "7534 Training Loss: tensor(0.0452)\n",
      "7535 Training Loss: tensor(0.0449)\n",
      "7536 Training Loss: tensor(0.0449)\n",
      "7537 Training Loss: tensor(0.0450)\n",
      "7538 Training Loss: tensor(0.0450)\n",
      "7539 Training Loss: tensor(0.0450)\n",
      "7540 Training Loss: tensor(0.0447)\n",
      "7541 Training Loss: tensor(0.0449)\n",
      "7542 Training Loss: tensor(0.0448)\n",
      "7543 Training Loss: tensor(0.0447)\n",
      "7544 Training Loss: tensor(0.0449)\n",
      "7545 Training Loss: tensor(0.0446)\n",
      "7546 Training Loss: tensor(0.0448)\n",
      "7547 Training Loss: tensor(0.0446)\n",
      "7548 Training Loss: tensor(0.0448)\n",
      "7549 Training Loss: tensor(0.0449)\n",
      "7550 Training Loss: tensor(0.0451)\n",
      "7551 Training Loss: tensor(0.0450)\n",
      "7552 Training Loss: tensor(0.0449)\n",
      "7553 Training Loss: tensor(0.0451)\n",
      "7554 Training Loss: tensor(0.0447)\n",
      "7555 Training Loss: tensor(0.0451)\n",
      "7556 Training Loss: tensor(0.0448)\n",
      "7557 Training Loss: tensor(0.0451)\n",
      "7558 Training Loss: tensor(0.0448)\n",
      "7559 Training Loss: tensor(0.0448)\n",
      "7560 Training Loss: tensor(0.0448)\n",
      "7561 Training Loss: tensor(0.0450)\n",
      "7562 Training Loss: tensor(0.0448)\n",
      "7563 Training Loss: tensor(0.0447)\n",
      "7564 Training Loss: tensor(0.0450)\n",
      "7565 Training Loss: tensor(0.0449)\n",
      "7566 Training Loss: tensor(0.0450)\n",
      "7567 Training Loss: tensor(0.0448)\n",
      "7568 Training Loss: tensor(0.0447)\n",
      "7569 Training Loss: tensor(0.0448)\n",
      "7570 Training Loss: tensor(0.0451)\n",
      "7571 Training Loss: tensor(0.0449)\n",
      "7572 Training Loss: tensor(0.0450)\n",
      "7573 Training Loss: tensor(0.0446)\n",
      "7574 Training Loss: tensor(0.0448)\n",
      "7575 Training Loss: tensor(0.0448)\n",
      "7576 Training Loss: tensor(0.0445)\n",
      "7577 Training Loss: tensor(0.0448)\n",
      "7578 Training Loss: tensor(0.0450)\n",
      "7579 Training Loss: tensor(0.0449)\n",
      "7580 Training Loss: tensor(0.0448)\n",
      "7581 Training Loss: tensor(0.0448)\n",
      "7582 Training Loss: tensor(0.0448)\n",
      "7583 Training Loss: tensor(0.0449)\n",
      "7584 Training Loss: tensor(0.0452)\n",
      "7585 Training Loss: tensor(0.0447)\n",
      "7586 Training Loss: tensor(0.0447)\n",
      "7587 Training Loss: tensor(0.0448)\n",
      "7588 Training Loss: tensor(0.0446)\n",
      "7589 Training Loss: tensor(0.0446)\n",
      "7590 Training Loss: tensor(0.0451)\n",
      "7591 Training Loss: tensor(0.0451)\n",
      "7592 Training Loss: tensor(0.0451)\n",
      "7593 Training Loss: tensor(0.0446)\n",
      "7594 Training Loss: tensor(0.0445)\n",
      "7595 Training Loss: tensor(0.0447)\n",
      "7596 Training Loss: tensor(0.0448)\n",
      "7597 Training Loss: tensor(0.0445)\n",
      "7598 Training Loss: tensor(0.0446)\n",
      "7599 Training Loss: tensor(0.0445)\n",
      "7600 Training Loss: tensor(0.0448)\n",
      "7601 Training Loss: tensor(0.0447)\n",
      "7602 Training Loss: tensor(0.0446)\n",
      "7603 Training Loss: tensor(0.0450)\n",
      "7604 Training Loss: tensor(0.0451)\n",
      "7605 Training Loss: tensor(0.0447)\n",
      "7606 Training Loss: tensor(0.0448)\n",
      "7607 Training Loss: tensor(0.0450)\n",
      "7608 Training Loss: tensor(0.0453)\n",
      "7609 Training Loss: tensor(0.0448)\n",
      "7610 Training Loss: tensor(0.0449)\n",
      "7611 Training Loss: tensor(0.0448)\n",
      "7612 Training Loss: tensor(0.0447)\n",
      "7613 Training Loss: tensor(0.0446)\n",
      "7614 Training Loss: tensor(0.0448)\n",
      "7615 Training Loss: tensor(0.0449)\n",
      "7616 Training Loss: tensor(0.0446)\n",
      "7617 Training Loss: tensor(0.0447)\n",
      "7618 Training Loss: tensor(0.0451)\n",
      "7619 Training Loss: tensor(0.0448)\n",
      "7620 Training Loss: tensor(0.0444)\n",
      "7621 Training Loss: tensor(0.0445)\n",
      "7622 Training Loss: tensor(0.0450)\n",
      "7623 Training Loss: tensor(0.0447)\n",
      "7624 Training Loss: tensor(0.0449)\n",
      "7625 Training Loss: tensor(0.0449)\n",
      "7626 Training Loss: tensor(0.0449)\n",
      "7627 Training Loss: tensor(0.0448)\n",
      "7628 Training Loss: tensor(0.0446)\n",
      "7629 Training Loss: tensor(0.0448)\n",
      "7630 Training Loss: tensor(0.0449)\n",
      "7631 Training Loss: tensor(0.0446)\n",
      "7632 Training Loss: tensor(0.0450)\n",
      "7633 Training Loss: tensor(0.0449)\n",
      "7634 Training Loss: tensor(0.0447)\n",
      "7635 Training Loss: tensor(0.0448)\n",
      "7636 Training Loss: tensor(0.0451)\n",
      "7637 Training Loss: tensor(0.0446)\n",
      "7638 Training Loss: tensor(0.0449)\n",
      "7639 Training Loss: tensor(0.0448)\n",
      "7640 Training Loss: tensor(0.0450)\n",
      "7641 Training Loss: tensor(0.0448)\n",
      "7642 Training Loss: tensor(0.0447)\n",
      "7643 Training Loss: tensor(0.0447)\n",
      "7644 Training Loss: tensor(0.0447)\n",
      "7645 Training Loss: tensor(0.0450)\n",
      "7646 Training Loss: tensor(0.0447)\n",
      "7647 Training Loss: tensor(0.0448)\n",
      "7648 Training Loss: tensor(0.0447)\n",
      "7649 Training Loss: tensor(0.0447)\n",
      "7650 Training Loss: tensor(0.0447)\n",
      "7651 Training Loss: tensor(0.0449)\n",
      "7652 Training Loss: tensor(0.0447)\n",
      "7653 Training Loss: tensor(0.0449)\n",
      "7654 Training Loss: tensor(0.0447)\n",
      "7655 Training Loss: tensor(0.0446)\n",
      "7656 Training Loss: tensor(0.0446)\n",
      "7657 Training Loss: tensor(0.0447)\n",
      "7658 Training Loss: tensor(0.0448)\n",
      "7659 Training Loss: tensor(0.0445)\n",
      "7660 Training Loss: tensor(0.0447)\n",
      "7661 Training Loss: tensor(0.0447)\n",
      "7662 Training Loss: tensor(0.0446)\n",
      "7663 Training Loss: tensor(0.0448)\n",
      "7664 Training Loss: tensor(0.0447)\n",
      "7665 Training Loss: tensor(0.0448)\n",
      "7666 Training Loss: tensor(0.0447)\n",
      "7667 Training Loss: tensor(0.0446)\n",
      "7668 Training Loss: tensor(0.0448)\n",
      "7669 Training Loss: tensor(0.0446)\n",
      "7670 Training Loss: tensor(0.0447)\n",
      "7671 Training Loss: tensor(0.0446)\n",
      "7672 Training Loss: tensor(0.0447)\n",
      "7673 Training Loss: tensor(0.0447)\n",
      "7674 Training Loss: tensor(0.0450)\n",
      "7675 Training Loss: tensor(0.0446)\n",
      "7676 Training Loss: tensor(0.0448)\n",
      "7677 Training Loss: tensor(0.0447)\n",
      "7678 Training Loss: tensor(0.0447)\n",
      "7679 Training Loss: tensor(0.0448)\n",
      "7680 Training Loss: tensor(0.0448)\n",
      "7681 Training Loss: tensor(0.0449)\n",
      "7682 Training Loss: tensor(0.0447)\n",
      "7683 Training Loss: tensor(0.0446)\n",
      "7684 Training Loss: tensor(0.0446)\n",
      "7685 Training Loss: tensor(0.0447)\n",
      "7686 Training Loss: tensor(0.0451)\n",
      "7687 Training Loss: tensor(0.0447)\n",
      "7688 Training Loss: tensor(0.0450)\n",
      "7689 Training Loss: tensor(0.0446)\n",
      "7690 Training Loss: tensor(0.0448)\n",
      "7691 Training Loss: tensor(0.0448)\n",
      "7692 Training Loss: tensor(0.0446)\n",
      "7693 Training Loss: tensor(0.0447)\n",
      "7694 Training Loss: tensor(0.0448)\n",
      "7695 Training Loss: tensor(0.0447)\n",
      "7696 Training Loss: tensor(0.0447)\n",
      "7697 Training Loss: tensor(0.0445)\n",
      "7698 Training Loss: tensor(0.0451)\n",
      "7699 Training Loss: tensor(0.0447)\n",
      "7700 Training Loss: tensor(0.0448)\n",
      "7701 Training Loss: tensor(0.0446)\n",
      "7702 Training Loss: tensor(0.0447)\n",
      "7703 Training Loss: tensor(0.0445)\n",
      "7704 Training Loss: tensor(0.0449)\n",
      "7705 Training Loss: tensor(0.0448)\n",
      "7706 Training Loss: tensor(0.0448)\n",
      "7707 Training Loss: tensor(0.0447)\n",
      "7708 Training Loss: tensor(0.0447)\n",
      "7709 Training Loss: tensor(0.0445)\n",
      "7710 Training Loss: tensor(0.0448)\n",
      "7711 Training Loss: tensor(0.0446)\n",
      "7712 Training Loss: tensor(0.0449)\n",
      "7713 Training Loss: tensor(0.0446)\n",
      "7714 Training Loss: tensor(0.0447)\n",
      "7715 Training Loss: tensor(0.0449)\n",
      "7716 Training Loss: tensor(0.0450)\n",
      "7717 Training Loss: tensor(0.0445)\n",
      "7718 Training Loss: tensor(0.0450)\n",
      "7719 Training Loss: tensor(0.0447)\n",
      "7720 Training Loss: tensor(0.0446)\n",
      "7721 Training Loss: tensor(0.0448)\n",
      "7722 Training Loss: tensor(0.0447)\n",
      "7723 Training Loss: tensor(0.0449)\n",
      "7724 Training Loss: tensor(0.0445)\n",
      "7725 Training Loss: tensor(0.0449)\n",
      "7726 Training Loss: tensor(0.0448)\n",
      "7727 Training Loss: tensor(0.0447)\n",
      "7728 Training Loss: tensor(0.0447)\n",
      "7729 Training Loss: tensor(0.0445)\n",
      "7730 Training Loss: tensor(0.0447)\n",
      "7731 Training Loss: tensor(0.0445)\n",
      "7732 Training Loss: tensor(0.0447)\n",
      "7733 Training Loss: tensor(0.0447)\n",
      "7734 Training Loss: tensor(0.0447)\n",
      "7735 Training Loss: tensor(0.0448)\n",
      "7736 Training Loss: tensor(0.0447)\n",
      "7737 Training Loss: tensor(0.0448)\n",
      "7738 Training Loss: tensor(0.0445)\n",
      "7739 Training Loss: tensor(0.0449)\n",
      "7740 Training Loss: tensor(0.0447)\n",
      "7741 Training Loss: tensor(0.0448)\n",
      "7742 Training Loss: tensor(0.0449)\n",
      "7743 Training Loss: tensor(0.0448)\n",
      "7744 Training Loss: tensor(0.0448)\n",
      "7745 Training Loss: tensor(0.0446)\n",
      "7746 Training Loss: tensor(0.0447)\n",
      "7747 Training Loss: tensor(0.0446)\n",
      "7748 Training Loss: tensor(0.0446)\n",
      "7749 Training Loss: tensor(0.0448)\n",
      "7750 Training Loss: tensor(0.0446)\n",
      "7751 Training Loss: tensor(0.0444)\n",
      "7752 Training Loss: tensor(0.0446)\n",
      "7753 Training Loss: tensor(0.0444)\n",
      "7754 Training Loss: tensor(0.0445)\n",
      "7755 Training Loss: tensor(0.0447)\n",
      "7756 Training Loss: tensor(0.0446)\n",
      "7757 Training Loss: tensor(0.0447)\n",
      "7758 Training Loss: tensor(0.0445)\n",
      "7759 Training Loss: tensor(0.0447)\n",
      "7760 Training Loss: tensor(0.0449)\n",
      "7761 Training Loss: tensor(0.0444)\n",
      "7762 Training Loss: tensor(0.0447)\n",
      "7763 Training Loss: tensor(0.0447)\n",
      "7764 Training Loss: tensor(0.0445)\n",
      "7765 Training Loss: tensor(0.0446)\n",
      "7766 Training Loss: tensor(0.0446)\n",
      "7767 Training Loss: tensor(0.0446)\n",
      "7768 Training Loss: tensor(0.0448)\n",
      "7769 Training Loss: tensor(0.0446)\n",
      "7770 Training Loss: tensor(0.0451)\n",
      "7771 Training Loss: tensor(0.0447)\n",
      "7772 Training Loss: tensor(0.0449)\n",
      "7773 Training Loss: tensor(0.0446)\n",
      "7774 Training Loss: tensor(0.0448)\n",
      "7775 Training Loss: tensor(0.0447)\n",
      "7776 Training Loss: tensor(0.0447)\n",
      "7777 Training Loss: tensor(0.0445)\n",
      "7778 Training Loss: tensor(0.0448)\n",
      "7779 Training Loss: tensor(0.0447)\n",
      "7780 Training Loss: tensor(0.0445)\n",
      "7781 Training Loss: tensor(0.0447)\n",
      "7782 Training Loss: tensor(0.0447)\n",
      "7783 Training Loss: tensor(0.0448)\n",
      "7784 Training Loss: tensor(0.0447)\n",
      "7785 Training Loss: tensor(0.0450)\n",
      "7786 Training Loss: tensor(0.0445)\n",
      "7787 Training Loss: tensor(0.0447)\n",
      "7788 Training Loss: tensor(0.0445)\n",
      "7789 Training Loss: tensor(0.0449)\n",
      "7790 Training Loss: tensor(0.0447)\n",
      "7791 Training Loss: tensor(0.0445)\n",
      "7792 Training Loss: tensor(0.0446)\n",
      "7793 Training Loss: tensor(0.0447)\n",
      "7794 Training Loss: tensor(0.0445)\n",
      "7795 Training Loss: tensor(0.0443)\n",
      "7796 Training Loss: tensor(0.0447)\n",
      "7797 Training Loss: tensor(0.0448)\n",
      "7798 Training Loss: tensor(0.0447)\n",
      "7799 Training Loss: tensor(0.0445)\n",
      "7800 Training Loss: tensor(0.0446)\n",
      "7801 Training Loss: tensor(0.0445)\n",
      "7802 Training Loss: tensor(0.0445)\n",
      "7803 Training Loss: tensor(0.0445)\n",
      "7804 Training Loss: tensor(0.0445)\n",
      "7805 Training Loss: tensor(0.0445)\n",
      "7806 Training Loss: tensor(0.0448)\n",
      "7807 Training Loss: tensor(0.0447)\n",
      "7808 Training Loss: tensor(0.0448)\n",
      "7809 Training Loss: tensor(0.0443)\n",
      "7810 Training Loss: tensor(0.0446)\n",
      "7811 Training Loss: tensor(0.0445)\n",
      "7812 Training Loss: tensor(0.0448)\n",
      "7813 Training Loss: tensor(0.0448)\n",
      "7814 Training Loss: tensor(0.0444)\n",
      "7815 Training Loss: tensor(0.0448)\n",
      "7816 Training Loss: tensor(0.0447)\n",
      "7817 Training Loss: tensor(0.0445)\n",
      "7818 Training Loss: tensor(0.0444)\n",
      "7819 Training Loss: tensor(0.0445)\n",
      "7820 Training Loss: tensor(0.0442)\n",
      "7821 Training Loss: tensor(0.0447)\n",
      "7822 Training Loss: tensor(0.0446)\n",
      "7823 Training Loss: tensor(0.0450)\n",
      "7824 Training Loss: tensor(0.0445)\n",
      "7825 Training Loss: tensor(0.0446)\n",
      "7826 Training Loss: tensor(0.0445)\n",
      "7827 Training Loss: tensor(0.0446)\n",
      "7828 Training Loss: tensor(0.0445)\n",
      "7829 Training Loss: tensor(0.0446)\n",
      "7830 Training Loss: tensor(0.0445)\n",
      "7831 Training Loss: tensor(0.0447)\n",
      "7832 Training Loss: tensor(0.0445)\n",
      "7833 Training Loss: tensor(0.0446)\n",
      "7834 Training Loss: tensor(0.0444)\n",
      "7835 Training Loss: tensor(0.0446)\n",
      "7836 Training Loss: tensor(0.0445)\n",
      "7837 Training Loss: tensor(0.0446)\n",
      "7838 Training Loss: tensor(0.0447)\n",
      "7839 Training Loss: tensor(0.0447)\n",
      "7840 Training Loss: tensor(0.0445)\n",
      "7841 Training Loss: tensor(0.0445)\n",
      "7842 Training Loss: tensor(0.0445)\n",
      "7843 Training Loss: tensor(0.0449)\n",
      "7844 Training Loss: tensor(0.0447)\n",
      "7845 Training Loss: tensor(0.0446)\n",
      "7846 Training Loss: tensor(0.0444)\n",
      "7847 Training Loss: tensor(0.0447)\n",
      "7848 Training Loss: tensor(0.0448)\n",
      "7849 Training Loss: tensor(0.0448)\n",
      "7850 Training Loss: tensor(0.0448)\n",
      "7851 Training Loss: tensor(0.0445)\n",
      "7852 Training Loss: tensor(0.0446)\n",
      "7853 Training Loss: tensor(0.0445)\n",
      "7854 Training Loss: tensor(0.0445)\n",
      "7855 Training Loss: tensor(0.0446)\n",
      "7856 Training Loss: tensor(0.0447)\n",
      "7857 Training Loss: tensor(0.0446)\n",
      "7858 Training Loss: tensor(0.0447)\n",
      "7859 Training Loss: tensor(0.0445)\n",
      "7860 Training Loss: tensor(0.0445)\n",
      "7861 Training Loss: tensor(0.0447)\n",
      "7862 Training Loss: tensor(0.0445)\n",
      "7863 Training Loss: tensor(0.0443)\n",
      "7864 Training Loss: tensor(0.0446)\n",
      "7865 Training Loss: tensor(0.0443)\n",
      "7866 Training Loss: tensor(0.0445)\n",
      "7867 Training Loss: tensor(0.0445)\n",
      "7868 Training Loss: tensor(0.0445)\n",
      "7869 Training Loss: tensor(0.0444)\n",
      "7870 Training Loss: tensor(0.0446)\n",
      "7871 Training Loss: tensor(0.0448)\n",
      "7872 Training Loss: tensor(0.0444)\n",
      "7873 Training Loss: tensor(0.0447)\n",
      "7874 Training Loss: tensor(0.0445)\n",
      "7875 Training Loss: tensor(0.0445)\n",
      "7876 Training Loss: tensor(0.0446)\n",
      "7877 Training Loss: tensor(0.0446)\n",
      "7878 Training Loss: tensor(0.0444)\n",
      "7879 Training Loss: tensor(0.0445)\n",
      "7880 Training Loss: tensor(0.0443)\n",
      "7881 Training Loss: tensor(0.0449)\n",
      "7882 Training Loss: tensor(0.0446)\n",
      "7883 Training Loss: tensor(0.0446)\n",
      "7884 Training Loss: tensor(0.0445)\n",
      "7885 Training Loss: tensor(0.0447)\n",
      "7886 Training Loss: tensor(0.0445)\n",
      "7887 Training Loss: tensor(0.0447)\n",
      "7888 Training Loss: tensor(0.0445)\n",
      "7889 Training Loss: tensor(0.0444)\n",
      "7890 Training Loss: tensor(0.0445)\n",
      "7891 Training Loss: tensor(0.0445)\n",
      "7892 Training Loss: tensor(0.0443)\n",
      "7893 Training Loss: tensor(0.0448)\n",
      "7894 Training Loss: tensor(0.0446)\n",
      "7895 Training Loss: tensor(0.0446)\n",
      "7896 Training Loss: tensor(0.0448)\n",
      "7897 Training Loss: tensor(0.0444)\n",
      "7898 Training Loss: tensor(0.0446)\n",
      "7899 Training Loss: tensor(0.0447)\n",
      "7900 Training Loss: tensor(0.0447)\n",
      "7901 Training Loss: tensor(0.0445)\n",
      "7902 Training Loss: tensor(0.0445)\n",
      "7903 Training Loss: tensor(0.0445)\n",
      "7904 Training Loss: tensor(0.0449)\n",
      "7905 Training Loss: tensor(0.0444)\n",
      "7906 Training Loss: tensor(0.0446)\n",
      "7907 Training Loss: tensor(0.0442)\n",
      "7908 Training Loss: tensor(0.0442)\n",
      "7909 Training Loss: tensor(0.0446)\n",
      "7910 Training Loss: tensor(0.0447)\n",
      "7911 Training Loss: tensor(0.0444)\n",
      "7912 Training Loss: tensor(0.0446)\n",
      "7913 Training Loss: tensor(0.0445)\n",
      "7914 Training Loss: tensor(0.0447)\n",
      "7915 Training Loss: tensor(0.0446)\n",
      "7916 Training Loss: tensor(0.0445)\n",
      "7917 Training Loss: tensor(0.0444)\n",
      "7918 Training Loss: tensor(0.0444)\n",
      "7919 Training Loss: tensor(0.0445)\n",
      "7920 Training Loss: tensor(0.0444)\n",
      "7921 Training Loss: tensor(0.0445)\n",
      "7922 Training Loss: tensor(0.0443)\n",
      "7923 Training Loss: tensor(0.0444)\n",
      "7924 Training Loss: tensor(0.0445)\n",
      "7925 Training Loss: tensor(0.0445)\n",
      "7926 Training Loss: tensor(0.0444)\n",
      "7927 Training Loss: tensor(0.0441)\n",
      "7928 Training Loss: tensor(0.0446)\n",
      "7929 Training Loss: tensor(0.0447)\n",
      "7930 Training Loss: tensor(0.0446)\n",
      "7931 Training Loss: tensor(0.0444)\n",
      "7932 Training Loss: tensor(0.0444)\n",
      "7933 Training Loss: tensor(0.0444)\n",
      "7934 Training Loss: tensor(0.0444)\n",
      "7935 Training Loss: tensor(0.0443)\n",
      "7936 Training Loss: tensor(0.0444)\n",
      "7937 Training Loss: tensor(0.0446)\n",
      "7938 Training Loss: tensor(0.0443)\n",
      "7939 Training Loss: tensor(0.0446)\n",
      "7940 Training Loss: tensor(0.0444)\n",
      "7941 Training Loss: tensor(0.0443)\n",
      "7942 Training Loss: tensor(0.0445)\n",
      "7943 Training Loss: tensor(0.0446)\n",
      "7944 Training Loss: tensor(0.0446)\n",
      "7945 Training Loss: tensor(0.0446)\n",
      "7946 Training Loss: tensor(0.0445)\n",
      "7947 Training Loss: tensor(0.0444)\n",
      "7948 Training Loss: tensor(0.0444)\n",
      "7949 Training Loss: tensor(0.0445)\n",
      "7950 Training Loss: tensor(0.0444)\n",
      "7951 Training Loss: tensor(0.0444)\n",
      "7952 Training Loss: tensor(0.0446)\n",
      "7953 Training Loss: tensor(0.0444)\n",
      "7954 Training Loss: tensor(0.0446)\n",
      "7955 Training Loss: tensor(0.0445)\n",
      "7956 Training Loss: tensor(0.0443)\n",
      "7957 Training Loss: tensor(0.0444)\n",
      "7958 Training Loss: tensor(0.0444)\n",
      "7959 Training Loss: tensor(0.0446)\n",
      "7960 Training Loss: tensor(0.0446)\n",
      "7961 Training Loss: tensor(0.0443)\n",
      "7962 Training Loss: tensor(0.0446)\n",
      "7963 Training Loss: tensor(0.0444)\n",
      "7964 Training Loss: tensor(0.0446)\n",
      "7965 Training Loss: tensor(0.0444)\n",
      "7966 Training Loss: tensor(0.0445)\n",
      "7967 Training Loss: tensor(0.0446)\n",
      "7968 Training Loss: tensor(0.0444)\n",
      "7969 Training Loss: tensor(0.0445)\n",
      "7970 Training Loss: tensor(0.0446)\n",
      "7971 Training Loss: tensor(0.0444)\n",
      "7972 Training Loss: tensor(0.0446)\n",
      "7973 Training Loss: tensor(0.0446)\n",
      "7974 Training Loss: tensor(0.0443)\n",
      "7975 Training Loss: tensor(0.0442)\n",
      "7976 Training Loss: tensor(0.0443)\n",
      "7977 Training Loss: tensor(0.0444)\n",
      "7978 Training Loss: tensor(0.0443)\n",
      "7979 Training Loss: tensor(0.0445)\n",
      "7980 Training Loss: tensor(0.0445)\n",
      "7981 Training Loss: tensor(0.0446)\n",
      "7982 Training Loss: tensor(0.0443)\n",
      "7983 Training Loss: tensor(0.0445)\n",
      "7984 Training Loss: tensor(0.0443)\n",
      "7985 Training Loss: tensor(0.0445)\n",
      "7986 Training Loss: tensor(0.0442)\n",
      "7987 Training Loss: tensor(0.0445)\n",
      "7988 Training Loss: tensor(0.0444)\n",
      "7989 Training Loss: tensor(0.0446)\n",
      "7990 Training Loss: tensor(0.0444)\n",
      "7991 Training Loss: tensor(0.0445)\n",
      "7992 Training Loss: tensor(0.0444)\n",
      "7993 Training Loss: tensor(0.0444)\n",
      "7994 Training Loss: tensor(0.0444)\n",
      "7995 Training Loss: tensor(0.0442)\n",
      "7996 Training Loss: tensor(0.0442)\n",
      "7997 Training Loss: tensor(0.0448)\n",
      "7998 Training Loss: tensor(0.0445)\n",
      "7999 Training Loss: tensor(0.0444)\n",
      "8000 Training Loss: tensor(0.0447)\n",
      "8001 Training Loss: tensor(0.0443)\n",
      "8002 Training Loss: tensor(0.0447)\n",
      "8003 Training Loss: tensor(0.0444)\n",
      "8004 Training Loss: tensor(0.0445)\n",
      "8005 Training Loss: tensor(0.0443)\n",
      "8006 Training Loss: tensor(0.0444)\n",
      "8007 Training Loss: tensor(0.0443)\n",
      "8008 Training Loss: tensor(0.0443)\n",
      "8009 Training Loss: tensor(0.0445)\n",
      "8010 Training Loss: tensor(0.0444)\n",
      "8011 Training Loss: tensor(0.0443)\n",
      "8012 Training Loss: tensor(0.0444)\n",
      "8013 Training Loss: tensor(0.0444)\n",
      "8014 Training Loss: tensor(0.0444)\n",
      "8015 Training Loss: tensor(0.0444)\n",
      "8016 Training Loss: tensor(0.0447)\n",
      "8017 Training Loss: tensor(0.0445)\n",
      "8018 Training Loss: tensor(0.0443)\n",
      "8019 Training Loss: tensor(0.0443)\n",
      "8020 Training Loss: tensor(0.0444)\n",
      "8021 Training Loss: tensor(0.0445)\n",
      "8022 Training Loss: tensor(0.0443)\n",
      "8023 Training Loss: tensor(0.0445)\n",
      "8024 Training Loss: tensor(0.0446)\n",
      "8025 Training Loss: tensor(0.0443)\n",
      "8026 Training Loss: tensor(0.0447)\n",
      "8027 Training Loss: tensor(0.0444)\n",
      "8028 Training Loss: tensor(0.0443)\n",
      "8029 Training Loss: tensor(0.0444)\n",
      "8030 Training Loss: tensor(0.0446)\n",
      "8031 Training Loss: tensor(0.0445)\n",
      "8032 Training Loss: tensor(0.0446)\n",
      "8033 Training Loss: tensor(0.0443)\n",
      "8034 Training Loss: tensor(0.0445)\n",
      "8035 Training Loss: tensor(0.0444)\n",
      "8036 Training Loss: tensor(0.0445)\n",
      "8037 Training Loss: tensor(0.0443)\n",
      "8038 Training Loss: tensor(0.0442)\n",
      "8039 Training Loss: tensor(0.0445)\n",
      "8040 Training Loss: tensor(0.0442)\n",
      "8041 Training Loss: tensor(0.0443)\n",
      "8042 Training Loss: tensor(0.0441)\n",
      "8043 Training Loss: tensor(0.0448)\n",
      "8044 Training Loss: tensor(0.0444)\n",
      "8045 Training Loss: tensor(0.0446)\n",
      "8046 Training Loss: tensor(0.0443)\n",
      "8047 Training Loss: tensor(0.0447)\n",
      "8048 Training Loss: tensor(0.0442)\n",
      "8049 Training Loss: tensor(0.0444)\n",
      "8050 Training Loss: tensor(0.0443)\n",
      "8051 Training Loss: tensor(0.0442)\n",
      "8052 Training Loss: tensor(0.0445)\n",
      "8053 Training Loss: tensor(0.0445)\n",
      "8054 Training Loss: tensor(0.0445)\n",
      "8055 Training Loss: tensor(0.0445)\n",
      "8056 Training Loss: tensor(0.0443)\n",
      "8057 Training Loss: tensor(0.0445)\n",
      "8058 Training Loss: tensor(0.0444)\n",
      "8059 Training Loss: tensor(0.0441)\n",
      "8060 Training Loss: tensor(0.0442)\n",
      "8061 Training Loss: tensor(0.0443)\n",
      "8062 Training Loss: tensor(0.0444)\n",
      "8063 Training Loss: tensor(0.0444)\n",
      "8064 Training Loss: tensor(0.0445)\n",
      "8065 Training Loss: tensor(0.0444)\n",
      "8066 Training Loss: tensor(0.0444)\n",
      "8067 Training Loss: tensor(0.0443)\n",
      "8068 Training Loss: tensor(0.0443)\n",
      "8069 Training Loss: tensor(0.0444)\n",
      "8070 Training Loss: tensor(0.0444)\n",
      "8071 Training Loss: tensor(0.0441)\n",
      "8072 Training Loss: tensor(0.0444)\n",
      "8073 Training Loss: tensor(0.0440)\n",
      "8074 Training Loss: tensor(0.0444)\n",
      "8075 Training Loss: tensor(0.0444)\n",
      "8076 Training Loss: tensor(0.0444)\n",
      "8077 Training Loss: tensor(0.0445)\n",
      "8078 Training Loss: tensor(0.0444)\n",
      "8079 Training Loss: tensor(0.0442)\n",
      "8080 Training Loss: tensor(0.0443)\n",
      "8081 Training Loss: tensor(0.0443)\n",
      "8082 Training Loss: tensor(0.0444)\n",
      "8083 Training Loss: tensor(0.0444)\n",
      "8084 Training Loss: tensor(0.0444)\n",
      "8085 Training Loss: tensor(0.0444)\n",
      "8086 Training Loss: tensor(0.0445)\n",
      "8087 Training Loss: tensor(0.0444)\n",
      "8088 Training Loss: tensor(0.0446)\n",
      "8089 Training Loss: tensor(0.0444)\n",
      "8090 Training Loss: tensor(0.0443)\n",
      "8091 Training Loss: tensor(0.0443)\n",
      "8092 Training Loss: tensor(0.0443)\n",
      "8093 Training Loss: tensor(0.0445)\n",
      "8094 Training Loss: tensor(0.0444)\n",
      "8095 Training Loss: tensor(0.0446)\n",
      "8096 Training Loss: tensor(0.0443)\n",
      "8097 Training Loss: tensor(0.0443)\n",
      "8098 Training Loss: tensor(0.0443)\n",
      "8099 Training Loss: tensor(0.0441)\n",
      "8100 Training Loss: tensor(0.0442)\n",
      "8101 Training Loss: tensor(0.0444)\n",
      "8102 Training Loss: tensor(0.0448)\n",
      "8103 Training Loss: tensor(0.0444)\n",
      "8104 Training Loss: tensor(0.0442)\n",
      "8105 Training Loss: tensor(0.0443)\n",
      "8106 Training Loss: tensor(0.0443)\n",
      "8107 Training Loss: tensor(0.0443)\n",
      "8108 Training Loss: tensor(0.0442)\n",
      "8109 Training Loss: tensor(0.0443)\n",
      "8110 Training Loss: tensor(0.0441)\n",
      "8111 Training Loss: tensor(0.0441)\n",
      "8112 Training Loss: tensor(0.0443)\n",
      "8113 Training Loss: tensor(0.0443)\n",
      "8114 Training Loss: tensor(0.0444)\n",
      "8115 Training Loss: tensor(0.0443)\n",
      "8116 Training Loss: tensor(0.0442)\n",
      "8117 Training Loss: tensor(0.0443)\n",
      "8118 Training Loss: tensor(0.0444)\n",
      "8119 Training Loss: tensor(0.0442)\n",
      "8120 Training Loss: tensor(0.0445)\n",
      "8121 Training Loss: tensor(0.0444)\n",
      "8122 Training Loss: tensor(0.0442)\n",
      "8123 Training Loss: tensor(0.0444)\n",
      "8124 Training Loss: tensor(0.0443)\n",
      "8125 Training Loss: tensor(0.0443)\n",
      "8126 Training Loss: tensor(0.0442)\n",
      "8127 Training Loss: tensor(0.0444)\n",
      "8128 Training Loss: tensor(0.0443)\n",
      "8129 Training Loss: tensor(0.0443)\n",
      "8130 Training Loss: tensor(0.0446)\n",
      "8131 Training Loss: tensor(0.0443)\n",
      "8132 Training Loss: tensor(0.0443)\n",
      "8133 Training Loss: tensor(0.0442)\n",
      "8134 Training Loss: tensor(0.0443)\n",
      "8135 Training Loss: tensor(0.0444)\n",
      "8136 Training Loss: tensor(0.0445)\n",
      "8137 Training Loss: tensor(0.0444)\n",
      "8138 Training Loss: tensor(0.0441)\n",
      "8139 Training Loss: tensor(0.0444)\n",
      "8140 Training Loss: tensor(0.0445)\n",
      "8141 Training Loss: tensor(0.0444)\n",
      "8142 Training Loss: tensor(0.0444)\n",
      "8143 Training Loss: tensor(0.0443)\n",
      "8144 Training Loss: tensor(0.0443)\n",
      "8145 Training Loss: tensor(0.0443)\n",
      "8146 Training Loss: tensor(0.0442)\n",
      "8147 Training Loss: tensor(0.0442)\n",
      "8148 Training Loss: tensor(0.0444)\n",
      "8149 Training Loss: tensor(0.0443)\n",
      "8150 Training Loss: tensor(0.0445)\n",
      "8151 Training Loss: tensor(0.0445)\n",
      "8152 Training Loss: tensor(0.0443)\n",
      "8153 Training Loss: tensor(0.0444)\n",
      "8154 Training Loss: tensor(0.0442)\n",
      "8155 Training Loss: tensor(0.0445)\n",
      "8156 Training Loss: tensor(0.0443)\n",
      "8157 Training Loss: tensor(0.0440)\n",
      "8158 Training Loss: tensor(0.0442)\n",
      "8159 Training Loss: tensor(0.0443)\n",
      "8160 Training Loss: tensor(0.0444)\n",
      "8161 Training Loss: tensor(0.0443)\n",
      "8162 Training Loss: tensor(0.0442)\n",
      "8163 Training Loss: tensor(0.0444)\n",
      "8164 Training Loss: tensor(0.0444)\n",
      "8165 Training Loss: tensor(0.0443)\n",
      "8166 Training Loss: tensor(0.0442)\n",
      "8167 Training Loss: tensor(0.0444)\n",
      "8168 Training Loss: tensor(0.0441)\n",
      "8169 Training Loss: tensor(0.0441)\n",
      "8170 Training Loss: tensor(0.0444)\n",
      "8171 Training Loss: tensor(0.0442)\n",
      "8172 Training Loss: tensor(0.0443)\n",
      "8173 Training Loss: tensor(0.0443)\n",
      "8174 Training Loss: tensor(0.0443)\n",
      "8175 Training Loss: tensor(0.0442)\n",
      "8176 Training Loss: tensor(0.0441)\n",
      "8177 Training Loss: tensor(0.0443)\n",
      "8178 Training Loss: tensor(0.0445)\n",
      "8179 Training Loss: tensor(0.0442)\n",
      "8180 Training Loss: tensor(0.0442)\n",
      "8181 Training Loss: tensor(0.0444)\n",
      "8182 Training Loss: tensor(0.0443)\n",
      "8183 Training Loss: tensor(0.0444)\n",
      "8184 Training Loss: tensor(0.0440)\n",
      "8185 Training Loss: tensor(0.0442)\n",
      "8186 Training Loss: tensor(0.0441)\n",
      "8187 Training Loss: tensor(0.0440)\n",
      "8188 Training Loss: tensor(0.0442)\n",
      "8189 Training Loss: tensor(0.0442)\n",
      "8190 Training Loss: tensor(0.0444)\n",
      "8191 Training Loss: tensor(0.0443)\n",
      "8192 Training Loss: tensor(0.0444)\n",
      "8193 Training Loss: tensor(0.0443)\n",
      "8194 Training Loss: tensor(0.0444)\n",
      "8195 Training Loss: tensor(0.0441)\n",
      "8196 Training Loss: tensor(0.0443)\n",
      "8197 Training Loss: tensor(0.0440)\n",
      "8198 Training Loss: tensor(0.0442)\n",
      "8199 Training Loss: tensor(0.0442)\n",
      "8200 Training Loss: tensor(0.0444)\n",
      "8201 Training Loss: tensor(0.0444)\n",
      "8202 Training Loss: tensor(0.0442)\n",
      "8203 Training Loss: tensor(0.0442)\n",
      "8204 Training Loss: tensor(0.0442)\n",
      "8205 Training Loss: tensor(0.0443)\n",
      "8206 Training Loss: tensor(0.0440)\n",
      "8207 Training Loss: tensor(0.0442)\n",
      "8208 Training Loss: tensor(0.0441)\n",
      "8209 Training Loss: tensor(0.0443)\n",
      "8210 Training Loss: tensor(0.0441)\n",
      "8211 Training Loss: tensor(0.0442)\n",
      "8212 Training Loss: tensor(0.0441)\n",
      "8213 Training Loss: tensor(0.0441)\n",
      "8214 Training Loss: tensor(0.0442)\n",
      "8215 Training Loss: tensor(0.0440)\n",
      "8216 Training Loss: tensor(0.0441)\n",
      "8217 Training Loss: tensor(0.0441)\n",
      "8218 Training Loss: tensor(0.0441)\n",
      "8219 Training Loss: tensor(0.0443)\n",
      "8220 Training Loss: tensor(0.0442)\n",
      "8221 Training Loss: tensor(0.0442)\n",
      "8222 Training Loss: tensor(0.0442)\n",
      "8223 Training Loss: tensor(0.0446)\n",
      "8224 Training Loss: tensor(0.0442)\n",
      "8225 Training Loss: tensor(0.0444)\n",
      "8226 Training Loss: tensor(0.0442)\n",
      "8227 Training Loss: tensor(0.0441)\n",
      "8228 Training Loss: tensor(0.0443)\n",
      "8229 Training Loss: tensor(0.0442)\n",
      "8230 Training Loss: tensor(0.0443)\n",
      "8231 Training Loss: tensor(0.0443)\n",
      "8232 Training Loss: tensor(0.0441)\n",
      "8233 Training Loss: tensor(0.0442)\n",
      "8234 Training Loss: tensor(0.0445)\n",
      "8235 Training Loss: tensor(0.0441)\n",
      "8236 Training Loss: tensor(0.0440)\n",
      "8237 Training Loss: tensor(0.0441)\n",
      "8238 Training Loss: tensor(0.0441)\n",
      "8239 Training Loss: tensor(0.0444)\n",
      "8240 Training Loss: tensor(0.0441)\n",
      "8241 Training Loss: tensor(0.0443)\n",
      "8242 Training Loss: tensor(0.0441)\n",
      "8243 Training Loss: tensor(0.0444)\n",
      "8244 Training Loss: tensor(0.0442)\n",
      "8245 Training Loss: tensor(0.0442)\n",
      "8246 Training Loss: tensor(0.0442)\n",
      "8247 Training Loss: tensor(0.0443)\n",
      "8248 Training Loss: tensor(0.0443)\n",
      "8249 Training Loss: tensor(0.0442)\n",
      "8250 Training Loss: tensor(0.0443)\n",
      "8251 Training Loss: tensor(0.0445)\n",
      "8252 Training Loss: tensor(0.0443)\n",
      "8253 Training Loss: tensor(0.0443)\n",
      "8254 Training Loss: tensor(0.0442)\n",
      "8255 Training Loss: tensor(0.0444)\n",
      "8256 Training Loss: tensor(0.0443)\n",
      "8257 Training Loss: tensor(0.0441)\n",
      "8258 Training Loss: tensor(0.0443)\n",
      "8259 Training Loss: tensor(0.0441)\n",
      "8260 Training Loss: tensor(0.0441)\n",
      "8261 Training Loss: tensor(0.0440)\n",
      "8262 Training Loss: tensor(0.0442)\n",
      "8263 Training Loss: tensor(0.0443)\n",
      "8264 Training Loss: tensor(0.0441)\n",
      "8265 Training Loss: tensor(0.0442)\n",
      "8266 Training Loss: tensor(0.0444)\n",
      "8267 Training Loss: tensor(0.0440)\n",
      "8268 Training Loss: tensor(0.0440)\n",
      "8269 Training Loss: tensor(0.0443)\n",
      "8270 Training Loss: tensor(0.0441)\n",
      "8271 Training Loss: tensor(0.0442)\n",
      "8272 Training Loss: tensor(0.0443)\n",
      "8273 Training Loss: tensor(0.0442)\n",
      "8274 Training Loss: tensor(0.0441)\n",
      "8275 Training Loss: tensor(0.0442)\n",
      "8276 Training Loss: tensor(0.0442)\n",
      "8277 Training Loss: tensor(0.0444)\n",
      "8278 Training Loss: tensor(0.0441)\n",
      "8279 Training Loss: tensor(0.0441)\n",
      "8280 Training Loss: tensor(0.0440)\n",
      "8281 Training Loss: tensor(0.0441)\n",
      "8282 Training Loss: tensor(0.0439)\n",
      "8283 Training Loss: tensor(0.0445)\n",
      "8284 Training Loss: tensor(0.0441)\n",
      "8285 Training Loss: tensor(0.0440)\n",
      "8286 Training Loss: tensor(0.0442)\n",
      "8287 Training Loss: tensor(0.0440)\n",
      "8288 Training Loss: tensor(0.0440)\n",
      "8289 Training Loss: tensor(0.0442)\n",
      "8290 Training Loss: tensor(0.0440)\n",
      "8291 Training Loss: tensor(0.0443)\n",
      "8292 Training Loss: tensor(0.0442)\n",
      "8293 Training Loss: tensor(0.0442)\n",
      "8294 Training Loss: tensor(0.0442)\n",
      "8295 Training Loss: tensor(0.0440)\n",
      "8296 Training Loss: tensor(0.0440)\n",
      "8297 Training Loss: tensor(0.0443)\n",
      "8298 Training Loss: tensor(0.0441)\n",
      "8299 Training Loss: tensor(0.0440)\n",
      "8300 Training Loss: tensor(0.0442)\n",
      "8301 Training Loss: tensor(0.0442)\n",
      "8302 Training Loss: tensor(0.0443)\n",
      "8303 Training Loss: tensor(0.0441)\n",
      "8304 Training Loss: tensor(0.0442)\n",
      "8305 Training Loss: tensor(0.0441)\n",
      "8306 Training Loss: tensor(0.0440)\n",
      "8307 Training Loss: tensor(0.0440)\n",
      "8308 Training Loss: tensor(0.0441)\n",
      "8309 Training Loss: tensor(0.0440)\n",
      "8310 Training Loss: tensor(0.0443)\n",
      "8311 Training Loss: tensor(0.0440)\n",
      "8312 Training Loss: tensor(0.0441)\n",
      "8313 Training Loss: tensor(0.0441)\n",
      "8314 Training Loss: tensor(0.0442)\n",
      "8315 Training Loss: tensor(0.0442)\n",
      "8316 Training Loss: tensor(0.0440)\n",
      "8317 Training Loss: tensor(0.0442)\n",
      "8318 Training Loss: tensor(0.0442)\n",
      "8319 Training Loss: tensor(0.0442)\n",
      "8320 Training Loss: tensor(0.0441)\n",
      "8321 Training Loss: tensor(0.0443)\n",
      "8322 Training Loss: tensor(0.0442)\n",
      "8323 Training Loss: tensor(0.0445)\n",
      "8324 Training Loss: tensor(0.0443)\n",
      "8325 Training Loss: tensor(0.0442)\n",
      "8326 Training Loss: tensor(0.0441)\n",
      "8327 Training Loss: tensor(0.0442)\n",
      "8328 Training Loss: tensor(0.0440)\n",
      "8329 Training Loss: tensor(0.0441)\n",
      "8330 Training Loss: tensor(0.0441)\n",
      "8331 Training Loss: tensor(0.0440)\n",
      "8332 Training Loss: tensor(0.0442)\n",
      "8333 Training Loss: tensor(0.0439)\n",
      "8334 Training Loss: tensor(0.0441)\n",
      "8335 Training Loss: tensor(0.0442)\n",
      "8336 Training Loss: tensor(0.0442)\n",
      "8337 Training Loss: tensor(0.0442)\n",
      "8338 Training Loss: tensor(0.0440)\n",
      "8339 Training Loss: tensor(0.0440)\n",
      "8340 Training Loss: tensor(0.0441)\n",
      "8341 Training Loss: tensor(0.0443)\n",
      "8342 Training Loss: tensor(0.0441)\n",
      "8343 Training Loss: tensor(0.0443)\n",
      "8344 Training Loss: tensor(0.0445)\n",
      "8345 Training Loss: tensor(0.0440)\n",
      "8346 Training Loss: tensor(0.0442)\n",
      "8347 Training Loss: tensor(0.0440)\n",
      "8348 Training Loss: tensor(0.0442)\n",
      "8349 Training Loss: tensor(0.0441)\n",
      "8350 Training Loss: tensor(0.0443)\n",
      "8351 Training Loss: tensor(0.0442)\n",
      "8352 Training Loss: tensor(0.0438)\n",
      "8353 Training Loss: tensor(0.0440)\n",
      "8354 Training Loss: tensor(0.0441)\n",
      "8355 Training Loss: tensor(0.0441)\n",
      "8356 Training Loss: tensor(0.0442)\n",
      "8357 Training Loss: tensor(0.0441)\n",
      "8358 Training Loss: tensor(0.0441)\n",
      "8359 Training Loss: tensor(0.0441)\n",
      "8360 Training Loss: tensor(0.0442)\n",
      "8361 Training Loss: tensor(0.0441)\n",
      "8362 Training Loss: tensor(0.0441)\n",
      "8363 Training Loss: tensor(0.0443)\n",
      "8364 Training Loss: tensor(0.0440)\n",
      "8365 Training Loss: tensor(0.0442)\n",
      "8366 Training Loss: tensor(0.0441)\n",
      "8367 Training Loss: tensor(0.0441)\n",
      "8368 Training Loss: tensor(0.0440)\n",
      "8369 Training Loss: tensor(0.0441)\n",
      "8370 Training Loss: tensor(0.0440)\n",
      "8371 Training Loss: tensor(0.0440)\n",
      "8372 Training Loss: tensor(0.0440)\n",
      "8373 Training Loss: tensor(0.0440)\n",
      "8374 Training Loss: tensor(0.0441)\n",
      "8375 Training Loss: tensor(0.0443)\n",
      "8376 Training Loss: tensor(0.0443)\n",
      "8377 Training Loss: tensor(0.0440)\n",
      "8378 Training Loss: tensor(0.0442)\n",
      "8379 Training Loss: tensor(0.0438)\n",
      "8380 Training Loss: tensor(0.0442)\n",
      "8381 Training Loss: tensor(0.0440)\n",
      "8382 Training Loss: tensor(0.0442)\n",
      "8383 Training Loss: tensor(0.0442)\n",
      "8384 Training Loss: tensor(0.0439)\n",
      "8385 Training Loss: tensor(0.0442)\n",
      "8386 Training Loss: tensor(0.0441)\n",
      "8387 Training Loss: tensor(0.0442)\n",
      "8388 Training Loss: tensor(0.0441)\n",
      "8389 Training Loss: tensor(0.0439)\n",
      "8390 Training Loss: tensor(0.0443)\n",
      "8391 Training Loss: tensor(0.0441)\n",
      "8392 Training Loss: tensor(0.0439)\n",
      "8393 Training Loss: tensor(0.0442)\n",
      "8394 Training Loss: tensor(0.0440)\n",
      "8395 Training Loss: tensor(0.0439)\n",
      "8396 Training Loss: tensor(0.0443)\n",
      "8397 Training Loss: tensor(0.0440)\n",
      "8398 Training Loss: tensor(0.0441)\n",
      "8399 Training Loss: tensor(0.0438)\n",
      "8400 Training Loss: tensor(0.0439)\n",
      "8401 Training Loss: tensor(0.0442)\n",
      "8402 Training Loss: tensor(0.0440)\n",
      "8403 Training Loss: tensor(0.0440)\n",
      "8404 Training Loss: tensor(0.0439)\n",
      "8405 Training Loss: tensor(0.0440)\n",
      "8406 Training Loss: tensor(0.0439)\n",
      "8407 Training Loss: tensor(0.0442)\n",
      "8408 Training Loss: tensor(0.0439)\n",
      "8409 Training Loss: tensor(0.0439)\n",
      "8410 Training Loss: tensor(0.0442)\n",
      "8411 Training Loss: tensor(0.0442)\n",
      "8412 Training Loss: tensor(0.0443)\n",
      "8413 Training Loss: tensor(0.0442)\n",
      "8414 Training Loss: tensor(0.0439)\n",
      "8415 Training Loss: tensor(0.0439)\n",
      "8416 Training Loss: tensor(0.0442)\n",
      "8417 Training Loss: tensor(0.0440)\n",
      "8418 Training Loss: tensor(0.0440)\n",
      "8419 Training Loss: tensor(0.0440)\n",
      "8420 Training Loss: tensor(0.0439)\n",
      "8421 Training Loss: tensor(0.0439)\n",
      "8422 Training Loss: tensor(0.0440)\n",
      "8423 Training Loss: tensor(0.0439)\n",
      "8424 Training Loss: tensor(0.0442)\n",
      "8425 Training Loss: tensor(0.0440)\n",
      "8426 Training Loss: tensor(0.0440)\n",
      "8427 Training Loss: tensor(0.0441)\n",
      "8428 Training Loss: tensor(0.0441)\n",
      "8429 Training Loss: tensor(0.0437)\n",
      "8430 Training Loss: tensor(0.0445)\n",
      "8431 Training Loss: tensor(0.0441)\n",
      "8432 Training Loss: tensor(0.0441)\n",
      "8433 Training Loss: tensor(0.0441)\n",
      "8434 Training Loss: tensor(0.0439)\n",
      "8435 Training Loss: tensor(0.0442)\n",
      "8436 Training Loss: tensor(0.0440)\n",
      "8437 Training Loss: tensor(0.0440)\n",
      "8438 Training Loss: tensor(0.0440)\n",
      "8439 Training Loss: tensor(0.0439)\n",
      "8440 Training Loss: tensor(0.0438)\n",
      "8441 Training Loss: tensor(0.0441)\n",
      "8442 Training Loss: tensor(0.0442)\n",
      "8443 Training Loss: tensor(0.0441)\n",
      "8444 Training Loss: tensor(0.0442)\n",
      "8445 Training Loss: tensor(0.0440)\n",
      "8446 Training Loss: tensor(0.0442)\n",
      "8447 Training Loss: tensor(0.0440)\n",
      "8448 Training Loss: tensor(0.0438)\n",
      "8449 Training Loss: tensor(0.0442)\n",
      "8450 Training Loss: tensor(0.0439)\n",
      "8451 Training Loss: tensor(0.0439)\n",
      "8452 Training Loss: tensor(0.0441)\n",
      "8453 Training Loss: tensor(0.0442)\n",
      "8454 Training Loss: tensor(0.0439)\n",
      "8455 Training Loss: tensor(0.0441)\n",
      "8456 Training Loss: tensor(0.0440)\n",
      "8457 Training Loss: tensor(0.0439)\n",
      "8458 Training Loss: tensor(0.0440)\n",
      "8459 Training Loss: tensor(0.0441)\n",
      "8460 Training Loss: tensor(0.0439)\n",
      "8461 Training Loss: tensor(0.0441)\n",
      "8462 Training Loss: tensor(0.0442)\n",
      "8463 Training Loss: tensor(0.0440)\n",
      "8464 Training Loss: tensor(0.0439)\n",
      "8465 Training Loss: tensor(0.0440)\n",
      "8466 Training Loss: tensor(0.0441)\n",
      "8467 Training Loss: tensor(0.0439)\n",
      "8468 Training Loss: tensor(0.0439)\n",
      "8469 Training Loss: tensor(0.0440)\n",
      "8470 Training Loss: tensor(0.0439)\n",
      "8471 Training Loss: tensor(0.0440)\n",
      "8472 Training Loss: tensor(0.0441)\n",
      "8473 Training Loss: tensor(0.0439)\n",
      "8474 Training Loss: tensor(0.0438)\n",
      "8475 Training Loss: tensor(0.0441)\n",
      "8476 Training Loss: tensor(0.0441)\n",
      "8477 Training Loss: tensor(0.0441)\n",
      "8478 Training Loss: tensor(0.0440)\n",
      "8479 Training Loss: tensor(0.0440)\n",
      "8480 Training Loss: tensor(0.0439)\n",
      "8481 Training Loss: tensor(0.0441)\n",
      "8482 Training Loss: tensor(0.0439)\n",
      "8483 Training Loss: tensor(0.0441)\n",
      "8484 Training Loss: tensor(0.0440)\n",
      "8485 Training Loss: tensor(0.0440)\n",
      "8486 Training Loss: tensor(0.0439)\n",
      "8487 Training Loss: tensor(0.0439)\n",
      "8488 Training Loss: tensor(0.0441)\n",
      "8489 Training Loss: tensor(0.0440)\n",
      "8490 Training Loss: tensor(0.0440)\n",
      "8491 Training Loss: tensor(0.0439)\n",
      "8492 Training Loss: tensor(0.0439)\n",
      "8493 Training Loss: tensor(0.0441)\n",
      "8494 Training Loss: tensor(0.0439)\n",
      "8495 Training Loss: tensor(0.0440)\n",
      "8496 Training Loss: tensor(0.0440)\n",
      "8497 Training Loss: tensor(0.0439)\n",
      "8498 Training Loss: tensor(0.0438)\n",
      "8499 Training Loss: tensor(0.0439)\n",
      "8500 Training Loss: tensor(0.0441)\n",
      "8501 Training Loss: tensor(0.0438)\n",
      "8502 Training Loss: tensor(0.0437)\n",
      "8503 Training Loss: tensor(0.0438)\n",
      "8504 Training Loss: tensor(0.0439)\n",
      "8505 Training Loss: tensor(0.0438)\n",
      "8506 Training Loss: tensor(0.0440)\n",
      "8507 Training Loss: tensor(0.0442)\n",
      "8508 Training Loss: tensor(0.0439)\n",
      "8509 Training Loss: tensor(0.0438)\n",
      "8510 Training Loss: tensor(0.0440)\n",
      "8511 Training Loss: tensor(0.0439)\n",
      "8512 Training Loss: tensor(0.0438)\n",
      "8513 Training Loss: tensor(0.0442)\n",
      "8514 Training Loss: tensor(0.0440)\n",
      "8515 Training Loss: tensor(0.0441)\n",
      "8516 Training Loss: tensor(0.0440)\n",
      "8517 Training Loss: tensor(0.0440)\n",
      "8518 Training Loss: tensor(0.0441)\n",
      "8519 Training Loss: tensor(0.0438)\n",
      "8520 Training Loss: tensor(0.0438)\n",
      "8521 Training Loss: tensor(0.0440)\n",
      "8522 Training Loss: tensor(0.0438)\n",
      "8523 Training Loss: tensor(0.0440)\n",
      "8524 Training Loss: tensor(0.0437)\n",
      "8525 Training Loss: tensor(0.0438)\n",
      "8526 Training Loss: tensor(0.0439)\n",
      "8527 Training Loss: tensor(0.0441)\n",
      "8528 Training Loss: tensor(0.0438)\n",
      "8529 Training Loss: tensor(0.0440)\n",
      "8530 Training Loss: tensor(0.0438)\n",
      "8531 Training Loss: tensor(0.0440)\n",
      "8532 Training Loss: tensor(0.0438)\n",
      "8533 Training Loss: tensor(0.0438)\n",
      "8534 Training Loss: tensor(0.0440)\n",
      "8535 Training Loss: tensor(0.0438)\n",
      "8536 Training Loss: tensor(0.0438)\n",
      "8537 Training Loss: tensor(0.0439)\n",
      "8538 Training Loss: tensor(0.0440)\n",
      "8539 Training Loss: tensor(0.0438)\n",
      "8540 Training Loss: tensor(0.0439)\n",
      "8541 Training Loss: tensor(0.0439)\n",
      "8542 Training Loss: tensor(0.0437)\n",
      "8543 Training Loss: tensor(0.0441)\n",
      "8544 Training Loss: tensor(0.0438)\n",
      "8545 Training Loss: tensor(0.0440)\n",
      "8546 Training Loss: tensor(0.0441)\n",
      "8547 Training Loss: tensor(0.0439)\n",
      "8548 Training Loss: tensor(0.0440)\n",
      "8549 Training Loss: tensor(0.0439)\n",
      "8550 Training Loss: tensor(0.0440)\n",
      "8551 Training Loss: tensor(0.0437)\n",
      "8552 Training Loss: tensor(0.0439)\n",
      "8553 Training Loss: tensor(0.0439)\n",
      "8554 Training Loss: tensor(0.0439)\n",
      "8555 Training Loss: tensor(0.0439)\n",
      "8556 Training Loss: tensor(0.0440)\n",
      "8557 Training Loss: tensor(0.0438)\n",
      "8558 Training Loss: tensor(0.0439)\n",
      "8559 Training Loss: tensor(0.0439)\n",
      "8560 Training Loss: tensor(0.0441)\n",
      "8561 Training Loss: tensor(0.0439)\n",
      "8562 Training Loss: tensor(0.0437)\n",
      "8563 Training Loss: tensor(0.0440)\n",
      "8564 Training Loss: tensor(0.0440)\n",
      "8565 Training Loss: tensor(0.0441)\n",
      "8566 Training Loss: tensor(0.0439)\n",
      "8567 Training Loss: tensor(0.0438)\n",
      "8568 Training Loss: tensor(0.0440)\n",
      "8569 Training Loss: tensor(0.0437)\n",
      "8570 Training Loss: tensor(0.0439)\n",
      "8571 Training Loss: tensor(0.0439)\n",
      "8572 Training Loss: tensor(0.0438)\n",
      "8573 Training Loss: tensor(0.0438)\n",
      "8574 Training Loss: tensor(0.0439)\n",
      "8575 Training Loss: tensor(0.0438)\n",
      "8576 Training Loss: tensor(0.0438)\n",
      "8577 Training Loss: tensor(0.0438)\n",
      "8578 Training Loss: tensor(0.0437)\n",
      "8579 Training Loss: tensor(0.0436)\n",
      "8580 Training Loss: tensor(0.0439)\n",
      "8581 Training Loss: tensor(0.0439)\n",
      "8582 Training Loss: tensor(0.0437)\n",
      "8583 Training Loss: tensor(0.0438)\n",
      "8584 Training Loss: tensor(0.0439)\n",
      "8585 Training Loss: tensor(0.0439)\n",
      "8586 Training Loss: tensor(0.0440)\n",
      "8587 Training Loss: tensor(0.0438)\n",
      "8588 Training Loss: tensor(0.0439)\n",
      "8589 Training Loss: tensor(0.0437)\n",
      "8590 Training Loss: tensor(0.0436)\n",
      "8591 Training Loss: tensor(0.0440)\n",
      "8592 Training Loss: tensor(0.0443)\n",
      "8593 Training Loss: tensor(0.0440)\n",
      "8594 Training Loss: tensor(0.0438)\n",
      "8595 Training Loss: tensor(0.0438)\n",
      "8596 Training Loss: tensor(0.0439)\n",
      "8597 Training Loss: tensor(0.0439)\n",
      "8598 Training Loss: tensor(0.0441)\n",
      "8599 Training Loss: tensor(0.0438)\n",
      "8600 Training Loss: tensor(0.0440)\n",
      "8601 Training Loss: tensor(0.0440)\n",
      "8602 Training Loss: tensor(0.0440)\n",
      "8603 Training Loss: tensor(0.0437)\n",
      "8604 Training Loss: tensor(0.0438)\n",
      "8605 Training Loss: tensor(0.0437)\n",
      "8606 Training Loss: tensor(0.0438)\n",
      "8607 Training Loss: tensor(0.0437)\n",
      "8608 Training Loss: tensor(0.0439)\n",
      "8609 Training Loss: tensor(0.0438)\n",
      "8610 Training Loss: tensor(0.0436)\n",
      "8611 Training Loss: tensor(0.0438)\n",
      "8612 Training Loss: tensor(0.0437)\n",
      "8613 Training Loss: tensor(0.0441)\n",
      "8614 Training Loss: tensor(0.0438)\n",
      "8615 Training Loss: tensor(0.0439)\n",
      "8616 Training Loss: tensor(0.0438)\n",
      "8617 Training Loss: tensor(0.0439)\n",
      "8618 Training Loss: tensor(0.0438)\n",
      "8619 Training Loss: tensor(0.0438)\n",
      "8620 Training Loss: tensor(0.0441)\n",
      "8621 Training Loss: tensor(0.0438)\n",
      "8622 Training Loss: tensor(0.0440)\n",
      "8623 Training Loss: tensor(0.0437)\n",
      "8624 Training Loss: tensor(0.0439)\n",
      "8625 Training Loss: tensor(0.0438)\n",
      "8626 Training Loss: tensor(0.0438)\n",
      "8627 Training Loss: tensor(0.0438)\n",
      "8628 Training Loss: tensor(0.0438)\n",
      "8629 Training Loss: tensor(0.0435)\n",
      "8630 Training Loss: tensor(0.0437)\n",
      "8631 Training Loss: tensor(0.0442)\n",
      "8632 Training Loss: tensor(0.0437)\n",
      "8633 Training Loss: tensor(0.0437)\n",
      "8634 Training Loss: tensor(0.0438)\n",
      "8635 Training Loss: tensor(0.0440)\n",
      "8636 Training Loss: tensor(0.0439)\n",
      "8637 Training Loss: tensor(0.0437)\n",
      "8638 Training Loss: tensor(0.0438)\n",
      "8639 Training Loss: tensor(0.0438)\n",
      "8640 Training Loss: tensor(0.0437)\n",
      "8641 Training Loss: tensor(0.0439)\n",
      "8642 Training Loss: tensor(0.0438)\n",
      "8643 Training Loss: tensor(0.0437)\n",
      "8644 Training Loss: tensor(0.0438)\n",
      "8645 Training Loss: tensor(0.0438)\n",
      "8646 Training Loss: tensor(0.0438)\n",
      "8647 Training Loss: tensor(0.0438)\n",
      "8648 Training Loss: tensor(0.0438)\n",
      "8649 Training Loss: tensor(0.0439)\n",
      "8650 Training Loss: tensor(0.0437)\n",
      "8651 Training Loss: tensor(0.0441)\n",
      "8652 Training Loss: tensor(0.0439)\n",
      "8653 Training Loss: tensor(0.0440)\n",
      "8654 Training Loss: tensor(0.0440)\n",
      "8655 Training Loss: tensor(0.0438)\n",
      "8656 Training Loss: tensor(0.0437)\n",
      "8657 Training Loss: tensor(0.0437)\n",
      "8658 Training Loss: tensor(0.0439)\n",
      "8659 Training Loss: tensor(0.0437)\n",
      "8660 Training Loss: tensor(0.0438)\n",
      "8661 Training Loss: tensor(0.0437)\n",
      "8662 Training Loss: tensor(0.0439)\n",
      "8663 Training Loss: tensor(0.0439)\n",
      "8664 Training Loss: tensor(0.0438)\n",
      "8665 Training Loss: tensor(0.0437)\n",
      "8666 Training Loss: tensor(0.0438)\n",
      "8667 Training Loss: tensor(0.0439)\n",
      "8668 Training Loss: tensor(0.0439)\n",
      "8669 Training Loss: tensor(0.0439)\n",
      "8670 Training Loss: tensor(0.0439)\n",
      "8671 Training Loss: tensor(0.0439)\n",
      "8672 Training Loss: tensor(0.0439)\n",
      "8673 Training Loss: tensor(0.0437)\n",
      "8674 Training Loss: tensor(0.0440)\n",
      "8675 Training Loss: tensor(0.0438)\n",
      "8676 Training Loss: tensor(0.0438)\n",
      "8677 Training Loss: tensor(0.0438)\n",
      "8678 Training Loss: tensor(0.0437)\n",
      "8679 Training Loss: tensor(0.0437)\n",
      "8680 Training Loss: tensor(0.0436)\n",
      "8681 Training Loss: tensor(0.0438)\n",
      "8682 Training Loss: tensor(0.0437)\n",
      "8683 Training Loss: tensor(0.0439)\n",
      "8684 Training Loss: tensor(0.0439)\n",
      "8685 Training Loss: tensor(0.0437)\n",
      "8686 Training Loss: tensor(0.0440)\n",
      "8687 Training Loss: tensor(0.0436)\n",
      "8688 Training Loss: tensor(0.0437)\n",
      "8689 Training Loss: tensor(0.0439)\n",
      "8690 Training Loss: tensor(0.0440)\n",
      "8691 Training Loss: tensor(0.0437)\n",
      "8692 Training Loss: tensor(0.0438)\n",
      "8693 Training Loss: tensor(0.0439)\n",
      "8694 Training Loss: tensor(0.0438)\n",
      "8695 Training Loss: tensor(0.0439)\n",
      "8696 Training Loss: tensor(0.0437)\n",
      "8697 Training Loss: tensor(0.0438)\n",
      "8698 Training Loss: tensor(0.0439)\n",
      "8699 Training Loss: tensor(0.0437)\n",
      "8700 Training Loss: tensor(0.0438)\n",
      "8701 Training Loss: tensor(0.0436)\n",
      "8702 Training Loss: tensor(0.0437)\n",
      "8703 Training Loss: tensor(0.0438)\n",
      "8704 Training Loss: tensor(0.0438)\n",
      "8705 Training Loss: tensor(0.0437)\n",
      "8706 Training Loss: tensor(0.0436)\n",
      "8707 Training Loss: tensor(0.0438)\n",
      "8708 Training Loss: tensor(0.0438)\n",
      "8709 Training Loss: tensor(0.0435)\n",
      "8710 Training Loss: tensor(0.0435)\n",
      "8711 Training Loss: tensor(0.0437)\n",
      "8712 Training Loss: tensor(0.0437)\n",
      "8713 Training Loss: tensor(0.0439)\n",
      "8714 Training Loss: tensor(0.0438)\n",
      "8715 Training Loss: tensor(0.0438)\n",
      "8716 Training Loss: tensor(0.0436)\n",
      "8717 Training Loss: tensor(0.0436)\n",
      "8718 Training Loss: tensor(0.0437)\n",
      "8719 Training Loss: tensor(0.0436)\n",
      "8720 Training Loss: tensor(0.0438)\n",
      "8721 Training Loss: tensor(0.0440)\n",
      "8722 Training Loss: tensor(0.0436)\n",
      "8723 Training Loss: tensor(0.0438)\n",
      "8724 Training Loss: tensor(0.0439)\n",
      "8725 Training Loss: tensor(0.0438)\n",
      "8726 Training Loss: tensor(0.0438)\n",
      "8727 Training Loss: tensor(0.0437)\n",
      "8728 Training Loss: tensor(0.0439)\n",
      "8729 Training Loss: tensor(0.0439)\n",
      "8730 Training Loss: tensor(0.0437)\n",
      "8731 Training Loss: tensor(0.0438)\n",
      "8732 Training Loss: tensor(0.0438)\n",
      "8733 Training Loss: tensor(0.0440)\n",
      "8734 Training Loss: tensor(0.0437)\n",
      "8735 Training Loss: tensor(0.0437)\n",
      "8736 Training Loss: tensor(0.0437)\n",
      "8737 Training Loss: tensor(0.0440)\n",
      "8738 Training Loss: tensor(0.0437)\n",
      "8739 Training Loss: tensor(0.0437)\n",
      "8740 Training Loss: tensor(0.0436)\n",
      "8741 Training Loss: tensor(0.0438)\n",
      "8742 Training Loss: tensor(0.0436)\n",
      "8743 Training Loss: tensor(0.0438)\n",
      "8744 Training Loss: tensor(0.0435)\n",
      "8745 Training Loss: tensor(0.0440)\n",
      "8746 Training Loss: tensor(0.0439)\n",
      "8747 Training Loss: tensor(0.0436)\n",
      "8748 Training Loss: tensor(0.0436)\n",
      "8749 Training Loss: tensor(0.0438)\n",
      "8750 Training Loss: tensor(0.0438)\n",
      "8751 Training Loss: tensor(0.0435)\n",
      "8752 Training Loss: tensor(0.0436)\n",
      "8753 Training Loss: tensor(0.0436)\n",
      "8754 Training Loss: tensor(0.0437)\n",
      "8755 Training Loss: tensor(0.0437)\n",
      "8756 Training Loss: tensor(0.0436)\n",
      "8757 Training Loss: tensor(0.0436)\n",
      "8758 Training Loss: tensor(0.0435)\n",
      "8759 Training Loss: tensor(0.0438)\n",
      "8760 Training Loss: tensor(0.0437)\n",
      "8761 Training Loss: tensor(0.0437)\n",
      "8762 Training Loss: tensor(0.0439)\n",
      "8763 Training Loss: tensor(0.0438)\n",
      "8764 Training Loss: tensor(0.0436)\n",
      "8765 Training Loss: tensor(0.0437)\n",
      "8766 Training Loss: tensor(0.0440)\n",
      "8767 Training Loss: tensor(0.0438)\n",
      "8768 Training Loss: tensor(0.0437)\n",
      "8769 Training Loss: tensor(0.0438)\n",
      "8770 Training Loss: tensor(0.0437)\n",
      "8771 Training Loss: tensor(0.0439)\n",
      "8772 Training Loss: tensor(0.0437)\n",
      "8773 Training Loss: tensor(0.0437)\n",
      "8774 Training Loss: tensor(0.0439)\n",
      "8775 Training Loss: tensor(0.0437)\n",
      "8776 Training Loss: tensor(0.0436)\n",
      "8777 Training Loss: tensor(0.0439)\n",
      "8778 Training Loss: tensor(0.0439)\n",
      "8779 Training Loss: tensor(0.0436)\n",
      "8780 Training Loss: tensor(0.0437)\n",
      "8781 Training Loss: tensor(0.0437)\n",
      "8782 Training Loss: tensor(0.0438)\n",
      "8783 Training Loss: tensor(0.0435)\n",
      "8784 Training Loss: tensor(0.0438)\n",
      "8785 Training Loss: tensor(0.0436)\n",
      "8786 Training Loss: tensor(0.0439)\n",
      "8787 Training Loss: tensor(0.0437)\n",
      "8788 Training Loss: tensor(0.0435)\n",
      "8789 Training Loss: tensor(0.0438)\n",
      "8790 Training Loss: tensor(0.0439)\n",
      "8791 Training Loss: tensor(0.0435)\n",
      "8792 Training Loss: tensor(0.0438)\n",
      "8793 Training Loss: tensor(0.0437)\n",
      "8794 Training Loss: tensor(0.0437)\n",
      "8795 Training Loss: tensor(0.0437)\n",
      "8796 Training Loss: tensor(0.0437)\n",
      "8797 Training Loss: tensor(0.0438)\n",
      "8798 Training Loss: tensor(0.0438)\n",
      "8799 Training Loss: tensor(0.0437)\n",
      "8800 Training Loss: tensor(0.0436)\n",
      "8801 Training Loss: tensor(0.0436)\n",
      "8802 Training Loss: tensor(0.0440)\n",
      "8803 Training Loss: tensor(0.0440)\n",
      "8804 Training Loss: tensor(0.0436)\n",
      "8805 Training Loss: tensor(0.0437)\n",
      "8806 Training Loss: tensor(0.0436)\n",
      "8807 Training Loss: tensor(0.0435)\n",
      "8808 Training Loss: tensor(0.0436)\n",
      "8809 Training Loss: tensor(0.0437)\n",
      "8810 Training Loss: tensor(0.0436)\n",
      "8811 Training Loss: tensor(0.0436)\n",
      "8812 Training Loss: tensor(0.0437)\n",
      "8813 Training Loss: tensor(0.0435)\n",
      "8814 Training Loss: tensor(0.0437)\n",
      "8815 Training Loss: tensor(0.0437)\n",
      "8816 Training Loss: tensor(0.0435)\n",
      "8817 Training Loss: tensor(0.0439)\n",
      "8818 Training Loss: tensor(0.0436)\n",
      "8819 Training Loss: tensor(0.0437)\n",
      "8820 Training Loss: tensor(0.0434)\n",
      "8821 Training Loss: tensor(0.0437)\n",
      "8822 Training Loss: tensor(0.0437)\n",
      "8823 Training Loss: tensor(0.0437)\n",
      "8824 Training Loss: tensor(0.0436)\n",
      "8825 Training Loss: tensor(0.0438)\n",
      "8826 Training Loss: tensor(0.0437)\n",
      "8827 Training Loss: tensor(0.0435)\n",
      "8828 Training Loss: tensor(0.0436)\n",
      "8829 Training Loss: tensor(0.0435)\n",
      "8830 Training Loss: tensor(0.0437)\n",
      "8831 Training Loss: tensor(0.0438)\n",
      "8832 Training Loss: tensor(0.0436)\n",
      "8833 Training Loss: tensor(0.0438)\n",
      "8834 Training Loss: tensor(0.0439)\n",
      "8835 Training Loss: tensor(0.0438)\n",
      "8836 Training Loss: tensor(0.0437)\n",
      "8837 Training Loss: tensor(0.0438)\n",
      "8838 Training Loss: tensor(0.0435)\n",
      "8839 Training Loss: tensor(0.0437)\n",
      "8840 Training Loss: tensor(0.0439)\n",
      "8841 Training Loss: tensor(0.0436)\n",
      "8842 Training Loss: tensor(0.0437)\n",
      "8843 Training Loss: tensor(0.0435)\n",
      "8844 Training Loss: tensor(0.0437)\n",
      "8845 Training Loss: tensor(0.0435)\n",
      "8846 Training Loss: tensor(0.0436)\n",
      "8847 Training Loss: tensor(0.0437)\n",
      "8848 Training Loss: tensor(0.0437)\n",
      "8849 Training Loss: tensor(0.0437)\n",
      "8850 Training Loss: tensor(0.0435)\n",
      "8851 Training Loss: tensor(0.0435)\n",
      "8852 Training Loss: tensor(0.0436)\n",
      "8853 Training Loss: tensor(0.0436)\n",
      "8854 Training Loss: tensor(0.0436)\n",
      "8855 Training Loss: tensor(0.0437)\n",
      "8856 Training Loss: tensor(0.0436)\n",
      "8857 Training Loss: tensor(0.0437)\n",
      "8858 Training Loss: tensor(0.0436)\n",
      "8859 Training Loss: tensor(0.0436)\n",
      "8860 Training Loss: tensor(0.0437)\n",
      "8861 Training Loss: tensor(0.0436)\n",
      "8862 Training Loss: tensor(0.0437)\n",
      "8863 Training Loss: tensor(0.0436)\n",
      "8864 Training Loss: tensor(0.0434)\n",
      "8865 Training Loss: tensor(0.0437)\n",
      "8866 Training Loss: tensor(0.0436)\n",
      "8867 Training Loss: tensor(0.0434)\n",
      "8868 Training Loss: tensor(0.0436)\n",
      "8869 Training Loss: tensor(0.0434)\n",
      "8870 Training Loss: tensor(0.0436)\n",
      "8871 Training Loss: tensor(0.0434)\n",
      "8872 Training Loss: tensor(0.0437)\n",
      "8873 Training Loss: tensor(0.0434)\n",
      "8874 Training Loss: tensor(0.0435)\n",
      "8875 Training Loss: tensor(0.0439)\n",
      "8876 Training Loss: tensor(0.0437)\n",
      "8877 Training Loss: tensor(0.0437)\n",
      "8878 Training Loss: tensor(0.0437)\n",
      "8879 Training Loss: tensor(0.0437)\n",
      "8880 Training Loss: tensor(0.0435)\n",
      "8881 Training Loss: tensor(0.0437)\n",
      "8882 Training Loss: tensor(0.0437)\n",
      "8883 Training Loss: tensor(0.0438)\n",
      "8884 Training Loss: tensor(0.0434)\n",
      "8885 Training Loss: tensor(0.0436)\n",
      "8886 Training Loss: tensor(0.0438)\n",
      "8887 Training Loss: tensor(0.0436)\n",
      "8888 Training Loss: tensor(0.0437)\n",
      "8889 Training Loss: tensor(0.0438)\n",
      "8890 Training Loss: tensor(0.0434)\n",
      "8891 Training Loss: tensor(0.0434)\n",
      "8892 Training Loss: tensor(0.0437)\n",
      "8893 Training Loss: tensor(0.0436)\n",
      "8894 Training Loss: tensor(0.0434)\n",
      "8895 Training Loss: tensor(0.0437)\n",
      "8896 Training Loss: tensor(0.0435)\n",
      "8897 Training Loss: tensor(0.0437)\n",
      "8898 Training Loss: tensor(0.0437)\n",
      "8899 Training Loss: tensor(0.0436)\n",
      "8900 Training Loss: tensor(0.0438)\n",
      "8901 Training Loss: tensor(0.0436)\n",
      "8902 Training Loss: tensor(0.0434)\n",
      "8903 Training Loss: tensor(0.0436)\n",
      "8904 Training Loss: tensor(0.0435)\n",
      "8905 Training Loss: tensor(0.0437)\n",
      "8906 Training Loss: tensor(0.0437)\n",
      "8907 Training Loss: tensor(0.0435)\n",
      "8908 Training Loss: tensor(0.0436)\n",
      "8909 Training Loss: tensor(0.0435)\n",
      "8910 Training Loss: tensor(0.0437)\n",
      "8911 Training Loss: tensor(0.0435)\n",
      "8912 Training Loss: tensor(0.0437)\n",
      "8913 Training Loss: tensor(0.0435)\n",
      "8914 Training Loss: tensor(0.0436)\n",
      "8915 Training Loss: tensor(0.0436)\n",
      "8916 Training Loss: tensor(0.0436)\n",
      "8917 Training Loss: tensor(0.0434)\n",
      "8918 Training Loss: tensor(0.0436)\n",
      "8919 Training Loss: tensor(0.0437)\n",
      "8920 Training Loss: tensor(0.0435)\n",
      "8921 Training Loss: tensor(0.0438)\n",
      "8922 Training Loss: tensor(0.0434)\n",
      "8923 Training Loss: tensor(0.0436)\n",
      "8924 Training Loss: tensor(0.0435)\n",
      "8925 Training Loss: tensor(0.0438)\n",
      "8926 Training Loss: tensor(0.0434)\n",
      "8927 Training Loss: tensor(0.0438)\n",
      "8928 Training Loss: tensor(0.0436)\n",
      "8929 Training Loss: tensor(0.0436)\n",
      "8930 Training Loss: tensor(0.0435)\n",
      "8931 Training Loss: tensor(0.0436)\n",
      "8932 Training Loss: tensor(0.0436)\n",
      "8933 Training Loss: tensor(0.0434)\n",
      "8934 Training Loss: tensor(0.0435)\n",
      "8935 Training Loss: tensor(0.0435)\n",
      "8936 Training Loss: tensor(0.0435)\n",
      "8937 Training Loss: tensor(0.0435)\n",
      "8938 Training Loss: tensor(0.0435)\n",
      "8939 Training Loss: tensor(0.0437)\n",
      "8940 Training Loss: tensor(0.0433)\n",
      "8941 Training Loss: tensor(0.0437)\n",
      "8942 Training Loss: tensor(0.0434)\n",
      "8943 Training Loss: tensor(0.0435)\n",
      "8944 Training Loss: tensor(0.0436)\n",
      "8945 Training Loss: tensor(0.0434)\n",
      "8946 Training Loss: tensor(0.0436)\n",
      "8947 Training Loss: tensor(0.0436)\n",
      "8948 Training Loss: tensor(0.0436)\n",
      "8949 Training Loss: tensor(0.0435)\n",
      "8950 Training Loss: tensor(0.0435)\n",
      "8951 Training Loss: tensor(0.0436)\n",
      "8952 Training Loss: tensor(0.0435)\n",
      "8953 Training Loss: tensor(0.0435)\n",
      "8954 Training Loss: tensor(0.0436)\n",
      "8955 Training Loss: tensor(0.0436)\n",
      "8956 Training Loss: tensor(0.0436)\n",
      "8957 Training Loss: tensor(0.0434)\n",
      "8958 Training Loss: tensor(0.0435)\n",
      "8959 Training Loss: tensor(0.0434)\n",
      "8960 Training Loss: tensor(0.0435)\n",
      "8961 Training Loss: tensor(0.0436)\n",
      "8962 Training Loss: tensor(0.0435)\n",
      "8963 Training Loss: tensor(0.0434)\n",
      "8964 Training Loss: tensor(0.0435)\n",
      "8965 Training Loss: tensor(0.0434)\n",
      "8966 Training Loss: tensor(0.0435)\n",
      "8967 Training Loss: tensor(0.0438)\n",
      "8968 Training Loss: tensor(0.0436)\n",
      "8969 Training Loss: tensor(0.0435)\n",
      "8970 Training Loss: tensor(0.0437)\n",
      "8971 Training Loss: tensor(0.0435)\n",
      "8972 Training Loss: tensor(0.0434)\n",
      "8973 Training Loss: tensor(0.0434)\n",
      "8974 Training Loss: tensor(0.0438)\n",
      "8975 Training Loss: tensor(0.0435)\n",
      "8976 Training Loss: tensor(0.0435)\n",
      "8977 Training Loss: tensor(0.0435)\n",
      "8978 Training Loss: tensor(0.0434)\n",
      "8979 Training Loss: tensor(0.0436)\n",
      "8980 Training Loss: tensor(0.0434)\n",
      "8981 Training Loss: tensor(0.0435)\n",
      "8982 Training Loss: tensor(0.0435)\n",
      "8983 Training Loss: tensor(0.0435)\n",
      "8984 Training Loss: tensor(0.0434)\n",
      "8985 Training Loss: tensor(0.0435)\n",
      "8986 Training Loss: tensor(0.0436)\n",
      "8987 Training Loss: tensor(0.0434)\n",
      "8988 Training Loss: tensor(0.0434)\n",
      "8989 Training Loss: tensor(0.0433)\n",
      "8990 Training Loss: tensor(0.0434)\n",
      "8991 Training Loss: tensor(0.0435)\n",
      "8992 Training Loss: tensor(0.0436)\n",
      "8993 Training Loss: tensor(0.0433)\n",
      "8994 Training Loss: tensor(0.0434)\n",
      "8995 Training Loss: tensor(0.0435)\n",
      "8996 Training Loss: tensor(0.0434)\n",
      "8997 Training Loss: tensor(0.0436)\n",
      "8998 Training Loss: tensor(0.0434)\n",
      "8999 Training Loss: tensor(0.0432)\n",
      "9000 Training Loss: tensor(0.0433)\n",
      "9001 Training Loss: tensor(0.0435)\n",
      "9002 Training Loss: tensor(0.0436)\n",
      "9003 Training Loss: tensor(0.0435)\n",
      "9004 Training Loss: tensor(0.0436)\n",
      "9005 Training Loss: tensor(0.0434)\n",
      "9006 Training Loss: tensor(0.0435)\n",
      "9007 Training Loss: tensor(0.0434)\n",
      "9008 Training Loss: tensor(0.0435)\n",
      "9009 Training Loss: tensor(0.0434)\n",
      "9010 Training Loss: tensor(0.0434)\n",
      "9011 Training Loss: tensor(0.0434)\n",
      "9012 Training Loss: tensor(0.0435)\n",
      "9013 Training Loss: tensor(0.0436)\n",
      "9014 Training Loss: tensor(0.0435)\n",
      "9015 Training Loss: tensor(0.0436)\n",
      "9016 Training Loss: tensor(0.0433)\n",
      "9017 Training Loss: tensor(0.0435)\n",
      "9018 Training Loss: tensor(0.0435)\n",
      "9019 Training Loss: tensor(0.0437)\n",
      "9020 Training Loss: tensor(0.0435)\n",
      "9021 Training Loss: tensor(0.0436)\n",
      "9022 Training Loss: tensor(0.0435)\n",
      "9023 Training Loss: tensor(0.0434)\n",
      "9024 Training Loss: tensor(0.0437)\n",
      "9025 Training Loss: tensor(0.0435)\n",
      "9026 Training Loss: tensor(0.0435)\n",
      "9027 Training Loss: tensor(0.0434)\n",
      "9028 Training Loss: tensor(0.0434)\n",
      "9029 Training Loss: tensor(0.0435)\n",
      "9030 Training Loss: tensor(0.0434)\n",
      "9031 Training Loss: tensor(0.0435)\n",
      "9032 Training Loss: tensor(0.0434)\n",
      "9033 Training Loss: tensor(0.0434)\n",
      "9034 Training Loss: tensor(0.0434)\n",
      "9035 Training Loss: tensor(0.0434)\n",
      "9036 Training Loss: tensor(0.0435)\n",
      "9037 Training Loss: tensor(0.0434)\n",
      "9038 Training Loss: tensor(0.0435)\n",
      "9039 Training Loss: tensor(0.0435)\n",
      "9040 Training Loss: tensor(0.0436)\n",
      "9041 Training Loss: tensor(0.0435)\n",
      "9042 Training Loss: tensor(0.0434)\n",
      "9043 Training Loss: tensor(0.0435)\n",
      "9044 Training Loss: tensor(0.0436)\n",
      "9045 Training Loss: tensor(0.0435)\n",
      "9046 Training Loss: tensor(0.0435)\n",
      "9047 Training Loss: tensor(0.0433)\n",
      "9048 Training Loss: tensor(0.0434)\n",
      "9049 Training Loss: tensor(0.0433)\n",
      "9050 Training Loss: tensor(0.0436)\n",
      "9051 Training Loss: tensor(0.0434)\n",
      "9052 Training Loss: tensor(0.0434)\n",
      "9053 Training Loss: tensor(0.0434)\n",
      "9054 Training Loss: tensor(0.0434)\n",
      "9055 Training Loss: tensor(0.0434)\n",
      "9056 Training Loss: tensor(0.0438)\n",
      "9057 Training Loss: tensor(0.0435)\n",
      "9058 Training Loss: tensor(0.0436)\n",
      "9059 Training Loss: tensor(0.0435)\n",
      "9060 Training Loss: tensor(0.0435)\n",
      "9061 Training Loss: tensor(0.0434)\n",
      "9062 Training Loss: tensor(0.0435)\n",
      "9063 Training Loss: tensor(0.0435)\n",
      "9064 Training Loss: tensor(0.0436)\n",
      "9065 Training Loss: tensor(0.0434)\n",
      "9066 Training Loss: tensor(0.0433)\n",
      "9067 Training Loss: tensor(0.0432)\n",
      "9068 Training Loss: tensor(0.0435)\n",
      "9069 Training Loss: tensor(0.0435)\n",
      "9070 Training Loss: tensor(0.0435)\n",
      "9071 Training Loss: tensor(0.0435)\n",
      "9072 Training Loss: tensor(0.0435)\n",
      "9073 Training Loss: tensor(0.0435)\n",
      "9074 Training Loss: tensor(0.0432)\n",
      "9075 Training Loss: tensor(0.0433)\n",
      "9076 Training Loss: tensor(0.0434)\n",
      "9077 Training Loss: tensor(0.0433)\n",
      "9078 Training Loss: tensor(0.0437)\n",
      "9079 Training Loss: tensor(0.0433)\n",
      "9080 Training Loss: tensor(0.0435)\n",
      "9081 Training Loss: tensor(0.0433)\n",
      "9082 Training Loss: tensor(0.0435)\n",
      "9083 Training Loss: tensor(0.0433)\n",
      "9084 Training Loss: tensor(0.0432)\n",
      "9085 Training Loss: tensor(0.0435)\n",
      "9086 Training Loss: tensor(0.0435)\n",
      "9087 Training Loss: tensor(0.0432)\n",
      "9088 Training Loss: tensor(0.0433)\n",
      "9089 Training Loss: tensor(0.0436)\n",
      "9090 Training Loss: tensor(0.0434)\n",
      "9091 Training Loss: tensor(0.0434)\n",
      "9092 Training Loss: tensor(0.0434)\n",
      "9093 Training Loss: tensor(0.0433)\n",
      "9094 Training Loss: tensor(0.0435)\n",
      "9095 Training Loss: tensor(0.0432)\n",
      "9096 Training Loss: tensor(0.0437)\n",
      "9097 Training Loss: tensor(0.0433)\n",
      "9098 Training Loss: tensor(0.0433)\n",
      "9099 Training Loss: tensor(0.0433)\n",
      "9100 Training Loss: tensor(0.0434)\n",
      "9101 Training Loss: tensor(0.0435)\n",
      "9102 Training Loss: tensor(0.0436)\n",
      "9103 Training Loss: tensor(0.0433)\n",
      "9104 Training Loss: tensor(0.0433)\n",
      "9105 Training Loss: tensor(0.0433)\n",
      "9106 Training Loss: tensor(0.0435)\n",
      "9107 Training Loss: tensor(0.0433)\n",
      "9108 Training Loss: tensor(0.0434)\n",
      "9109 Training Loss: tensor(0.0435)\n",
      "9110 Training Loss: tensor(0.0437)\n",
      "9111 Training Loss: tensor(0.0433)\n",
      "9112 Training Loss: tensor(0.0434)\n",
      "9113 Training Loss: tensor(0.0436)\n",
      "9114 Training Loss: tensor(0.0434)\n",
      "9115 Training Loss: tensor(0.0435)\n",
      "9116 Training Loss: tensor(0.0436)\n",
      "9117 Training Loss: tensor(0.0434)\n",
      "9118 Training Loss: tensor(0.0433)\n",
      "9119 Training Loss: tensor(0.0434)\n",
      "9120 Training Loss: tensor(0.0435)\n",
      "9121 Training Loss: tensor(0.0434)\n",
      "9122 Training Loss: tensor(0.0433)\n",
      "9123 Training Loss: tensor(0.0435)\n",
      "9124 Training Loss: tensor(0.0433)\n",
      "9125 Training Loss: tensor(0.0433)\n",
      "9126 Training Loss: tensor(0.0435)\n",
      "9127 Training Loss: tensor(0.0433)\n",
      "9128 Training Loss: tensor(0.0432)\n",
      "9129 Training Loss: tensor(0.0433)\n",
      "9130 Training Loss: tensor(0.0432)\n",
      "9131 Training Loss: tensor(0.0433)\n",
      "9132 Training Loss: tensor(0.0435)\n",
      "9133 Training Loss: tensor(0.0436)\n",
      "9134 Training Loss: tensor(0.0433)\n",
      "9135 Training Loss: tensor(0.0432)\n",
      "9136 Training Loss: tensor(0.0433)\n",
      "9137 Training Loss: tensor(0.0435)\n",
      "9138 Training Loss: tensor(0.0435)\n",
      "9139 Training Loss: tensor(0.0432)\n",
      "9140 Training Loss: tensor(0.0431)\n",
      "9141 Training Loss: tensor(0.0434)\n",
      "9142 Training Loss: tensor(0.0435)\n",
      "9143 Training Loss: tensor(0.0434)\n",
      "9144 Training Loss: tensor(0.0434)\n",
      "9145 Training Loss: tensor(0.0435)\n",
      "9146 Training Loss: tensor(0.0433)\n",
      "9147 Training Loss: tensor(0.0431)\n",
      "9148 Training Loss: tensor(0.0432)\n",
      "9149 Training Loss: tensor(0.0434)\n",
      "9150 Training Loss: tensor(0.0434)\n",
      "9151 Training Loss: tensor(0.0432)\n",
      "9152 Training Loss: tensor(0.0431)\n",
      "9153 Training Loss: tensor(0.0434)\n",
      "9154 Training Loss: tensor(0.0435)\n",
      "9155 Training Loss: tensor(0.0434)\n",
      "9156 Training Loss: tensor(0.0434)\n",
      "9157 Training Loss: tensor(0.0435)\n",
      "9158 Training Loss: tensor(0.0434)\n",
      "9159 Training Loss: tensor(0.0433)\n",
      "9160 Training Loss: tensor(0.0432)\n",
      "9161 Training Loss: tensor(0.0435)\n",
      "9162 Training Loss: tensor(0.0434)\n",
      "9163 Training Loss: tensor(0.0434)\n",
      "9164 Training Loss: tensor(0.0433)\n",
      "9165 Training Loss: tensor(0.0432)\n",
      "9166 Training Loss: tensor(0.0434)\n",
      "9167 Training Loss: tensor(0.0433)\n",
      "9168 Training Loss: tensor(0.0434)\n",
      "9169 Training Loss: tensor(0.0433)\n",
      "9170 Training Loss: tensor(0.0433)\n",
      "9171 Training Loss: tensor(0.0433)\n",
      "9172 Training Loss: tensor(0.0433)\n",
      "9173 Training Loss: tensor(0.0434)\n",
      "9174 Training Loss: tensor(0.0433)\n",
      "9175 Training Loss: tensor(0.0433)\n",
      "9176 Training Loss: tensor(0.0433)\n",
      "9177 Training Loss: tensor(0.0434)\n",
      "9178 Training Loss: tensor(0.0434)\n",
      "9179 Training Loss: tensor(0.0431)\n",
      "9180 Training Loss: tensor(0.0436)\n",
      "9181 Training Loss: tensor(0.0435)\n",
      "9182 Training Loss: tensor(0.0432)\n",
      "9183 Training Loss: tensor(0.0432)\n",
      "9184 Training Loss: tensor(0.0432)\n",
      "9185 Training Loss: tensor(0.0435)\n",
      "9186 Training Loss: tensor(0.0433)\n",
      "9187 Training Loss: tensor(0.0432)\n",
      "9188 Training Loss: tensor(0.0434)\n",
      "9189 Training Loss: tensor(0.0430)\n",
      "9190 Training Loss: tensor(0.0432)\n",
      "9191 Training Loss: tensor(0.0434)\n",
      "9192 Training Loss: tensor(0.0434)\n",
      "9193 Training Loss: tensor(0.0433)\n",
      "9194 Training Loss: tensor(0.0434)\n",
      "9195 Training Loss: tensor(0.0433)\n",
      "9196 Training Loss: tensor(0.0436)\n",
      "9197 Training Loss: tensor(0.0433)\n",
      "9198 Training Loss: tensor(0.0432)\n",
      "9199 Training Loss: tensor(0.0433)\n",
      "9200 Training Loss: tensor(0.0432)\n",
      "9201 Training Loss: tensor(0.0434)\n",
      "9202 Training Loss: tensor(0.0433)\n",
      "9203 Training Loss: tensor(0.0434)\n",
      "9204 Training Loss: tensor(0.0432)\n",
      "9205 Training Loss: tensor(0.0433)\n",
      "9206 Training Loss: tensor(0.0435)\n",
      "9207 Training Loss: tensor(0.0434)\n",
      "9208 Training Loss: tensor(0.0433)\n",
      "9209 Training Loss: tensor(0.0434)\n",
      "9210 Training Loss: tensor(0.0431)\n",
      "9211 Training Loss: tensor(0.0432)\n",
      "9212 Training Loss: tensor(0.0434)\n",
      "9213 Training Loss: tensor(0.0432)\n",
      "9214 Training Loss: tensor(0.0431)\n",
      "9215 Training Loss: tensor(0.0433)\n",
      "9216 Training Loss: tensor(0.0435)\n",
      "9217 Training Loss: tensor(0.0433)\n",
      "9218 Training Loss: tensor(0.0432)\n",
      "9219 Training Loss: tensor(0.0431)\n",
      "9220 Training Loss: tensor(0.0433)\n",
      "9221 Training Loss: tensor(0.0432)\n",
      "9222 Training Loss: tensor(0.0433)\n",
      "9223 Training Loss: tensor(0.0431)\n",
      "9224 Training Loss: tensor(0.0432)\n",
      "9225 Training Loss: tensor(0.0432)\n",
      "9226 Training Loss: tensor(0.0434)\n",
      "9227 Training Loss: tensor(0.0433)\n",
      "9228 Training Loss: tensor(0.0434)\n",
      "9229 Training Loss: tensor(0.0433)\n",
      "9230 Training Loss: tensor(0.0433)\n",
      "9231 Training Loss: tensor(0.0433)\n",
      "9232 Training Loss: tensor(0.0432)\n",
      "9233 Training Loss: tensor(0.0432)\n",
      "9234 Training Loss: tensor(0.0435)\n",
      "9235 Training Loss: tensor(0.0433)\n",
      "9236 Training Loss: tensor(0.0433)\n",
      "9237 Training Loss: tensor(0.0433)\n",
      "9238 Training Loss: tensor(0.0432)\n",
      "9239 Training Loss: tensor(0.0432)\n",
      "9240 Training Loss: tensor(0.0433)\n",
      "9241 Training Loss: tensor(0.0430)\n",
      "9242 Training Loss: tensor(0.0433)\n",
      "9243 Training Loss: tensor(0.0431)\n",
      "9244 Training Loss: tensor(0.0433)\n",
      "9245 Training Loss: tensor(0.0431)\n",
      "9246 Training Loss: tensor(0.0432)\n",
      "9247 Training Loss: tensor(0.0434)\n",
      "9248 Training Loss: tensor(0.0433)\n",
      "9249 Training Loss: tensor(0.0434)\n",
      "9250 Training Loss: tensor(0.0433)\n",
      "9251 Training Loss: tensor(0.0433)\n",
      "9252 Training Loss: tensor(0.0432)\n",
      "9253 Training Loss: tensor(0.0434)\n",
      "9254 Training Loss: tensor(0.0433)\n",
      "9255 Training Loss: tensor(0.0434)\n",
      "9256 Training Loss: tensor(0.0433)\n",
      "9257 Training Loss: tensor(0.0433)\n",
      "9258 Training Loss: tensor(0.0433)\n",
      "9259 Training Loss: tensor(0.0433)\n",
      "9260 Training Loss: tensor(0.0433)\n",
      "9261 Training Loss: tensor(0.0431)\n",
      "9262 Training Loss: tensor(0.0432)\n",
      "9263 Training Loss: tensor(0.0434)\n",
      "9264 Training Loss: tensor(0.0432)\n",
      "9265 Training Loss: tensor(0.0432)\n",
      "9266 Training Loss: tensor(0.0433)\n",
      "9267 Training Loss: tensor(0.0432)\n",
      "9268 Training Loss: tensor(0.0431)\n",
      "9269 Training Loss: tensor(0.0432)\n",
      "9270 Training Loss: tensor(0.0431)\n",
      "9271 Training Loss: tensor(0.0435)\n",
      "9272 Training Loss: tensor(0.0432)\n",
      "9273 Training Loss: tensor(0.0433)\n",
      "9274 Training Loss: tensor(0.0433)\n",
      "9275 Training Loss: tensor(0.0432)\n",
      "9276 Training Loss: tensor(0.0431)\n",
      "9277 Training Loss: tensor(0.0432)\n",
      "9278 Training Loss: tensor(0.0432)\n",
      "9279 Training Loss: tensor(0.0435)\n",
      "9280 Training Loss: tensor(0.0433)\n",
      "9281 Training Loss: tensor(0.0433)\n",
      "9282 Training Loss: tensor(0.0433)\n",
      "9283 Training Loss: tensor(0.0433)\n",
      "9284 Training Loss: tensor(0.0433)\n",
      "9285 Training Loss: tensor(0.0430)\n",
      "9286 Training Loss: tensor(0.0432)\n",
      "9287 Training Loss: tensor(0.0432)\n",
      "9288 Training Loss: tensor(0.0431)\n",
      "9289 Training Loss: tensor(0.0432)\n",
      "9290 Training Loss: tensor(0.0434)\n",
      "9291 Training Loss: tensor(0.0433)\n",
      "9292 Training Loss: tensor(0.0435)\n",
      "9293 Training Loss: tensor(0.0433)\n",
      "9294 Training Loss: tensor(0.0433)\n",
      "9295 Training Loss: tensor(0.0433)\n",
      "9296 Training Loss: tensor(0.0433)\n",
      "9297 Training Loss: tensor(0.0432)\n",
      "9298 Training Loss: tensor(0.0430)\n",
      "9299 Training Loss: tensor(0.0435)\n",
      "9300 Training Loss: tensor(0.0431)\n",
      "9301 Training Loss: tensor(0.0432)\n",
      "9302 Training Loss: tensor(0.0432)\n",
      "9303 Training Loss: tensor(0.0432)\n",
      "9304 Training Loss: tensor(0.0434)\n",
      "9305 Training Loss: tensor(0.0432)\n",
      "9306 Training Loss: tensor(0.0431)\n",
      "9307 Training Loss: tensor(0.0432)\n",
      "9308 Training Loss: tensor(0.0432)\n",
      "9309 Training Loss: tensor(0.0431)\n",
      "9310 Training Loss: tensor(0.0430)\n",
      "9311 Training Loss: tensor(0.0433)\n",
      "9312 Training Loss: tensor(0.0433)\n",
      "9313 Training Loss: tensor(0.0432)\n",
      "9314 Training Loss: tensor(0.0432)\n",
      "9315 Training Loss: tensor(0.0431)\n",
      "9316 Training Loss: tensor(0.0431)\n",
      "9317 Training Loss: tensor(0.0431)\n",
      "9318 Training Loss: tensor(0.0432)\n",
      "9319 Training Loss: tensor(0.0433)\n",
      "9320 Training Loss: tensor(0.0430)\n",
      "9321 Training Loss: tensor(0.0432)\n",
      "9322 Training Loss: tensor(0.0432)\n",
      "9323 Training Loss: tensor(0.0430)\n",
      "9324 Training Loss: tensor(0.0431)\n",
      "9325 Training Loss: tensor(0.0430)\n",
      "9326 Training Loss: tensor(0.0430)\n",
      "9327 Training Loss: tensor(0.0433)\n",
      "9328 Training Loss: tensor(0.0433)\n",
      "9329 Training Loss: tensor(0.0434)\n",
      "9330 Training Loss: tensor(0.0431)\n",
      "9331 Training Loss: tensor(0.0430)\n",
      "9332 Training Loss: tensor(0.0431)\n",
      "9333 Training Loss: tensor(0.0430)\n",
      "9334 Training Loss: tensor(0.0433)\n",
      "9335 Training Loss: tensor(0.0431)\n",
      "9336 Training Loss: tensor(0.0431)\n",
      "9337 Training Loss: tensor(0.0431)\n",
      "9338 Training Loss: tensor(0.0433)\n",
      "9339 Training Loss: tensor(0.0432)\n",
      "9340 Training Loss: tensor(0.0433)\n",
      "9341 Training Loss: tensor(0.0431)\n",
      "9342 Training Loss: tensor(0.0432)\n",
      "9343 Training Loss: tensor(0.0430)\n",
      "9344 Training Loss: tensor(0.0432)\n",
      "9345 Training Loss: tensor(0.0432)\n",
      "9346 Training Loss: tensor(0.0432)\n",
      "9347 Training Loss: tensor(0.0432)\n",
      "9348 Training Loss: tensor(0.0430)\n",
      "9349 Training Loss: tensor(0.0433)\n",
      "9350 Training Loss: tensor(0.0431)\n",
      "9351 Training Loss: tensor(0.0432)\n",
      "9352 Training Loss: tensor(0.0432)\n",
      "9353 Training Loss: tensor(0.0432)\n",
      "9354 Training Loss: tensor(0.0431)\n",
      "9355 Training Loss: tensor(0.0432)\n",
      "9356 Training Loss: tensor(0.0430)\n",
      "9357 Training Loss: tensor(0.0431)\n",
      "9358 Training Loss: tensor(0.0432)\n",
      "9359 Training Loss: tensor(0.0432)\n",
      "9360 Training Loss: tensor(0.0431)\n",
      "9361 Training Loss: tensor(0.0432)\n",
      "9362 Training Loss: tensor(0.0431)\n",
      "9363 Training Loss: tensor(0.0434)\n",
      "9364 Training Loss: tensor(0.0433)\n",
      "9365 Training Loss: tensor(0.0432)\n",
      "9366 Training Loss: tensor(0.0433)\n",
      "9367 Training Loss: tensor(0.0430)\n",
      "9368 Training Loss: tensor(0.0433)\n",
      "9369 Training Loss: tensor(0.0432)\n",
      "9370 Training Loss: tensor(0.0431)\n",
      "9371 Training Loss: tensor(0.0431)\n",
      "9372 Training Loss: tensor(0.0432)\n",
      "9373 Training Loss: tensor(0.0431)\n",
      "9374 Training Loss: tensor(0.0432)\n",
      "9375 Training Loss: tensor(0.0431)\n",
      "9376 Training Loss: tensor(0.0431)\n",
      "9377 Training Loss: tensor(0.0431)\n",
      "9378 Training Loss: tensor(0.0431)\n",
      "9379 Training Loss: tensor(0.0430)\n",
      "9380 Training Loss: tensor(0.0432)\n",
      "9381 Training Loss: tensor(0.0429)\n",
      "9382 Training Loss: tensor(0.0434)\n",
      "9383 Training Loss: tensor(0.0431)\n",
      "9384 Training Loss: tensor(0.0430)\n",
      "9385 Training Loss: tensor(0.0430)\n",
      "9386 Training Loss: tensor(0.0431)\n",
      "9387 Training Loss: tensor(0.0432)\n",
      "9388 Training Loss: tensor(0.0430)\n",
      "9389 Training Loss: tensor(0.0429)\n",
      "9390 Training Loss: tensor(0.0431)\n",
      "9391 Training Loss: tensor(0.0431)\n",
      "9392 Training Loss: tensor(0.0429)\n",
      "9393 Training Loss: tensor(0.0431)\n",
      "9394 Training Loss: tensor(0.0430)\n",
      "9395 Training Loss: tensor(0.0431)\n",
      "9396 Training Loss: tensor(0.0430)\n",
      "9397 Training Loss: tensor(0.0430)\n",
      "9398 Training Loss: tensor(0.0433)\n",
      "9399 Training Loss: tensor(0.0432)\n",
      "9400 Training Loss: tensor(0.0430)\n",
      "9401 Training Loss: tensor(0.0431)\n",
      "9402 Training Loss: tensor(0.0432)\n",
      "9403 Training Loss: tensor(0.0431)\n",
      "9404 Training Loss: tensor(0.0431)\n",
      "9405 Training Loss: tensor(0.0431)\n",
      "9406 Training Loss: tensor(0.0431)\n",
      "9407 Training Loss: tensor(0.0430)\n",
      "9408 Training Loss: tensor(0.0430)\n",
      "9409 Training Loss: tensor(0.0429)\n",
      "9410 Training Loss: tensor(0.0431)\n",
      "9411 Training Loss: tensor(0.0431)\n",
      "9412 Training Loss: tensor(0.0429)\n",
      "9413 Training Loss: tensor(0.0432)\n",
      "9414 Training Loss: tensor(0.0433)\n",
      "9415 Training Loss: tensor(0.0430)\n",
      "9416 Training Loss: tensor(0.0430)\n",
      "9417 Training Loss: tensor(0.0430)\n",
      "9418 Training Loss: tensor(0.0431)\n",
      "9419 Training Loss: tensor(0.0432)\n",
      "9420 Training Loss: tensor(0.0432)\n",
      "9421 Training Loss: tensor(0.0430)\n",
      "9422 Training Loss: tensor(0.0431)\n",
      "9423 Training Loss: tensor(0.0431)\n",
      "9424 Training Loss: tensor(0.0431)\n",
      "9425 Training Loss: tensor(0.0431)\n",
      "9426 Training Loss: tensor(0.0431)\n",
      "9427 Training Loss: tensor(0.0430)\n",
      "9428 Training Loss: tensor(0.0429)\n",
      "9429 Training Loss: tensor(0.0431)\n",
      "9430 Training Loss: tensor(0.0432)\n",
      "9431 Training Loss: tensor(0.0432)\n",
      "9432 Training Loss: tensor(0.0432)\n",
      "9433 Training Loss: tensor(0.0431)\n",
      "9434 Training Loss: tensor(0.0431)\n",
      "9435 Training Loss: tensor(0.0429)\n",
      "9436 Training Loss: tensor(0.0431)\n",
      "9437 Training Loss: tensor(0.0430)\n",
      "9438 Training Loss: tensor(0.0433)\n",
      "9439 Training Loss: tensor(0.0430)\n",
      "9440 Training Loss: tensor(0.0431)\n",
      "9441 Training Loss: tensor(0.0430)\n",
      "9442 Training Loss: tensor(0.0430)\n",
      "9443 Training Loss: tensor(0.0432)\n",
      "9444 Training Loss: tensor(0.0430)\n",
      "9445 Training Loss: tensor(0.0430)\n",
      "9446 Training Loss: tensor(0.0430)\n",
      "9447 Training Loss: tensor(0.0429)\n",
      "9448 Training Loss: tensor(0.0433)\n",
      "9449 Training Loss: tensor(0.0431)\n",
      "9450 Training Loss: tensor(0.0429)\n",
      "9451 Training Loss: tensor(0.0429)\n",
      "9452 Training Loss: tensor(0.0430)\n",
      "9453 Training Loss: tensor(0.0432)\n",
      "9454 Training Loss: tensor(0.0430)\n",
      "9455 Training Loss: tensor(0.0430)\n",
      "9456 Training Loss: tensor(0.0430)\n",
      "9457 Training Loss: tensor(0.0431)\n",
      "9458 Training Loss: tensor(0.0432)\n",
      "9459 Training Loss: tensor(0.0431)\n",
      "9460 Training Loss: tensor(0.0430)\n",
      "9461 Training Loss: tensor(0.0431)\n",
      "9462 Training Loss: tensor(0.0432)\n",
      "9463 Training Loss: tensor(0.0430)\n",
      "9464 Training Loss: tensor(0.0430)\n",
      "9465 Training Loss: tensor(0.0431)\n",
      "9466 Training Loss: tensor(0.0430)\n",
      "9467 Training Loss: tensor(0.0431)\n",
      "9468 Training Loss: tensor(0.0431)\n",
      "9469 Training Loss: tensor(0.0432)\n",
      "9470 Training Loss: tensor(0.0431)\n",
      "9471 Training Loss: tensor(0.0430)\n",
      "9472 Training Loss: tensor(0.0430)\n",
      "9473 Training Loss: tensor(0.0430)\n",
      "9474 Training Loss: tensor(0.0430)\n",
      "9475 Training Loss: tensor(0.0432)\n",
      "9476 Training Loss: tensor(0.0430)\n",
      "9477 Training Loss: tensor(0.0430)\n",
      "9478 Training Loss: tensor(0.0429)\n",
      "9479 Training Loss: tensor(0.0430)\n",
      "9480 Training Loss: tensor(0.0432)\n",
      "9481 Training Loss: tensor(0.0430)\n",
      "9482 Training Loss: tensor(0.0431)\n",
      "9483 Training Loss: tensor(0.0431)\n",
      "9484 Training Loss: tensor(0.0431)\n",
      "9485 Training Loss: tensor(0.0429)\n",
      "9486 Training Loss: tensor(0.0432)\n",
      "9487 Training Loss: tensor(0.0430)\n",
      "9488 Training Loss: tensor(0.0431)\n",
      "9489 Training Loss: tensor(0.0429)\n",
      "9490 Training Loss: tensor(0.0429)\n",
      "9491 Training Loss: tensor(0.0429)\n",
      "9492 Training Loss: tensor(0.0432)\n",
      "9493 Training Loss: tensor(0.0429)\n",
      "9494 Training Loss: tensor(0.0429)\n",
      "9495 Training Loss: tensor(0.0430)\n",
      "9496 Training Loss: tensor(0.0429)\n",
      "9497 Training Loss: tensor(0.0430)\n",
      "9498 Training Loss: tensor(0.0429)\n",
      "9499 Training Loss: tensor(0.0429)\n",
      "9500 Training Loss: tensor(0.0430)\n",
      "9501 Training Loss: tensor(0.0430)\n",
      "9502 Training Loss: tensor(0.0429)\n",
      "9503 Training Loss: tensor(0.0430)\n",
      "9504 Training Loss: tensor(0.0427)\n",
      "9505 Training Loss: tensor(0.0432)\n",
      "9506 Training Loss: tensor(0.0429)\n",
      "9507 Training Loss: tensor(0.0429)\n",
      "9508 Training Loss: tensor(0.0430)\n",
      "9509 Training Loss: tensor(0.0430)\n",
      "9510 Training Loss: tensor(0.0430)\n",
      "9511 Training Loss: tensor(0.0432)\n",
      "9512 Training Loss: tensor(0.0430)\n",
      "9513 Training Loss: tensor(0.0431)\n",
      "9514 Training Loss: tensor(0.0430)\n",
      "9515 Training Loss: tensor(0.0431)\n",
      "9516 Training Loss: tensor(0.0429)\n",
      "9517 Training Loss: tensor(0.0430)\n",
      "9518 Training Loss: tensor(0.0429)\n",
      "9519 Training Loss: tensor(0.0430)\n",
      "9520 Training Loss: tensor(0.0430)\n",
      "9521 Training Loss: tensor(0.0429)\n",
      "9522 Training Loss: tensor(0.0430)\n",
      "9523 Training Loss: tensor(0.0429)\n",
      "9524 Training Loss: tensor(0.0429)\n",
      "9525 Training Loss: tensor(0.0428)\n",
      "9526 Training Loss: tensor(0.0430)\n",
      "9527 Training Loss: tensor(0.0430)\n",
      "9528 Training Loss: tensor(0.0432)\n",
      "9529 Training Loss: tensor(0.0429)\n",
      "9530 Training Loss: tensor(0.0430)\n",
      "9531 Training Loss: tensor(0.0429)\n",
      "9532 Training Loss: tensor(0.0429)\n",
      "9533 Training Loss: tensor(0.0430)\n",
      "9534 Training Loss: tensor(0.0431)\n",
      "9535 Training Loss: tensor(0.0430)\n",
      "9536 Training Loss: tensor(0.0432)\n",
      "9537 Training Loss: tensor(0.0431)\n",
      "9538 Training Loss: tensor(0.0428)\n",
      "9539 Training Loss: tensor(0.0430)\n",
      "9540 Training Loss: tensor(0.0429)\n",
      "9541 Training Loss: tensor(0.0429)\n",
      "9542 Training Loss: tensor(0.0429)\n",
      "9543 Training Loss: tensor(0.0430)\n",
      "9544 Training Loss: tensor(0.0430)\n",
      "9545 Training Loss: tensor(0.0428)\n",
      "9546 Training Loss: tensor(0.0431)\n",
      "9547 Training Loss: tensor(0.0430)\n",
      "9548 Training Loss: tensor(0.0428)\n",
      "9549 Training Loss: tensor(0.0429)\n",
      "9550 Training Loss: tensor(0.0431)\n",
      "9551 Training Loss: tensor(0.0429)\n",
      "9552 Training Loss: tensor(0.0432)\n",
      "9553 Training Loss: tensor(0.0429)\n",
      "9554 Training Loss: tensor(0.0431)\n",
      "9555 Training Loss: tensor(0.0429)\n",
      "9556 Training Loss: tensor(0.0431)\n",
      "9557 Training Loss: tensor(0.0429)\n",
      "9558 Training Loss: tensor(0.0429)\n",
      "9559 Training Loss: tensor(0.0431)\n",
      "9560 Training Loss: tensor(0.0429)\n",
      "9561 Training Loss: tensor(0.0431)\n",
      "9562 Training Loss: tensor(0.0429)\n",
      "9563 Training Loss: tensor(0.0428)\n",
      "9564 Training Loss: tensor(0.0427)\n",
      "9565 Training Loss: tensor(0.0429)\n",
      "9566 Training Loss: tensor(0.0426)\n",
      "9567 Training Loss: tensor(0.0430)\n",
      "9568 Training Loss: tensor(0.0429)\n",
      "9569 Training Loss: tensor(0.0429)\n",
      "9570 Training Loss: tensor(0.0429)\n",
      "9571 Training Loss: tensor(0.0429)\n",
      "9572 Training Loss: tensor(0.0428)\n",
      "9573 Training Loss: tensor(0.0428)\n",
      "9574 Training Loss: tensor(0.0431)\n",
      "9575 Training Loss: tensor(0.0430)\n",
      "9576 Training Loss: tensor(0.0430)\n",
      "9577 Training Loss: tensor(0.0431)\n",
      "9578 Training Loss: tensor(0.0429)\n",
      "9579 Training Loss: tensor(0.0429)\n",
      "9580 Training Loss: tensor(0.0429)\n",
      "9581 Training Loss: tensor(0.0428)\n",
      "9582 Training Loss: tensor(0.0429)\n",
      "9583 Training Loss: tensor(0.0428)\n",
      "9584 Training Loss: tensor(0.0428)\n",
      "9585 Training Loss: tensor(0.0427)\n",
      "9586 Training Loss: tensor(0.0430)\n",
      "9587 Training Loss: tensor(0.0429)\n",
      "9588 Training Loss: tensor(0.0430)\n",
      "9589 Training Loss: tensor(0.0428)\n",
      "9590 Training Loss: tensor(0.0429)\n",
      "9591 Training Loss: tensor(0.0430)\n",
      "9592 Training Loss: tensor(0.0429)\n",
      "9593 Training Loss: tensor(0.0431)\n",
      "9594 Training Loss: tensor(0.0429)\n",
      "9595 Training Loss: tensor(0.0429)\n",
      "9596 Training Loss: tensor(0.0429)\n",
      "9597 Training Loss: tensor(0.0429)\n",
      "9598 Training Loss: tensor(0.0427)\n",
      "9599 Training Loss: tensor(0.0428)\n",
      "9600 Training Loss: tensor(0.0428)\n",
      "9601 Training Loss: tensor(0.0429)\n",
      "9602 Training Loss: tensor(0.0427)\n",
      "9603 Training Loss: tensor(0.0431)\n",
      "9604 Training Loss: tensor(0.0431)\n",
      "9605 Training Loss: tensor(0.0430)\n",
      "9606 Training Loss: tensor(0.0428)\n",
      "9607 Training Loss: tensor(0.0429)\n",
      "9608 Training Loss: tensor(0.0429)\n",
      "9609 Training Loss: tensor(0.0428)\n",
      "9610 Training Loss: tensor(0.0429)\n",
      "9611 Training Loss: tensor(0.0429)\n",
      "9612 Training Loss: tensor(0.0428)\n",
      "9613 Training Loss: tensor(0.0428)\n",
      "9614 Training Loss: tensor(0.0430)\n",
      "9615 Training Loss: tensor(0.0427)\n",
      "9616 Training Loss: tensor(0.0428)\n",
      "9617 Training Loss: tensor(0.0428)\n",
      "9618 Training Loss: tensor(0.0429)\n",
      "9619 Training Loss: tensor(0.0428)\n",
      "9620 Training Loss: tensor(0.0430)\n",
      "9621 Training Loss: tensor(0.0429)\n",
      "9622 Training Loss: tensor(0.0428)\n",
      "9623 Training Loss: tensor(0.0429)\n",
      "9624 Training Loss: tensor(0.0429)\n",
      "9625 Training Loss: tensor(0.0428)\n",
      "9626 Training Loss: tensor(0.0431)\n",
      "9627 Training Loss: tensor(0.0427)\n",
      "9628 Training Loss: tensor(0.0427)\n",
      "9629 Training Loss: tensor(0.0427)\n",
      "9630 Training Loss: tensor(0.0428)\n",
      "9631 Training Loss: tensor(0.0430)\n",
      "9632 Training Loss: tensor(0.0427)\n",
      "9633 Training Loss: tensor(0.0427)\n",
      "9634 Training Loss: tensor(0.0428)\n",
      "9635 Training Loss: tensor(0.0430)\n",
      "9636 Training Loss: tensor(0.0429)\n",
      "9637 Training Loss: tensor(0.0428)\n",
      "9638 Training Loss: tensor(0.0429)\n",
      "9639 Training Loss: tensor(0.0429)\n",
      "9640 Training Loss: tensor(0.0429)\n",
      "9641 Training Loss: tensor(0.0430)\n",
      "9642 Training Loss: tensor(0.0427)\n",
      "9643 Training Loss: tensor(0.0428)\n",
      "9644 Training Loss: tensor(0.0428)\n",
      "9645 Training Loss: tensor(0.0428)\n",
      "9646 Training Loss: tensor(0.0428)\n",
      "9647 Training Loss: tensor(0.0429)\n",
      "9648 Training Loss: tensor(0.0430)\n",
      "9649 Training Loss: tensor(0.0429)\n",
      "9650 Training Loss: tensor(0.0429)\n",
      "9651 Training Loss: tensor(0.0427)\n",
      "9652 Training Loss: tensor(0.0427)\n",
      "9653 Training Loss: tensor(0.0428)\n",
      "9654 Training Loss: tensor(0.0428)\n",
      "9655 Training Loss: tensor(0.0427)\n",
      "9656 Training Loss: tensor(0.0428)\n",
      "9657 Training Loss: tensor(0.0428)\n",
      "9658 Training Loss: tensor(0.0430)\n",
      "9659 Training Loss: tensor(0.0428)\n",
      "9660 Training Loss: tensor(0.0428)\n",
      "9661 Training Loss: tensor(0.0428)\n",
      "9662 Training Loss: tensor(0.0429)\n",
      "9663 Training Loss: tensor(0.0431)\n",
      "9664 Training Loss: tensor(0.0428)\n",
      "9665 Training Loss: tensor(0.0428)\n",
      "9666 Training Loss: tensor(0.0428)\n",
      "9667 Training Loss: tensor(0.0429)\n",
      "9668 Training Loss: tensor(0.0427)\n",
      "9669 Training Loss: tensor(0.0429)\n",
      "9670 Training Loss: tensor(0.0427)\n",
      "9671 Training Loss: tensor(0.0429)\n",
      "9672 Training Loss: tensor(0.0429)\n",
      "9673 Training Loss: tensor(0.0429)\n",
      "9674 Training Loss: tensor(0.0427)\n",
      "9675 Training Loss: tensor(0.0427)\n",
      "9676 Training Loss: tensor(0.0429)\n",
      "9677 Training Loss: tensor(0.0430)\n",
      "9678 Training Loss: tensor(0.0429)\n",
      "9679 Training Loss: tensor(0.0429)\n",
      "9680 Training Loss: tensor(0.0427)\n",
      "9681 Training Loss: tensor(0.0428)\n",
      "9682 Training Loss: tensor(0.0427)\n",
      "9683 Training Loss: tensor(0.0427)\n",
      "9684 Training Loss: tensor(0.0429)\n",
      "9685 Training Loss: tensor(0.0427)\n",
      "9686 Training Loss: tensor(0.0428)\n",
      "9687 Training Loss: tensor(0.0428)\n",
      "9688 Training Loss: tensor(0.0427)\n",
      "9689 Training Loss: tensor(0.0428)\n",
      "9690 Training Loss: tensor(0.0430)\n",
      "9691 Training Loss: tensor(0.0428)\n",
      "9692 Training Loss: tensor(0.0429)\n",
      "9693 Training Loss: tensor(0.0427)\n",
      "9694 Training Loss: tensor(0.0427)\n",
      "9695 Training Loss: tensor(0.0428)\n",
      "9696 Training Loss: tensor(0.0428)\n",
      "9697 Training Loss: tensor(0.0427)\n",
      "9698 Training Loss: tensor(0.0428)\n",
      "9699 Training Loss: tensor(0.0426)\n",
      "9700 Training Loss: tensor(0.0428)\n",
      "9701 Training Loss: tensor(0.0426)\n",
      "9702 Training Loss: tensor(0.0429)\n",
      "9703 Training Loss: tensor(0.0426)\n",
      "9704 Training Loss: tensor(0.0427)\n",
      "9705 Training Loss: tensor(0.0426)\n",
      "9706 Training Loss: tensor(0.0428)\n",
      "9707 Training Loss: tensor(0.0427)\n",
      "9708 Training Loss: tensor(0.0427)\n",
      "9709 Training Loss: tensor(0.0426)\n",
      "9710 Training Loss: tensor(0.0426)\n",
      "9711 Training Loss: tensor(0.0427)\n",
      "9712 Training Loss: tensor(0.0427)\n",
      "9713 Training Loss: tensor(0.0428)\n",
      "9714 Training Loss: tensor(0.0429)\n",
      "9715 Training Loss: tensor(0.0429)\n",
      "9716 Training Loss: tensor(0.0427)\n",
      "9717 Training Loss: tensor(0.0428)\n",
      "9718 Training Loss: tensor(0.0427)\n",
      "9719 Training Loss: tensor(0.0429)\n",
      "9720 Training Loss: tensor(0.0430)\n",
      "9721 Training Loss: tensor(0.0428)\n",
      "9722 Training Loss: tensor(0.0426)\n",
      "9723 Training Loss: tensor(0.0428)\n",
      "9724 Training Loss: tensor(0.0427)\n",
      "9725 Training Loss: tensor(0.0427)\n",
      "9726 Training Loss: tensor(0.0429)\n",
      "9727 Training Loss: tensor(0.0427)\n",
      "9728 Training Loss: tensor(0.0428)\n",
      "9729 Training Loss: tensor(0.0427)\n",
      "9730 Training Loss: tensor(0.0429)\n",
      "9731 Training Loss: tensor(0.0427)\n",
      "9732 Training Loss: tensor(0.0427)\n",
      "9733 Training Loss: tensor(0.0428)\n",
      "9734 Training Loss: tensor(0.0427)\n",
      "9735 Training Loss: tensor(0.0426)\n",
      "9736 Training Loss: tensor(0.0430)\n",
      "9737 Training Loss: tensor(0.0426)\n",
      "9738 Training Loss: tensor(0.0427)\n",
      "9739 Training Loss: tensor(0.0428)\n",
      "9740 Training Loss: tensor(0.0428)\n",
      "9741 Training Loss: tensor(0.0426)\n",
      "9742 Training Loss: tensor(0.0427)\n",
      "9743 Training Loss: tensor(0.0430)\n",
      "9744 Training Loss: tensor(0.0425)\n",
      "9745 Training Loss: tensor(0.0429)\n",
      "9746 Training Loss: tensor(0.0427)\n",
      "9747 Training Loss: tensor(0.0426)\n",
      "9748 Training Loss: tensor(0.0425)\n",
      "9749 Training Loss: tensor(0.0427)\n",
      "9750 Training Loss: tensor(0.0427)\n",
      "9751 Training Loss: tensor(0.0426)\n",
      "9752 Training Loss: tensor(0.0426)\n",
      "9753 Training Loss: tensor(0.0428)\n",
      "9754 Training Loss: tensor(0.0426)\n",
      "9755 Training Loss: tensor(0.0426)\n",
      "9756 Training Loss: tensor(0.0427)\n",
      "9757 Training Loss: tensor(0.0426)\n",
      "9758 Training Loss: tensor(0.0427)\n",
      "9759 Training Loss: tensor(0.0427)\n",
      "9760 Training Loss: tensor(0.0428)\n",
      "9761 Training Loss: tensor(0.0426)\n",
      "9762 Training Loss: tensor(0.0427)\n",
      "9763 Training Loss: tensor(0.0427)\n",
      "9764 Training Loss: tensor(0.0428)\n",
      "9765 Training Loss: tensor(0.0426)\n",
      "9766 Training Loss: tensor(0.0427)\n",
      "9767 Training Loss: tensor(0.0426)\n",
      "9768 Training Loss: tensor(0.0428)\n",
      "9769 Training Loss: tensor(0.0426)\n",
      "9770 Training Loss: tensor(0.0427)\n",
      "9771 Training Loss: tensor(0.0427)\n",
      "9772 Training Loss: tensor(0.0427)\n",
      "9773 Training Loss: tensor(0.0427)\n",
      "9774 Training Loss: tensor(0.0426)\n",
      "9775 Training Loss: tensor(0.0425)\n",
      "9776 Training Loss: tensor(0.0428)\n",
      "9777 Training Loss: tensor(0.0428)\n",
      "9778 Training Loss: tensor(0.0430)\n",
      "9779 Training Loss: tensor(0.0426)\n",
      "9780 Training Loss: tensor(0.0428)\n",
      "9781 Training Loss: tensor(0.0428)\n",
      "9782 Training Loss: tensor(0.0428)\n",
      "9783 Training Loss: tensor(0.0426)\n",
      "9784 Training Loss: tensor(0.0425)\n",
      "9785 Training Loss: tensor(0.0427)\n",
      "9786 Training Loss: tensor(0.0426)\n",
      "9787 Training Loss: tensor(0.0426)\n",
      "9788 Training Loss: tensor(0.0427)\n",
      "9789 Training Loss: tensor(0.0427)\n",
      "9790 Training Loss: tensor(0.0426)\n",
      "9791 Training Loss: tensor(0.0427)\n",
      "9792 Training Loss: tensor(0.0427)\n",
      "9793 Training Loss: tensor(0.0426)\n",
      "9794 Training Loss: tensor(0.0426)\n",
      "9795 Training Loss: tensor(0.0428)\n",
      "9796 Training Loss: tensor(0.0426)\n",
      "9797 Training Loss: tensor(0.0427)\n",
      "9798 Training Loss: tensor(0.0425)\n",
      "9799 Training Loss: tensor(0.0425)\n",
      "9800 Training Loss: tensor(0.0426)\n",
      "9801 Training Loss: tensor(0.0429)\n",
      "9802 Training Loss: tensor(0.0428)\n",
      "9803 Training Loss: tensor(0.0428)\n",
      "9804 Training Loss: tensor(0.0426)\n",
      "9805 Training Loss: tensor(0.0426)\n",
      "9806 Training Loss: tensor(0.0427)\n",
      "9807 Training Loss: tensor(0.0428)\n",
      "9808 Training Loss: tensor(0.0425)\n",
      "9809 Training Loss: tensor(0.0425)\n",
      "9810 Training Loss: tensor(0.0429)\n",
      "9811 Training Loss: tensor(0.0426)\n",
      "9812 Training Loss: tensor(0.0425)\n",
      "9813 Training Loss: tensor(0.0427)\n",
      "9814 Training Loss: tensor(0.0427)\n",
      "9815 Training Loss: tensor(0.0426)\n",
      "9816 Training Loss: tensor(0.0426)\n",
      "9817 Training Loss: tensor(0.0428)\n",
      "9818 Training Loss: tensor(0.0425)\n",
      "9819 Training Loss: tensor(0.0429)\n",
      "9820 Training Loss: tensor(0.0427)\n",
      "9821 Training Loss: tensor(0.0427)\n",
      "9822 Training Loss: tensor(0.0426)\n",
      "9823 Training Loss: tensor(0.0427)\n",
      "9824 Training Loss: tensor(0.0426)\n",
      "9825 Training Loss: tensor(0.0426)\n",
      "9826 Training Loss: tensor(0.0426)\n",
      "9827 Training Loss: tensor(0.0425)\n",
      "9828 Training Loss: tensor(0.0426)\n",
      "9829 Training Loss: tensor(0.0424)\n",
      "9830 Training Loss: tensor(0.0429)\n",
      "9831 Training Loss: tensor(0.0427)\n",
      "9832 Training Loss: tensor(0.0425)\n",
      "9833 Training Loss: tensor(0.0424)\n",
      "9834 Training Loss: tensor(0.0425)\n",
      "9835 Training Loss: tensor(0.0426)\n",
      "9836 Training Loss: tensor(0.0427)\n",
      "9837 Training Loss: tensor(0.0426)\n",
      "9838 Training Loss: tensor(0.0426)\n",
      "9839 Training Loss: tensor(0.0426)\n",
      "9840 Training Loss: tensor(0.0428)\n",
      "9841 Training Loss: tensor(0.0426)\n",
      "9842 Training Loss: tensor(0.0429)\n",
      "9843 Training Loss: tensor(0.0425)\n",
      "9844 Training Loss: tensor(0.0426)\n",
      "9845 Training Loss: tensor(0.0425)\n",
      "9846 Training Loss: tensor(0.0427)\n",
      "9847 Training Loss: tensor(0.0427)\n",
      "9848 Training Loss: tensor(0.0427)\n",
      "9849 Training Loss: tensor(0.0428)\n",
      "9850 Training Loss: tensor(0.0424)\n",
      "9851 Training Loss: tensor(0.0425)\n",
      "9852 Training Loss: tensor(0.0427)\n",
      "9853 Training Loss: tensor(0.0426)\n",
      "9854 Training Loss: tensor(0.0428)\n",
      "9855 Training Loss: tensor(0.0426)\n",
      "9856 Training Loss: tensor(0.0427)\n",
      "9857 Training Loss: tensor(0.0427)\n",
      "9858 Training Loss: tensor(0.0425)\n",
      "9859 Training Loss: tensor(0.0424)\n",
      "9860 Training Loss: tensor(0.0427)\n",
      "9861 Training Loss: tensor(0.0426)\n",
      "9862 Training Loss: tensor(0.0428)\n",
      "9863 Training Loss: tensor(0.0425)\n",
      "9864 Training Loss: tensor(0.0427)\n",
      "9865 Training Loss: tensor(0.0426)\n",
      "9866 Training Loss: tensor(0.0426)\n",
      "9867 Training Loss: tensor(0.0425)\n",
      "9868 Training Loss: tensor(0.0425)\n",
      "9869 Training Loss: tensor(0.0425)\n",
      "9870 Training Loss: tensor(0.0425)\n",
      "9871 Training Loss: tensor(0.0427)\n",
      "9872 Training Loss: tensor(0.0425)\n",
      "9873 Training Loss: tensor(0.0425)\n",
      "9874 Training Loss: tensor(0.0423)\n",
      "9875 Training Loss: tensor(0.0423)\n",
      "9876 Training Loss: tensor(0.0425)\n",
      "9877 Training Loss: tensor(0.0424)\n",
      "9878 Training Loss: tensor(0.0425)\n",
      "9879 Training Loss: tensor(0.0425)\n",
      "9880 Training Loss: tensor(0.0426)\n",
      "9881 Training Loss: tensor(0.0425)\n",
      "9882 Training Loss: tensor(0.0424)\n",
      "9883 Training Loss: tensor(0.0424)\n",
      "9884 Training Loss: tensor(0.0426)\n",
      "9885 Training Loss: tensor(0.0427)\n",
      "9886 Training Loss: tensor(0.0426)\n",
      "9887 Training Loss: tensor(0.0425)\n",
      "9888 Training Loss: tensor(0.0426)\n",
      "9889 Training Loss: tensor(0.0424)\n",
      "9890 Training Loss: tensor(0.0426)\n",
      "9891 Training Loss: tensor(0.0427)\n",
      "9892 Training Loss: tensor(0.0426)\n",
      "9893 Training Loss: tensor(0.0426)\n",
      "9894 Training Loss: tensor(0.0425)\n",
      "9895 Training Loss: tensor(0.0426)\n",
      "9896 Training Loss: tensor(0.0427)\n",
      "9897 Training Loss: tensor(0.0426)\n",
      "9898 Training Loss: tensor(0.0427)\n",
      "9899 Training Loss: tensor(0.0425)\n",
      "9900 Training Loss: tensor(0.0424)\n",
      "9901 Training Loss: tensor(0.0425)\n",
      "9902 Training Loss: tensor(0.0425)\n",
      "9903 Training Loss: tensor(0.0424)\n",
      "9904 Training Loss: tensor(0.0425)\n",
      "9905 Training Loss: tensor(0.0426)\n",
      "9906 Training Loss: tensor(0.0425)\n",
      "9907 Training Loss: tensor(0.0425)\n",
      "9908 Training Loss: tensor(0.0425)\n",
      "9909 Training Loss: tensor(0.0424)\n",
      "9910 Training Loss: tensor(0.0424)\n",
      "9911 Training Loss: tensor(0.0426)\n",
      "9912 Training Loss: tensor(0.0425)\n",
      "9913 Training Loss: tensor(0.0425)\n",
      "9914 Training Loss: tensor(0.0424)\n",
      "9915 Training Loss: tensor(0.0424)\n",
      "9916 Training Loss: tensor(0.0425)\n",
      "9917 Training Loss: tensor(0.0425)\n",
      "9918 Training Loss: tensor(0.0425)\n",
      "9919 Training Loss: tensor(0.0425)\n",
      "9920 Training Loss: tensor(0.0425)\n",
      "9921 Training Loss: tensor(0.0425)\n",
      "9922 Training Loss: tensor(0.0425)\n",
      "9923 Training Loss: tensor(0.0424)\n",
      "9924 Training Loss: tensor(0.0424)\n",
      "9925 Training Loss: tensor(0.0425)\n",
      "9926 Training Loss: tensor(0.0424)\n",
      "9927 Training Loss: tensor(0.0426)\n",
      "9928 Training Loss: tensor(0.0424)\n",
      "9929 Training Loss: tensor(0.0425)\n",
      "9930 Training Loss: tensor(0.0426)\n",
      "9931 Training Loss: tensor(0.0425)\n",
      "9932 Training Loss: tensor(0.0425)\n",
      "9933 Training Loss: tensor(0.0427)\n",
      "9934 Training Loss: tensor(0.0424)\n",
      "9935 Training Loss: tensor(0.0423)\n",
      "9936 Training Loss: tensor(0.0427)\n",
      "9937 Training Loss: tensor(0.0426)\n",
      "9938 Training Loss: tensor(0.0425)\n",
      "9939 Training Loss: tensor(0.0425)\n",
      "9940 Training Loss: tensor(0.0424)\n",
      "9941 Training Loss: tensor(0.0426)\n",
      "9942 Training Loss: tensor(0.0425)\n",
      "9943 Training Loss: tensor(0.0425)\n",
      "9944 Training Loss: tensor(0.0425)\n",
      "9945 Training Loss: tensor(0.0424)\n",
      "9946 Training Loss: tensor(0.0425)\n",
      "9947 Training Loss: tensor(0.0424)\n",
      "9948 Training Loss: tensor(0.0424)\n",
      "9949 Training Loss: tensor(0.0425)\n",
      "9950 Training Loss: tensor(0.0426)\n",
      "9951 Training Loss: tensor(0.0424)\n",
      "9952 Training Loss: tensor(0.0425)\n",
      "9953 Training Loss: tensor(0.0425)\n",
      "9954 Training Loss: tensor(0.0426)\n",
      "9955 Training Loss: tensor(0.0422)\n",
      "9956 Training Loss: tensor(0.0424)\n",
      "9957 Training Loss: tensor(0.0425)\n",
      "9958 Training Loss: tensor(0.0425)\n",
      "9959 Training Loss: tensor(0.0424)\n",
      "9960 Training Loss: tensor(0.0424)\n",
      "9961 Training Loss: tensor(0.0425)\n",
      "9962 Training Loss: tensor(0.0422)\n",
      "9963 Training Loss: tensor(0.0426)\n",
      "9964 Training Loss: tensor(0.0426)\n",
      "9965 Training Loss: tensor(0.0426)\n",
      "9966 Training Loss: tensor(0.0426)\n",
      "9967 Training Loss: tensor(0.0423)\n",
      "9968 Training Loss: tensor(0.0425)\n",
      "9969 Training Loss: tensor(0.0424)\n",
      "9970 Training Loss: tensor(0.0426)\n",
      "9971 Training Loss: tensor(0.0425)\n",
      "9972 Training Loss: tensor(0.0425)\n",
      "9973 Training Loss: tensor(0.0424)\n",
      "9974 Training Loss: tensor(0.0423)\n",
      "9975 Training Loss: tensor(0.0425)\n",
      "9976 Training Loss: tensor(0.0426)\n",
      "9977 Training Loss: tensor(0.0423)\n",
      "9978 Training Loss: tensor(0.0424)\n",
      "9979 Training Loss: tensor(0.0425)\n",
      "9980 Training Loss: tensor(0.0425)\n",
      "9981 Training Loss: tensor(0.0424)\n",
      "9982 Training Loss: tensor(0.0424)\n",
      "9983 Training Loss: tensor(0.0424)\n",
      "9984 Training Loss: tensor(0.0424)\n",
      "9985 Training Loss: tensor(0.0425)\n",
      "9986 Training Loss: tensor(0.0423)\n",
      "9987 Training Loss: tensor(0.0424)\n",
      "9988 Training Loss: tensor(0.0424)\n",
      "9989 Training Loss: tensor(0.0425)\n",
      "9990 Training Loss: tensor(0.0424)\n",
      "9991 Training Loss: tensor(0.0425)\n",
      "9992 Training Loss: tensor(0.0424)\n",
      "9993 Training Loss: tensor(0.0427)\n",
      "9994 Training Loss: tensor(0.0425)\n",
      "9995 Training Loss: tensor(0.0424)\n",
      "9996 Training Loss: tensor(0.0424)\n",
      "9997 Training Loss: tensor(0.0423)\n",
      "9998 Training Loss: tensor(0.0424)\n",
      "9999 Training Loss: tensor(0.0424)\n",
      "10000 Training Loss: tensor(0.0424)\n",
      "10001 Training Loss: tensor(0.0424)\n",
      "10002 Training Loss: tensor(0.0425)\n",
      "10003 Training Loss: tensor(0.0423)\n",
      "10004 Training Loss: tensor(0.0424)\n",
      "10005 Training Loss: tensor(0.0424)\n",
      "10006 Training Loss: tensor(0.0424)\n",
      "10007 Training Loss: tensor(0.0423)\n",
      "10008 Training Loss: tensor(0.0423)\n",
      "10009 Training Loss: tensor(0.0425)\n",
      "10010 Training Loss: tensor(0.0425)\n",
      "10011 Training Loss: tensor(0.0422)\n",
      "10012 Training Loss: tensor(0.0426)\n",
      "10013 Training Loss: tensor(0.0423)\n",
      "10014 Training Loss: tensor(0.0423)\n",
      "10015 Training Loss: tensor(0.0424)\n",
      "10016 Training Loss: tensor(0.0425)\n",
      "10017 Training Loss: tensor(0.0422)\n",
      "10018 Training Loss: tensor(0.0422)\n",
      "10019 Training Loss: tensor(0.0424)\n",
      "10020 Training Loss: tensor(0.0425)\n",
      "10021 Training Loss: tensor(0.0425)\n",
      "10022 Training Loss: tensor(0.0424)\n",
      "10023 Training Loss: tensor(0.0424)\n",
      "10024 Training Loss: tensor(0.0424)\n",
      "10025 Training Loss: tensor(0.0422)\n",
      "10026 Training Loss: tensor(0.0425)\n",
      "10027 Training Loss: tensor(0.0423)\n",
      "10028 Training Loss: tensor(0.0424)\n",
      "10029 Training Loss: tensor(0.0422)\n",
      "10030 Training Loss: tensor(0.0422)\n",
      "10031 Training Loss: tensor(0.0424)\n",
      "10032 Training Loss: tensor(0.0423)\n",
      "10033 Training Loss: tensor(0.0423)\n",
      "10034 Training Loss: tensor(0.0424)\n",
      "10035 Training Loss: tensor(0.0423)\n",
      "10036 Training Loss: tensor(0.0423)\n",
      "10037 Training Loss: tensor(0.0423)\n",
      "10038 Training Loss: tensor(0.0425)\n",
      "10039 Training Loss: tensor(0.0423)\n",
      "10040 Training Loss: tensor(0.0423)\n",
      "10041 Training Loss: tensor(0.0423)\n",
      "10042 Training Loss: tensor(0.0425)\n",
      "10043 Training Loss: tensor(0.0424)\n",
      "10044 Training Loss: tensor(0.0424)\n",
      "10045 Training Loss: tensor(0.0424)\n",
      "10046 Training Loss: tensor(0.0424)\n",
      "10047 Training Loss: tensor(0.0423)\n",
      "10048 Training Loss: tensor(0.0423)\n",
      "10049 Training Loss: tensor(0.0423)\n",
      "10050 Training Loss: tensor(0.0422)\n",
      "10051 Training Loss: tensor(0.0424)\n",
      "10052 Training Loss: tensor(0.0424)\n",
      "10053 Training Loss: tensor(0.0423)\n",
      "10054 Training Loss: tensor(0.0424)\n",
      "10055 Training Loss: tensor(0.0421)\n",
      "10056 Training Loss: tensor(0.0423)\n",
      "10057 Training Loss: tensor(0.0423)\n",
      "10058 Training Loss: tensor(0.0424)\n",
      "10059 Training Loss: tensor(0.0423)\n",
      "10060 Training Loss: tensor(0.0424)\n",
      "10061 Training Loss: tensor(0.0422)\n",
      "10062 Training Loss: tensor(0.0422)\n",
      "10063 Training Loss: tensor(0.0423)\n",
      "10064 Training Loss: tensor(0.0424)\n",
      "10065 Training Loss: tensor(0.0423)\n",
      "10066 Training Loss: tensor(0.0423)\n",
      "10067 Training Loss: tensor(0.0422)\n",
      "10068 Training Loss: tensor(0.0422)\n",
      "10069 Training Loss: tensor(0.0424)\n",
      "10070 Training Loss: tensor(0.0422)\n",
      "10071 Training Loss: tensor(0.0423)\n",
      "10072 Training Loss: tensor(0.0423)\n",
      "10073 Training Loss: tensor(0.0424)\n",
      "10074 Training Loss: tensor(0.0424)\n",
      "10075 Training Loss: tensor(0.0422)\n",
      "10076 Training Loss: tensor(0.0423)\n",
      "10077 Training Loss: tensor(0.0424)\n",
      "10078 Training Loss: tensor(0.0422)\n",
      "10079 Training Loss: tensor(0.0423)\n",
      "10080 Training Loss: tensor(0.0423)\n",
      "10081 Training Loss: tensor(0.0426)\n",
      "10082 Training Loss: tensor(0.0423)\n",
      "10083 Training Loss: tensor(0.0422)\n",
      "10084 Training Loss: tensor(0.0422)\n",
      "10085 Training Loss: tensor(0.0422)\n",
      "10086 Training Loss: tensor(0.0421)\n",
      "10087 Training Loss: tensor(0.0422)\n",
      "10088 Training Loss: tensor(0.0422)\n",
      "10089 Training Loss: tensor(0.0423)\n",
      "10090 Training Loss: tensor(0.0423)\n",
      "10091 Training Loss: tensor(0.0422)\n",
      "10092 Training Loss: tensor(0.0423)\n",
      "10093 Training Loss: tensor(0.0424)\n",
      "10094 Training Loss: tensor(0.0422)\n",
      "10095 Training Loss: tensor(0.0424)\n",
      "10096 Training Loss: tensor(0.0423)\n",
      "10097 Training Loss: tensor(0.0421)\n",
      "10098 Training Loss: tensor(0.0422)\n",
      "10099 Training Loss: tensor(0.0423)\n",
      "10100 Training Loss: tensor(0.0423)\n",
      "10101 Training Loss: tensor(0.0421)\n",
      "10102 Training Loss: tensor(0.0423)\n",
      "10103 Training Loss: tensor(0.0422)\n",
      "10104 Training Loss: tensor(0.0423)\n",
      "10105 Training Loss: tensor(0.0423)\n",
      "10106 Training Loss: tensor(0.0422)\n",
      "10107 Training Loss: tensor(0.0422)\n",
      "10108 Training Loss: tensor(0.0422)\n",
      "10109 Training Loss: tensor(0.0423)\n",
      "10110 Training Loss: tensor(0.0422)\n",
      "10111 Training Loss: tensor(0.0424)\n",
      "10112 Training Loss: tensor(0.0422)\n",
      "10113 Training Loss: tensor(0.0423)\n",
      "10114 Training Loss: tensor(0.0422)\n",
      "10115 Training Loss: tensor(0.0421)\n",
      "10116 Training Loss: tensor(0.0423)\n",
      "10117 Training Loss: tensor(0.0422)\n",
      "10118 Training Loss: tensor(0.0422)\n",
      "10119 Training Loss: tensor(0.0423)\n",
      "10120 Training Loss: tensor(0.0422)\n",
      "10121 Training Loss: tensor(0.0421)\n",
      "10122 Training Loss: tensor(0.0422)\n",
      "10123 Training Loss: tensor(0.0422)\n",
      "10124 Training Loss: tensor(0.0422)\n",
      "10125 Training Loss: tensor(0.0423)\n",
      "10126 Training Loss: tensor(0.0422)\n",
      "10127 Training Loss: tensor(0.0423)\n",
      "10128 Training Loss: tensor(0.0422)\n",
      "10129 Training Loss: tensor(0.0422)\n",
      "10130 Training Loss: tensor(0.0422)\n",
      "10131 Training Loss: tensor(0.0421)\n",
      "10132 Training Loss: tensor(0.0423)\n",
      "10133 Training Loss: tensor(0.0422)\n",
      "10134 Training Loss: tensor(0.0423)\n",
      "10135 Training Loss: tensor(0.0423)\n",
      "10136 Training Loss: tensor(0.0424)\n",
      "10137 Training Loss: tensor(0.0422)\n",
      "10138 Training Loss: tensor(0.0422)\n",
      "10139 Training Loss: tensor(0.0421)\n",
      "10140 Training Loss: tensor(0.0423)\n",
      "10141 Training Loss: tensor(0.0423)\n",
      "10142 Training Loss: tensor(0.0421)\n",
      "10143 Training Loss: tensor(0.0421)\n",
      "10144 Training Loss: tensor(0.0423)\n",
      "10145 Training Loss: tensor(0.0422)\n",
      "10146 Training Loss: tensor(0.0423)\n",
      "10147 Training Loss: tensor(0.0422)\n",
      "10148 Training Loss: tensor(0.0422)\n",
      "10149 Training Loss: tensor(0.0422)\n",
      "10150 Training Loss: tensor(0.0423)\n",
      "10151 Training Loss: tensor(0.0421)\n",
      "10152 Training Loss: tensor(0.0421)\n",
      "10153 Training Loss: tensor(0.0422)\n",
      "10154 Training Loss: tensor(0.0421)\n",
      "10155 Training Loss: tensor(0.0422)\n",
      "10156 Training Loss: tensor(0.0423)\n",
      "10157 Training Loss: tensor(0.0421)\n",
      "10158 Training Loss: tensor(0.0422)\n",
      "10159 Training Loss: tensor(0.0421)\n",
      "10160 Training Loss: tensor(0.0422)\n",
      "10161 Training Loss: tensor(0.0422)\n",
      "10162 Training Loss: tensor(0.0421)\n",
      "10163 Training Loss: tensor(0.0422)\n",
      "10164 Training Loss: tensor(0.0421)\n",
      "10165 Training Loss: tensor(0.0420)\n",
      "10166 Training Loss: tensor(0.0420)\n",
      "10167 Training Loss: tensor(0.0422)\n",
      "10168 Training Loss: tensor(0.0422)\n",
      "10169 Training Loss: tensor(0.0421)\n",
      "10170 Training Loss: tensor(0.0421)\n",
      "10171 Training Loss: tensor(0.0422)\n",
      "10172 Training Loss: tensor(0.0423)\n",
      "10173 Training Loss: tensor(0.0422)\n",
      "10174 Training Loss: tensor(0.0421)\n",
      "10175 Training Loss: tensor(0.0422)\n",
      "10176 Training Loss: tensor(0.0421)\n",
      "10177 Training Loss: tensor(0.0422)\n",
      "10178 Training Loss: tensor(0.0422)\n",
      "10179 Training Loss: tensor(0.0421)\n",
      "10180 Training Loss: tensor(0.0420)\n",
      "10181 Training Loss: tensor(0.0422)\n",
      "10182 Training Loss: tensor(0.0423)\n",
      "10183 Training Loss: tensor(0.0421)\n",
      "10184 Training Loss: tensor(0.0423)\n",
      "10185 Training Loss: tensor(0.0422)\n",
      "10186 Training Loss: tensor(0.0421)\n",
      "10187 Training Loss: tensor(0.0421)\n",
      "10188 Training Loss: tensor(0.0424)\n",
      "10189 Training Loss: tensor(0.0422)\n",
      "10190 Training Loss: tensor(0.0421)\n",
      "10191 Training Loss: tensor(0.0423)\n",
      "10192 Training Loss: tensor(0.0421)\n",
      "10193 Training Loss: tensor(0.0422)\n",
      "10194 Training Loss: tensor(0.0422)\n",
      "10195 Training Loss: tensor(0.0421)\n",
      "10196 Training Loss: tensor(0.0422)\n",
      "10197 Training Loss: tensor(0.0421)\n",
      "10198 Training Loss: tensor(0.0422)\n",
      "10199 Training Loss: tensor(0.0422)\n",
      "10200 Training Loss: tensor(0.0421)\n",
      "10201 Training Loss: tensor(0.0421)\n",
      "10202 Training Loss: tensor(0.0420)\n",
      "10203 Training Loss: tensor(0.0421)\n",
      "10204 Training Loss: tensor(0.0422)\n",
      "10205 Training Loss: tensor(0.0421)\n",
      "10206 Training Loss: tensor(0.0421)\n",
      "10207 Training Loss: tensor(0.0419)\n",
      "10208 Training Loss: tensor(0.0421)\n",
      "10209 Training Loss: tensor(0.0421)\n",
      "10210 Training Loss: tensor(0.0420)\n",
      "10211 Training Loss: tensor(0.0420)\n",
      "10212 Training Loss: tensor(0.0420)\n",
      "10213 Training Loss: tensor(0.0420)\n",
      "10214 Training Loss: tensor(0.0421)\n",
      "10215 Training Loss: tensor(0.0421)\n",
      "10216 Training Loss: tensor(0.0420)\n",
      "10217 Training Loss: tensor(0.0421)\n",
      "10218 Training Loss: tensor(0.0423)\n",
      "10219 Training Loss: tensor(0.0422)\n",
      "10220 Training Loss: tensor(0.0421)\n",
      "10221 Training Loss: tensor(0.0422)\n",
      "10222 Training Loss: tensor(0.0421)\n",
      "10223 Training Loss: tensor(0.0422)\n",
      "10224 Training Loss: tensor(0.0420)\n",
      "10225 Training Loss: tensor(0.0422)\n",
      "10226 Training Loss: tensor(0.0421)\n",
      "10227 Training Loss: tensor(0.0420)\n",
      "10228 Training Loss: tensor(0.0421)\n",
      "10229 Training Loss: tensor(0.0420)\n",
      "10230 Training Loss: tensor(0.0422)\n",
      "10231 Training Loss: tensor(0.0421)\n",
      "10232 Training Loss: tensor(0.0420)\n",
      "10233 Training Loss: tensor(0.0421)\n",
      "10234 Training Loss: tensor(0.0419)\n",
      "10235 Training Loss: tensor(0.0421)\n",
      "10236 Training Loss: tensor(0.0421)\n",
      "10237 Training Loss: tensor(0.0419)\n",
      "10238 Training Loss: tensor(0.0421)\n",
      "10239 Training Loss: tensor(0.0421)\n",
      "10240 Training Loss: tensor(0.0420)\n",
      "10241 Training Loss: tensor(0.0421)\n",
      "10242 Training Loss: tensor(0.0421)\n",
      "10243 Training Loss: tensor(0.0421)\n",
      "10244 Training Loss: tensor(0.0420)\n",
      "10245 Training Loss: tensor(0.0420)\n",
      "10246 Training Loss: tensor(0.0422)\n",
      "10247 Training Loss: tensor(0.0420)\n",
      "10248 Training Loss: tensor(0.0421)\n",
      "10249 Training Loss: tensor(0.0419)\n",
      "10250 Training Loss: tensor(0.0421)\n",
      "10251 Training Loss: tensor(0.0421)\n",
      "10252 Training Loss: tensor(0.0420)\n",
      "10253 Training Loss: tensor(0.0420)\n",
      "10254 Training Loss: tensor(0.0421)\n",
      "10255 Training Loss: tensor(0.0422)\n",
      "10256 Training Loss: tensor(0.0421)\n",
      "10257 Training Loss: tensor(0.0420)\n",
      "10258 Training Loss: tensor(0.0422)\n",
      "10259 Training Loss: tensor(0.0420)\n",
      "10260 Training Loss: tensor(0.0420)\n",
      "10261 Training Loss: tensor(0.0421)\n",
      "10262 Training Loss: tensor(0.0421)\n",
      "10263 Training Loss: tensor(0.0421)\n",
      "10264 Training Loss: tensor(0.0422)\n",
      "10265 Training Loss: tensor(0.0421)\n",
      "10266 Training Loss: tensor(0.0421)\n",
      "10267 Training Loss: tensor(0.0421)\n",
      "10268 Training Loss: tensor(0.0420)\n",
      "10269 Training Loss: tensor(0.0420)\n",
      "10270 Training Loss: tensor(0.0419)\n",
      "10271 Training Loss: tensor(0.0419)\n",
      "10272 Training Loss: tensor(0.0419)\n",
      "10273 Training Loss: tensor(0.0419)\n",
      "10274 Training Loss: tensor(0.0419)\n",
      "10275 Training Loss: tensor(0.0420)\n",
      "10276 Training Loss: tensor(0.0419)\n",
      "10277 Training Loss: tensor(0.0420)\n",
      "10278 Training Loss: tensor(0.0420)\n",
      "10279 Training Loss: tensor(0.0419)\n",
      "10280 Training Loss: tensor(0.0421)\n",
      "10281 Training Loss: tensor(0.0420)\n",
      "10282 Training Loss: tensor(0.0420)\n",
      "10283 Training Loss: tensor(0.0420)\n",
      "10284 Training Loss: tensor(0.0419)\n",
      "10285 Training Loss: tensor(0.0420)\n",
      "10286 Training Loss: tensor(0.0421)\n",
      "10287 Training Loss: tensor(0.0419)\n",
      "10288 Training Loss: tensor(0.0419)\n",
      "10289 Training Loss: tensor(0.0419)\n",
      "10290 Training Loss: tensor(0.0421)\n",
      "10291 Training Loss: tensor(0.0420)\n",
      "10292 Training Loss: tensor(0.0419)\n",
      "10293 Training Loss: tensor(0.0421)\n",
      "10294 Training Loss: tensor(0.0420)\n",
      "10295 Training Loss: tensor(0.0419)\n",
      "10296 Training Loss: tensor(0.0420)\n",
      "10297 Training Loss: tensor(0.0421)\n",
      "10298 Training Loss: tensor(0.0419)\n",
      "10299 Training Loss: tensor(0.0420)\n",
      "10300 Training Loss: tensor(0.0419)\n",
      "10301 Training Loss: tensor(0.0419)\n",
      "10302 Training Loss: tensor(0.0419)\n",
      "10303 Training Loss: tensor(0.0420)\n",
      "10304 Training Loss: tensor(0.0419)\n",
      "10305 Training Loss: tensor(0.0420)\n",
      "10306 Training Loss: tensor(0.0421)\n",
      "10307 Training Loss: tensor(0.0420)\n",
      "10308 Training Loss: tensor(0.0422)\n",
      "10309 Training Loss: tensor(0.0420)\n",
      "10310 Training Loss: tensor(0.0420)\n",
      "10311 Training Loss: tensor(0.0419)\n",
      "10312 Training Loss: tensor(0.0420)\n",
      "10313 Training Loss: tensor(0.0419)\n",
      "10314 Training Loss: tensor(0.0419)\n",
      "10315 Training Loss: tensor(0.0420)\n",
      "10316 Training Loss: tensor(0.0420)\n",
      "10317 Training Loss: tensor(0.0420)\n",
      "10318 Training Loss: tensor(0.0419)\n",
      "10319 Training Loss: tensor(0.0420)\n",
      "10320 Training Loss: tensor(0.0419)\n",
      "10321 Training Loss: tensor(0.0420)\n",
      "10322 Training Loss: tensor(0.0419)\n",
      "10323 Training Loss: tensor(0.0420)\n",
      "10324 Training Loss: tensor(0.0420)\n",
      "10325 Training Loss: tensor(0.0419)\n",
      "10326 Training Loss: tensor(0.0419)\n",
      "10327 Training Loss: tensor(0.0418)\n",
      "10328 Training Loss: tensor(0.0418)\n",
      "10329 Training Loss: tensor(0.0419)\n",
      "10330 Training Loss: tensor(0.0420)\n",
      "10331 Training Loss: tensor(0.0420)\n",
      "10332 Training Loss: tensor(0.0419)\n",
      "10333 Training Loss: tensor(0.0418)\n",
      "10334 Training Loss: tensor(0.0418)\n",
      "10335 Training Loss: tensor(0.0420)\n",
      "10336 Training Loss: tensor(0.0419)\n",
      "10337 Training Loss: tensor(0.0419)\n",
      "10338 Training Loss: tensor(0.0419)\n",
      "10339 Training Loss: tensor(0.0419)\n",
      "10340 Training Loss: tensor(0.0419)\n",
      "10341 Training Loss: tensor(0.0419)\n",
      "10342 Training Loss: tensor(0.0420)\n",
      "10343 Training Loss: tensor(0.0419)\n",
      "10344 Training Loss: tensor(0.0419)\n",
      "10345 Training Loss: tensor(0.0419)\n",
      "10346 Training Loss: tensor(0.0419)\n",
      "10347 Training Loss: tensor(0.0418)\n",
      "10348 Training Loss: tensor(0.0419)\n",
      "10349 Training Loss: tensor(0.0418)\n",
      "10350 Training Loss: tensor(0.0419)\n",
      "10351 Training Loss: tensor(0.0418)\n",
      "10352 Training Loss: tensor(0.0419)\n",
      "10353 Training Loss: tensor(0.0420)\n",
      "10354 Training Loss: tensor(0.0419)\n",
      "10355 Training Loss: tensor(0.0419)\n",
      "10356 Training Loss: tensor(0.0419)\n",
      "10357 Training Loss: tensor(0.0418)\n",
      "10358 Training Loss: tensor(0.0419)\n",
      "10359 Training Loss: tensor(0.0419)\n",
      "10360 Training Loss: tensor(0.0418)\n",
      "10361 Training Loss: tensor(0.0420)\n",
      "10362 Training Loss: tensor(0.0418)\n",
      "10363 Training Loss: tensor(0.0418)\n",
      "10364 Training Loss: tensor(0.0419)\n",
      "10365 Training Loss: tensor(0.0418)\n",
      "10366 Training Loss: tensor(0.0420)\n",
      "10367 Training Loss: tensor(0.0419)\n",
      "10368 Training Loss: tensor(0.0420)\n",
      "10369 Training Loss: tensor(0.0419)\n",
      "10370 Training Loss: tensor(0.0418)\n",
      "10371 Training Loss: tensor(0.0420)\n",
      "10372 Training Loss: tensor(0.0419)\n",
      "10373 Training Loss: tensor(0.0419)\n",
      "10374 Training Loss: tensor(0.0418)\n",
      "10375 Training Loss: tensor(0.0419)\n",
      "10376 Training Loss: tensor(0.0419)\n",
      "10377 Training Loss: tensor(0.0418)\n",
      "10378 Training Loss: tensor(0.0419)\n",
      "10379 Training Loss: tensor(0.0419)\n",
      "10380 Training Loss: tensor(0.0418)\n",
      "10381 Training Loss: tensor(0.0419)\n",
      "10382 Training Loss: tensor(0.0418)\n",
      "10383 Training Loss: tensor(0.0419)\n",
      "10384 Training Loss: tensor(0.0418)\n",
      "10385 Training Loss: tensor(0.0419)\n",
      "10386 Training Loss: tensor(0.0419)\n",
      "10387 Training Loss: tensor(0.0418)\n",
      "10388 Training Loss: tensor(0.0419)\n",
      "10389 Training Loss: tensor(0.0419)\n",
      "10390 Training Loss: tensor(0.0418)\n",
      "10391 Training Loss: tensor(0.0418)\n",
      "10392 Training Loss: tensor(0.0419)\n",
      "10393 Training Loss: tensor(0.0417)\n",
      "10394 Training Loss: tensor(0.0419)\n",
      "10395 Training Loss: tensor(0.0419)\n",
      "10396 Training Loss: tensor(0.0417)\n",
      "10397 Training Loss: tensor(0.0418)\n",
      "10398 Training Loss: tensor(0.0419)\n",
      "10399 Training Loss: tensor(0.0419)\n",
      "10400 Training Loss: tensor(0.0419)\n",
      "10401 Training Loss: tensor(0.0417)\n",
      "10402 Training Loss: tensor(0.0418)\n",
      "10403 Training Loss: tensor(0.0417)\n",
      "10404 Training Loss: tensor(0.0419)\n",
      "10405 Training Loss: tensor(0.0417)\n",
      "10406 Training Loss: tensor(0.0419)\n",
      "10407 Training Loss: tensor(0.0419)\n",
      "10408 Training Loss: tensor(0.0418)\n",
      "10409 Training Loss: tensor(0.0417)\n",
      "10410 Training Loss: tensor(0.0419)\n",
      "10411 Training Loss: tensor(0.0418)\n",
      "10412 Training Loss: tensor(0.0418)\n",
      "10413 Training Loss: tensor(0.0417)\n",
      "10414 Training Loss: tensor(0.0417)\n",
      "10415 Training Loss: tensor(0.0418)\n",
      "10416 Training Loss: tensor(0.0417)\n",
      "10417 Training Loss: tensor(0.0418)\n",
      "10418 Training Loss: tensor(0.0420)\n",
      "10419 Training Loss: tensor(0.0418)\n",
      "10420 Training Loss: tensor(0.0418)\n",
      "10421 Training Loss: tensor(0.0417)\n",
      "10422 Training Loss: tensor(0.0418)\n",
      "10423 Training Loss: tensor(0.0416)\n",
      "10424 Training Loss: tensor(0.0419)\n",
      "10425 Training Loss: tensor(0.0418)\n",
      "10426 Training Loss: tensor(0.0418)\n",
      "10427 Training Loss: tensor(0.0418)\n",
      "10428 Training Loss: tensor(0.0419)\n",
      "10429 Training Loss: tensor(0.0419)\n",
      "10430 Training Loss: tensor(0.0418)\n",
      "10431 Training Loss: tensor(0.0418)\n",
      "10432 Training Loss: tensor(0.0418)\n",
      "10433 Training Loss: tensor(0.0417)\n",
      "10434 Training Loss: tensor(0.0418)\n",
      "10435 Training Loss: tensor(0.0419)\n",
      "10436 Training Loss: tensor(0.0418)\n",
      "10437 Training Loss: tensor(0.0418)\n",
      "10438 Training Loss: tensor(0.0417)\n",
      "10439 Training Loss: tensor(0.0417)\n",
      "10440 Training Loss: tensor(0.0417)\n",
      "10441 Training Loss: tensor(0.0418)\n",
      "10442 Training Loss: tensor(0.0419)\n",
      "10443 Training Loss: tensor(0.0418)\n",
      "10444 Training Loss: tensor(0.0418)\n",
      "10445 Training Loss: tensor(0.0418)\n",
      "10446 Training Loss: tensor(0.0417)\n",
      "10447 Training Loss: tensor(0.0417)\n",
      "10448 Training Loss: tensor(0.0418)\n",
      "10449 Training Loss: tensor(0.0417)\n",
      "10450 Training Loss: tensor(0.0418)\n",
      "10451 Training Loss: tensor(0.0418)\n",
      "10452 Training Loss: tensor(0.0416)\n",
      "10453 Training Loss: tensor(0.0417)\n",
      "10454 Training Loss: tensor(0.0416)\n",
      "10455 Training Loss: tensor(0.0417)\n",
      "10456 Training Loss: tensor(0.0417)\n",
      "10457 Training Loss: tensor(0.0417)\n",
      "10458 Training Loss: tensor(0.0417)\n",
      "10459 Training Loss: tensor(0.0418)\n",
      "10460 Training Loss: tensor(0.0417)\n",
      "10461 Training Loss: tensor(0.0418)\n",
      "10462 Training Loss: tensor(0.0417)\n",
      "10463 Training Loss: tensor(0.0416)\n",
      "10464 Training Loss: tensor(0.0417)\n",
      "10465 Training Loss: tensor(0.0418)\n",
      "10466 Training Loss: tensor(0.0419)\n",
      "10467 Training Loss: tensor(0.0417)\n",
      "10468 Training Loss: tensor(0.0416)\n",
      "10469 Training Loss: tensor(0.0416)\n",
      "10470 Training Loss: tensor(0.0416)\n",
      "10471 Training Loss: tensor(0.0417)\n",
      "10472 Training Loss: tensor(0.0418)\n",
      "10473 Training Loss: tensor(0.0417)\n",
      "10474 Training Loss: tensor(0.0416)\n",
      "10475 Training Loss: tensor(0.0417)\n",
      "10476 Training Loss: tensor(0.0418)\n",
      "10477 Training Loss: tensor(0.0417)\n",
      "10478 Training Loss: tensor(0.0416)\n",
      "10479 Training Loss: tensor(0.0418)\n",
      "10480 Training Loss: tensor(0.0417)\n",
      "10481 Training Loss: tensor(0.0416)\n",
      "10482 Training Loss: tensor(0.0416)\n",
      "10483 Training Loss: tensor(0.0417)\n",
      "10484 Training Loss: tensor(0.0417)\n",
      "10485 Training Loss: tensor(0.0418)\n",
      "10486 Training Loss: tensor(0.0417)\n",
      "10487 Training Loss: tensor(0.0418)\n",
      "10488 Training Loss: tensor(0.0415)\n",
      "10489 Training Loss: tensor(0.0417)\n",
      "10490 Training Loss: tensor(0.0418)\n",
      "10491 Training Loss: tensor(0.0417)\n",
      "10492 Training Loss: tensor(0.0417)\n",
      "10493 Training Loss: tensor(0.0416)\n",
      "10494 Training Loss: tensor(0.0418)\n",
      "10495 Training Loss: tensor(0.0418)\n",
      "10496 Training Loss: tensor(0.0417)\n",
      "10497 Training Loss: tensor(0.0419)\n",
      "10498 Training Loss: tensor(0.0418)\n",
      "10499 Training Loss: tensor(0.0418)\n",
      "10500 Training Loss: tensor(0.0417)\n",
      "10501 Training Loss: tensor(0.0417)\n",
      "10502 Training Loss: tensor(0.0416)\n",
      "10503 Training Loss: tensor(0.0416)\n",
      "10504 Training Loss: tensor(0.0417)\n",
      "10505 Training Loss: tensor(0.0417)\n",
      "10506 Training Loss: tensor(0.0416)\n",
      "10507 Training Loss: tensor(0.0416)\n",
      "10508 Training Loss: tensor(0.0417)\n",
      "10509 Training Loss: tensor(0.0417)\n",
      "10510 Training Loss: tensor(0.0416)\n",
      "10511 Training Loss: tensor(0.0417)\n",
      "10512 Training Loss: tensor(0.0418)\n",
      "10513 Training Loss: tensor(0.0416)\n",
      "10514 Training Loss: tensor(0.0416)\n",
      "10515 Training Loss: tensor(0.0415)\n",
      "10516 Training Loss: tensor(0.0416)\n",
      "10517 Training Loss: tensor(0.0418)\n",
      "10518 Training Loss: tensor(0.0415)\n",
      "10519 Training Loss: tensor(0.0418)\n",
      "10520 Training Loss: tensor(0.0417)\n",
      "10521 Training Loss: tensor(0.0416)\n",
      "10522 Training Loss: tensor(0.0416)\n",
      "10523 Training Loss: tensor(0.0416)\n",
      "10524 Training Loss: tensor(0.0416)\n",
      "10525 Training Loss: tensor(0.0416)\n",
      "10526 Training Loss: tensor(0.0417)\n",
      "10527 Training Loss: tensor(0.0417)\n",
      "10528 Training Loss: tensor(0.0417)\n",
      "10529 Training Loss: tensor(0.0415)\n",
      "10530 Training Loss: tensor(0.0417)\n",
      "10531 Training Loss: tensor(0.0415)\n",
      "10532 Training Loss: tensor(0.0416)\n",
      "10533 Training Loss: tensor(0.0416)\n",
      "10534 Training Loss: tensor(0.0416)\n",
      "10535 Training Loss: tensor(0.0416)\n",
      "10536 Training Loss: tensor(0.0415)\n",
      "10537 Training Loss: tensor(0.0416)\n",
      "10538 Training Loss: tensor(0.0416)\n",
      "10539 Training Loss: tensor(0.0416)\n",
      "10540 Training Loss: tensor(0.0417)\n",
      "10541 Training Loss: tensor(0.0418)\n",
      "10542 Training Loss: tensor(0.0417)\n",
      "10543 Training Loss: tensor(0.0416)\n",
      "10544 Training Loss: tensor(0.0417)\n",
      "10545 Training Loss: tensor(0.0416)\n",
      "10546 Training Loss: tensor(0.0416)\n",
      "10547 Training Loss: tensor(0.0416)\n",
      "10548 Training Loss: tensor(0.0415)\n",
      "10549 Training Loss: tensor(0.0414)\n",
      "10550 Training Loss: tensor(0.0417)\n",
      "10551 Training Loss: tensor(0.0415)\n",
      "10552 Training Loss: tensor(0.0416)\n",
      "10553 Training Loss: tensor(0.0416)\n",
      "10554 Training Loss: tensor(0.0416)\n",
      "10555 Training Loss: tensor(0.0416)\n",
      "10556 Training Loss: tensor(0.0416)\n",
      "10557 Training Loss: tensor(0.0418)\n",
      "10558 Training Loss: tensor(0.0415)\n",
      "10559 Training Loss: tensor(0.0416)\n",
      "10560 Training Loss: tensor(0.0415)\n",
      "10561 Training Loss: tensor(0.0415)\n",
      "10562 Training Loss: tensor(0.0416)\n",
      "10563 Training Loss: tensor(0.0416)\n",
      "10564 Training Loss: tensor(0.0416)\n",
      "10565 Training Loss: tensor(0.0416)\n",
      "10566 Training Loss: tensor(0.0416)\n",
      "10567 Training Loss: tensor(0.0417)\n",
      "10568 Training Loss: tensor(0.0415)\n",
      "10569 Training Loss: tensor(0.0416)\n",
      "10570 Training Loss: tensor(0.0416)\n",
      "10571 Training Loss: tensor(0.0418)\n",
      "10572 Training Loss: tensor(0.0416)\n",
      "10573 Training Loss: tensor(0.0416)\n",
      "10574 Training Loss: tensor(0.0416)\n",
      "10575 Training Loss: tensor(0.0414)\n",
      "10576 Training Loss: tensor(0.0416)\n",
      "10577 Training Loss: tensor(0.0415)\n",
      "10578 Training Loss: tensor(0.0416)\n",
      "10579 Training Loss: tensor(0.0414)\n",
      "10580 Training Loss: tensor(0.0415)\n",
      "10581 Training Loss: tensor(0.0417)\n",
      "10582 Training Loss: tensor(0.0416)\n",
      "10583 Training Loss: tensor(0.0416)\n",
      "10584 Training Loss: tensor(0.0415)\n",
      "10585 Training Loss: tensor(0.0415)\n",
      "10586 Training Loss: tensor(0.0415)\n",
      "10587 Training Loss: tensor(0.0417)\n",
      "10588 Training Loss: tensor(0.0416)\n",
      "10589 Training Loss: tensor(0.0415)\n",
      "10590 Training Loss: tensor(0.0415)\n",
      "10591 Training Loss: tensor(0.0416)\n",
      "10592 Training Loss: tensor(0.0415)\n",
      "10593 Training Loss: tensor(0.0416)\n",
      "10594 Training Loss: tensor(0.0414)\n",
      "10595 Training Loss: tensor(0.0414)\n",
      "10596 Training Loss: tensor(0.0416)\n",
      "10597 Training Loss: tensor(0.0415)\n",
      "10598 Training Loss: tensor(0.0415)\n",
      "10599 Training Loss: tensor(0.0415)\n",
      "10600 Training Loss: tensor(0.0416)\n",
      "10601 Training Loss: tensor(0.0415)\n",
      "10602 Training Loss: tensor(0.0416)\n",
      "10603 Training Loss: tensor(0.0417)\n",
      "10604 Training Loss: tensor(0.0415)\n",
      "10605 Training Loss: tensor(0.0415)\n",
      "10606 Training Loss: tensor(0.0414)\n",
      "10607 Training Loss: tensor(0.0415)\n",
      "10608 Training Loss: tensor(0.0415)\n",
      "10609 Training Loss: tensor(0.0416)\n",
      "10610 Training Loss: tensor(0.0416)\n",
      "10611 Training Loss: tensor(0.0415)\n",
      "10612 Training Loss: tensor(0.0415)\n",
      "10613 Training Loss: tensor(0.0414)\n",
      "10614 Training Loss: tensor(0.0414)\n",
      "10615 Training Loss: tensor(0.0415)\n",
      "10616 Training Loss: tensor(0.0415)\n",
      "10617 Training Loss: tensor(0.0415)\n",
      "10618 Training Loss: tensor(0.0415)\n",
      "10619 Training Loss: tensor(0.0415)\n",
      "10620 Training Loss: tensor(0.0415)\n",
      "10621 Training Loss: tensor(0.0416)\n",
      "10622 Training Loss: tensor(0.0414)\n",
      "10623 Training Loss: tensor(0.0416)\n",
      "10624 Training Loss: tensor(0.0414)\n",
      "10625 Training Loss: tensor(0.0415)\n",
      "10626 Training Loss: tensor(0.0416)\n",
      "10627 Training Loss: tensor(0.0415)\n",
      "10628 Training Loss: tensor(0.0415)\n",
      "10629 Training Loss: tensor(0.0416)\n",
      "10630 Training Loss: tensor(0.0414)\n",
      "10631 Training Loss: tensor(0.0415)\n",
      "10632 Training Loss: tensor(0.0414)\n",
      "10633 Training Loss: tensor(0.0414)\n",
      "10634 Training Loss: tensor(0.0415)\n",
      "10635 Training Loss: tensor(0.0414)\n",
      "10636 Training Loss: tensor(0.0414)\n",
      "10637 Training Loss: tensor(0.0414)\n",
      "10638 Training Loss: tensor(0.0415)\n",
      "10639 Training Loss: tensor(0.0415)\n",
      "10640 Training Loss: tensor(0.0416)\n",
      "10641 Training Loss: tensor(0.0415)\n",
      "10642 Training Loss: tensor(0.0415)\n",
      "10643 Training Loss: tensor(0.0414)\n",
      "10644 Training Loss: tensor(0.0414)\n",
      "10645 Training Loss: tensor(0.0413)\n",
      "10646 Training Loss: tensor(0.0414)\n",
      "10647 Training Loss: tensor(0.0415)\n",
      "10648 Training Loss: tensor(0.0416)\n",
      "10649 Training Loss: tensor(0.0414)\n",
      "10650 Training Loss: tensor(0.0415)\n",
      "10651 Training Loss: tensor(0.0413)\n",
      "10652 Training Loss: tensor(0.0414)\n",
      "10653 Training Loss: tensor(0.0414)\n",
      "10654 Training Loss: tensor(0.0413)\n",
      "10655 Training Loss: tensor(0.0414)\n",
      "10656 Training Loss: tensor(0.0414)\n",
      "10657 Training Loss: tensor(0.0414)\n",
      "10658 Training Loss: tensor(0.0414)\n",
      "10659 Training Loss: tensor(0.0414)\n",
      "10660 Training Loss: tensor(0.0415)\n",
      "10661 Training Loss: tensor(0.0413)\n",
      "10662 Training Loss: tensor(0.0414)\n",
      "10663 Training Loss: tensor(0.0413)\n",
      "10664 Training Loss: tensor(0.0415)\n",
      "10665 Training Loss: tensor(0.0413)\n",
      "10666 Training Loss: tensor(0.0414)\n",
      "10667 Training Loss: tensor(0.0412)\n",
      "10668 Training Loss: tensor(0.0414)\n",
      "10669 Training Loss: tensor(0.0415)\n",
      "10670 Training Loss: tensor(0.0414)\n",
      "10671 Training Loss: tensor(0.0415)\n",
      "10672 Training Loss: tensor(0.0415)\n",
      "10673 Training Loss: tensor(0.0414)\n",
      "10674 Training Loss: tensor(0.0414)\n",
      "10675 Training Loss: tensor(0.0414)\n",
      "10676 Training Loss: tensor(0.0414)\n",
      "10677 Training Loss: tensor(0.0415)\n",
      "10678 Training Loss: tensor(0.0413)\n",
      "10679 Training Loss: tensor(0.0413)\n",
      "10680 Training Loss: tensor(0.0415)\n",
      "10681 Training Loss: tensor(0.0413)\n",
      "10682 Training Loss: tensor(0.0413)\n",
      "10683 Training Loss: tensor(0.0412)\n",
      "10684 Training Loss: tensor(0.0414)\n",
      "10685 Training Loss: tensor(0.0412)\n",
      "10686 Training Loss: tensor(0.0414)\n",
      "10687 Training Loss: tensor(0.0413)\n",
      "10688 Training Loss: tensor(0.0414)\n",
      "10689 Training Loss: tensor(0.0414)\n",
      "10690 Training Loss: tensor(0.0415)\n",
      "10691 Training Loss: tensor(0.0413)\n",
      "10692 Training Loss: tensor(0.0413)\n",
      "10693 Training Loss: tensor(0.0413)\n",
      "10694 Training Loss: tensor(0.0413)\n",
      "10695 Training Loss: tensor(0.0412)\n",
      "10696 Training Loss: tensor(0.0413)\n",
      "10697 Training Loss: tensor(0.0413)\n",
      "10698 Training Loss: tensor(0.0412)\n",
      "10699 Training Loss: tensor(0.0415)\n",
      "10700 Training Loss: tensor(0.0414)\n",
      "10701 Training Loss: tensor(0.0414)\n",
      "10702 Training Loss: tensor(0.0414)\n",
      "10703 Training Loss: tensor(0.0414)\n",
      "10704 Training Loss: tensor(0.0414)\n",
      "10705 Training Loss: tensor(0.0413)\n",
      "10706 Training Loss: tensor(0.0413)\n",
      "10707 Training Loss: tensor(0.0413)\n",
      "10708 Training Loss: tensor(0.0413)\n",
      "10709 Training Loss: tensor(0.0413)\n",
      "10710 Training Loss: tensor(0.0413)\n",
      "10711 Training Loss: tensor(0.0415)\n",
      "10712 Training Loss: tensor(0.0413)\n",
      "10713 Training Loss: tensor(0.0414)\n",
      "10714 Training Loss: tensor(0.0414)\n",
      "10715 Training Loss: tensor(0.0413)\n",
      "10716 Training Loss: tensor(0.0413)\n",
      "10717 Training Loss: tensor(0.0413)\n",
      "10718 Training Loss: tensor(0.0413)\n",
      "10719 Training Loss: tensor(0.0413)\n",
      "10720 Training Loss: tensor(0.0413)\n",
      "10721 Training Loss: tensor(0.0414)\n",
      "10722 Training Loss: tensor(0.0412)\n",
      "10723 Training Loss: tensor(0.0414)\n",
      "10724 Training Loss: tensor(0.0412)\n",
      "10725 Training Loss: tensor(0.0413)\n",
      "10726 Training Loss: tensor(0.0414)\n",
      "10727 Training Loss: tensor(0.0413)\n",
      "10728 Training Loss: tensor(0.0414)\n",
      "10729 Training Loss: tensor(0.0412)\n",
      "10730 Training Loss: tensor(0.0413)\n",
      "10731 Training Loss: tensor(0.0412)\n",
      "10732 Training Loss: tensor(0.0412)\n",
      "10733 Training Loss: tensor(0.0413)\n",
      "10734 Training Loss: tensor(0.0412)\n",
      "10735 Training Loss: tensor(0.0414)\n",
      "10736 Training Loss: tensor(0.0412)\n",
      "10737 Training Loss: tensor(0.0413)\n",
      "10738 Training Loss: tensor(0.0412)\n",
      "10739 Training Loss: tensor(0.0412)\n",
      "10740 Training Loss: tensor(0.0413)\n",
      "10741 Training Loss: tensor(0.0412)\n",
      "10742 Training Loss: tensor(0.0413)\n",
      "10743 Training Loss: tensor(0.0413)\n",
      "10744 Training Loss: tensor(0.0413)\n",
      "10745 Training Loss: tensor(0.0413)\n",
      "10746 Training Loss: tensor(0.0412)\n",
      "10747 Training Loss: tensor(0.0413)\n",
      "10748 Training Loss: tensor(0.0412)\n",
      "10749 Training Loss: tensor(0.0412)\n",
      "10750 Training Loss: tensor(0.0412)\n",
      "10751 Training Loss: tensor(0.0412)\n",
      "10752 Training Loss: tensor(0.0414)\n",
      "10753 Training Loss: tensor(0.0413)\n",
      "10754 Training Loss: tensor(0.0413)\n",
      "10755 Training Loss: tensor(0.0412)\n",
      "10756 Training Loss: tensor(0.0412)\n",
      "10757 Training Loss: tensor(0.0415)\n",
      "10758 Training Loss: tensor(0.0412)\n",
      "10759 Training Loss: tensor(0.0413)\n",
      "10760 Training Loss: tensor(0.0413)\n",
      "10761 Training Loss: tensor(0.0413)\n",
      "10762 Training Loss: tensor(0.0413)\n",
      "10763 Training Loss: tensor(0.0411)\n",
      "10764 Training Loss: tensor(0.0411)\n",
      "10765 Training Loss: tensor(0.0413)\n",
      "10766 Training Loss: tensor(0.0412)\n",
      "10767 Training Loss: tensor(0.0413)\n",
      "10768 Training Loss: tensor(0.0412)\n",
      "10769 Training Loss: tensor(0.0412)\n",
      "10770 Training Loss: tensor(0.0412)\n",
      "10771 Training Loss: tensor(0.0412)\n",
      "10772 Training Loss: tensor(0.0412)\n",
      "10773 Training Loss: tensor(0.0412)\n",
      "10774 Training Loss: tensor(0.0413)\n",
      "10775 Training Loss: tensor(0.0412)\n",
      "10776 Training Loss: tensor(0.0413)\n",
      "10777 Training Loss: tensor(0.0413)\n",
      "10778 Training Loss: tensor(0.0411)\n",
      "10779 Training Loss: tensor(0.0412)\n",
      "10780 Training Loss: tensor(0.0411)\n",
      "10781 Training Loss: tensor(0.0411)\n",
      "10782 Training Loss: tensor(0.0411)\n",
      "10783 Training Loss: tensor(0.0411)\n",
      "10784 Training Loss: tensor(0.0412)\n",
      "10785 Training Loss: tensor(0.0411)\n",
      "10786 Training Loss: tensor(0.0412)\n",
      "10787 Training Loss: tensor(0.0412)\n",
      "10788 Training Loss: tensor(0.0413)\n",
      "10789 Training Loss: tensor(0.0411)\n",
      "10790 Training Loss: tensor(0.0411)\n",
      "10791 Training Loss: tensor(0.0412)\n",
      "10792 Training Loss: tensor(0.0412)\n",
      "10793 Training Loss: tensor(0.0411)\n",
      "10794 Training Loss: tensor(0.0411)\n",
      "10795 Training Loss: tensor(0.0411)\n",
      "10796 Training Loss: tensor(0.0412)\n",
      "10797 Training Loss: tensor(0.0412)\n",
      "10798 Training Loss: tensor(0.0411)\n",
      "10799 Training Loss: tensor(0.0412)\n",
      "10800 Training Loss: tensor(0.0412)\n",
      "10801 Training Loss: tensor(0.0412)\n",
      "10802 Training Loss: tensor(0.0413)\n",
      "10803 Training Loss: tensor(0.0413)\n",
      "10804 Training Loss: tensor(0.0412)\n",
      "10805 Training Loss: tensor(0.0412)\n",
      "10806 Training Loss: tensor(0.0412)\n",
      "10807 Training Loss: tensor(0.0412)\n",
      "10808 Training Loss: tensor(0.0413)\n",
      "10809 Training Loss: tensor(0.0412)\n",
      "10810 Training Loss: tensor(0.0411)\n",
      "10811 Training Loss: tensor(0.0414)\n",
      "10812 Training Loss: tensor(0.0411)\n",
      "10813 Training Loss: tensor(0.0413)\n",
      "10814 Training Loss: tensor(0.0413)\n",
      "10815 Training Loss: tensor(0.0411)\n",
      "10816 Training Loss: tensor(0.0411)\n",
      "10817 Training Loss: tensor(0.0411)\n",
      "10818 Training Loss: tensor(0.0411)\n",
      "10819 Training Loss: tensor(0.0412)\n",
      "10820 Training Loss: tensor(0.0412)\n",
      "10821 Training Loss: tensor(0.0413)\n",
      "10822 Training Loss: tensor(0.0412)\n",
      "10823 Training Loss: tensor(0.0412)\n",
      "10824 Training Loss: tensor(0.0412)\n",
      "10825 Training Loss: tensor(0.0413)\n",
      "10826 Training Loss: tensor(0.0410)\n",
      "10827 Training Loss: tensor(0.0410)\n",
      "10828 Training Loss: tensor(0.0412)\n",
      "10829 Training Loss: tensor(0.0413)\n",
      "10830 Training Loss: tensor(0.0412)\n",
      "10831 Training Loss: tensor(0.0410)\n",
      "10832 Training Loss: tensor(0.0412)\n",
      "10833 Training Loss: tensor(0.0411)\n",
      "10834 Training Loss: tensor(0.0411)\n",
      "10835 Training Loss: tensor(0.0411)\n",
      "10836 Training Loss: tensor(0.0411)\n",
      "10837 Training Loss: tensor(0.0411)\n",
      "10838 Training Loss: tensor(0.0411)\n",
      "10839 Training Loss: tensor(0.0410)\n",
      "10840 Training Loss: tensor(0.0412)\n",
      "10841 Training Loss: tensor(0.0413)\n",
      "10842 Training Loss: tensor(0.0412)\n",
      "10843 Training Loss: tensor(0.0412)\n",
      "10844 Training Loss: tensor(0.0410)\n",
      "10845 Training Loss: tensor(0.0411)\n",
      "10846 Training Loss: tensor(0.0410)\n",
      "10847 Training Loss: tensor(0.0410)\n",
      "10848 Training Loss: tensor(0.0411)\n",
      "10849 Training Loss: tensor(0.0412)\n",
      "10850 Training Loss: tensor(0.0411)\n",
      "10851 Training Loss: tensor(0.0411)\n",
      "10852 Training Loss: tensor(0.0411)\n",
      "10853 Training Loss: tensor(0.0412)\n",
      "10854 Training Loss: tensor(0.0410)\n",
      "10855 Training Loss: tensor(0.0410)\n",
      "10856 Training Loss: tensor(0.0411)\n",
      "10857 Training Loss: tensor(0.0411)\n",
      "10858 Training Loss: tensor(0.0411)\n",
      "10859 Training Loss: tensor(0.0411)\n",
      "10860 Training Loss: tensor(0.0411)\n",
      "10861 Training Loss: tensor(0.0412)\n",
      "10862 Training Loss: tensor(0.0410)\n",
      "10863 Training Loss: tensor(0.0412)\n",
      "10864 Training Loss: tensor(0.0411)\n",
      "10865 Training Loss: tensor(0.0411)\n",
      "10866 Training Loss: tensor(0.0410)\n",
      "10867 Training Loss: tensor(0.0410)\n",
      "10868 Training Loss: tensor(0.0410)\n",
      "10869 Training Loss: tensor(0.0410)\n",
      "10870 Training Loss: tensor(0.0411)\n",
      "10871 Training Loss: tensor(0.0411)\n",
      "10872 Training Loss: tensor(0.0411)\n",
      "10873 Training Loss: tensor(0.0411)\n",
      "10874 Training Loss: tensor(0.0412)\n",
      "10875 Training Loss: tensor(0.0412)\n",
      "10876 Training Loss: tensor(0.0411)\n",
      "10877 Training Loss: tensor(0.0411)\n",
      "10878 Training Loss: tensor(0.0412)\n",
      "10879 Training Loss: tensor(0.0411)\n",
      "10880 Training Loss: tensor(0.0410)\n",
      "10881 Training Loss: tensor(0.0410)\n",
      "10882 Training Loss: tensor(0.0412)\n",
      "10883 Training Loss: tensor(0.0411)\n",
      "10884 Training Loss: tensor(0.0409)\n",
      "10885 Training Loss: tensor(0.0411)\n",
      "10886 Training Loss: tensor(0.0411)\n",
      "10887 Training Loss: tensor(0.0410)\n",
      "10888 Training Loss: tensor(0.0411)\n",
      "10889 Training Loss: tensor(0.0411)\n",
      "10890 Training Loss: tensor(0.0410)\n",
      "10891 Training Loss: tensor(0.0410)\n",
      "10892 Training Loss: tensor(0.0410)\n",
      "10893 Training Loss: tensor(0.0411)\n",
      "10894 Training Loss: tensor(0.0410)\n",
      "10895 Training Loss: tensor(0.0410)\n",
      "10896 Training Loss: tensor(0.0409)\n",
      "10897 Training Loss: tensor(0.0411)\n",
      "10898 Training Loss: tensor(0.0410)\n",
      "10899 Training Loss: tensor(0.0413)\n",
      "10900 Training Loss: tensor(0.0409)\n",
      "10901 Training Loss: tensor(0.0412)\n",
      "10902 Training Loss: tensor(0.0410)\n",
      "10903 Training Loss: tensor(0.0411)\n",
      "10904 Training Loss: tensor(0.0411)\n",
      "10905 Training Loss: tensor(0.0410)\n",
      "10906 Training Loss: tensor(0.0411)\n",
      "10907 Training Loss: tensor(0.0410)\n",
      "10908 Training Loss: tensor(0.0410)\n",
      "10909 Training Loss: tensor(0.0410)\n",
      "10910 Training Loss: tensor(0.0409)\n",
      "10911 Training Loss: tensor(0.0410)\n",
      "10912 Training Loss: tensor(0.0410)\n",
      "10913 Training Loss: tensor(0.0410)\n",
      "10914 Training Loss: tensor(0.0411)\n",
      "10915 Training Loss: tensor(0.0411)\n",
      "10916 Training Loss: tensor(0.0410)\n",
      "10917 Training Loss: tensor(0.0411)\n",
      "10918 Training Loss: tensor(0.0412)\n",
      "10919 Training Loss: tensor(0.0410)\n",
      "10920 Training Loss: tensor(0.0410)\n",
      "10921 Training Loss: tensor(0.0410)\n",
      "10922 Training Loss: tensor(0.0409)\n",
      "10923 Training Loss: tensor(0.0410)\n",
      "10924 Training Loss: tensor(0.0412)\n",
      "10925 Training Loss: tensor(0.0408)\n",
      "10926 Training Loss: tensor(0.0410)\n",
      "10927 Training Loss: tensor(0.0410)\n",
      "10928 Training Loss: tensor(0.0410)\n",
      "10929 Training Loss: tensor(0.0409)\n",
      "10930 Training Loss: tensor(0.0409)\n",
      "10931 Training Loss: tensor(0.0410)\n",
      "10932 Training Loss: tensor(0.0409)\n",
      "10933 Training Loss: tensor(0.0411)\n",
      "10934 Training Loss: tensor(0.0409)\n",
      "10935 Training Loss: tensor(0.0410)\n",
      "10936 Training Loss: tensor(0.0410)\n",
      "10937 Training Loss: tensor(0.0409)\n",
      "10938 Training Loss: tensor(0.0409)\n",
      "10939 Training Loss: tensor(0.0410)\n",
      "10940 Training Loss: tensor(0.0412)\n",
      "10941 Training Loss: tensor(0.0411)\n",
      "10942 Training Loss: tensor(0.0411)\n",
      "10943 Training Loss: tensor(0.0410)\n",
      "10944 Training Loss: tensor(0.0409)\n",
      "10945 Training Loss: tensor(0.0410)\n",
      "10946 Training Loss: tensor(0.0410)\n",
      "10947 Training Loss: tensor(0.0410)\n",
      "10948 Training Loss: tensor(0.0410)\n",
      "10949 Training Loss: tensor(0.0409)\n",
      "10950 Training Loss: tensor(0.0410)\n",
      "10951 Training Loss: tensor(0.0410)\n",
      "10952 Training Loss: tensor(0.0410)\n",
      "10953 Training Loss: tensor(0.0410)\n",
      "10954 Training Loss: tensor(0.0410)\n",
      "10955 Training Loss: tensor(0.0410)\n",
      "10956 Training Loss: tensor(0.0409)\n",
      "10957 Training Loss: tensor(0.0410)\n",
      "10958 Training Loss: tensor(0.0410)\n",
      "10959 Training Loss: tensor(0.0411)\n",
      "10960 Training Loss: tensor(0.0409)\n",
      "10961 Training Loss: tensor(0.0410)\n",
      "10962 Training Loss: tensor(0.0410)\n",
      "10963 Training Loss: tensor(0.0409)\n",
      "10964 Training Loss: tensor(0.0410)\n",
      "10965 Training Loss: tensor(0.0408)\n",
      "10966 Training Loss: tensor(0.0408)\n",
      "10967 Training Loss: tensor(0.0407)\n",
      "10968 Training Loss: tensor(0.0409)\n",
      "10969 Training Loss: tensor(0.0409)\n",
      "10970 Training Loss: tensor(0.0410)\n",
      "10971 Training Loss: tensor(0.0410)\n",
      "10972 Training Loss: tensor(0.0408)\n",
      "10973 Training Loss: tensor(0.0411)\n",
      "10974 Training Loss: tensor(0.0410)\n",
      "10975 Training Loss: tensor(0.0409)\n",
      "10976 Training Loss: tensor(0.0409)\n",
      "10977 Training Loss: tensor(0.0410)\n",
      "10978 Training Loss: tensor(0.0410)\n",
      "10979 Training Loss: tensor(0.0409)\n",
      "10980 Training Loss: tensor(0.0409)\n",
      "10981 Training Loss: tensor(0.0409)\n",
      "10982 Training Loss: tensor(0.0409)\n",
      "10983 Training Loss: tensor(0.0410)\n",
      "10984 Training Loss: tensor(0.0409)\n",
      "10985 Training Loss: tensor(0.0408)\n",
      "10986 Training Loss: tensor(0.0410)\n",
      "10987 Training Loss: tensor(0.0410)\n",
      "10988 Training Loss: tensor(0.0409)\n",
      "10989 Training Loss: tensor(0.0410)\n",
      "10990 Training Loss: tensor(0.0409)\n",
      "10991 Training Loss: tensor(0.0409)\n",
      "10992 Training Loss: tensor(0.0410)\n",
      "10993 Training Loss: tensor(0.0410)\n",
      "10994 Training Loss: tensor(0.0408)\n",
      "10995 Training Loss: tensor(0.0408)\n",
      "10996 Training Loss: tensor(0.0409)\n",
      "10997 Training Loss: tensor(0.0409)\n",
      "10998 Training Loss: tensor(0.0409)\n",
      "10999 Training Loss: tensor(0.0408)\n",
      "11000 Training Loss: tensor(0.0409)\n",
      "11001 Training Loss: tensor(0.0409)\n",
      "11002 Training Loss: tensor(0.0409)\n",
      "11003 Training Loss: tensor(0.0410)\n",
      "11004 Training Loss: tensor(0.0407)\n",
      "11005 Training Loss: tensor(0.0411)\n",
      "11006 Training Loss: tensor(0.0410)\n",
      "11007 Training Loss: tensor(0.0409)\n",
      "11008 Training Loss: tensor(0.0409)\n",
      "11009 Training Loss: tensor(0.0408)\n",
      "11010 Training Loss: tensor(0.0408)\n",
      "11011 Training Loss: tensor(0.0409)\n",
      "11012 Training Loss: tensor(0.0408)\n",
      "11013 Training Loss: tensor(0.0409)\n",
      "11014 Training Loss: tensor(0.0409)\n",
      "11015 Training Loss: tensor(0.0409)\n",
      "11016 Training Loss: tensor(0.0408)\n",
      "11017 Training Loss: tensor(0.0409)\n",
      "11018 Training Loss: tensor(0.0408)\n",
      "11019 Training Loss: tensor(0.0407)\n",
      "11020 Training Loss: tensor(0.0408)\n",
      "11021 Training Loss: tensor(0.0408)\n",
      "11022 Training Loss: tensor(0.0409)\n",
      "11023 Training Loss: tensor(0.0409)\n",
      "11024 Training Loss: tensor(0.0410)\n",
      "11025 Training Loss: tensor(0.0408)\n",
      "11026 Training Loss: tensor(0.0409)\n",
      "11027 Training Loss: tensor(0.0409)\n",
      "11028 Training Loss: tensor(0.0407)\n",
      "11029 Training Loss: tensor(0.0411)\n",
      "11030 Training Loss: tensor(0.0409)\n",
      "11031 Training Loss: tensor(0.0408)\n",
      "11032 Training Loss: tensor(0.0408)\n",
      "11033 Training Loss: tensor(0.0408)\n",
      "11034 Training Loss: tensor(0.0409)\n",
      "11035 Training Loss: tensor(0.0407)\n",
      "11036 Training Loss: tensor(0.0409)\n",
      "11037 Training Loss: tensor(0.0409)\n",
      "11038 Training Loss: tensor(0.0409)\n",
      "11039 Training Loss: tensor(0.0408)\n",
      "11040 Training Loss: tensor(0.0411)\n",
      "11041 Training Loss: tensor(0.0409)\n",
      "11042 Training Loss: tensor(0.0409)\n",
      "11043 Training Loss: tensor(0.0408)\n",
      "11044 Training Loss: tensor(0.0407)\n",
      "11045 Training Loss: tensor(0.0409)\n",
      "11046 Training Loss: tensor(0.0407)\n",
      "11047 Training Loss: tensor(0.0408)\n",
      "11048 Training Loss: tensor(0.0409)\n",
      "11049 Training Loss: tensor(0.0407)\n",
      "11050 Training Loss: tensor(0.0409)\n",
      "11051 Training Loss: tensor(0.0407)\n",
      "11052 Training Loss: tensor(0.0408)\n",
      "11053 Training Loss: tensor(0.0408)\n",
      "11054 Training Loss: tensor(0.0409)\n",
      "11055 Training Loss: tensor(0.0408)\n",
      "11056 Training Loss: tensor(0.0410)\n",
      "11057 Training Loss: tensor(0.0409)\n",
      "11058 Training Loss: tensor(0.0409)\n",
      "11059 Training Loss: tensor(0.0406)\n",
      "11060 Training Loss: tensor(0.0407)\n",
      "11061 Training Loss: tensor(0.0408)\n",
      "11062 Training Loss: tensor(0.0409)\n",
      "11063 Training Loss: tensor(0.0408)\n",
      "11064 Training Loss: tensor(0.0407)\n",
      "11065 Training Loss: tensor(0.0408)\n",
      "11066 Training Loss: tensor(0.0407)\n",
      "11067 Training Loss: tensor(0.0408)\n",
      "11068 Training Loss: tensor(0.0407)\n",
      "11069 Training Loss: tensor(0.0409)\n",
      "11070 Training Loss: tensor(0.0409)\n",
      "11071 Training Loss: tensor(0.0408)\n",
      "11072 Training Loss: tensor(0.0408)\n",
      "11073 Training Loss: tensor(0.0409)\n",
      "11074 Training Loss: tensor(0.0407)\n",
      "11075 Training Loss: tensor(0.0407)\n",
      "11076 Training Loss: tensor(0.0410)\n",
      "11077 Training Loss: tensor(0.0407)\n",
      "11078 Training Loss: tensor(0.0408)\n",
      "11079 Training Loss: tensor(0.0409)\n",
      "11080 Training Loss: tensor(0.0407)\n",
      "11081 Training Loss: tensor(0.0407)\n",
      "11082 Training Loss: tensor(0.0409)\n",
      "11083 Training Loss: tensor(0.0409)\n",
      "11084 Training Loss: tensor(0.0407)\n",
      "11085 Training Loss: tensor(0.0408)\n",
      "11086 Training Loss: tensor(0.0408)\n",
      "11087 Training Loss: tensor(0.0406)\n",
      "11088 Training Loss: tensor(0.0406)\n",
      "11089 Training Loss: tensor(0.0407)\n",
      "11090 Training Loss: tensor(0.0407)\n",
      "11091 Training Loss: tensor(0.0407)\n",
      "11092 Training Loss: tensor(0.0408)\n",
      "11093 Training Loss: tensor(0.0408)\n",
      "11094 Training Loss: tensor(0.0408)\n",
      "11095 Training Loss: tensor(0.0408)\n",
      "11096 Training Loss: tensor(0.0408)\n",
      "11097 Training Loss: tensor(0.0408)\n",
      "11098 Training Loss: tensor(0.0406)\n",
      "11099 Training Loss: tensor(0.0407)\n",
      "11100 Training Loss: tensor(0.0408)\n",
      "11101 Training Loss: tensor(0.0408)\n",
      "11102 Training Loss: tensor(0.0407)\n",
      "11103 Training Loss: tensor(0.0408)\n",
      "11104 Training Loss: tensor(0.0408)\n",
      "11105 Training Loss: tensor(0.0406)\n",
      "11106 Training Loss: tensor(0.0408)\n",
      "11107 Training Loss: tensor(0.0407)\n",
      "11108 Training Loss: tensor(0.0407)\n",
      "11109 Training Loss: tensor(0.0408)\n",
      "11110 Training Loss: tensor(0.0408)\n",
      "11111 Training Loss: tensor(0.0407)\n",
      "11112 Training Loss: tensor(0.0407)\n",
      "11113 Training Loss: tensor(0.0406)\n",
      "11114 Training Loss: tensor(0.0406)\n",
      "11115 Training Loss: tensor(0.0409)\n",
      "11116 Training Loss: tensor(0.0407)\n",
      "11117 Training Loss: tensor(0.0407)\n",
      "11118 Training Loss: tensor(0.0407)\n",
      "11119 Training Loss: tensor(0.0407)\n",
      "11120 Training Loss: tensor(0.0406)\n",
      "11121 Training Loss: tensor(0.0407)\n",
      "11122 Training Loss: tensor(0.0407)\n",
      "11123 Training Loss: tensor(0.0407)\n",
      "11124 Training Loss: tensor(0.0407)\n",
      "11125 Training Loss: tensor(0.0408)\n",
      "11126 Training Loss: tensor(0.0407)\n",
      "11127 Training Loss: tensor(0.0408)\n",
      "11128 Training Loss: tensor(0.0407)\n",
      "11129 Training Loss: tensor(0.0407)\n",
      "11130 Training Loss: tensor(0.0406)\n",
      "11131 Training Loss: tensor(0.0407)\n",
      "11132 Training Loss: tensor(0.0408)\n",
      "11133 Training Loss: tensor(0.0407)\n",
      "11134 Training Loss: tensor(0.0407)\n",
      "11135 Training Loss: tensor(0.0406)\n",
      "11136 Training Loss: tensor(0.0406)\n",
      "11137 Training Loss: tensor(0.0406)\n",
      "11138 Training Loss: tensor(0.0407)\n",
      "11139 Training Loss: tensor(0.0406)\n",
      "11140 Training Loss: tensor(0.0406)\n",
      "11141 Training Loss: tensor(0.0407)\n",
      "11142 Training Loss: tensor(0.0407)\n",
      "11143 Training Loss: tensor(0.0406)\n",
      "11144 Training Loss: tensor(0.0405)\n",
      "11145 Training Loss: tensor(0.0406)\n",
      "11146 Training Loss: tensor(0.0407)\n",
      "11147 Training Loss: tensor(0.0405)\n",
      "11148 Training Loss: tensor(0.0406)\n",
      "11149 Training Loss: tensor(0.0407)\n",
      "11150 Training Loss: tensor(0.0406)\n",
      "11151 Training Loss: tensor(0.0409)\n",
      "11152 Training Loss: tensor(0.0407)\n",
      "11153 Training Loss: tensor(0.0405)\n",
      "11154 Training Loss: tensor(0.0406)\n",
      "11155 Training Loss: tensor(0.0406)\n",
      "11156 Training Loss: tensor(0.0406)\n",
      "11157 Training Loss: tensor(0.0406)\n",
      "11158 Training Loss: tensor(0.0406)\n",
      "11159 Training Loss: tensor(0.0407)\n",
      "11160 Training Loss: tensor(0.0406)\n",
      "11161 Training Loss: tensor(0.0406)\n",
      "11162 Training Loss: tensor(0.0406)\n",
      "11163 Training Loss: tensor(0.0406)\n",
      "11164 Training Loss: tensor(0.0405)\n",
      "11165 Training Loss: tensor(0.0407)\n",
      "11166 Training Loss: tensor(0.0406)\n",
      "11167 Training Loss: tensor(0.0406)\n",
      "11168 Training Loss: tensor(0.0406)\n",
      "11169 Training Loss: tensor(0.0407)\n",
      "11170 Training Loss: tensor(0.0407)\n",
      "11171 Training Loss: tensor(0.0406)\n",
      "11172 Training Loss: tensor(0.0407)\n",
      "11173 Training Loss: tensor(0.0408)\n",
      "11174 Training Loss: tensor(0.0405)\n",
      "11175 Training Loss: tensor(0.0408)\n",
      "11176 Training Loss: tensor(0.0406)\n",
      "11177 Training Loss: tensor(0.0408)\n",
      "11178 Training Loss: tensor(0.0408)\n",
      "11179 Training Loss: tensor(0.0407)\n",
      "11180 Training Loss: tensor(0.0406)\n",
      "11181 Training Loss: tensor(0.0406)\n",
      "11182 Training Loss: tensor(0.0406)\n",
      "11183 Training Loss: tensor(0.0407)\n",
      "11184 Training Loss: tensor(0.0407)\n",
      "11185 Training Loss: tensor(0.0406)\n",
      "11186 Training Loss: tensor(0.0407)\n",
      "11187 Training Loss: tensor(0.0408)\n",
      "11188 Training Loss: tensor(0.0406)\n",
      "11189 Training Loss: tensor(0.0407)\n",
      "11190 Training Loss: tensor(0.0407)\n",
      "11191 Training Loss: tensor(0.0404)\n",
      "11192 Training Loss: tensor(0.0406)\n",
      "11193 Training Loss: tensor(0.0406)\n",
      "11194 Training Loss: tensor(0.0406)\n",
      "11195 Training Loss: tensor(0.0408)\n",
      "11196 Training Loss: tensor(0.0406)\n",
      "11197 Training Loss: tensor(0.0407)\n",
      "11198 Training Loss: tensor(0.0406)\n",
      "11199 Training Loss: tensor(0.0406)\n",
      "11200 Training Loss: tensor(0.0406)\n",
      "11201 Training Loss: tensor(0.0405)\n",
      "11202 Training Loss: tensor(0.0407)\n",
      "11203 Training Loss: tensor(0.0404)\n",
      "11204 Training Loss: tensor(0.0407)\n",
      "11205 Training Loss: tensor(0.0406)\n",
      "11206 Training Loss: tensor(0.0405)\n",
      "11207 Training Loss: tensor(0.0407)\n",
      "11208 Training Loss: tensor(0.0407)\n",
      "11209 Training Loss: tensor(0.0406)\n",
      "11210 Training Loss: tensor(0.0407)\n",
      "11211 Training Loss: tensor(0.0406)\n",
      "11212 Training Loss: tensor(0.0404)\n",
      "11213 Training Loss: tensor(0.0407)\n",
      "11214 Training Loss: tensor(0.0406)\n",
      "11215 Training Loss: tensor(0.0406)\n",
      "11216 Training Loss: tensor(0.0406)\n",
      "11217 Training Loss: tensor(0.0406)\n",
      "11218 Training Loss: tensor(0.0405)\n",
      "11219 Training Loss: tensor(0.0407)\n",
      "11220 Training Loss: tensor(0.0406)\n",
      "11221 Training Loss: tensor(0.0407)\n",
      "11222 Training Loss: tensor(0.0406)\n",
      "11223 Training Loss: tensor(0.0404)\n",
      "11224 Training Loss: tensor(0.0406)\n",
      "11225 Training Loss: tensor(0.0406)\n",
      "11226 Training Loss: tensor(0.0406)\n",
      "11227 Training Loss: tensor(0.0405)\n",
      "11228 Training Loss: tensor(0.0406)\n",
      "11229 Training Loss: tensor(0.0407)\n",
      "11230 Training Loss: tensor(0.0407)\n",
      "11231 Training Loss: tensor(0.0407)\n",
      "11232 Training Loss: tensor(0.0405)\n",
      "11233 Training Loss: tensor(0.0405)\n",
      "11234 Training Loss: tensor(0.0406)\n",
      "11235 Training Loss: tensor(0.0407)\n",
      "11236 Training Loss: tensor(0.0407)\n",
      "11237 Training Loss: tensor(0.0406)\n",
      "11238 Training Loss: tensor(0.0406)\n",
      "11239 Training Loss: tensor(0.0405)\n",
      "11240 Training Loss: tensor(0.0406)\n",
      "11241 Training Loss: tensor(0.0404)\n",
      "11242 Training Loss: tensor(0.0406)\n",
      "11243 Training Loss: tensor(0.0406)\n",
      "11244 Training Loss: tensor(0.0405)\n",
      "11245 Training Loss: tensor(0.0405)\n",
      "11246 Training Loss: tensor(0.0405)\n",
      "11247 Training Loss: tensor(0.0407)\n",
      "11248 Training Loss: tensor(0.0407)\n",
      "11249 Training Loss: tensor(0.0406)\n",
      "11250 Training Loss: tensor(0.0405)\n",
      "11251 Training Loss: tensor(0.0406)\n",
      "11252 Training Loss: tensor(0.0406)\n",
      "11253 Training Loss: tensor(0.0405)\n",
      "11254 Training Loss: tensor(0.0405)\n",
      "11255 Training Loss: tensor(0.0406)\n",
      "11256 Training Loss: tensor(0.0405)\n",
      "11257 Training Loss: tensor(0.0406)\n",
      "11258 Training Loss: tensor(0.0405)\n",
      "11259 Training Loss: tensor(0.0404)\n",
      "11260 Training Loss: tensor(0.0406)\n",
      "11261 Training Loss: tensor(0.0406)\n",
      "11262 Training Loss: tensor(0.0405)\n",
      "11263 Training Loss: tensor(0.0405)\n",
      "11264 Training Loss: tensor(0.0406)\n",
      "11265 Training Loss: tensor(0.0407)\n",
      "11266 Training Loss: tensor(0.0405)\n",
      "11267 Training Loss: tensor(0.0406)\n",
      "11268 Training Loss: tensor(0.0405)\n",
      "11269 Training Loss: tensor(0.0405)\n",
      "11270 Training Loss: tensor(0.0406)\n",
      "11271 Training Loss: tensor(0.0406)\n",
      "11272 Training Loss: tensor(0.0405)\n",
      "11273 Training Loss: tensor(0.0405)\n",
      "11274 Training Loss: tensor(0.0405)\n",
      "11275 Training Loss: tensor(0.0405)\n",
      "11276 Training Loss: tensor(0.0404)\n",
      "11277 Training Loss: tensor(0.0404)\n",
      "11278 Training Loss: tensor(0.0404)\n",
      "11279 Training Loss: tensor(0.0403)\n",
      "11280 Training Loss: tensor(0.0406)\n",
      "11281 Training Loss: tensor(0.0405)\n",
      "11282 Training Loss: tensor(0.0407)\n",
      "11283 Training Loss: tensor(0.0405)\n",
      "11284 Training Loss: tensor(0.0407)\n",
      "11285 Training Loss: tensor(0.0406)\n",
      "11286 Training Loss: tensor(0.0406)\n",
      "11287 Training Loss: tensor(0.0405)\n",
      "11288 Training Loss: tensor(0.0404)\n",
      "11289 Training Loss: tensor(0.0405)\n",
      "11290 Training Loss: tensor(0.0405)\n",
      "11291 Training Loss: tensor(0.0406)\n",
      "11292 Training Loss: tensor(0.0404)\n",
      "11293 Training Loss: tensor(0.0405)\n",
      "11294 Training Loss: tensor(0.0405)\n",
      "11295 Training Loss: tensor(0.0405)\n",
      "11296 Training Loss: tensor(0.0405)\n",
      "11297 Training Loss: tensor(0.0404)\n",
      "11298 Training Loss: tensor(0.0406)\n",
      "11299 Training Loss: tensor(0.0404)\n",
      "11300 Training Loss: tensor(0.0405)\n",
      "11301 Training Loss: tensor(0.0407)\n",
      "11302 Training Loss: tensor(0.0405)\n",
      "11303 Training Loss: tensor(0.0404)\n",
      "11304 Training Loss: tensor(0.0405)\n",
      "11305 Training Loss: tensor(0.0406)\n",
      "11306 Training Loss: tensor(0.0406)\n",
      "11307 Training Loss: tensor(0.0404)\n",
      "11308 Training Loss: tensor(0.0405)\n",
      "11309 Training Loss: tensor(0.0404)\n",
      "11310 Training Loss: tensor(0.0405)\n",
      "11311 Training Loss: tensor(0.0406)\n",
      "11312 Training Loss: tensor(0.0405)\n",
      "11313 Training Loss: tensor(0.0406)\n",
      "11314 Training Loss: tensor(0.0403)\n",
      "11315 Training Loss: tensor(0.0404)\n",
      "11316 Training Loss: tensor(0.0404)\n",
      "11317 Training Loss: tensor(0.0403)\n",
      "11318 Training Loss: tensor(0.0404)\n",
      "11319 Training Loss: tensor(0.0405)\n",
      "11320 Training Loss: tensor(0.0405)\n",
      "11321 Training Loss: tensor(0.0405)\n",
      "11322 Training Loss: tensor(0.0406)\n",
      "11323 Training Loss: tensor(0.0405)\n",
      "11324 Training Loss: tensor(0.0404)\n",
      "11325 Training Loss: tensor(0.0405)\n",
      "11326 Training Loss: tensor(0.0405)\n",
      "11327 Training Loss: tensor(0.0405)\n",
      "11328 Training Loss: tensor(0.0403)\n",
      "11329 Training Loss: tensor(0.0404)\n",
      "11330 Training Loss: tensor(0.0406)\n",
      "11331 Training Loss: tensor(0.0404)\n",
      "11332 Training Loss: tensor(0.0405)\n",
      "11333 Training Loss: tensor(0.0404)\n",
      "11334 Training Loss: tensor(0.0403)\n",
      "11335 Training Loss: tensor(0.0404)\n",
      "11336 Training Loss: tensor(0.0404)\n",
      "11337 Training Loss: tensor(0.0405)\n",
      "11338 Training Loss: tensor(0.0404)\n",
      "11339 Training Loss: tensor(0.0404)\n",
      "11340 Training Loss: tensor(0.0403)\n",
      "11341 Training Loss: tensor(0.0405)\n",
      "11342 Training Loss: tensor(0.0404)\n",
      "11343 Training Loss: tensor(0.0403)\n",
      "11344 Training Loss: tensor(0.0404)\n",
      "11345 Training Loss: tensor(0.0405)\n",
      "11346 Training Loss: tensor(0.0404)\n",
      "11347 Training Loss: tensor(0.0404)\n",
      "11348 Training Loss: tensor(0.0404)\n",
      "11349 Training Loss: tensor(0.0404)\n",
      "11350 Training Loss: tensor(0.0404)\n",
      "11351 Training Loss: tensor(0.0404)\n",
      "11352 Training Loss: tensor(0.0403)\n",
      "11353 Training Loss: tensor(0.0404)\n",
      "11354 Training Loss: tensor(0.0403)\n",
      "11355 Training Loss: tensor(0.0404)\n",
      "11356 Training Loss: tensor(0.0403)\n",
      "11357 Training Loss: tensor(0.0404)\n",
      "11358 Training Loss: tensor(0.0405)\n",
      "11359 Training Loss: tensor(0.0403)\n",
      "11360 Training Loss: tensor(0.0404)\n",
      "11361 Training Loss: tensor(0.0404)\n",
      "11362 Training Loss: tensor(0.0403)\n",
      "11363 Training Loss: tensor(0.0403)\n",
      "11364 Training Loss: tensor(0.0402)\n",
      "11365 Training Loss: tensor(0.0403)\n",
      "11366 Training Loss: tensor(0.0402)\n",
      "11367 Training Loss: tensor(0.0404)\n",
      "11368 Training Loss: tensor(0.0404)\n",
      "11369 Training Loss: tensor(0.0404)\n",
      "11370 Training Loss: tensor(0.0403)\n",
      "11371 Training Loss: tensor(0.0406)\n",
      "11372 Training Loss: tensor(0.0403)\n",
      "11373 Training Loss: tensor(0.0404)\n",
      "11374 Training Loss: tensor(0.0403)\n",
      "11375 Training Loss: tensor(0.0405)\n",
      "11376 Training Loss: tensor(0.0405)\n",
      "11377 Training Loss: tensor(0.0404)\n",
      "11378 Training Loss: tensor(0.0403)\n",
      "11379 Training Loss: tensor(0.0403)\n",
      "11380 Training Loss: tensor(0.0405)\n",
      "11381 Training Loss: tensor(0.0403)\n",
      "11382 Training Loss: tensor(0.0404)\n",
      "11383 Training Loss: tensor(0.0403)\n",
      "11384 Training Loss: tensor(0.0404)\n",
      "11385 Training Loss: tensor(0.0405)\n",
      "11386 Training Loss: tensor(0.0406)\n",
      "11387 Training Loss: tensor(0.0403)\n",
      "11388 Training Loss: tensor(0.0403)\n",
      "11389 Training Loss: tensor(0.0405)\n",
      "11390 Training Loss: tensor(0.0402)\n",
      "11391 Training Loss: tensor(0.0404)\n",
      "11392 Training Loss: tensor(0.0405)\n",
      "11393 Training Loss: tensor(0.0404)\n",
      "11394 Training Loss: tensor(0.0404)\n",
      "11395 Training Loss: tensor(0.0403)\n",
      "11396 Training Loss: tensor(0.0402)\n",
      "11397 Training Loss: tensor(0.0404)\n",
      "11398 Training Loss: tensor(0.0403)\n",
      "11399 Training Loss: tensor(0.0403)\n",
      "11400 Training Loss: tensor(0.0404)\n",
      "11401 Training Loss: tensor(0.0403)\n",
      "11402 Training Loss: tensor(0.0406)\n",
      "11403 Training Loss: tensor(0.0404)\n",
      "11404 Training Loss: tensor(0.0402)\n",
      "11405 Training Loss: tensor(0.0403)\n",
      "11406 Training Loss: tensor(0.0403)\n",
      "11407 Training Loss: tensor(0.0403)\n",
      "11408 Training Loss: tensor(0.0403)\n",
      "11409 Training Loss: tensor(0.0405)\n",
      "11410 Training Loss: tensor(0.0405)\n",
      "11411 Training Loss: tensor(0.0402)\n",
      "11412 Training Loss: tensor(0.0404)\n",
      "11413 Training Loss: tensor(0.0403)\n",
      "11414 Training Loss: tensor(0.0404)\n",
      "11415 Training Loss: tensor(0.0402)\n",
      "11416 Training Loss: tensor(0.0403)\n",
      "11417 Training Loss: tensor(0.0403)\n",
      "11418 Training Loss: tensor(0.0402)\n",
      "11419 Training Loss: tensor(0.0402)\n",
      "11420 Training Loss: tensor(0.0404)\n",
      "11421 Training Loss: tensor(0.0403)\n",
      "11422 Training Loss: tensor(0.0404)\n",
      "11423 Training Loss: tensor(0.0402)\n",
      "11424 Training Loss: tensor(0.0403)\n",
      "11425 Training Loss: tensor(0.0404)\n",
      "11426 Training Loss: tensor(0.0405)\n",
      "11427 Training Loss: tensor(0.0402)\n",
      "11428 Training Loss: tensor(0.0404)\n",
      "11429 Training Loss: tensor(0.0403)\n",
      "11430 Training Loss: tensor(0.0403)\n",
      "11431 Training Loss: tensor(0.0403)\n",
      "11432 Training Loss: tensor(0.0403)\n",
      "11433 Training Loss: tensor(0.0403)\n",
      "11434 Training Loss: tensor(0.0404)\n",
      "11435 Training Loss: tensor(0.0402)\n",
      "11436 Training Loss: tensor(0.0403)\n",
      "11437 Training Loss: tensor(0.0404)\n",
      "11438 Training Loss: tensor(0.0403)\n",
      "11439 Training Loss: tensor(0.0401)\n",
      "11440 Training Loss: tensor(0.0403)\n",
      "11441 Training Loss: tensor(0.0403)\n",
      "11442 Training Loss: tensor(0.0402)\n",
      "11443 Training Loss: tensor(0.0403)\n",
      "11444 Training Loss: tensor(0.0403)\n",
      "11445 Training Loss: tensor(0.0404)\n",
      "11446 Training Loss: tensor(0.0402)\n",
      "11447 Training Loss: tensor(0.0403)\n",
      "11448 Training Loss: tensor(0.0403)\n",
      "11449 Training Loss: tensor(0.0402)\n",
      "11450 Training Loss: tensor(0.0404)\n",
      "11451 Training Loss: tensor(0.0402)\n",
      "11452 Training Loss: tensor(0.0403)\n",
      "11453 Training Loss: tensor(0.0402)\n",
      "11454 Training Loss: tensor(0.0402)\n",
      "11455 Training Loss: tensor(0.0403)\n",
      "11456 Training Loss: tensor(0.0403)\n",
      "11457 Training Loss: tensor(0.0403)\n",
      "11458 Training Loss: tensor(0.0402)\n",
      "11459 Training Loss: tensor(0.0402)\n",
      "11460 Training Loss: tensor(0.0402)\n",
      "11461 Training Loss: tensor(0.0400)\n",
      "11462 Training Loss: tensor(0.0402)\n",
      "11463 Training Loss: tensor(0.0403)\n",
      "11464 Training Loss: tensor(0.0403)\n",
      "11465 Training Loss: tensor(0.0404)\n",
      "11466 Training Loss: tensor(0.0402)\n",
      "11467 Training Loss: tensor(0.0402)\n",
      "11468 Training Loss: tensor(0.0402)\n",
      "11469 Training Loss: tensor(0.0405)\n",
      "11470 Training Loss: tensor(0.0403)\n",
      "11471 Training Loss: tensor(0.0403)\n",
      "11472 Training Loss: tensor(0.0403)\n",
      "11473 Training Loss: tensor(0.0402)\n",
      "11474 Training Loss: tensor(0.0404)\n",
      "11475 Training Loss: tensor(0.0403)\n",
      "11476 Training Loss: tensor(0.0402)\n",
      "11477 Training Loss: tensor(0.0403)\n",
      "11478 Training Loss: tensor(0.0403)\n",
      "11479 Training Loss: tensor(0.0402)\n",
      "11480 Training Loss: tensor(0.0403)\n",
      "11481 Training Loss: tensor(0.0402)\n",
      "11482 Training Loss: tensor(0.0402)\n",
      "11483 Training Loss: tensor(0.0403)\n",
      "11484 Training Loss: tensor(0.0403)\n",
      "11485 Training Loss: tensor(0.0403)\n",
      "11486 Training Loss: tensor(0.0403)\n",
      "11487 Training Loss: tensor(0.0402)\n",
      "11488 Training Loss: tensor(0.0402)\n",
      "11489 Training Loss: tensor(0.0403)\n",
      "11490 Training Loss: tensor(0.0403)\n",
      "11491 Training Loss: tensor(0.0401)\n",
      "11492 Training Loss: tensor(0.0403)\n",
      "11493 Training Loss: tensor(0.0403)\n",
      "11494 Training Loss: tensor(0.0403)\n",
      "11495 Training Loss: tensor(0.0402)\n",
      "11496 Training Loss: tensor(0.0401)\n",
      "11497 Training Loss: tensor(0.0402)\n",
      "11498 Training Loss: tensor(0.0401)\n",
      "11499 Training Loss: tensor(0.0402)\n",
      "11500 Training Loss: tensor(0.0402)\n",
      "11501 Training Loss: tensor(0.0404)\n",
      "11502 Training Loss: tensor(0.0401)\n",
      "11503 Training Loss: tensor(0.0401)\n",
      "11504 Training Loss: tensor(0.0404)\n",
      "11505 Training Loss: tensor(0.0402)\n",
      "11506 Training Loss: tensor(0.0400)\n",
      "11507 Training Loss: tensor(0.0403)\n",
      "11508 Training Loss: tensor(0.0401)\n",
      "11509 Training Loss: tensor(0.0404)\n",
      "11510 Training Loss: tensor(0.0402)\n",
      "11511 Training Loss: tensor(0.0402)\n",
      "11512 Training Loss: tensor(0.0401)\n",
      "11513 Training Loss: tensor(0.0402)\n",
      "11514 Training Loss: tensor(0.0402)\n",
      "11515 Training Loss: tensor(0.0402)\n",
      "11516 Training Loss: tensor(0.0401)\n",
      "11517 Training Loss: tensor(0.0404)\n",
      "11518 Training Loss: tensor(0.0401)\n",
      "11519 Training Loss: tensor(0.0401)\n",
      "11520 Training Loss: tensor(0.0404)\n",
      "11521 Training Loss: tensor(0.0401)\n",
      "11522 Training Loss: tensor(0.0401)\n",
      "11523 Training Loss: tensor(0.0402)\n",
      "11524 Training Loss: tensor(0.0402)\n",
      "11525 Training Loss: tensor(0.0402)\n",
      "11526 Training Loss: tensor(0.0402)\n",
      "11527 Training Loss: tensor(0.0401)\n",
      "11528 Training Loss: tensor(0.0401)\n",
      "11529 Training Loss: tensor(0.0401)\n",
      "11530 Training Loss: tensor(0.0403)\n",
      "11531 Training Loss: tensor(0.0402)\n",
      "11532 Training Loss: tensor(0.0401)\n",
      "11533 Training Loss: tensor(0.0402)\n",
      "11534 Training Loss: tensor(0.0401)\n",
      "11535 Training Loss: tensor(0.0402)\n",
      "11536 Training Loss: tensor(0.0403)\n",
      "11537 Training Loss: tensor(0.0402)\n",
      "11538 Training Loss: tensor(0.0400)\n",
      "11539 Training Loss: tensor(0.0402)\n",
      "11540 Training Loss: tensor(0.0404)\n",
      "11541 Training Loss: tensor(0.0403)\n",
      "11542 Training Loss: tensor(0.0401)\n",
      "11543 Training Loss: tensor(0.0401)\n",
      "11544 Training Loss: tensor(0.0400)\n",
      "11545 Training Loss: tensor(0.0401)\n",
      "11546 Training Loss: tensor(0.0402)\n",
      "11547 Training Loss: tensor(0.0401)\n",
      "11548 Training Loss: tensor(0.0402)\n",
      "11549 Training Loss: tensor(0.0402)\n",
      "11550 Training Loss: tensor(0.0401)\n",
      "11551 Training Loss: tensor(0.0400)\n",
      "11552 Training Loss: tensor(0.0402)\n",
      "11553 Training Loss: tensor(0.0401)\n",
      "11554 Training Loss: tensor(0.0401)\n",
      "11555 Training Loss: tensor(0.0400)\n",
      "11556 Training Loss: tensor(0.0402)\n",
      "11557 Training Loss: tensor(0.0400)\n",
      "11558 Training Loss: tensor(0.0400)\n",
      "11559 Training Loss: tensor(0.0402)\n",
      "11560 Training Loss: tensor(0.0402)\n",
      "11561 Training Loss: tensor(0.0401)\n",
      "11562 Training Loss: tensor(0.0402)\n",
      "11563 Training Loss: tensor(0.0401)\n",
      "11564 Training Loss: tensor(0.0401)\n",
      "11565 Training Loss: tensor(0.0401)\n",
      "11566 Training Loss: tensor(0.0402)\n",
      "11567 Training Loss: tensor(0.0403)\n",
      "11568 Training Loss: tensor(0.0401)\n",
      "11569 Training Loss: tensor(0.0403)\n",
      "11570 Training Loss: tensor(0.0401)\n",
      "11571 Training Loss: tensor(0.0402)\n",
      "11572 Training Loss: tensor(0.0401)\n",
      "11573 Training Loss: tensor(0.0401)\n",
      "11574 Training Loss: tensor(0.0401)\n",
      "11575 Training Loss: tensor(0.0401)\n",
      "11576 Training Loss: tensor(0.0402)\n",
      "11577 Training Loss: tensor(0.0400)\n",
      "11578 Training Loss: tensor(0.0400)\n",
      "11579 Training Loss: tensor(0.0401)\n",
      "11580 Training Loss: tensor(0.0402)\n",
      "11581 Training Loss: tensor(0.0399)\n",
      "11582 Training Loss: tensor(0.0400)\n",
      "11583 Training Loss: tensor(0.0400)\n",
      "11584 Training Loss: tensor(0.0400)\n",
      "11585 Training Loss: tensor(0.0401)\n",
      "11586 Training Loss: tensor(0.0404)\n",
      "11587 Training Loss: tensor(0.0400)\n",
      "11588 Training Loss: tensor(0.0401)\n",
      "11589 Training Loss: tensor(0.0402)\n",
      "11590 Training Loss: tensor(0.0401)\n",
      "11591 Training Loss: tensor(0.0400)\n",
      "11592 Training Loss: tensor(0.0402)\n",
      "11593 Training Loss: tensor(0.0402)\n",
      "11594 Training Loss: tensor(0.0403)\n",
      "11595 Training Loss: tensor(0.0400)\n",
      "11596 Training Loss: tensor(0.0401)\n",
      "11597 Training Loss: tensor(0.0399)\n",
      "11598 Training Loss: tensor(0.0401)\n",
      "11599 Training Loss: tensor(0.0402)\n",
      "11600 Training Loss: tensor(0.0401)\n",
      "11601 Training Loss: tensor(0.0400)\n",
      "11602 Training Loss: tensor(0.0400)\n",
      "11603 Training Loss: tensor(0.0403)\n",
      "11604 Training Loss: tensor(0.0400)\n",
      "11605 Training Loss: tensor(0.0400)\n",
      "11606 Training Loss: tensor(0.0400)\n",
      "11607 Training Loss: tensor(0.0400)\n",
      "11608 Training Loss: tensor(0.0402)\n",
      "11609 Training Loss: tensor(0.0401)\n",
      "11610 Training Loss: tensor(0.0401)\n",
      "11611 Training Loss: tensor(0.0402)\n",
      "11612 Training Loss: tensor(0.0400)\n",
      "11613 Training Loss: tensor(0.0400)\n",
      "11614 Training Loss: tensor(0.0400)\n",
      "11615 Training Loss: tensor(0.0400)\n",
      "11616 Training Loss: tensor(0.0402)\n",
      "11617 Training Loss: tensor(0.0402)\n",
      "11618 Training Loss: tensor(0.0400)\n",
      "11619 Training Loss: tensor(0.0402)\n",
      "11620 Training Loss: tensor(0.0401)\n",
      "11621 Training Loss: tensor(0.0400)\n",
      "11622 Training Loss: tensor(0.0400)\n",
      "11623 Training Loss: tensor(0.0400)\n",
      "11624 Training Loss: tensor(0.0400)\n",
      "11625 Training Loss: tensor(0.0401)\n",
      "11626 Training Loss: tensor(0.0400)\n",
      "11627 Training Loss: tensor(0.0400)\n",
      "11628 Training Loss: tensor(0.0401)\n",
      "11629 Training Loss: tensor(0.0402)\n",
      "11630 Training Loss: tensor(0.0401)\n",
      "11631 Training Loss: tensor(0.0403)\n",
      "11632 Training Loss: tensor(0.0400)\n",
      "11633 Training Loss: tensor(0.0399)\n",
      "11634 Training Loss: tensor(0.0401)\n",
      "11635 Training Loss: tensor(0.0401)\n",
      "11636 Training Loss: tensor(0.0399)\n",
      "11637 Training Loss: tensor(0.0400)\n",
      "11638 Training Loss: tensor(0.0400)\n",
      "11639 Training Loss: tensor(0.0400)\n",
      "11640 Training Loss: tensor(0.0399)\n",
      "11641 Training Loss: tensor(0.0400)\n",
      "11642 Training Loss: tensor(0.0400)\n",
      "11643 Training Loss: tensor(0.0402)\n",
      "11644 Training Loss: tensor(0.0401)\n",
      "11645 Training Loss: tensor(0.0401)\n",
      "11646 Training Loss: tensor(0.0399)\n",
      "11647 Training Loss: tensor(0.0399)\n",
      "11648 Training Loss: tensor(0.0400)\n",
      "11649 Training Loss: tensor(0.0399)\n",
      "11650 Training Loss: tensor(0.0400)\n",
      "11651 Training Loss: tensor(0.0399)\n",
      "11652 Training Loss: tensor(0.0398)\n",
      "11653 Training Loss: tensor(0.0400)\n",
      "11654 Training Loss: tensor(0.0399)\n",
      "11655 Training Loss: tensor(0.0400)\n",
      "11656 Training Loss: tensor(0.0401)\n",
      "11657 Training Loss: tensor(0.0401)\n",
      "11658 Training Loss: tensor(0.0401)\n",
      "11659 Training Loss: tensor(0.0400)\n",
      "11660 Training Loss: tensor(0.0401)\n",
      "11661 Training Loss: tensor(0.0399)\n",
      "11662 Training Loss: tensor(0.0401)\n",
      "11663 Training Loss: tensor(0.0399)\n",
      "11664 Training Loss: tensor(0.0398)\n",
      "11665 Training Loss: tensor(0.0401)\n",
      "11666 Training Loss: tensor(0.0400)\n",
      "11667 Training Loss: tensor(0.0399)\n",
      "11668 Training Loss: tensor(0.0401)\n",
      "11669 Training Loss: tensor(0.0399)\n",
      "11670 Training Loss: tensor(0.0401)\n",
      "11671 Training Loss: tensor(0.0399)\n",
      "11672 Training Loss: tensor(0.0399)\n",
      "11673 Training Loss: tensor(0.0401)\n",
      "11674 Training Loss: tensor(0.0399)\n",
      "11675 Training Loss: tensor(0.0399)\n",
      "11676 Training Loss: tensor(0.0401)\n",
      "11677 Training Loss: tensor(0.0399)\n",
      "11678 Training Loss: tensor(0.0400)\n",
      "11679 Training Loss: tensor(0.0400)\n",
      "11680 Training Loss: tensor(0.0399)\n",
      "11681 Training Loss: tensor(0.0399)\n",
      "11682 Training Loss: tensor(0.0399)\n",
      "11683 Training Loss: tensor(0.0399)\n",
      "11684 Training Loss: tensor(0.0399)\n",
      "11685 Training Loss: tensor(0.0400)\n",
      "11686 Training Loss: tensor(0.0399)\n",
      "11687 Training Loss: tensor(0.0400)\n",
      "11688 Training Loss: tensor(0.0400)\n",
      "11689 Training Loss: tensor(0.0399)\n",
      "11690 Training Loss: tensor(0.0400)\n",
      "11691 Training Loss: tensor(0.0399)\n",
      "11692 Training Loss: tensor(0.0400)\n",
      "11693 Training Loss: tensor(0.0400)\n",
      "11694 Training Loss: tensor(0.0400)\n",
      "11695 Training Loss: tensor(0.0400)\n",
      "11696 Training Loss: tensor(0.0400)\n",
      "11697 Training Loss: tensor(0.0399)\n",
      "11698 Training Loss: tensor(0.0399)\n",
      "11699 Training Loss: tensor(0.0399)\n",
      "11700 Training Loss: tensor(0.0399)\n",
      "11701 Training Loss: tensor(0.0400)\n",
      "11702 Training Loss: tensor(0.0401)\n",
      "11703 Training Loss: tensor(0.0400)\n",
      "11704 Training Loss: tensor(0.0398)\n",
      "11705 Training Loss: tensor(0.0399)\n",
      "11706 Training Loss: tensor(0.0400)\n",
      "11707 Training Loss: tensor(0.0399)\n",
      "11708 Training Loss: tensor(0.0398)\n",
      "11709 Training Loss: tensor(0.0398)\n",
      "11710 Training Loss: tensor(0.0401)\n",
      "11711 Training Loss: tensor(0.0399)\n",
      "11712 Training Loss: tensor(0.0398)\n",
      "11713 Training Loss: tensor(0.0399)\n",
      "11714 Training Loss: tensor(0.0399)\n",
      "11715 Training Loss: tensor(0.0399)\n",
      "11716 Training Loss: tensor(0.0400)\n",
      "11717 Training Loss: tensor(0.0400)\n",
      "11718 Training Loss: tensor(0.0399)\n",
      "11719 Training Loss: tensor(0.0399)\n",
      "11720 Training Loss: tensor(0.0399)\n",
      "11721 Training Loss: tensor(0.0399)\n",
      "11722 Training Loss: tensor(0.0401)\n",
      "11723 Training Loss: tensor(0.0399)\n",
      "11724 Training Loss: tensor(0.0399)\n",
      "11725 Training Loss: tensor(0.0399)\n",
      "11726 Training Loss: tensor(0.0399)\n",
      "11727 Training Loss: tensor(0.0400)\n",
      "11728 Training Loss: tensor(0.0400)\n",
      "11729 Training Loss: tensor(0.0399)\n",
      "11730 Training Loss: tensor(0.0400)\n",
      "11731 Training Loss: tensor(0.0398)\n",
      "11732 Training Loss: tensor(0.0398)\n",
      "11733 Training Loss: tensor(0.0398)\n",
      "11734 Training Loss: tensor(0.0399)\n",
      "11735 Training Loss: tensor(0.0399)\n",
      "11736 Training Loss: tensor(0.0400)\n",
      "11737 Training Loss: tensor(0.0399)\n",
      "11738 Training Loss: tensor(0.0400)\n",
      "11739 Training Loss: tensor(0.0399)\n",
      "11740 Training Loss: tensor(0.0398)\n",
      "11741 Training Loss: tensor(0.0400)\n",
      "11742 Training Loss: tensor(0.0399)\n",
      "11743 Training Loss: tensor(0.0401)\n",
      "11744 Training Loss: tensor(0.0399)\n",
      "11745 Training Loss: tensor(0.0400)\n",
      "11746 Training Loss: tensor(0.0399)\n",
      "11747 Training Loss: tensor(0.0399)\n",
      "11748 Training Loss: tensor(0.0400)\n",
      "11749 Training Loss: tensor(0.0398)\n",
      "11750 Training Loss: tensor(0.0399)\n",
      "11751 Training Loss: tensor(0.0398)\n",
      "11752 Training Loss: tensor(0.0398)\n",
      "11753 Training Loss: tensor(0.0399)\n",
      "11754 Training Loss: tensor(0.0400)\n",
      "11755 Training Loss: tensor(0.0400)\n",
      "11756 Training Loss: tensor(0.0399)\n",
      "11757 Training Loss: tensor(0.0400)\n",
      "11758 Training Loss: tensor(0.0397)\n",
      "11759 Training Loss: tensor(0.0398)\n",
      "11760 Training Loss: tensor(0.0400)\n",
      "11761 Training Loss: tensor(0.0399)\n",
      "11762 Training Loss: tensor(0.0401)\n",
      "11763 Training Loss: tensor(0.0401)\n",
      "11764 Training Loss: tensor(0.0400)\n",
      "11765 Training Loss: tensor(0.0399)\n",
      "11766 Training Loss: tensor(0.0398)\n",
      "11767 Training Loss: tensor(0.0398)\n",
      "11768 Training Loss: tensor(0.0398)\n",
      "11769 Training Loss: tensor(0.0398)\n",
      "11770 Training Loss: tensor(0.0399)\n",
      "11771 Training Loss: tensor(0.0399)\n",
      "11772 Training Loss: tensor(0.0398)\n",
      "11773 Training Loss: tensor(0.0399)\n",
      "11774 Training Loss: tensor(0.0400)\n",
      "11775 Training Loss: tensor(0.0399)\n",
      "11776 Training Loss: tensor(0.0400)\n",
      "11777 Training Loss: tensor(0.0398)\n",
      "11778 Training Loss: tensor(0.0397)\n",
      "11779 Training Loss: tensor(0.0398)\n",
      "11780 Training Loss: tensor(0.0399)\n",
      "11781 Training Loss: tensor(0.0400)\n",
      "11782 Training Loss: tensor(0.0399)\n",
      "11783 Training Loss: tensor(0.0398)\n",
      "11784 Training Loss: tensor(0.0397)\n",
      "11785 Training Loss: tensor(0.0399)\n",
      "11786 Training Loss: tensor(0.0398)\n",
      "11787 Training Loss: tensor(0.0398)\n",
      "11788 Training Loss: tensor(0.0398)\n",
      "11789 Training Loss: tensor(0.0401)\n",
      "11790 Training Loss: tensor(0.0399)\n",
      "11791 Training Loss: tensor(0.0399)\n",
      "11792 Training Loss: tensor(0.0398)\n",
      "11793 Training Loss: tensor(0.0399)\n",
      "11794 Training Loss: tensor(0.0398)\n",
      "11795 Training Loss: tensor(0.0400)\n",
      "11796 Training Loss: tensor(0.0398)\n",
      "11797 Training Loss: tensor(0.0399)\n",
      "11798 Training Loss: tensor(0.0398)\n",
      "11799 Training Loss: tensor(0.0397)\n",
      "11800 Training Loss: tensor(0.0400)\n",
      "11801 Training Loss: tensor(0.0398)\n",
      "11802 Training Loss: tensor(0.0398)\n",
      "11803 Training Loss: tensor(0.0400)\n",
      "11804 Training Loss: tensor(0.0397)\n",
      "11805 Training Loss: tensor(0.0398)\n",
      "11806 Training Loss: tensor(0.0397)\n",
      "11807 Training Loss: tensor(0.0398)\n",
      "11808 Training Loss: tensor(0.0398)\n",
      "11809 Training Loss: tensor(0.0397)\n",
      "11810 Training Loss: tensor(0.0400)\n",
      "11811 Training Loss: tensor(0.0399)\n",
      "11812 Training Loss: tensor(0.0397)\n",
      "11813 Training Loss: tensor(0.0398)\n",
      "11814 Training Loss: tensor(0.0398)\n",
      "11815 Training Loss: tensor(0.0398)\n",
      "11816 Training Loss: tensor(0.0398)\n",
      "11817 Training Loss: tensor(0.0399)\n",
      "11818 Training Loss: tensor(0.0398)\n",
      "11819 Training Loss: tensor(0.0400)\n",
      "11820 Training Loss: tensor(0.0397)\n",
      "11821 Training Loss: tensor(0.0397)\n",
      "11822 Training Loss: tensor(0.0398)\n",
      "11823 Training Loss: tensor(0.0398)\n",
      "11824 Training Loss: tensor(0.0398)\n",
      "11825 Training Loss: tensor(0.0398)\n",
      "11826 Training Loss: tensor(0.0398)\n",
      "11827 Training Loss: tensor(0.0398)\n",
      "11828 Training Loss: tensor(0.0397)\n",
      "11829 Training Loss: tensor(0.0397)\n",
      "11830 Training Loss: tensor(0.0397)\n",
      "11831 Training Loss: tensor(0.0397)\n",
      "11832 Training Loss: tensor(0.0398)\n",
      "11833 Training Loss: tensor(0.0397)\n",
      "11834 Training Loss: tensor(0.0398)\n",
      "11835 Training Loss: tensor(0.0397)\n",
      "11836 Training Loss: tensor(0.0397)\n",
      "11837 Training Loss: tensor(0.0398)\n",
      "11838 Training Loss: tensor(0.0396)\n",
      "11839 Training Loss: tensor(0.0398)\n",
      "11840 Training Loss: tensor(0.0398)\n",
      "11841 Training Loss: tensor(0.0397)\n",
      "11842 Training Loss: tensor(0.0397)\n",
      "11843 Training Loss: tensor(0.0397)\n",
      "11844 Training Loss: tensor(0.0398)\n",
      "11845 Training Loss: tensor(0.0397)\n",
      "11846 Training Loss: tensor(0.0396)\n",
      "11847 Training Loss: tensor(0.0399)\n",
      "11848 Training Loss: tensor(0.0399)\n",
      "11849 Training Loss: tensor(0.0398)\n",
      "11850 Training Loss: tensor(0.0398)\n",
      "11851 Training Loss: tensor(0.0396)\n",
      "11852 Training Loss: tensor(0.0397)\n",
      "11853 Training Loss: tensor(0.0398)\n",
      "11854 Training Loss: tensor(0.0396)\n",
      "11855 Training Loss: tensor(0.0398)\n",
      "11856 Training Loss: tensor(0.0398)\n",
      "11857 Training Loss: tensor(0.0398)\n",
      "11858 Training Loss: tensor(0.0397)\n",
      "11859 Training Loss: tensor(0.0398)\n",
      "11860 Training Loss: tensor(0.0398)\n",
      "11861 Training Loss: tensor(0.0398)\n",
      "11862 Training Loss: tensor(0.0397)\n",
      "11863 Training Loss: tensor(0.0397)\n",
      "11864 Training Loss: tensor(0.0397)\n",
      "11865 Training Loss: tensor(0.0397)\n",
      "11866 Training Loss: tensor(0.0395)\n",
      "11867 Training Loss: tensor(0.0397)\n",
      "11868 Training Loss: tensor(0.0397)\n",
      "11869 Training Loss: tensor(0.0398)\n",
      "11870 Training Loss: tensor(0.0395)\n",
      "11871 Training Loss: tensor(0.0396)\n",
      "11872 Training Loss: tensor(0.0397)\n",
      "11873 Training Loss: tensor(0.0397)\n",
      "11874 Training Loss: tensor(0.0398)\n",
      "11875 Training Loss: tensor(0.0397)\n",
      "11876 Training Loss: tensor(0.0396)\n",
      "11877 Training Loss: tensor(0.0398)\n",
      "11878 Training Loss: tensor(0.0397)\n",
      "11879 Training Loss: tensor(0.0398)\n",
      "11880 Training Loss: tensor(0.0397)\n",
      "11881 Training Loss: tensor(0.0396)\n",
      "11882 Training Loss: tensor(0.0398)\n",
      "11883 Training Loss: tensor(0.0397)\n",
      "11884 Training Loss: tensor(0.0398)\n",
      "11885 Training Loss: tensor(0.0397)\n",
      "11886 Training Loss: tensor(0.0396)\n",
      "11887 Training Loss: tensor(0.0397)\n",
      "11888 Training Loss: tensor(0.0396)\n",
      "11889 Training Loss: tensor(0.0395)\n",
      "11890 Training Loss: tensor(0.0397)\n",
      "11891 Training Loss: tensor(0.0397)\n",
      "11892 Training Loss: tensor(0.0396)\n",
      "11893 Training Loss: tensor(0.0396)\n",
      "11894 Training Loss: tensor(0.0396)\n",
      "11895 Training Loss: tensor(0.0396)\n",
      "11896 Training Loss: tensor(0.0398)\n",
      "11897 Training Loss: tensor(0.0395)\n",
      "11898 Training Loss: tensor(0.0396)\n",
      "11899 Training Loss: tensor(0.0396)\n",
      "11900 Training Loss: tensor(0.0395)\n",
      "11901 Training Loss: tensor(0.0399)\n",
      "11902 Training Loss: tensor(0.0397)\n",
      "11903 Training Loss: tensor(0.0396)\n",
      "11904 Training Loss: tensor(0.0395)\n",
      "11905 Training Loss: tensor(0.0398)\n",
      "11906 Training Loss: tensor(0.0397)\n",
      "11907 Training Loss: tensor(0.0396)\n",
      "11908 Training Loss: tensor(0.0397)\n",
      "11909 Training Loss: tensor(0.0397)\n",
      "11910 Training Loss: tensor(0.0396)\n",
      "11911 Training Loss: tensor(0.0396)\n",
      "11912 Training Loss: tensor(0.0397)\n",
      "11913 Training Loss: tensor(0.0396)\n",
      "11914 Training Loss: tensor(0.0395)\n",
      "11915 Training Loss: tensor(0.0396)\n",
      "11916 Training Loss: tensor(0.0396)\n",
      "11917 Training Loss: tensor(0.0396)\n",
      "11918 Training Loss: tensor(0.0397)\n",
      "11919 Training Loss: tensor(0.0396)\n",
      "11920 Training Loss: tensor(0.0397)\n",
      "11921 Training Loss: tensor(0.0395)\n",
      "11922 Training Loss: tensor(0.0400)\n",
      "11923 Training Loss: tensor(0.0396)\n",
      "11924 Training Loss: tensor(0.0397)\n",
      "11925 Training Loss: tensor(0.0396)\n",
      "11926 Training Loss: tensor(0.0395)\n",
      "11927 Training Loss: tensor(0.0397)\n",
      "11928 Training Loss: tensor(0.0397)\n",
      "11929 Training Loss: tensor(0.0396)\n",
      "11930 Training Loss: tensor(0.0396)\n",
      "11931 Training Loss: tensor(0.0396)\n",
      "11932 Training Loss: tensor(0.0396)\n",
      "11933 Training Loss: tensor(0.0398)\n",
      "11934 Training Loss: tensor(0.0395)\n",
      "11935 Training Loss: tensor(0.0395)\n",
      "11936 Training Loss: tensor(0.0396)\n",
      "11937 Training Loss: tensor(0.0396)\n",
      "11938 Training Loss: tensor(0.0397)\n",
      "11939 Training Loss: tensor(0.0397)\n",
      "11940 Training Loss: tensor(0.0395)\n",
      "11941 Training Loss: tensor(0.0397)\n",
      "11942 Training Loss: tensor(0.0396)\n",
      "11943 Training Loss: tensor(0.0396)\n",
      "11944 Training Loss: tensor(0.0396)\n",
      "11945 Training Loss: tensor(0.0395)\n",
      "11946 Training Loss: tensor(0.0396)\n",
      "11947 Training Loss: tensor(0.0396)\n",
      "11948 Training Loss: tensor(0.0398)\n",
      "11949 Training Loss: tensor(0.0395)\n",
      "11950 Training Loss: tensor(0.0396)\n",
      "11951 Training Loss: tensor(0.0396)\n",
      "11952 Training Loss: tensor(0.0397)\n",
      "11953 Training Loss: tensor(0.0396)\n",
      "11954 Training Loss: tensor(0.0398)\n",
      "11955 Training Loss: tensor(0.0395)\n",
      "11956 Training Loss: tensor(0.0396)\n",
      "11957 Training Loss: tensor(0.0396)\n",
      "11958 Training Loss: tensor(0.0394)\n",
      "11959 Training Loss: tensor(0.0394)\n",
      "11960 Training Loss: tensor(0.0396)\n",
      "11961 Training Loss: tensor(0.0395)\n",
      "11962 Training Loss: tensor(0.0397)\n",
      "11963 Training Loss: tensor(0.0396)\n",
      "11964 Training Loss: tensor(0.0395)\n",
      "11965 Training Loss: tensor(0.0395)\n",
      "11966 Training Loss: tensor(0.0395)\n",
      "11967 Training Loss: tensor(0.0396)\n",
      "11968 Training Loss: tensor(0.0395)\n",
      "11969 Training Loss: tensor(0.0396)\n",
      "11970 Training Loss: tensor(0.0396)\n",
      "11971 Training Loss: tensor(0.0395)\n",
      "11972 Training Loss: tensor(0.0394)\n",
      "11973 Training Loss: tensor(0.0395)\n",
      "11974 Training Loss: tensor(0.0396)\n",
      "11975 Training Loss: tensor(0.0396)\n",
      "11976 Training Loss: tensor(0.0397)\n",
      "11977 Training Loss: tensor(0.0395)\n",
      "11978 Training Loss: tensor(0.0397)\n",
      "11979 Training Loss: tensor(0.0397)\n",
      "11980 Training Loss: tensor(0.0397)\n",
      "11981 Training Loss: tensor(0.0394)\n",
      "11982 Training Loss: tensor(0.0395)\n",
      "11983 Training Loss: tensor(0.0395)\n",
      "11984 Training Loss: tensor(0.0396)\n",
      "11985 Training Loss: tensor(0.0396)\n",
      "11986 Training Loss: tensor(0.0397)\n",
      "11987 Training Loss: tensor(0.0395)\n",
      "11988 Training Loss: tensor(0.0395)\n",
      "11989 Training Loss: tensor(0.0394)\n",
      "11990 Training Loss: tensor(0.0395)\n",
      "11991 Training Loss: tensor(0.0396)\n",
      "11992 Training Loss: tensor(0.0395)\n",
      "11993 Training Loss: tensor(0.0395)\n",
      "11994 Training Loss: tensor(0.0396)\n",
      "11995 Training Loss: tensor(0.0394)\n",
      "11996 Training Loss: tensor(0.0396)\n",
      "11997 Training Loss: tensor(0.0394)\n",
      "11998 Training Loss: tensor(0.0396)\n",
      "11999 Training Loss: tensor(0.0396)\n",
      "12000 Training Loss: tensor(0.0396)\n",
      "12001 Training Loss: tensor(0.0395)\n",
      "12002 Training Loss: tensor(0.0394)\n",
      "12003 Training Loss: tensor(0.0397)\n",
      "12004 Training Loss: tensor(0.0397)\n",
      "12005 Training Loss: tensor(0.0394)\n",
      "12006 Training Loss: tensor(0.0395)\n",
      "12007 Training Loss: tensor(0.0395)\n",
      "12008 Training Loss: tensor(0.0396)\n",
      "12009 Training Loss: tensor(0.0394)\n",
      "12010 Training Loss: tensor(0.0394)\n",
      "12011 Training Loss: tensor(0.0394)\n",
      "12012 Training Loss: tensor(0.0397)\n",
      "12013 Training Loss: tensor(0.0395)\n",
      "12014 Training Loss: tensor(0.0396)\n",
      "12015 Training Loss: tensor(0.0395)\n",
      "12016 Training Loss: tensor(0.0396)\n",
      "12017 Training Loss: tensor(0.0395)\n",
      "12018 Training Loss: tensor(0.0395)\n",
      "12019 Training Loss: tensor(0.0395)\n",
      "12020 Training Loss: tensor(0.0395)\n",
      "12021 Training Loss: tensor(0.0395)\n",
      "12022 Training Loss: tensor(0.0394)\n",
      "12023 Training Loss: tensor(0.0394)\n",
      "12024 Training Loss: tensor(0.0395)\n",
      "12025 Training Loss: tensor(0.0394)\n",
      "12026 Training Loss: tensor(0.0395)\n",
      "12027 Training Loss: tensor(0.0395)\n",
      "12028 Training Loss: tensor(0.0395)\n",
      "12029 Training Loss: tensor(0.0396)\n",
      "12030 Training Loss: tensor(0.0394)\n",
      "12031 Training Loss: tensor(0.0394)\n",
      "12032 Training Loss: tensor(0.0394)\n",
      "12033 Training Loss: tensor(0.0396)\n",
      "12034 Training Loss: tensor(0.0395)\n",
      "12035 Training Loss: tensor(0.0394)\n",
      "12036 Training Loss: tensor(0.0395)\n",
      "12037 Training Loss: tensor(0.0396)\n",
      "12038 Training Loss: tensor(0.0396)\n",
      "12039 Training Loss: tensor(0.0397)\n",
      "12040 Training Loss: tensor(0.0394)\n",
      "12041 Training Loss: tensor(0.0394)\n",
      "12042 Training Loss: tensor(0.0396)\n",
      "12043 Training Loss: tensor(0.0394)\n",
      "12044 Training Loss: tensor(0.0394)\n",
      "12045 Training Loss: tensor(0.0396)\n",
      "12046 Training Loss: tensor(0.0396)\n",
      "12047 Training Loss: tensor(0.0394)\n",
      "12048 Training Loss: tensor(0.0395)\n",
      "12049 Training Loss: tensor(0.0394)\n",
      "12050 Training Loss: tensor(0.0395)\n",
      "12051 Training Loss: tensor(0.0394)\n",
      "12052 Training Loss: tensor(0.0393)\n",
      "12053 Training Loss: tensor(0.0395)\n",
      "12054 Training Loss: tensor(0.0395)\n",
      "12055 Training Loss: tensor(0.0394)\n",
      "12056 Training Loss: tensor(0.0396)\n",
      "12057 Training Loss: tensor(0.0395)\n",
      "12058 Training Loss: tensor(0.0395)\n",
      "12059 Training Loss: tensor(0.0392)\n",
      "12060 Training Loss: tensor(0.0396)\n",
      "12061 Training Loss: tensor(0.0395)\n",
      "12062 Training Loss: tensor(0.0394)\n",
      "12063 Training Loss: tensor(0.0394)\n",
      "12064 Training Loss: tensor(0.0394)\n",
      "12065 Training Loss: tensor(0.0395)\n",
      "12066 Training Loss: tensor(0.0393)\n",
      "12067 Training Loss: tensor(0.0395)\n",
      "12068 Training Loss: tensor(0.0394)\n",
      "12069 Training Loss: tensor(0.0394)\n",
      "12070 Training Loss: tensor(0.0394)\n",
      "12071 Training Loss: tensor(0.0392)\n",
      "12072 Training Loss: tensor(0.0395)\n",
      "12073 Training Loss: tensor(0.0395)\n",
      "12074 Training Loss: tensor(0.0394)\n",
      "12075 Training Loss: tensor(0.0394)\n",
      "12076 Training Loss: tensor(0.0394)\n",
      "12077 Training Loss: tensor(0.0395)\n",
      "12078 Training Loss: tensor(0.0394)\n",
      "12079 Training Loss: tensor(0.0394)\n",
      "12080 Training Loss: tensor(0.0393)\n",
      "12081 Training Loss: tensor(0.0394)\n",
      "12082 Training Loss: tensor(0.0394)\n",
      "12083 Training Loss: tensor(0.0394)\n",
      "12084 Training Loss: tensor(0.0394)\n",
      "12085 Training Loss: tensor(0.0396)\n",
      "12086 Training Loss: tensor(0.0393)\n",
      "12087 Training Loss: tensor(0.0394)\n",
      "12088 Training Loss: tensor(0.0394)\n",
      "12089 Training Loss: tensor(0.0394)\n",
      "12090 Training Loss: tensor(0.0394)\n",
      "12091 Training Loss: tensor(0.0394)\n",
      "12092 Training Loss: tensor(0.0392)\n",
      "12093 Training Loss: tensor(0.0395)\n",
      "12094 Training Loss: tensor(0.0393)\n",
      "12095 Training Loss: tensor(0.0395)\n",
      "12096 Training Loss: tensor(0.0397)\n",
      "12097 Training Loss: tensor(0.0393)\n",
      "12098 Training Loss: tensor(0.0393)\n",
      "12099 Training Loss: tensor(0.0393)\n",
      "12100 Training Loss: tensor(0.0394)\n",
      "12101 Training Loss: tensor(0.0394)\n",
      "12102 Training Loss: tensor(0.0394)\n",
      "12103 Training Loss: tensor(0.0393)\n",
      "12104 Training Loss: tensor(0.0395)\n",
      "12105 Training Loss: tensor(0.0394)\n",
      "12106 Training Loss: tensor(0.0393)\n",
      "12107 Training Loss: tensor(0.0394)\n",
      "12108 Training Loss: tensor(0.0394)\n",
      "12109 Training Loss: tensor(0.0395)\n",
      "12110 Training Loss: tensor(0.0394)\n",
      "12111 Training Loss: tensor(0.0392)\n",
      "12112 Training Loss: tensor(0.0393)\n",
      "12113 Training Loss: tensor(0.0393)\n",
      "12114 Training Loss: tensor(0.0394)\n",
      "12115 Training Loss: tensor(0.0393)\n",
      "12116 Training Loss: tensor(0.0394)\n",
      "12117 Training Loss: tensor(0.0394)\n",
      "12118 Training Loss: tensor(0.0393)\n",
      "12119 Training Loss: tensor(0.0394)\n",
      "12120 Training Loss: tensor(0.0394)\n",
      "12121 Training Loss: tensor(0.0393)\n",
      "12122 Training Loss: tensor(0.0393)\n",
      "12123 Training Loss: tensor(0.0393)\n",
      "12124 Training Loss: tensor(0.0393)\n",
      "12125 Training Loss: tensor(0.0392)\n",
      "12126 Training Loss: tensor(0.0393)\n",
      "12127 Training Loss: tensor(0.0393)\n",
      "12128 Training Loss: tensor(0.0394)\n",
      "12129 Training Loss: tensor(0.0392)\n",
      "12130 Training Loss: tensor(0.0393)\n",
      "12131 Training Loss: tensor(0.0393)\n",
      "12132 Training Loss: tensor(0.0394)\n",
      "12133 Training Loss: tensor(0.0394)\n",
      "12134 Training Loss: tensor(0.0394)\n",
      "12135 Training Loss: tensor(0.0393)\n",
      "12136 Training Loss: tensor(0.0393)\n",
      "12137 Training Loss: tensor(0.0394)\n",
      "12138 Training Loss: tensor(0.0392)\n",
      "12139 Training Loss: tensor(0.0394)\n",
      "12140 Training Loss: tensor(0.0395)\n",
      "12141 Training Loss: tensor(0.0394)\n",
      "12142 Training Loss: tensor(0.0393)\n",
      "12143 Training Loss: tensor(0.0392)\n",
      "12144 Training Loss: tensor(0.0393)\n",
      "12145 Training Loss: tensor(0.0394)\n",
      "12146 Training Loss: tensor(0.0392)\n",
      "12147 Training Loss: tensor(0.0393)\n",
      "12148 Training Loss: tensor(0.0393)\n",
      "12149 Training Loss: tensor(0.0394)\n",
      "12150 Training Loss: tensor(0.0394)\n",
      "12151 Training Loss: tensor(0.0391)\n",
      "12152 Training Loss: tensor(0.0394)\n",
      "12153 Training Loss: tensor(0.0392)\n",
      "12154 Training Loss: tensor(0.0394)\n",
      "12155 Training Loss: tensor(0.0392)\n",
      "12156 Training Loss: tensor(0.0392)\n",
      "12157 Training Loss: tensor(0.0393)\n",
      "12158 Training Loss: tensor(0.0394)\n",
      "12159 Training Loss: tensor(0.0394)\n",
      "12160 Training Loss: tensor(0.0392)\n",
      "12161 Training Loss: tensor(0.0392)\n",
      "12162 Training Loss: tensor(0.0393)\n",
      "12163 Training Loss: tensor(0.0391)\n",
      "12164 Training Loss: tensor(0.0393)\n",
      "12165 Training Loss: tensor(0.0392)\n",
      "12166 Training Loss: tensor(0.0392)\n",
      "12167 Training Loss: tensor(0.0394)\n",
      "12168 Training Loss: tensor(0.0392)\n",
      "12169 Training Loss: tensor(0.0392)\n",
      "12170 Training Loss: tensor(0.0392)\n",
      "12171 Training Loss: tensor(0.0394)\n",
      "12172 Training Loss: tensor(0.0391)\n",
      "12173 Training Loss: tensor(0.0392)\n",
      "12174 Training Loss: tensor(0.0393)\n",
      "12175 Training Loss: tensor(0.0392)\n",
      "12176 Training Loss: tensor(0.0394)\n",
      "12177 Training Loss: tensor(0.0393)\n",
      "12178 Training Loss: tensor(0.0392)\n",
      "12179 Training Loss: tensor(0.0393)\n",
      "12180 Training Loss: tensor(0.0392)\n",
      "12181 Training Loss: tensor(0.0392)\n",
      "12182 Training Loss: tensor(0.0391)\n",
      "12183 Training Loss: tensor(0.0393)\n",
      "12184 Training Loss: tensor(0.0392)\n",
      "12185 Training Loss: tensor(0.0391)\n",
      "12186 Training Loss: tensor(0.0391)\n",
      "12187 Training Loss: tensor(0.0393)\n",
      "12188 Training Loss: tensor(0.0392)\n",
      "12189 Training Loss: tensor(0.0393)\n",
      "12190 Training Loss: tensor(0.0393)\n",
      "12191 Training Loss: tensor(0.0391)\n",
      "12192 Training Loss: tensor(0.0393)\n",
      "12193 Training Loss: tensor(0.0392)\n",
      "12194 Training Loss: tensor(0.0391)\n",
      "12195 Training Loss: tensor(0.0393)\n",
      "12196 Training Loss: tensor(0.0391)\n",
      "12197 Training Loss: tensor(0.0393)\n",
      "12198 Training Loss: tensor(0.0394)\n",
      "12199 Training Loss: tensor(0.0393)\n",
      "12200 Training Loss: tensor(0.0393)\n",
      "12201 Training Loss: tensor(0.0392)\n",
      "12202 Training Loss: tensor(0.0393)\n",
      "12203 Training Loss: tensor(0.0393)\n",
      "12204 Training Loss: tensor(0.0393)\n",
      "12205 Training Loss: tensor(0.0393)\n",
      "12206 Training Loss: tensor(0.0391)\n",
      "12207 Training Loss: tensor(0.0393)\n",
      "12208 Training Loss: tensor(0.0391)\n",
      "12209 Training Loss: tensor(0.0392)\n",
      "12210 Training Loss: tensor(0.0393)\n",
      "12211 Training Loss: tensor(0.0392)\n",
      "12212 Training Loss: tensor(0.0391)\n",
      "12213 Training Loss: tensor(0.0391)\n",
      "12214 Training Loss: tensor(0.0391)\n",
      "12215 Training Loss: tensor(0.0393)\n",
      "12216 Training Loss: tensor(0.0393)\n",
      "12217 Training Loss: tensor(0.0393)\n",
      "12218 Training Loss: tensor(0.0393)\n",
      "12219 Training Loss: tensor(0.0392)\n",
      "12220 Training Loss: tensor(0.0392)\n",
      "12221 Training Loss: tensor(0.0391)\n",
      "12222 Training Loss: tensor(0.0391)\n",
      "12223 Training Loss: tensor(0.0392)\n",
      "12224 Training Loss: tensor(0.0392)\n",
      "12225 Training Loss: tensor(0.0392)\n",
      "12226 Training Loss: tensor(0.0391)\n",
      "12227 Training Loss: tensor(0.0391)\n",
      "12228 Training Loss: tensor(0.0392)\n",
      "12229 Training Loss: tensor(0.0392)\n",
      "12230 Training Loss: tensor(0.0393)\n",
      "12231 Training Loss: tensor(0.0391)\n",
      "12232 Training Loss: tensor(0.0390)\n",
      "12233 Training Loss: tensor(0.0392)\n",
      "12234 Training Loss: tensor(0.0391)\n",
      "12235 Training Loss: tensor(0.0393)\n",
      "12236 Training Loss: tensor(0.0393)\n",
      "12237 Training Loss: tensor(0.0392)\n",
      "12238 Training Loss: tensor(0.0391)\n",
      "12239 Training Loss: tensor(0.0392)\n",
      "12240 Training Loss: tensor(0.0390)\n",
      "12241 Training Loss: tensor(0.0391)\n",
      "12242 Training Loss: tensor(0.0391)\n",
      "12243 Training Loss: tensor(0.0393)\n",
      "12244 Training Loss: tensor(0.0393)\n",
      "12245 Training Loss: tensor(0.0391)\n",
      "12246 Training Loss: tensor(0.0390)\n",
      "12247 Training Loss: tensor(0.0390)\n",
      "12248 Training Loss: tensor(0.0392)\n",
      "12249 Training Loss: tensor(0.0392)\n",
      "12250 Training Loss: tensor(0.0391)\n",
      "12251 Training Loss: tensor(0.0392)\n",
      "12252 Training Loss: tensor(0.0391)\n",
      "12253 Training Loss: tensor(0.0392)\n",
      "12254 Training Loss: tensor(0.0391)\n",
      "12255 Training Loss: tensor(0.0392)\n",
      "12256 Training Loss: tensor(0.0391)\n",
      "12257 Training Loss: tensor(0.0393)\n",
      "12258 Training Loss: tensor(0.0391)\n",
      "12259 Training Loss: tensor(0.0391)\n",
      "12260 Training Loss: tensor(0.0392)\n",
      "12261 Training Loss: tensor(0.0391)\n",
      "12262 Training Loss: tensor(0.0392)\n",
      "12263 Training Loss: tensor(0.0391)\n",
      "12264 Training Loss: tensor(0.0390)\n",
      "12265 Training Loss: tensor(0.0391)\n",
      "12266 Training Loss: tensor(0.0391)\n",
      "12267 Training Loss: tensor(0.0391)\n",
      "12268 Training Loss: tensor(0.0390)\n",
      "12269 Training Loss: tensor(0.0391)\n",
      "12270 Training Loss: tensor(0.0393)\n",
      "12271 Training Loss: tensor(0.0391)\n",
      "12272 Training Loss: tensor(0.0390)\n",
      "12273 Training Loss: tensor(0.0391)\n",
      "12274 Training Loss: tensor(0.0392)\n",
      "12275 Training Loss: tensor(0.0391)\n",
      "12276 Training Loss: tensor(0.0390)\n",
      "12277 Training Loss: tensor(0.0391)\n",
      "12278 Training Loss: tensor(0.0390)\n",
      "12279 Training Loss: tensor(0.0391)\n",
      "12280 Training Loss: tensor(0.0391)\n",
      "12281 Training Loss: tensor(0.0390)\n",
      "12282 Training Loss: tensor(0.0392)\n",
      "12283 Training Loss: tensor(0.0390)\n",
      "12284 Training Loss: tensor(0.0390)\n",
      "12285 Training Loss: tensor(0.0392)\n",
      "12286 Training Loss: tensor(0.0390)\n",
      "12287 Training Loss: tensor(0.0391)\n",
      "12288 Training Loss: tensor(0.0391)\n",
      "12289 Training Loss: tensor(0.0392)\n",
      "12290 Training Loss: tensor(0.0391)\n",
      "12291 Training Loss: tensor(0.0390)\n",
      "12292 Training Loss: tensor(0.0391)\n",
      "12293 Training Loss: tensor(0.0392)\n",
      "12294 Training Loss: tensor(0.0390)\n",
      "12295 Training Loss: tensor(0.0391)\n",
      "12296 Training Loss: tensor(0.0391)\n",
      "12297 Training Loss: tensor(0.0392)\n",
      "12298 Training Loss: tensor(0.0390)\n",
      "12299 Training Loss: tensor(0.0390)\n",
      "12300 Training Loss: tensor(0.0389)\n",
      "12301 Training Loss: tensor(0.0391)\n",
      "12302 Training Loss: tensor(0.0390)\n",
      "12303 Training Loss: tensor(0.0392)\n",
      "12304 Training Loss: tensor(0.0391)\n",
      "12305 Training Loss: tensor(0.0390)\n",
      "12306 Training Loss: tensor(0.0389)\n",
      "12307 Training Loss: tensor(0.0390)\n",
      "12308 Training Loss: tensor(0.0389)\n",
      "12309 Training Loss: tensor(0.0391)\n",
      "12310 Training Loss: tensor(0.0391)\n",
      "12311 Training Loss: tensor(0.0390)\n",
      "12312 Training Loss: tensor(0.0389)\n",
      "12313 Training Loss: tensor(0.0391)\n",
      "12314 Training Loss: tensor(0.0391)\n",
      "12315 Training Loss: tensor(0.0390)\n",
      "12316 Training Loss: tensor(0.0388)\n",
      "12317 Training Loss: tensor(0.0390)\n",
      "12318 Training Loss: tensor(0.0391)\n",
      "12319 Training Loss: tensor(0.0391)\n",
      "12320 Training Loss: tensor(0.0390)\n",
      "12321 Training Loss: tensor(0.0391)\n",
      "12322 Training Loss: tensor(0.0392)\n",
      "12323 Training Loss: tensor(0.0390)\n",
      "12324 Training Loss: tensor(0.0390)\n",
      "12325 Training Loss: tensor(0.0390)\n",
      "12326 Training Loss: tensor(0.0390)\n",
      "12327 Training Loss: tensor(0.0389)\n",
      "12328 Training Loss: tensor(0.0389)\n",
      "12329 Training Loss: tensor(0.0392)\n",
      "12330 Training Loss: tensor(0.0391)\n",
      "12331 Training Loss: tensor(0.0390)\n",
      "12332 Training Loss: tensor(0.0391)\n",
      "12333 Training Loss: tensor(0.0390)\n",
      "12334 Training Loss: tensor(0.0390)\n",
      "12335 Training Loss: tensor(0.0390)\n",
      "12336 Training Loss: tensor(0.0389)\n",
      "12337 Training Loss: tensor(0.0389)\n",
      "12338 Training Loss: tensor(0.0391)\n",
      "12339 Training Loss: tensor(0.0389)\n",
      "12340 Training Loss: tensor(0.0390)\n",
      "12341 Training Loss: tensor(0.0389)\n",
      "12342 Training Loss: tensor(0.0390)\n",
      "12343 Training Loss: tensor(0.0390)\n",
      "12344 Training Loss: tensor(0.0389)\n",
      "12345 Training Loss: tensor(0.0391)\n",
      "12346 Training Loss: tensor(0.0388)\n",
      "12347 Training Loss: tensor(0.0389)\n",
      "12348 Training Loss: tensor(0.0388)\n",
      "12349 Training Loss: tensor(0.0388)\n",
      "12350 Training Loss: tensor(0.0390)\n",
      "12351 Training Loss: tensor(0.0391)\n",
      "12352 Training Loss: tensor(0.0391)\n",
      "12353 Training Loss: tensor(0.0389)\n",
      "12354 Training Loss: tensor(0.0390)\n",
      "12355 Training Loss: tensor(0.0390)\n",
      "12356 Training Loss: tensor(0.0391)\n",
      "12357 Training Loss: tensor(0.0390)\n",
      "12358 Training Loss: tensor(0.0389)\n",
      "12359 Training Loss: tensor(0.0389)\n",
      "12360 Training Loss: tensor(0.0390)\n",
      "12361 Training Loss: tensor(0.0389)\n",
      "12362 Training Loss: tensor(0.0389)\n",
      "12363 Training Loss: tensor(0.0390)\n",
      "12364 Training Loss: tensor(0.0391)\n",
      "12365 Training Loss: tensor(0.0390)\n",
      "12366 Training Loss: tensor(0.0388)\n",
      "12367 Training Loss: tensor(0.0390)\n",
      "12368 Training Loss: tensor(0.0389)\n",
      "12369 Training Loss: tensor(0.0390)\n",
      "12370 Training Loss: tensor(0.0389)\n",
      "12371 Training Loss: tensor(0.0389)\n",
      "12372 Training Loss: tensor(0.0390)\n",
      "12373 Training Loss: tensor(0.0389)\n",
      "12374 Training Loss: tensor(0.0388)\n",
      "12375 Training Loss: tensor(0.0389)\n",
      "12376 Training Loss: tensor(0.0389)\n",
      "12377 Training Loss: tensor(0.0392)\n",
      "12378 Training Loss: tensor(0.0389)\n",
      "12379 Training Loss: tensor(0.0389)\n",
      "12380 Training Loss: tensor(0.0388)\n",
      "12381 Training Loss: tensor(0.0388)\n",
      "12382 Training Loss: tensor(0.0390)\n",
      "12383 Training Loss: tensor(0.0391)\n",
      "12384 Training Loss: tensor(0.0388)\n",
      "12385 Training Loss: tensor(0.0388)\n",
      "12386 Training Loss: tensor(0.0390)\n",
      "12387 Training Loss: tensor(0.0390)\n",
      "12388 Training Loss: tensor(0.0389)\n",
      "12389 Training Loss: tensor(0.0388)\n",
      "12390 Training Loss: tensor(0.0389)\n",
      "12391 Training Loss: tensor(0.0388)\n",
      "12392 Training Loss: tensor(0.0389)\n",
      "12393 Training Loss: tensor(0.0389)\n",
      "12394 Training Loss: tensor(0.0387)\n",
      "12395 Training Loss: tensor(0.0390)\n",
      "12396 Training Loss: tensor(0.0389)\n",
      "12397 Training Loss: tensor(0.0387)\n",
      "12398 Training Loss: tensor(0.0388)\n",
      "12399 Training Loss: tensor(0.0389)\n",
      "12400 Training Loss: tensor(0.0389)\n",
      "12401 Training Loss: tensor(0.0389)\n",
      "12402 Training Loss: tensor(0.0389)\n",
      "12403 Training Loss: tensor(0.0390)\n",
      "12404 Training Loss: tensor(0.0389)\n",
      "12405 Training Loss: tensor(0.0388)\n",
      "12406 Training Loss: tensor(0.0390)\n",
      "12407 Training Loss: tensor(0.0387)\n",
      "12408 Training Loss: tensor(0.0388)\n",
      "12409 Training Loss: tensor(0.0388)\n",
      "12410 Training Loss: tensor(0.0388)\n",
      "12411 Training Loss: tensor(0.0387)\n",
      "12412 Training Loss: tensor(0.0390)\n",
      "12413 Training Loss: tensor(0.0388)\n",
      "12414 Training Loss: tensor(0.0389)\n",
      "12415 Training Loss: tensor(0.0389)\n",
      "12416 Training Loss: tensor(0.0390)\n",
      "12417 Training Loss: tensor(0.0391)\n",
      "12418 Training Loss: tensor(0.0388)\n",
      "12419 Training Loss: tensor(0.0388)\n",
      "12420 Training Loss: tensor(0.0391)\n",
      "12421 Training Loss: tensor(0.0388)\n",
      "12422 Training Loss: tensor(0.0389)\n",
      "12423 Training Loss: tensor(0.0389)\n",
      "12424 Training Loss: tensor(0.0388)\n",
      "12425 Training Loss: tensor(0.0387)\n",
      "12426 Training Loss: tensor(0.0388)\n",
      "12427 Training Loss: tensor(0.0389)\n",
      "12428 Training Loss: tensor(0.0388)\n",
      "12429 Training Loss: tensor(0.0389)\n",
      "12430 Training Loss: tensor(0.0388)\n",
      "12431 Training Loss: tensor(0.0389)\n",
      "12432 Training Loss: tensor(0.0388)\n",
      "12433 Training Loss: tensor(0.0388)\n",
      "12434 Training Loss: tensor(0.0387)\n",
      "12435 Training Loss: tensor(0.0389)\n",
      "12436 Training Loss: tensor(0.0389)\n",
      "12437 Training Loss: tensor(0.0389)\n",
      "12438 Training Loss: tensor(0.0388)\n",
      "12439 Training Loss: tensor(0.0389)\n",
      "12440 Training Loss: tensor(0.0389)\n",
      "12441 Training Loss: tensor(0.0387)\n",
      "12442 Training Loss: tensor(0.0388)\n",
      "12443 Training Loss: tensor(0.0387)\n",
      "12444 Training Loss: tensor(0.0389)\n",
      "12445 Training Loss: tensor(0.0386)\n",
      "12446 Training Loss: tensor(0.0387)\n",
      "12447 Training Loss: tensor(0.0389)\n",
      "12448 Training Loss: tensor(0.0388)\n",
      "12449 Training Loss: tensor(0.0388)\n",
      "12450 Training Loss: tensor(0.0388)\n",
      "12451 Training Loss: tensor(0.0390)\n",
      "12452 Training Loss: tensor(0.0388)\n",
      "12453 Training Loss: tensor(0.0388)\n",
      "12454 Training Loss: tensor(0.0388)\n",
      "12455 Training Loss: tensor(0.0389)\n",
      "12456 Training Loss: tensor(0.0390)\n",
      "12457 Training Loss: tensor(0.0387)\n",
      "12458 Training Loss: tensor(0.0388)\n",
      "12459 Training Loss: tensor(0.0386)\n",
      "12460 Training Loss: tensor(0.0388)\n",
      "12461 Training Loss: tensor(0.0390)\n",
      "12462 Training Loss: tensor(0.0388)\n",
      "12463 Training Loss: tensor(0.0387)\n",
      "12464 Training Loss: tensor(0.0388)\n",
      "12465 Training Loss: tensor(0.0387)\n",
      "12466 Training Loss: tensor(0.0388)\n",
      "12467 Training Loss: tensor(0.0387)\n",
      "12468 Training Loss: tensor(0.0388)\n",
      "12469 Training Loss: tensor(0.0388)\n",
      "12470 Training Loss: tensor(0.0388)\n",
      "12471 Training Loss: tensor(0.0385)\n",
      "12472 Training Loss: tensor(0.0386)\n",
      "12473 Training Loss: tensor(0.0389)\n",
      "12474 Training Loss: tensor(0.0387)\n",
      "12475 Training Loss: tensor(0.0387)\n",
      "12476 Training Loss: tensor(0.0388)\n",
      "12477 Training Loss: tensor(0.0388)\n",
      "12478 Training Loss: tensor(0.0389)\n",
      "12479 Training Loss: tensor(0.0391)\n",
      "12480 Training Loss: tensor(0.0388)\n",
      "12481 Training Loss: tensor(0.0389)\n",
      "12482 Training Loss: tensor(0.0386)\n",
      "12483 Training Loss: tensor(0.0388)\n",
      "12484 Training Loss: tensor(0.0388)\n",
      "12485 Training Loss: tensor(0.0388)\n",
      "12486 Training Loss: tensor(0.0387)\n",
      "12487 Training Loss: tensor(0.0388)\n",
      "12488 Training Loss: tensor(0.0387)\n",
      "12489 Training Loss: tensor(0.0387)\n",
      "12490 Training Loss: tensor(0.0387)\n",
      "12491 Training Loss: tensor(0.0387)\n",
      "12492 Training Loss: tensor(0.0387)\n",
      "12493 Training Loss: tensor(0.0388)\n",
      "12494 Training Loss: tensor(0.0387)\n",
      "12495 Training Loss: tensor(0.0387)\n",
      "12496 Training Loss: tensor(0.0388)\n",
      "12497 Training Loss: tensor(0.0387)\n",
      "12498 Training Loss: tensor(0.0388)\n",
      "12499 Training Loss: tensor(0.0387)\n",
      "12500 Training Loss: tensor(0.0388)\n",
      "12501 Training Loss: tensor(0.0389)\n",
      "12502 Training Loss: tensor(0.0387)\n",
      "12503 Training Loss: tensor(0.0387)\n",
      "12504 Training Loss: tensor(0.0388)\n",
      "12505 Training Loss: tensor(0.0387)\n",
      "12506 Training Loss: tensor(0.0386)\n",
      "12507 Training Loss: tensor(0.0388)\n",
      "12508 Training Loss: tensor(0.0386)\n",
      "12509 Training Loss: tensor(0.0387)\n",
      "12510 Training Loss: tensor(0.0388)\n",
      "12511 Training Loss: tensor(0.0388)\n",
      "12512 Training Loss: tensor(0.0387)\n",
      "12513 Training Loss: tensor(0.0387)\n",
      "12514 Training Loss: tensor(0.0387)\n",
      "12515 Training Loss: tensor(0.0389)\n",
      "12516 Training Loss: tensor(0.0388)\n",
      "12517 Training Loss: tensor(0.0388)\n",
      "12518 Training Loss: tensor(0.0386)\n",
      "12519 Training Loss: tensor(0.0387)\n",
      "12520 Training Loss: tensor(0.0388)\n",
      "12521 Training Loss: tensor(0.0387)\n",
      "12522 Training Loss: tensor(0.0386)\n",
      "12523 Training Loss: tensor(0.0387)\n",
      "12524 Training Loss: tensor(0.0387)\n",
      "12525 Training Loss: tensor(0.0388)\n",
      "12526 Training Loss: tensor(0.0386)\n",
      "12527 Training Loss: tensor(0.0388)\n",
      "12528 Training Loss: tensor(0.0386)\n",
      "12529 Training Loss: tensor(0.0387)\n",
      "12530 Training Loss: tensor(0.0386)\n",
      "12531 Training Loss: tensor(0.0387)\n",
      "12532 Training Loss: tensor(0.0386)\n",
      "12533 Training Loss: tensor(0.0386)\n",
      "12534 Training Loss: tensor(0.0386)\n",
      "12535 Training Loss: tensor(0.0387)\n",
      "12536 Training Loss: tensor(0.0386)\n",
      "12537 Training Loss: tensor(0.0386)\n",
      "12538 Training Loss: tensor(0.0386)\n",
      "12539 Training Loss: tensor(0.0386)\n",
      "12540 Training Loss: tensor(0.0386)\n",
      "12541 Training Loss: tensor(0.0385)\n",
      "12542 Training Loss: tensor(0.0386)\n",
      "12543 Training Loss: tensor(0.0386)\n",
      "12544 Training Loss: tensor(0.0389)\n",
      "12545 Training Loss: tensor(0.0387)\n",
      "12546 Training Loss: tensor(0.0387)\n",
      "12547 Training Loss: tensor(0.0387)\n",
      "12548 Training Loss: tensor(0.0386)\n",
      "12549 Training Loss: tensor(0.0386)\n",
      "12550 Training Loss: tensor(0.0386)\n",
      "12551 Training Loss: tensor(0.0387)\n",
      "12552 Training Loss: tensor(0.0386)\n",
      "12553 Training Loss: tensor(0.0388)\n",
      "12554 Training Loss: tensor(0.0387)\n",
      "12555 Training Loss: tensor(0.0386)\n",
      "12556 Training Loss: tensor(0.0385)\n",
      "12557 Training Loss: tensor(0.0385)\n",
      "12558 Training Loss: tensor(0.0385)\n",
      "12559 Training Loss: tensor(0.0388)\n",
      "12560 Training Loss: tensor(0.0386)\n",
      "12561 Training Loss: tensor(0.0386)\n",
      "12562 Training Loss: tensor(0.0387)\n",
      "12563 Training Loss: tensor(0.0387)\n",
      "12564 Training Loss: tensor(0.0389)\n",
      "12565 Training Loss: tensor(0.0386)\n",
      "12566 Training Loss: tensor(0.0385)\n",
      "12567 Training Loss: tensor(0.0386)\n",
      "12568 Training Loss: tensor(0.0386)\n",
      "12569 Training Loss: tensor(0.0386)\n",
      "12570 Training Loss: tensor(0.0387)\n",
      "12571 Training Loss: tensor(0.0385)\n",
      "12572 Training Loss: tensor(0.0385)\n",
      "12573 Training Loss: tensor(0.0387)\n",
      "12574 Training Loss: tensor(0.0386)\n",
      "12575 Training Loss: tensor(0.0385)\n",
      "12576 Training Loss: tensor(0.0385)\n",
      "12577 Training Loss: tensor(0.0387)\n",
      "12578 Training Loss: tensor(0.0386)\n",
      "12579 Training Loss: tensor(0.0387)\n",
      "12580 Training Loss: tensor(0.0385)\n",
      "12581 Training Loss: tensor(0.0388)\n",
      "12582 Training Loss: tensor(0.0388)\n",
      "12583 Training Loss: tensor(0.0384)\n",
      "12584 Training Loss: tensor(0.0386)\n",
      "12585 Training Loss: tensor(0.0386)\n",
      "12586 Training Loss: tensor(0.0385)\n",
      "12587 Training Loss: tensor(0.0386)\n",
      "12588 Training Loss: tensor(0.0385)\n",
      "12589 Training Loss: tensor(0.0387)\n",
      "12590 Training Loss: tensor(0.0386)\n",
      "12591 Training Loss: tensor(0.0386)\n",
      "12592 Training Loss: tensor(0.0387)\n",
      "12593 Training Loss: tensor(0.0385)\n",
      "12594 Training Loss: tensor(0.0387)\n",
      "12595 Training Loss: tensor(0.0387)\n",
      "12596 Training Loss: tensor(0.0384)\n",
      "12597 Training Loss: tensor(0.0385)\n",
      "12598 Training Loss: tensor(0.0385)\n",
      "12599 Training Loss: tensor(0.0387)\n",
      "12600 Training Loss: tensor(0.0386)\n",
      "12601 Training Loss: tensor(0.0388)\n",
      "12602 Training Loss: tensor(0.0386)\n",
      "12603 Training Loss: tensor(0.0385)\n",
      "12604 Training Loss: tensor(0.0385)\n",
      "12605 Training Loss: tensor(0.0385)\n",
      "12606 Training Loss: tensor(0.0387)\n",
      "12607 Training Loss: tensor(0.0385)\n",
      "12608 Training Loss: tensor(0.0385)\n",
      "12609 Training Loss: tensor(0.0386)\n",
      "12610 Training Loss: tensor(0.0385)\n",
      "12611 Training Loss: tensor(0.0385)\n",
      "12612 Training Loss: tensor(0.0387)\n",
      "12613 Training Loss: tensor(0.0385)\n",
      "12614 Training Loss: tensor(0.0386)\n",
      "12615 Training Loss: tensor(0.0385)\n",
      "12616 Training Loss: tensor(0.0387)\n",
      "12617 Training Loss: tensor(0.0386)\n",
      "12618 Training Loss: tensor(0.0385)\n",
      "12619 Training Loss: tensor(0.0387)\n",
      "12620 Training Loss: tensor(0.0383)\n",
      "12621 Training Loss: tensor(0.0385)\n",
      "12622 Training Loss: tensor(0.0386)\n",
      "12623 Training Loss: tensor(0.0386)\n",
      "12624 Training Loss: tensor(0.0384)\n",
      "12625 Training Loss: tensor(0.0385)\n",
      "12626 Training Loss: tensor(0.0385)\n",
      "12627 Training Loss: tensor(0.0384)\n",
      "12628 Training Loss: tensor(0.0385)\n",
      "12629 Training Loss: tensor(0.0385)\n",
      "12630 Training Loss: tensor(0.0386)\n",
      "12631 Training Loss: tensor(0.0384)\n",
      "12632 Training Loss: tensor(0.0384)\n",
      "12633 Training Loss: tensor(0.0384)\n",
      "12634 Training Loss: tensor(0.0386)\n",
      "12635 Training Loss: tensor(0.0384)\n",
      "12636 Training Loss: tensor(0.0385)\n",
      "12637 Training Loss: tensor(0.0385)\n",
      "12638 Training Loss: tensor(0.0383)\n",
      "12639 Training Loss: tensor(0.0385)\n",
      "12640 Training Loss: tensor(0.0385)\n",
      "12641 Training Loss: tensor(0.0384)\n",
      "12642 Training Loss: tensor(0.0385)\n",
      "12643 Training Loss: tensor(0.0383)\n",
      "12644 Training Loss: tensor(0.0384)\n",
      "12645 Training Loss: tensor(0.0385)\n",
      "12646 Training Loss: tensor(0.0384)\n",
      "12647 Training Loss: tensor(0.0385)\n",
      "12648 Training Loss: tensor(0.0383)\n",
      "12649 Training Loss: tensor(0.0386)\n",
      "12650 Training Loss: tensor(0.0386)\n",
      "12651 Training Loss: tensor(0.0384)\n",
      "12652 Training Loss: tensor(0.0385)\n",
      "12653 Training Loss: tensor(0.0385)\n",
      "12654 Training Loss: tensor(0.0384)\n",
      "12655 Training Loss: tensor(0.0387)\n",
      "12656 Training Loss: tensor(0.0385)\n",
      "12657 Training Loss: tensor(0.0384)\n",
      "12658 Training Loss: tensor(0.0384)\n",
      "12659 Training Loss: tensor(0.0384)\n",
      "12660 Training Loss: tensor(0.0384)\n",
      "12661 Training Loss: tensor(0.0384)\n",
      "12662 Training Loss: tensor(0.0385)\n",
      "12663 Training Loss: tensor(0.0386)\n",
      "12664 Training Loss: tensor(0.0385)\n",
      "12665 Training Loss: tensor(0.0385)\n",
      "12666 Training Loss: tensor(0.0385)\n",
      "12667 Training Loss: tensor(0.0384)\n",
      "12668 Training Loss: tensor(0.0384)\n",
      "12669 Training Loss: tensor(0.0385)\n",
      "12670 Training Loss: tensor(0.0384)\n",
      "12671 Training Loss: tensor(0.0384)\n",
      "12672 Training Loss: tensor(0.0385)\n",
      "12673 Training Loss: tensor(0.0383)\n",
      "12674 Training Loss: tensor(0.0383)\n",
      "12675 Training Loss: tensor(0.0385)\n",
      "12676 Training Loss: tensor(0.0383)\n",
      "12677 Training Loss: tensor(0.0385)\n",
      "12678 Training Loss: tensor(0.0384)\n",
      "12679 Training Loss: tensor(0.0383)\n",
      "12680 Training Loss: tensor(0.0384)\n",
      "12681 Training Loss: tensor(0.0384)\n",
      "12682 Training Loss: tensor(0.0385)\n",
      "12683 Training Loss: tensor(0.0385)\n",
      "12684 Training Loss: tensor(0.0384)\n",
      "12685 Training Loss: tensor(0.0384)\n",
      "12686 Training Loss: tensor(0.0387)\n",
      "12687 Training Loss: tensor(0.0383)\n",
      "12688 Training Loss: tensor(0.0385)\n",
      "12689 Training Loss: tensor(0.0383)\n",
      "12690 Training Loss: tensor(0.0384)\n",
      "12691 Training Loss: tensor(0.0385)\n",
      "12692 Training Loss: tensor(0.0382)\n",
      "12693 Training Loss: tensor(0.0385)\n",
      "12694 Training Loss: tensor(0.0383)\n",
      "12695 Training Loss: tensor(0.0384)\n",
      "12696 Training Loss: tensor(0.0385)\n",
      "12697 Training Loss: tensor(0.0384)\n",
      "12698 Training Loss: tensor(0.0384)\n",
      "12699 Training Loss: tensor(0.0385)\n",
      "12700 Training Loss: tensor(0.0383)\n",
      "12701 Training Loss: tensor(0.0383)\n",
      "12702 Training Loss: tensor(0.0383)\n",
      "12703 Training Loss: tensor(0.0384)\n",
      "12704 Training Loss: tensor(0.0384)\n",
      "12705 Training Loss: tensor(0.0384)\n",
      "12706 Training Loss: tensor(0.0383)\n",
      "12707 Training Loss: tensor(0.0383)\n",
      "12708 Training Loss: tensor(0.0384)\n",
      "12709 Training Loss: tensor(0.0384)\n",
      "12710 Training Loss: tensor(0.0385)\n",
      "12711 Training Loss: tensor(0.0384)\n",
      "12712 Training Loss: tensor(0.0383)\n",
      "12713 Training Loss: tensor(0.0384)\n",
      "12714 Training Loss: tensor(0.0385)\n",
      "12715 Training Loss: tensor(0.0383)\n",
      "12716 Training Loss: tensor(0.0384)\n",
      "12717 Training Loss: tensor(0.0384)\n",
      "12718 Training Loss: tensor(0.0385)\n",
      "12719 Training Loss: tensor(0.0384)\n",
      "12720 Training Loss: tensor(0.0383)\n",
      "12721 Training Loss: tensor(0.0383)\n",
      "12722 Training Loss: tensor(0.0383)\n",
      "12723 Training Loss: tensor(0.0383)\n",
      "12724 Training Loss: tensor(0.0381)\n",
      "12725 Training Loss: tensor(0.0384)\n",
      "12726 Training Loss: tensor(0.0387)\n",
      "12727 Training Loss: tensor(0.0383)\n",
      "12728 Training Loss: tensor(0.0382)\n",
      "12729 Training Loss: tensor(0.0382)\n",
      "12730 Training Loss: tensor(0.0383)\n",
      "12731 Training Loss: tensor(0.0384)\n",
      "12732 Training Loss: tensor(0.0384)\n",
      "12733 Training Loss: tensor(0.0385)\n",
      "12734 Training Loss: tensor(0.0382)\n",
      "12735 Training Loss: tensor(0.0384)\n",
      "12736 Training Loss: tensor(0.0383)\n",
      "12737 Training Loss: tensor(0.0383)\n",
      "12738 Training Loss: tensor(0.0382)\n",
      "12739 Training Loss: tensor(0.0385)\n",
      "12740 Training Loss: tensor(0.0383)\n",
      "12741 Training Loss: tensor(0.0383)\n",
      "12742 Training Loss: tensor(0.0383)\n",
      "12743 Training Loss: tensor(0.0385)\n",
      "12744 Training Loss: tensor(0.0383)\n",
      "12745 Training Loss: tensor(0.0382)\n",
      "12746 Training Loss: tensor(0.0384)\n",
      "12747 Training Loss: tensor(0.0382)\n",
      "12748 Training Loss: tensor(0.0382)\n",
      "12749 Training Loss: tensor(0.0383)\n",
      "12750 Training Loss: tensor(0.0385)\n",
      "12751 Training Loss: tensor(0.0383)\n",
      "12752 Training Loss: tensor(0.0384)\n",
      "12753 Training Loss: tensor(0.0384)\n",
      "12754 Training Loss: tensor(0.0384)\n",
      "12755 Training Loss: tensor(0.0384)\n",
      "12756 Training Loss: tensor(0.0384)\n",
      "12757 Training Loss: tensor(0.0383)\n",
      "12758 Training Loss: tensor(0.0383)\n",
      "12759 Training Loss: tensor(0.0381)\n",
      "12760 Training Loss: tensor(0.0383)\n",
      "12761 Training Loss: tensor(0.0383)\n",
      "12762 Training Loss: tensor(0.0383)\n",
      "12763 Training Loss: tensor(0.0383)\n",
      "12764 Training Loss: tensor(0.0382)\n",
      "12765 Training Loss: tensor(0.0384)\n",
      "12766 Training Loss: tensor(0.0383)\n",
      "12767 Training Loss: tensor(0.0382)\n",
      "12768 Training Loss: tensor(0.0383)\n",
      "12769 Training Loss: tensor(0.0382)\n",
      "12770 Training Loss: tensor(0.0382)\n",
      "12771 Training Loss: tensor(0.0383)\n",
      "12772 Training Loss: tensor(0.0383)\n",
      "12773 Training Loss: tensor(0.0383)\n",
      "12774 Training Loss: tensor(0.0382)\n",
      "12775 Training Loss: tensor(0.0382)\n",
      "12776 Training Loss: tensor(0.0382)\n",
      "12777 Training Loss: tensor(0.0382)\n",
      "12778 Training Loss: tensor(0.0382)\n",
      "12779 Training Loss: tensor(0.0386)\n",
      "12780 Training Loss: tensor(0.0381)\n",
      "12781 Training Loss: tensor(0.0382)\n",
      "12782 Training Loss: tensor(0.0383)\n",
      "12783 Training Loss: tensor(0.0381)\n",
      "12784 Training Loss: tensor(0.0381)\n",
      "12785 Training Loss: tensor(0.0383)\n",
      "12786 Training Loss: tensor(0.0384)\n",
      "12787 Training Loss: tensor(0.0383)\n",
      "12788 Training Loss: tensor(0.0381)\n",
      "12789 Training Loss: tensor(0.0384)\n",
      "12790 Training Loss: tensor(0.0381)\n",
      "12791 Training Loss: tensor(0.0381)\n",
      "12792 Training Loss: tensor(0.0381)\n",
      "12793 Training Loss: tensor(0.0381)\n",
      "12794 Training Loss: tensor(0.0381)\n",
      "12795 Training Loss: tensor(0.0381)\n",
      "12796 Training Loss: tensor(0.0384)\n",
      "12797 Training Loss: tensor(0.0382)\n",
      "12798 Training Loss: tensor(0.0382)\n",
      "12799 Training Loss: tensor(0.0384)\n",
      "12800 Training Loss: tensor(0.0381)\n",
      "12801 Training Loss: tensor(0.0383)\n",
      "12802 Training Loss: tensor(0.0381)\n",
      "12803 Training Loss: tensor(0.0383)\n",
      "12804 Training Loss: tensor(0.0382)\n",
      "12805 Training Loss: tensor(0.0383)\n",
      "12806 Training Loss: tensor(0.0381)\n",
      "12807 Training Loss: tensor(0.0381)\n",
      "12808 Training Loss: tensor(0.0381)\n",
      "12809 Training Loss: tensor(0.0381)\n",
      "12810 Training Loss: tensor(0.0382)\n",
      "12811 Training Loss: tensor(0.0382)\n",
      "12812 Training Loss: tensor(0.0382)\n",
      "12813 Training Loss: tensor(0.0381)\n",
      "12814 Training Loss: tensor(0.0382)\n",
      "12815 Training Loss: tensor(0.0381)\n",
      "12816 Training Loss: tensor(0.0381)\n",
      "12817 Training Loss: tensor(0.0384)\n",
      "12818 Training Loss: tensor(0.0381)\n",
      "12819 Training Loss: tensor(0.0383)\n",
      "12820 Training Loss: tensor(0.0381)\n",
      "12821 Training Loss: tensor(0.0380)\n",
      "12822 Training Loss: tensor(0.0380)\n",
      "12823 Training Loss: tensor(0.0381)\n",
      "12824 Training Loss: tensor(0.0381)\n",
      "12825 Training Loss: tensor(0.0383)\n",
      "12826 Training Loss: tensor(0.0381)\n",
      "12827 Training Loss: tensor(0.0381)\n",
      "12828 Training Loss: tensor(0.0381)\n",
      "12829 Training Loss: tensor(0.0382)\n",
      "12830 Training Loss: tensor(0.0383)\n",
      "12831 Training Loss: tensor(0.0381)\n",
      "12832 Training Loss: tensor(0.0381)\n",
      "12833 Training Loss: tensor(0.0383)\n",
      "12834 Training Loss: tensor(0.0380)\n",
      "12835 Training Loss: tensor(0.0380)\n",
      "12836 Training Loss: tensor(0.0381)\n",
      "12837 Training Loss: tensor(0.0381)\n",
      "12838 Training Loss: tensor(0.0381)\n",
      "12839 Training Loss: tensor(0.0382)\n",
      "12840 Training Loss: tensor(0.0382)\n",
      "12841 Training Loss: tensor(0.0382)\n",
      "12842 Training Loss: tensor(0.0381)\n",
      "12843 Training Loss: tensor(0.0381)\n",
      "12844 Training Loss: tensor(0.0382)\n",
      "12845 Training Loss: tensor(0.0381)\n",
      "12846 Training Loss: tensor(0.0380)\n",
      "12847 Training Loss: tensor(0.0381)\n",
      "12848 Training Loss: tensor(0.0381)\n",
      "12849 Training Loss: tensor(0.0382)\n",
      "12850 Training Loss: tensor(0.0380)\n",
      "12851 Training Loss: tensor(0.0380)\n",
      "12852 Training Loss: tensor(0.0382)\n",
      "12853 Training Loss: tensor(0.0380)\n",
      "12854 Training Loss: tensor(0.0381)\n",
      "12855 Training Loss: tensor(0.0380)\n",
      "12856 Training Loss: tensor(0.0382)\n",
      "12857 Training Loss: tensor(0.0381)\n",
      "12858 Training Loss: tensor(0.0380)\n",
      "12859 Training Loss: tensor(0.0381)\n",
      "12860 Training Loss: tensor(0.0381)\n",
      "12861 Training Loss: tensor(0.0380)\n",
      "12862 Training Loss: tensor(0.0381)\n",
      "12863 Training Loss: tensor(0.0381)\n",
      "12864 Training Loss: tensor(0.0381)\n",
      "12865 Training Loss: tensor(0.0381)\n",
      "12866 Training Loss: tensor(0.0380)\n",
      "12867 Training Loss: tensor(0.0380)\n",
      "12868 Training Loss: tensor(0.0381)\n",
      "12869 Training Loss: tensor(0.0382)\n",
      "12870 Training Loss: tensor(0.0381)\n",
      "12871 Training Loss: tensor(0.0381)\n",
      "12872 Training Loss: tensor(0.0380)\n",
      "12873 Training Loss: tensor(0.0383)\n",
      "12874 Training Loss: tensor(0.0382)\n",
      "12875 Training Loss: tensor(0.0381)\n",
      "12876 Training Loss: tensor(0.0383)\n",
      "12877 Training Loss: tensor(0.0381)\n",
      "12878 Training Loss: tensor(0.0381)\n",
      "12879 Training Loss: tensor(0.0381)\n",
      "12880 Training Loss: tensor(0.0382)\n",
      "12881 Training Loss: tensor(0.0382)\n",
      "12882 Training Loss: tensor(0.0379)\n",
      "12883 Training Loss: tensor(0.0381)\n",
      "12884 Training Loss: tensor(0.0380)\n",
      "12885 Training Loss: tensor(0.0381)\n",
      "12886 Training Loss: tensor(0.0381)\n",
      "12887 Training Loss: tensor(0.0381)\n",
      "12888 Training Loss: tensor(0.0380)\n",
      "12889 Training Loss: tensor(0.0379)\n",
      "12890 Training Loss: tensor(0.0380)\n",
      "12891 Training Loss: tensor(0.0380)\n",
      "12892 Training Loss: tensor(0.0379)\n",
      "12893 Training Loss: tensor(0.0381)\n",
      "12894 Training Loss: tensor(0.0380)\n",
      "12895 Training Loss: tensor(0.0382)\n",
      "12896 Training Loss: tensor(0.0379)\n",
      "12897 Training Loss: tensor(0.0379)\n",
      "12898 Training Loss: tensor(0.0380)\n",
      "12899 Training Loss: tensor(0.0379)\n",
      "12900 Training Loss: tensor(0.0380)\n",
      "12901 Training Loss: tensor(0.0381)\n",
      "12902 Training Loss: tensor(0.0380)\n",
      "12903 Training Loss: tensor(0.0381)\n",
      "12904 Training Loss: tensor(0.0380)\n",
      "12905 Training Loss: tensor(0.0380)\n",
      "12906 Training Loss: tensor(0.0381)\n",
      "12907 Training Loss: tensor(0.0380)\n",
      "12908 Training Loss: tensor(0.0380)\n",
      "12909 Training Loss: tensor(0.0380)\n",
      "12910 Training Loss: tensor(0.0380)\n",
      "12911 Training Loss: tensor(0.0379)\n",
      "12912 Training Loss: tensor(0.0381)\n",
      "12913 Training Loss: tensor(0.0380)\n",
      "12914 Training Loss: tensor(0.0381)\n",
      "12915 Training Loss: tensor(0.0380)\n",
      "12916 Training Loss: tensor(0.0379)\n",
      "12917 Training Loss: tensor(0.0380)\n",
      "12918 Training Loss: tensor(0.0379)\n",
      "12919 Training Loss: tensor(0.0379)\n",
      "12920 Training Loss: tensor(0.0380)\n",
      "12921 Training Loss: tensor(0.0379)\n",
      "12922 Training Loss: tensor(0.0380)\n",
      "12923 Training Loss: tensor(0.0378)\n",
      "12924 Training Loss: tensor(0.0379)\n",
      "12925 Training Loss: tensor(0.0379)\n",
      "12926 Training Loss: tensor(0.0381)\n",
      "12927 Training Loss: tensor(0.0379)\n",
      "12928 Training Loss: tensor(0.0379)\n",
      "12929 Training Loss: tensor(0.0381)\n",
      "12930 Training Loss: tensor(0.0380)\n",
      "12931 Training Loss: tensor(0.0380)\n",
      "12932 Training Loss: tensor(0.0382)\n",
      "12933 Training Loss: tensor(0.0382)\n",
      "12934 Training Loss: tensor(0.0380)\n",
      "12935 Training Loss: tensor(0.0380)\n",
      "12936 Training Loss: tensor(0.0378)\n",
      "12937 Training Loss: tensor(0.0381)\n",
      "12938 Training Loss: tensor(0.0380)\n",
      "12939 Training Loss: tensor(0.0380)\n",
      "12940 Training Loss: tensor(0.0380)\n",
      "12941 Training Loss: tensor(0.0380)\n",
      "12942 Training Loss: tensor(0.0380)\n",
      "12943 Training Loss: tensor(0.0378)\n",
      "12944 Training Loss: tensor(0.0380)\n",
      "12945 Training Loss: tensor(0.0379)\n",
      "12946 Training Loss: tensor(0.0379)\n",
      "12947 Training Loss: tensor(0.0380)\n",
      "12948 Training Loss: tensor(0.0379)\n",
      "12949 Training Loss: tensor(0.0379)\n",
      "12950 Training Loss: tensor(0.0379)\n",
      "12951 Training Loss: tensor(0.0379)\n",
      "12952 Training Loss: tensor(0.0380)\n",
      "12953 Training Loss: tensor(0.0379)\n",
      "12954 Training Loss: tensor(0.0381)\n",
      "12955 Training Loss: tensor(0.0379)\n",
      "12956 Training Loss: tensor(0.0380)\n",
      "12957 Training Loss: tensor(0.0378)\n",
      "12958 Training Loss: tensor(0.0378)\n",
      "12959 Training Loss: tensor(0.0381)\n",
      "12960 Training Loss: tensor(0.0379)\n",
      "12961 Training Loss: tensor(0.0378)\n",
      "12962 Training Loss: tensor(0.0379)\n",
      "12963 Training Loss: tensor(0.0379)\n",
      "12964 Training Loss: tensor(0.0378)\n",
      "12965 Training Loss: tensor(0.0379)\n",
      "12966 Training Loss: tensor(0.0380)\n",
      "12967 Training Loss: tensor(0.0379)\n",
      "12968 Training Loss: tensor(0.0377)\n",
      "12969 Training Loss: tensor(0.0378)\n",
      "12970 Training Loss: tensor(0.0378)\n",
      "12971 Training Loss: tensor(0.0378)\n",
      "12972 Training Loss: tensor(0.0379)\n",
      "12973 Training Loss: tensor(0.0380)\n",
      "12974 Training Loss: tensor(0.0380)\n",
      "12975 Training Loss: tensor(0.0377)\n",
      "12976 Training Loss: tensor(0.0379)\n",
      "12977 Training Loss: tensor(0.0380)\n",
      "12978 Training Loss: tensor(0.0379)\n",
      "12979 Training Loss: tensor(0.0380)\n",
      "12980 Training Loss: tensor(0.0380)\n",
      "12981 Training Loss: tensor(0.0378)\n",
      "12982 Training Loss: tensor(0.0377)\n",
      "12983 Training Loss: tensor(0.0378)\n",
      "12984 Training Loss: tensor(0.0380)\n",
      "12985 Training Loss: tensor(0.0378)\n",
      "12986 Training Loss: tensor(0.0381)\n",
      "12987 Training Loss: tensor(0.0378)\n",
      "12988 Training Loss: tensor(0.0379)\n",
      "12989 Training Loss: tensor(0.0379)\n",
      "12990 Training Loss: tensor(0.0379)\n",
      "12991 Training Loss: tensor(0.0379)\n",
      "12992 Training Loss: tensor(0.0379)\n",
      "12993 Training Loss: tensor(0.0380)\n",
      "12994 Training Loss: tensor(0.0378)\n",
      "12995 Training Loss: tensor(0.0379)\n",
      "12996 Training Loss: tensor(0.0378)\n",
      "12997 Training Loss: tensor(0.0380)\n",
      "12998 Training Loss: tensor(0.0380)\n",
      "12999 Training Loss: tensor(0.0379)\n",
      "13000 Training Loss: tensor(0.0380)\n",
      "13001 Training Loss: tensor(0.0378)\n",
      "13002 Training Loss: tensor(0.0378)\n",
      "13003 Training Loss: tensor(0.0380)\n",
      "13004 Training Loss: tensor(0.0379)\n",
      "13005 Training Loss: tensor(0.0379)\n",
      "13006 Training Loss: tensor(0.0378)\n",
      "13007 Training Loss: tensor(0.0379)\n",
      "13008 Training Loss: tensor(0.0379)\n",
      "13009 Training Loss: tensor(0.0378)\n",
      "13010 Training Loss: tensor(0.0380)\n",
      "13011 Training Loss: tensor(0.0379)\n",
      "13012 Training Loss: tensor(0.0379)\n",
      "13013 Training Loss: tensor(0.0379)\n",
      "13014 Training Loss: tensor(0.0378)\n",
      "13015 Training Loss: tensor(0.0379)\n",
      "13016 Training Loss: tensor(0.0378)\n",
      "13017 Training Loss: tensor(0.0380)\n",
      "13018 Training Loss: tensor(0.0378)\n",
      "13019 Training Loss: tensor(0.0378)\n",
      "13020 Training Loss: tensor(0.0378)\n",
      "13021 Training Loss: tensor(0.0377)\n",
      "13022 Training Loss: tensor(0.0379)\n",
      "13023 Training Loss: tensor(0.0377)\n",
      "13024 Training Loss: tensor(0.0380)\n",
      "13025 Training Loss: tensor(0.0378)\n",
      "13026 Training Loss: tensor(0.0378)\n",
      "13027 Training Loss: tensor(0.0378)\n",
      "13028 Training Loss: tensor(0.0377)\n",
      "13029 Training Loss: tensor(0.0378)\n",
      "13030 Training Loss: tensor(0.0378)\n",
      "13031 Training Loss: tensor(0.0379)\n",
      "13032 Training Loss: tensor(0.0376)\n",
      "13033 Training Loss: tensor(0.0379)\n",
      "13034 Training Loss: tensor(0.0379)\n",
      "13035 Training Loss: tensor(0.0380)\n",
      "13036 Training Loss: tensor(0.0378)\n",
      "13037 Training Loss: tensor(0.0379)\n",
      "13038 Training Loss: tensor(0.0379)\n",
      "13039 Training Loss: tensor(0.0376)\n",
      "13040 Training Loss: tensor(0.0377)\n",
      "13041 Training Loss: tensor(0.0377)\n",
      "13042 Training Loss: tensor(0.0378)\n",
      "13043 Training Loss: tensor(0.0378)\n",
      "13044 Training Loss: tensor(0.0379)\n",
      "13045 Training Loss: tensor(0.0378)\n",
      "13046 Training Loss: tensor(0.0378)\n",
      "13047 Training Loss: tensor(0.0377)\n",
      "13048 Training Loss: tensor(0.0379)\n",
      "13049 Training Loss: tensor(0.0378)\n",
      "13050 Training Loss: tensor(0.0378)\n",
      "13051 Training Loss: tensor(0.0378)\n",
      "13052 Training Loss: tensor(0.0379)\n",
      "13053 Training Loss: tensor(0.0378)\n",
      "13054 Training Loss: tensor(0.0377)\n",
      "13055 Training Loss: tensor(0.0378)\n",
      "13056 Training Loss: tensor(0.0379)\n",
      "13057 Training Loss: tensor(0.0378)\n",
      "13058 Training Loss: tensor(0.0376)\n",
      "13059 Training Loss: tensor(0.0377)\n",
      "13060 Training Loss: tensor(0.0378)\n",
      "13061 Training Loss: tensor(0.0377)\n",
      "13062 Training Loss: tensor(0.0377)\n",
      "13063 Training Loss: tensor(0.0378)\n",
      "13064 Training Loss: tensor(0.0376)\n",
      "13065 Training Loss: tensor(0.0378)\n",
      "13066 Training Loss: tensor(0.0377)\n",
      "13067 Training Loss: tensor(0.0378)\n",
      "13068 Training Loss: tensor(0.0377)\n",
      "13069 Training Loss: tensor(0.0378)\n",
      "13070 Training Loss: tensor(0.0377)\n",
      "13071 Training Loss: tensor(0.0377)\n",
      "13072 Training Loss: tensor(0.0377)\n",
      "13073 Training Loss: tensor(0.0376)\n",
      "13074 Training Loss: tensor(0.0378)\n",
      "13075 Training Loss: tensor(0.0376)\n",
      "13076 Training Loss: tensor(0.0376)\n",
      "13077 Training Loss: tensor(0.0377)\n",
      "13078 Training Loss: tensor(0.0379)\n",
      "13079 Training Loss: tensor(0.0378)\n",
      "13080 Training Loss: tensor(0.0378)\n",
      "13081 Training Loss: tensor(0.0377)\n",
      "13082 Training Loss: tensor(0.0377)\n",
      "13083 Training Loss: tensor(0.0379)\n",
      "13084 Training Loss: tensor(0.0377)\n",
      "13085 Training Loss: tensor(0.0376)\n",
      "13086 Training Loss: tensor(0.0380)\n",
      "13087 Training Loss: tensor(0.0376)\n",
      "13088 Training Loss: tensor(0.0377)\n",
      "13089 Training Loss: tensor(0.0377)\n",
      "13090 Training Loss: tensor(0.0376)\n",
      "13091 Training Loss: tensor(0.0377)\n",
      "13092 Training Loss: tensor(0.0376)\n",
      "13093 Training Loss: tensor(0.0377)\n",
      "13094 Training Loss: tensor(0.0378)\n",
      "13095 Training Loss: tensor(0.0375)\n",
      "13096 Training Loss: tensor(0.0378)\n",
      "13097 Training Loss: tensor(0.0378)\n",
      "13098 Training Loss: tensor(0.0377)\n",
      "13099 Training Loss: tensor(0.0378)\n",
      "13100 Training Loss: tensor(0.0377)\n",
      "13101 Training Loss: tensor(0.0377)\n",
      "13102 Training Loss: tensor(0.0377)\n",
      "13103 Training Loss: tensor(0.0377)\n",
      "13104 Training Loss: tensor(0.0378)\n",
      "13105 Training Loss: tensor(0.0377)\n",
      "13106 Training Loss: tensor(0.0375)\n",
      "13107 Training Loss: tensor(0.0375)\n",
      "13108 Training Loss: tensor(0.0376)\n",
      "13109 Training Loss: tensor(0.0377)\n",
      "13110 Training Loss: tensor(0.0377)\n",
      "13111 Training Loss: tensor(0.0377)\n",
      "13112 Training Loss: tensor(0.0377)\n",
      "13113 Training Loss: tensor(0.0376)\n",
      "13114 Training Loss: tensor(0.0377)\n",
      "13115 Training Loss: tensor(0.0376)\n",
      "13116 Training Loss: tensor(0.0378)\n",
      "13117 Training Loss: tensor(0.0377)\n",
      "13118 Training Loss: tensor(0.0377)\n",
      "13119 Training Loss: tensor(0.0379)\n",
      "13120 Training Loss: tensor(0.0377)\n",
      "13121 Training Loss: tensor(0.0377)\n",
      "13122 Training Loss: tensor(0.0376)\n",
      "13123 Training Loss: tensor(0.0377)\n",
      "13124 Training Loss: tensor(0.0376)\n",
      "13125 Training Loss: tensor(0.0378)\n",
      "13126 Training Loss: tensor(0.0379)\n",
      "13127 Training Loss: tensor(0.0377)\n",
      "13128 Training Loss: tensor(0.0376)\n",
      "13129 Training Loss: tensor(0.0376)\n",
      "13130 Training Loss: tensor(0.0376)\n",
      "13131 Training Loss: tensor(0.0376)\n",
      "13132 Training Loss: tensor(0.0375)\n",
      "13133 Training Loss: tensor(0.0376)\n",
      "13134 Training Loss: tensor(0.0378)\n",
      "13135 Training Loss: tensor(0.0376)\n",
      "13136 Training Loss: tensor(0.0376)\n",
      "13137 Training Loss: tensor(0.0375)\n",
      "13138 Training Loss: tensor(0.0377)\n",
      "13139 Training Loss: tensor(0.0375)\n",
      "13140 Training Loss: tensor(0.0377)\n",
      "13141 Training Loss: tensor(0.0375)\n",
      "13142 Training Loss: tensor(0.0375)\n",
      "13143 Training Loss: tensor(0.0376)\n",
      "13144 Training Loss: tensor(0.0375)\n",
      "13145 Training Loss: tensor(0.0377)\n",
      "13146 Training Loss: tensor(0.0377)\n",
      "13147 Training Loss: tensor(0.0376)\n",
      "13148 Training Loss: tensor(0.0376)\n",
      "13149 Training Loss: tensor(0.0374)\n",
      "13150 Training Loss: tensor(0.0375)\n",
      "13151 Training Loss: tensor(0.0376)\n",
      "13152 Training Loss: tensor(0.0376)\n",
      "13153 Training Loss: tensor(0.0376)\n",
      "13154 Training Loss: tensor(0.0376)\n",
      "13155 Training Loss: tensor(0.0377)\n",
      "13156 Training Loss: tensor(0.0376)\n",
      "13157 Training Loss: tensor(0.0376)\n",
      "13158 Training Loss: tensor(0.0377)\n",
      "13159 Training Loss: tensor(0.0376)\n",
      "13160 Training Loss: tensor(0.0377)\n",
      "13161 Training Loss: tensor(0.0377)\n",
      "13162 Training Loss: tensor(0.0379)\n",
      "13163 Training Loss: tensor(0.0375)\n",
      "13164 Training Loss: tensor(0.0376)\n",
      "13165 Training Loss: tensor(0.0376)\n",
      "13166 Training Loss: tensor(0.0375)\n",
      "13167 Training Loss: tensor(0.0376)\n",
      "13168 Training Loss: tensor(0.0377)\n",
      "13169 Training Loss: tensor(0.0376)\n",
      "13170 Training Loss: tensor(0.0377)\n",
      "13171 Training Loss: tensor(0.0374)\n",
      "13172 Training Loss: tensor(0.0376)\n",
      "13173 Training Loss: tensor(0.0376)\n",
      "13174 Training Loss: tensor(0.0375)\n",
      "13175 Training Loss: tensor(0.0377)\n",
      "13176 Training Loss: tensor(0.0375)\n",
      "13177 Training Loss: tensor(0.0374)\n",
      "13178 Training Loss: tensor(0.0376)\n",
      "13179 Training Loss: tensor(0.0375)\n",
      "13180 Training Loss: tensor(0.0375)\n",
      "13181 Training Loss: tensor(0.0378)\n",
      "13182 Training Loss: tensor(0.0376)\n",
      "13183 Training Loss: tensor(0.0375)\n",
      "13184 Training Loss: tensor(0.0376)\n",
      "13185 Training Loss: tensor(0.0376)\n",
      "13186 Training Loss: tensor(0.0375)\n",
      "13187 Training Loss: tensor(0.0375)\n",
      "13188 Training Loss: tensor(0.0376)\n",
      "13189 Training Loss: tensor(0.0375)\n",
      "13190 Training Loss: tensor(0.0377)\n",
      "13191 Training Loss: tensor(0.0375)\n",
      "13192 Training Loss: tensor(0.0375)\n",
      "13193 Training Loss: tensor(0.0374)\n",
      "13194 Training Loss: tensor(0.0378)\n",
      "13195 Training Loss: tensor(0.0375)\n",
      "13196 Training Loss: tensor(0.0375)\n",
      "13197 Training Loss: tensor(0.0375)\n",
      "13198 Training Loss: tensor(0.0376)\n",
      "13199 Training Loss: tensor(0.0374)\n",
      "13200 Training Loss: tensor(0.0376)\n",
      "13201 Training Loss: tensor(0.0377)\n",
      "13202 Training Loss: tensor(0.0375)\n",
      "13203 Training Loss: tensor(0.0375)\n",
      "13204 Training Loss: tensor(0.0375)\n",
      "13205 Training Loss: tensor(0.0375)\n",
      "13206 Training Loss: tensor(0.0376)\n",
      "13207 Training Loss: tensor(0.0376)\n",
      "13208 Training Loss: tensor(0.0374)\n",
      "13209 Training Loss: tensor(0.0373)\n",
      "13210 Training Loss: tensor(0.0377)\n",
      "13211 Training Loss: tensor(0.0375)\n",
      "13212 Training Loss: tensor(0.0375)\n",
      "13213 Training Loss: tensor(0.0374)\n",
      "13214 Training Loss: tensor(0.0375)\n",
      "13215 Training Loss: tensor(0.0378)\n",
      "13216 Training Loss: tensor(0.0375)\n",
      "13217 Training Loss: tensor(0.0376)\n",
      "13218 Training Loss: tensor(0.0374)\n",
      "13219 Training Loss: tensor(0.0375)\n",
      "13220 Training Loss: tensor(0.0376)\n",
      "13221 Training Loss: tensor(0.0374)\n",
      "13222 Training Loss: tensor(0.0374)\n",
      "13223 Training Loss: tensor(0.0374)\n",
      "13224 Training Loss: tensor(0.0376)\n",
      "13225 Training Loss: tensor(0.0376)\n",
      "13226 Training Loss: tensor(0.0375)\n",
      "13227 Training Loss: tensor(0.0374)\n",
      "13228 Training Loss: tensor(0.0375)\n",
      "13229 Training Loss: tensor(0.0376)\n",
      "13230 Training Loss: tensor(0.0374)\n",
      "13231 Training Loss: tensor(0.0374)\n",
      "13232 Training Loss: tensor(0.0374)\n",
      "13233 Training Loss: tensor(0.0375)\n",
      "13234 Training Loss: tensor(0.0373)\n",
      "13235 Training Loss: tensor(0.0376)\n",
      "13236 Training Loss: tensor(0.0374)\n",
      "13237 Training Loss: tensor(0.0373)\n",
      "13238 Training Loss: tensor(0.0376)\n",
      "13239 Training Loss: tensor(0.0375)\n",
      "13240 Training Loss: tensor(0.0375)\n",
      "13241 Training Loss: tensor(0.0374)\n",
      "13242 Training Loss: tensor(0.0374)\n",
      "13243 Training Loss: tensor(0.0375)\n",
      "13244 Training Loss: tensor(0.0375)\n",
      "13245 Training Loss: tensor(0.0374)\n",
      "13246 Training Loss: tensor(0.0376)\n",
      "13247 Training Loss: tensor(0.0375)\n",
      "13248 Training Loss: tensor(0.0374)\n",
      "13249 Training Loss: tensor(0.0375)\n",
      "13250 Training Loss: tensor(0.0374)\n",
      "13251 Training Loss: tensor(0.0376)\n",
      "13252 Training Loss: tensor(0.0375)\n",
      "13253 Training Loss: tensor(0.0376)\n",
      "13254 Training Loss: tensor(0.0376)\n",
      "13255 Training Loss: tensor(0.0374)\n",
      "13256 Training Loss: tensor(0.0374)\n",
      "13257 Training Loss: tensor(0.0374)\n",
      "13258 Training Loss: tensor(0.0373)\n",
      "13259 Training Loss: tensor(0.0374)\n",
      "13260 Training Loss: tensor(0.0375)\n",
      "13261 Training Loss: tensor(0.0374)\n",
      "13262 Training Loss: tensor(0.0373)\n",
      "13263 Training Loss: tensor(0.0373)\n",
      "13264 Training Loss: tensor(0.0374)\n",
      "13265 Training Loss: tensor(0.0373)\n",
      "13266 Training Loss: tensor(0.0373)\n",
      "13267 Training Loss: tensor(0.0375)\n",
      "13268 Training Loss: tensor(0.0373)\n",
      "13269 Training Loss: tensor(0.0374)\n",
      "13270 Training Loss: tensor(0.0373)\n",
      "13271 Training Loss: tensor(0.0374)\n",
      "13272 Training Loss: tensor(0.0372)\n",
      "13273 Training Loss: tensor(0.0375)\n",
      "13274 Training Loss: tensor(0.0375)\n",
      "13275 Training Loss: tensor(0.0375)\n",
      "13276 Training Loss: tensor(0.0375)\n",
      "13277 Training Loss: tensor(0.0376)\n",
      "13278 Training Loss: tensor(0.0375)\n",
      "13279 Training Loss: tensor(0.0375)\n",
      "13280 Training Loss: tensor(0.0374)\n",
      "13281 Training Loss: tensor(0.0374)\n",
      "13282 Training Loss: tensor(0.0375)\n",
      "13283 Training Loss: tensor(0.0374)\n",
      "13284 Training Loss: tensor(0.0374)\n",
      "13285 Training Loss: tensor(0.0375)\n",
      "13286 Training Loss: tensor(0.0375)\n",
      "13287 Training Loss: tensor(0.0373)\n",
      "13288 Training Loss: tensor(0.0376)\n",
      "13289 Training Loss: tensor(0.0373)\n",
      "13290 Training Loss: tensor(0.0373)\n",
      "13291 Training Loss: tensor(0.0374)\n",
      "13292 Training Loss: tensor(0.0374)\n",
      "13293 Training Loss: tensor(0.0374)\n",
      "13294 Training Loss: tensor(0.0375)\n",
      "13295 Training Loss: tensor(0.0373)\n",
      "13296 Training Loss: tensor(0.0373)\n",
      "13297 Training Loss: tensor(0.0374)\n",
      "13298 Training Loss: tensor(0.0374)\n",
      "13299 Training Loss: tensor(0.0374)\n",
      "13300 Training Loss: tensor(0.0374)\n",
      "13301 Training Loss: tensor(0.0374)\n",
      "13302 Training Loss: tensor(0.0374)\n",
      "13303 Training Loss: tensor(0.0374)\n",
      "13304 Training Loss: tensor(0.0374)\n",
      "13305 Training Loss: tensor(0.0373)\n",
      "13306 Training Loss: tensor(0.0374)\n",
      "13307 Training Loss: tensor(0.0373)\n",
      "13308 Training Loss: tensor(0.0374)\n",
      "13309 Training Loss: tensor(0.0374)\n",
      "13310 Training Loss: tensor(0.0374)\n",
      "13311 Training Loss: tensor(0.0373)\n",
      "13312 Training Loss: tensor(0.0373)\n",
      "13313 Training Loss: tensor(0.0373)\n",
      "13314 Training Loss: tensor(0.0374)\n",
      "13315 Training Loss: tensor(0.0375)\n",
      "13316 Training Loss: tensor(0.0373)\n",
      "13317 Training Loss: tensor(0.0374)\n",
      "13318 Training Loss: tensor(0.0371)\n",
      "13319 Training Loss: tensor(0.0373)\n",
      "13320 Training Loss: tensor(0.0372)\n",
      "13321 Training Loss: tensor(0.0372)\n",
      "13322 Training Loss: tensor(0.0373)\n",
      "13323 Training Loss: tensor(0.0371)\n",
      "13324 Training Loss: tensor(0.0375)\n",
      "13325 Training Loss: tensor(0.0375)\n",
      "13326 Training Loss: tensor(0.0374)\n",
      "13327 Training Loss: tensor(0.0376)\n",
      "13328 Training Loss: tensor(0.0375)\n",
      "13329 Training Loss: tensor(0.0374)\n",
      "13330 Training Loss: tensor(0.0373)\n",
      "13331 Training Loss: tensor(0.0373)\n",
      "13332 Training Loss: tensor(0.0374)\n",
      "13333 Training Loss: tensor(0.0374)\n",
      "13334 Training Loss: tensor(0.0373)\n",
      "13335 Training Loss: tensor(0.0374)\n",
      "13336 Training Loss: tensor(0.0373)\n",
      "13337 Training Loss: tensor(0.0373)\n",
      "13338 Training Loss: tensor(0.0372)\n",
      "13339 Training Loss: tensor(0.0372)\n",
      "13340 Training Loss: tensor(0.0371)\n",
      "13341 Training Loss: tensor(0.0372)\n",
      "13342 Training Loss: tensor(0.0373)\n",
      "13343 Training Loss: tensor(0.0372)\n",
      "13344 Training Loss: tensor(0.0374)\n",
      "13345 Training Loss: tensor(0.0372)\n",
      "13346 Training Loss: tensor(0.0372)\n",
      "13347 Training Loss: tensor(0.0373)\n",
      "13348 Training Loss: tensor(0.0373)\n",
      "13349 Training Loss: tensor(0.0373)\n",
      "13350 Training Loss: tensor(0.0371)\n",
      "13351 Training Loss: tensor(0.0374)\n",
      "13352 Training Loss: tensor(0.0373)\n",
      "13353 Training Loss: tensor(0.0373)\n",
      "13354 Training Loss: tensor(0.0374)\n",
      "13355 Training Loss: tensor(0.0371)\n",
      "13356 Training Loss: tensor(0.0373)\n",
      "13357 Training Loss: tensor(0.0371)\n",
      "13358 Training Loss: tensor(0.0373)\n",
      "13359 Training Loss: tensor(0.0371)\n",
      "13360 Training Loss: tensor(0.0374)\n",
      "13361 Training Loss: tensor(0.0374)\n",
      "13362 Training Loss: tensor(0.0374)\n",
      "13363 Training Loss: tensor(0.0372)\n",
      "13364 Training Loss: tensor(0.0374)\n",
      "13365 Training Loss: tensor(0.0374)\n",
      "13366 Training Loss: tensor(0.0373)\n",
      "13367 Training Loss: tensor(0.0373)\n",
      "13368 Training Loss: tensor(0.0373)\n",
      "13369 Training Loss: tensor(0.0372)\n",
      "13370 Training Loss: tensor(0.0372)\n",
      "13371 Training Loss: tensor(0.0371)\n",
      "13372 Training Loss: tensor(0.0372)\n",
      "13373 Training Loss: tensor(0.0372)\n",
      "13374 Training Loss: tensor(0.0373)\n",
      "13375 Training Loss: tensor(0.0372)\n",
      "13376 Training Loss: tensor(0.0375)\n",
      "13377 Training Loss: tensor(0.0372)\n",
      "13378 Training Loss: tensor(0.0372)\n",
      "13379 Training Loss: tensor(0.0374)\n",
      "13380 Training Loss: tensor(0.0370)\n",
      "13381 Training Loss: tensor(0.0374)\n",
      "13382 Training Loss: tensor(0.0371)\n",
      "13383 Training Loss: tensor(0.0373)\n",
      "13384 Training Loss: tensor(0.0371)\n",
      "13385 Training Loss: tensor(0.0373)\n",
      "13386 Training Loss: tensor(0.0371)\n",
      "13387 Training Loss: tensor(0.0373)\n",
      "13388 Training Loss: tensor(0.0373)\n",
      "13389 Training Loss: tensor(0.0371)\n",
      "13390 Training Loss: tensor(0.0372)\n",
      "13391 Training Loss: tensor(0.0373)\n",
      "13392 Training Loss: tensor(0.0372)\n",
      "13393 Training Loss: tensor(0.0374)\n",
      "13394 Training Loss: tensor(0.0372)\n",
      "13395 Training Loss: tensor(0.0372)\n",
      "13396 Training Loss: tensor(0.0371)\n",
      "13397 Training Loss: tensor(0.0372)\n",
      "13398 Training Loss: tensor(0.0371)\n",
      "13399 Training Loss: tensor(0.0371)\n",
      "13400 Training Loss: tensor(0.0372)\n",
      "13401 Training Loss: tensor(0.0373)\n",
      "13402 Training Loss: tensor(0.0371)\n",
      "13403 Training Loss: tensor(0.0375)\n",
      "13404 Training Loss: tensor(0.0373)\n",
      "13405 Training Loss: tensor(0.0373)\n",
      "13406 Training Loss: tensor(0.0371)\n",
      "13407 Training Loss: tensor(0.0372)\n",
      "13408 Training Loss: tensor(0.0371)\n",
      "13409 Training Loss: tensor(0.0374)\n",
      "13410 Training Loss: tensor(0.0373)\n",
      "13411 Training Loss: tensor(0.0371)\n",
      "13412 Training Loss: tensor(0.0373)\n",
      "13413 Training Loss: tensor(0.0374)\n",
      "13414 Training Loss: tensor(0.0373)\n",
      "13415 Training Loss: tensor(0.0372)\n",
      "13416 Training Loss: tensor(0.0373)\n",
      "13417 Training Loss: tensor(0.0372)\n",
      "13418 Training Loss: tensor(0.0372)\n",
      "13419 Training Loss: tensor(0.0371)\n",
      "13420 Training Loss: tensor(0.0374)\n",
      "13421 Training Loss: tensor(0.0373)\n",
      "13422 Training Loss: tensor(0.0371)\n",
      "13423 Training Loss: tensor(0.0373)\n",
      "13424 Training Loss: tensor(0.0372)\n",
      "13425 Training Loss: tensor(0.0372)\n",
      "13426 Training Loss: tensor(0.0371)\n",
      "13427 Training Loss: tensor(0.0372)\n",
      "13428 Training Loss: tensor(0.0371)\n",
      "13429 Training Loss: tensor(0.0372)\n",
      "13430 Training Loss: tensor(0.0371)\n",
      "13431 Training Loss: tensor(0.0371)\n",
      "13432 Training Loss: tensor(0.0373)\n",
      "13433 Training Loss: tensor(0.0373)\n",
      "13434 Training Loss: tensor(0.0372)\n",
      "13435 Training Loss: tensor(0.0372)\n",
      "13436 Training Loss: tensor(0.0370)\n",
      "13437 Training Loss: tensor(0.0372)\n",
      "13438 Training Loss: tensor(0.0370)\n",
      "13439 Training Loss: tensor(0.0371)\n",
      "13440 Training Loss: tensor(0.0371)\n",
      "13441 Training Loss: tensor(0.0373)\n",
      "13442 Training Loss: tensor(0.0371)\n",
      "13443 Training Loss: tensor(0.0373)\n",
      "13444 Training Loss: tensor(0.0371)\n",
      "13445 Training Loss: tensor(0.0371)\n",
      "13446 Training Loss: tensor(0.0371)\n",
      "13447 Training Loss: tensor(0.0371)\n",
      "13448 Training Loss: tensor(0.0370)\n",
      "13449 Training Loss: tensor(0.0369)\n",
      "13450 Training Loss: tensor(0.0370)\n",
      "13451 Training Loss: tensor(0.0372)\n",
      "13452 Training Loss: tensor(0.0373)\n",
      "13453 Training Loss: tensor(0.0371)\n",
      "13454 Training Loss: tensor(0.0374)\n",
      "13455 Training Loss: tensor(0.0369)\n",
      "13456 Training Loss: tensor(0.0371)\n",
      "13457 Training Loss: tensor(0.0371)\n",
      "13458 Training Loss: tensor(0.0372)\n",
      "13459 Training Loss: tensor(0.0372)\n",
      "13460 Training Loss: tensor(0.0371)\n",
      "13461 Training Loss: tensor(0.0370)\n",
      "13462 Training Loss: tensor(0.0371)\n",
      "13463 Training Loss: tensor(0.0370)\n",
      "13464 Training Loss: tensor(0.0370)\n",
      "13465 Training Loss: tensor(0.0369)\n",
      "13466 Training Loss: tensor(0.0370)\n",
      "13467 Training Loss: tensor(0.0371)\n",
      "13468 Training Loss: tensor(0.0375)\n",
      "13469 Training Loss: tensor(0.0370)\n",
      "13470 Training Loss: tensor(0.0373)\n",
      "13471 Training Loss: tensor(0.0371)\n",
      "13472 Training Loss: tensor(0.0371)\n",
      "13473 Training Loss: tensor(0.0371)\n",
      "13474 Training Loss: tensor(0.0371)\n",
      "13475 Training Loss: tensor(0.0371)\n",
      "13476 Training Loss: tensor(0.0371)\n",
      "13477 Training Loss: tensor(0.0372)\n",
      "13478 Training Loss: tensor(0.0373)\n",
      "13479 Training Loss: tensor(0.0370)\n",
      "13480 Training Loss: tensor(0.0372)\n",
      "13481 Training Loss: tensor(0.0369)\n",
      "13482 Training Loss: tensor(0.0370)\n",
      "13483 Training Loss: tensor(0.0371)\n",
      "13484 Training Loss: tensor(0.0370)\n",
      "13485 Training Loss: tensor(0.0370)\n",
      "13486 Training Loss: tensor(0.0370)\n",
      "13487 Training Loss: tensor(0.0372)\n",
      "13488 Training Loss: tensor(0.0369)\n",
      "13489 Training Loss: tensor(0.0372)\n",
      "13490 Training Loss: tensor(0.0369)\n",
      "13491 Training Loss: tensor(0.0370)\n",
      "13492 Training Loss: tensor(0.0371)\n",
      "13493 Training Loss: tensor(0.0371)\n",
      "13494 Training Loss: tensor(0.0369)\n",
      "13495 Training Loss: tensor(0.0372)\n",
      "13496 Training Loss: tensor(0.0370)\n",
      "13497 Training Loss: tensor(0.0370)\n",
      "13498 Training Loss: tensor(0.0371)\n",
      "13499 Training Loss: tensor(0.0369)\n",
      "13500 Training Loss: tensor(0.0370)\n",
      "13501 Training Loss: tensor(0.0371)\n",
      "13502 Training Loss: tensor(0.0372)\n",
      "13503 Training Loss: tensor(0.0370)\n",
      "13504 Training Loss: tensor(0.0370)\n",
      "13505 Training Loss: tensor(0.0368)\n",
      "13506 Training Loss: tensor(0.0370)\n",
      "13507 Training Loss: tensor(0.0370)\n",
      "13508 Training Loss: tensor(0.0370)\n",
      "13509 Training Loss: tensor(0.0370)\n",
      "13510 Training Loss: tensor(0.0370)\n",
      "13511 Training Loss: tensor(0.0374)\n",
      "13512 Training Loss: tensor(0.0369)\n",
      "13513 Training Loss: tensor(0.0372)\n",
      "13514 Training Loss: tensor(0.0370)\n",
      "13515 Training Loss: tensor(0.0371)\n",
      "13516 Training Loss: tensor(0.0371)\n",
      "13517 Training Loss: tensor(0.0370)\n",
      "13518 Training Loss: tensor(0.0370)\n",
      "13519 Training Loss: tensor(0.0370)\n",
      "13520 Training Loss: tensor(0.0369)\n",
      "13521 Training Loss: tensor(0.0371)\n",
      "13522 Training Loss: tensor(0.0370)\n",
      "13523 Training Loss: tensor(0.0371)\n",
      "13524 Training Loss: tensor(0.0370)\n",
      "13525 Training Loss: tensor(0.0369)\n",
      "13526 Training Loss: tensor(0.0370)\n",
      "13527 Training Loss: tensor(0.0368)\n",
      "13528 Training Loss: tensor(0.0369)\n",
      "13529 Training Loss: tensor(0.0370)\n",
      "13530 Training Loss: tensor(0.0372)\n",
      "13531 Training Loss: tensor(0.0368)\n",
      "13532 Training Loss: tensor(0.0371)\n",
      "13533 Training Loss: tensor(0.0370)\n",
      "13534 Training Loss: tensor(0.0371)\n",
      "13535 Training Loss: tensor(0.0370)\n",
      "13536 Training Loss: tensor(0.0369)\n",
      "13537 Training Loss: tensor(0.0370)\n",
      "13538 Training Loss: tensor(0.0370)\n",
      "13539 Training Loss: tensor(0.0370)\n",
      "13540 Training Loss: tensor(0.0366)\n",
      "13541 Training Loss: tensor(0.0368)\n",
      "13542 Training Loss: tensor(0.0370)\n",
      "13543 Training Loss: tensor(0.0369)\n",
      "13544 Training Loss: tensor(0.0370)\n",
      "13545 Training Loss: tensor(0.0371)\n",
      "13546 Training Loss: tensor(0.0369)\n",
      "13547 Training Loss: tensor(0.0369)\n",
      "13548 Training Loss: tensor(0.0373)\n",
      "13549 Training Loss: tensor(0.0369)\n",
      "13550 Training Loss: tensor(0.0370)\n",
      "13551 Training Loss: tensor(0.0370)\n",
      "13552 Training Loss: tensor(0.0368)\n",
      "13553 Training Loss: tensor(0.0370)\n",
      "13554 Training Loss: tensor(0.0370)\n",
      "13555 Training Loss: tensor(0.0370)\n",
      "13556 Training Loss: tensor(0.0372)\n",
      "13557 Training Loss: tensor(0.0370)\n",
      "13558 Training Loss: tensor(0.0372)\n",
      "13559 Training Loss: tensor(0.0369)\n",
      "13560 Training Loss: tensor(0.0370)\n",
      "13561 Training Loss: tensor(0.0370)\n",
      "13562 Training Loss: tensor(0.0370)\n",
      "13563 Training Loss: tensor(0.0369)\n",
      "13564 Training Loss: tensor(0.0369)\n",
      "13565 Training Loss: tensor(0.0369)\n",
      "13566 Training Loss: tensor(0.0370)\n",
      "13567 Training Loss: tensor(0.0369)\n",
      "13568 Training Loss: tensor(0.0367)\n",
      "13569 Training Loss: tensor(0.0370)\n",
      "13570 Training Loss: tensor(0.0369)\n",
      "13571 Training Loss: tensor(0.0368)\n",
      "13572 Training Loss: tensor(0.0372)\n",
      "13573 Training Loss: tensor(0.0368)\n",
      "13574 Training Loss: tensor(0.0369)\n",
      "13575 Training Loss: tensor(0.0369)\n",
      "13576 Training Loss: tensor(0.0370)\n",
      "13577 Training Loss: tensor(0.0367)\n",
      "13578 Training Loss: tensor(0.0368)\n",
      "13579 Training Loss: tensor(0.0369)\n",
      "13580 Training Loss: tensor(0.0369)\n",
      "13581 Training Loss: tensor(0.0372)\n",
      "13582 Training Loss: tensor(0.0369)\n",
      "13583 Training Loss: tensor(0.0369)\n",
      "13584 Training Loss: tensor(0.0370)\n",
      "13585 Training Loss: tensor(0.0369)\n",
      "13586 Training Loss: tensor(0.0369)\n",
      "13587 Training Loss: tensor(0.0371)\n",
      "13588 Training Loss: tensor(0.0369)\n",
      "13589 Training Loss: tensor(0.0369)\n",
      "13590 Training Loss: tensor(0.0368)\n",
      "13591 Training Loss: tensor(0.0371)\n",
      "13592 Training Loss: tensor(0.0369)\n",
      "13593 Training Loss: tensor(0.0368)\n",
      "13594 Training Loss: tensor(0.0369)\n",
      "13595 Training Loss: tensor(0.0367)\n",
      "13596 Training Loss: tensor(0.0369)\n",
      "13597 Training Loss: tensor(0.0369)\n",
      "13598 Training Loss: tensor(0.0368)\n",
      "13599 Training Loss: tensor(0.0371)\n",
      "13600 Training Loss: tensor(0.0370)\n",
      "13601 Training Loss: tensor(0.0368)\n",
      "13602 Training Loss: tensor(0.0368)\n",
      "13603 Training Loss: tensor(0.0369)\n",
      "13604 Training Loss: tensor(0.0369)\n",
      "13605 Training Loss: tensor(0.0369)\n",
      "13606 Training Loss: tensor(0.0369)\n",
      "13607 Training Loss: tensor(0.0369)\n",
      "13608 Training Loss: tensor(0.0369)\n",
      "13609 Training Loss: tensor(0.0368)\n",
      "13610 Training Loss: tensor(0.0370)\n",
      "13611 Training Loss: tensor(0.0370)\n",
      "13612 Training Loss: tensor(0.0371)\n",
      "13613 Training Loss: tensor(0.0370)\n",
      "13614 Training Loss: tensor(0.0369)\n",
      "13615 Training Loss: tensor(0.0368)\n",
      "13616 Training Loss: tensor(0.0368)\n",
      "13617 Training Loss: tensor(0.0371)\n",
      "13618 Training Loss: tensor(0.0370)\n",
      "13619 Training Loss: tensor(0.0368)\n",
      "13620 Training Loss: tensor(0.0367)\n",
      "13621 Training Loss: tensor(0.0370)\n",
      "13622 Training Loss: tensor(0.0368)\n",
      "13623 Training Loss: tensor(0.0368)\n",
      "13624 Training Loss: tensor(0.0368)\n",
      "13625 Training Loss: tensor(0.0368)\n",
      "13626 Training Loss: tensor(0.0369)\n",
      "13627 Training Loss: tensor(0.0369)\n",
      "13628 Training Loss: tensor(0.0368)\n",
      "13629 Training Loss: tensor(0.0368)\n",
      "13630 Training Loss: tensor(0.0367)\n",
      "13631 Training Loss: tensor(0.0368)\n",
      "13632 Training Loss: tensor(0.0370)\n",
      "13633 Training Loss: tensor(0.0367)\n",
      "13634 Training Loss: tensor(0.0370)\n",
      "13635 Training Loss: tensor(0.0368)\n",
      "13636 Training Loss: tensor(0.0368)\n",
      "13637 Training Loss: tensor(0.0369)\n",
      "13638 Training Loss: tensor(0.0367)\n",
      "13639 Training Loss: tensor(0.0368)\n",
      "13640 Training Loss: tensor(0.0369)\n",
      "13641 Training Loss: tensor(0.0366)\n",
      "13642 Training Loss: tensor(0.0368)\n",
      "13643 Training Loss: tensor(0.0368)\n",
      "13644 Training Loss: tensor(0.0371)\n",
      "13645 Training Loss: tensor(0.0367)\n",
      "13646 Training Loss: tensor(0.0369)\n",
      "13647 Training Loss: tensor(0.0368)\n",
      "13648 Training Loss: tensor(0.0367)\n",
      "13649 Training Loss: tensor(0.0369)\n",
      "13650 Training Loss: tensor(0.0368)\n",
      "13651 Training Loss: tensor(0.0367)\n",
      "13652 Training Loss: tensor(0.0368)\n",
      "13653 Training Loss: tensor(0.0367)\n",
      "13654 Training Loss: tensor(0.0368)\n",
      "13655 Training Loss: tensor(0.0370)\n",
      "13656 Training Loss: tensor(0.0369)\n",
      "13657 Training Loss: tensor(0.0368)\n",
      "13658 Training Loss: tensor(0.0369)\n",
      "13659 Training Loss: tensor(0.0369)\n",
      "13660 Training Loss: tensor(0.0369)\n",
      "13661 Training Loss: tensor(0.0367)\n",
      "13662 Training Loss: tensor(0.0368)\n",
      "13663 Training Loss: tensor(0.0369)\n",
      "13664 Training Loss: tensor(0.0368)\n",
      "13665 Training Loss: tensor(0.0366)\n",
      "13666 Training Loss: tensor(0.0369)\n",
      "13667 Training Loss: tensor(0.0367)\n",
      "13668 Training Loss: tensor(0.0366)\n",
      "13669 Training Loss: tensor(0.0369)\n",
      "13670 Training Loss: tensor(0.0369)\n",
      "13671 Training Loss: tensor(0.0368)\n",
      "13672 Training Loss: tensor(0.0368)\n",
      "13673 Training Loss: tensor(0.0367)\n",
      "13674 Training Loss: tensor(0.0367)\n",
      "13675 Training Loss: tensor(0.0367)\n",
      "13676 Training Loss: tensor(0.0368)\n",
      "13677 Training Loss: tensor(0.0369)\n",
      "13678 Training Loss: tensor(0.0366)\n",
      "13679 Training Loss: tensor(0.0368)\n",
      "13680 Training Loss: tensor(0.0368)\n",
      "13681 Training Loss: tensor(0.0367)\n",
      "13682 Training Loss: tensor(0.0366)\n",
      "13683 Training Loss: tensor(0.0368)\n",
      "13684 Training Loss: tensor(0.0366)\n",
      "13685 Training Loss: tensor(0.0367)\n",
      "13686 Training Loss: tensor(0.0367)\n",
      "13687 Training Loss: tensor(0.0370)\n",
      "13688 Training Loss: tensor(0.0368)\n",
      "13689 Training Loss: tensor(0.0369)\n",
      "13690 Training Loss: tensor(0.0368)\n",
      "13691 Training Loss: tensor(0.0367)\n",
      "13692 Training Loss: tensor(0.0366)\n",
      "13693 Training Loss: tensor(0.0366)\n",
      "13694 Training Loss: tensor(0.0368)\n",
      "13695 Training Loss: tensor(0.0367)\n",
      "13696 Training Loss: tensor(0.0368)\n",
      "13697 Training Loss: tensor(0.0368)\n",
      "13698 Training Loss: tensor(0.0368)\n",
      "13699 Training Loss: tensor(0.0367)\n",
      "13700 Training Loss: tensor(0.0367)\n",
      "13701 Training Loss: tensor(0.0368)\n",
      "13702 Training Loss: tensor(0.0368)\n",
      "13703 Training Loss: tensor(0.0367)\n",
      "13704 Training Loss: tensor(0.0365)\n",
      "13705 Training Loss: tensor(0.0367)\n",
      "13706 Training Loss: tensor(0.0368)\n",
      "13707 Training Loss: tensor(0.0368)\n",
      "13708 Training Loss: tensor(0.0366)\n",
      "13709 Training Loss: tensor(0.0367)\n",
      "13710 Training Loss: tensor(0.0367)\n",
      "13711 Training Loss: tensor(0.0371)\n",
      "13712 Training Loss: tensor(0.0369)\n",
      "13713 Training Loss: tensor(0.0367)\n",
      "13714 Training Loss: tensor(0.0368)\n",
      "13715 Training Loss: tensor(0.0368)\n",
      "13716 Training Loss: tensor(0.0367)\n",
      "13717 Training Loss: tensor(0.0366)\n",
      "13718 Training Loss: tensor(0.0368)\n",
      "13719 Training Loss: tensor(0.0366)\n",
      "13720 Training Loss: tensor(0.0370)\n",
      "13721 Training Loss: tensor(0.0367)\n",
      "13722 Training Loss: tensor(0.0368)\n",
      "13723 Training Loss: tensor(0.0367)\n",
      "13724 Training Loss: tensor(0.0367)\n",
      "13725 Training Loss: tensor(0.0367)\n",
      "13726 Training Loss: tensor(0.0367)\n",
      "13727 Training Loss: tensor(0.0367)\n",
      "13728 Training Loss: tensor(0.0369)\n",
      "13729 Training Loss: tensor(0.0366)\n",
      "13730 Training Loss: tensor(0.0367)\n",
      "13731 Training Loss: tensor(0.0368)\n",
      "13732 Training Loss: tensor(0.0366)\n",
      "13733 Training Loss: tensor(0.0367)\n",
      "13734 Training Loss: tensor(0.0366)\n",
      "13735 Training Loss: tensor(0.0366)\n",
      "13736 Training Loss: tensor(0.0366)\n",
      "13737 Training Loss: tensor(0.0370)\n",
      "13738 Training Loss: tensor(0.0367)\n",
      "13739 Training Loss: tensor(0.0369)\n",
      "13740 Training Loss: tensor(0.0368)\n",
      "13741 Training Loss: tensor(0.0365)\n",
      "13742 Training Loss: tensor(0.0365)\n",
      "13743 Training Loss: tensor(0.0364)\n",
      "13744 Training Loss: tensor(0.0366)\n",
      "13745 Training Loss: tensor(0.0367)\n",
      "13746 Training Loss: tensor(0.0366)\n",
      "13747 Training Loss: tensor(0.0365)\n",
      "13748 Training Loss: tensor(0.0366)\n",
      "13749 Training Loss: tensor(0.0365)\n",
      "13750 Training Loss: tensor(0.0366)\n",
      "13751 Training Loss: tensor(0.0366)\n",
      "13752 Training Loss: tensor(0.0366)\n",
      "13753 Training Loss: tensor(0.0368)\n",
      "13754 Training Loss: tensor(0.0367)\n",
      "13755 Training Loss: tensor(0.0368)\n",
      "13756 Training Loss: tensor(0.0368)\n",
      "13757 Training Loss: tensor(0.0366)\n",
      "13758 Training Loss: tensor(0.0366)\n",
      "13759 Training Loss: tensor(0.0366)\n",
      "13760 Training Loss: tensor(0.0370)\n",
      "13761 Training Loss: tensor(0.0365)\n",
      "13762 Training Loss: tensor(0.0365)\n",
      "13763 Training Loss: tensor(0.0366)\n",
      "13764 Training Loss: tensor(0.0366)\n",
      "13765 Training Loss: tensor(0.0366)\n",
      "13766 Training Loss: tensor(0.0364)\n",
      "13767 Training Loss: tensor(0.0367)\n",
      "13768 Training Loss: tensor(0.0367)\n",
      "13769 Training Loss: tensor(0.0367)\n",
      "13770 Training Loss: tensor(0.0367)\n",
      "13771 Training Loss: tensor(0.0365)\n",
      "13772 Training Loss: tensor(0.0365)\n",
      "13773 Training Loss: tensor(0.0365)\n",
      "13774 Training Loss: tensor(0.0368)\n",
      "13775 Training Loss: tensor(0.0365)\n",
      "13776 Training Loss: tensor(0.0367)\n",
      "13777 Training Loss: tensor(0.0366)\n",
      "13778 Training Loss: tensor(0.0366)\n",
      "13779 Training Loss: tensor(0.0364)\n",
      "13780 Training Loss: tensor(0.0366)\n",
      "13781 Training Loss: tensor(0.0366)\n",
      "13782 Training Loss: tensor(0.0366)\n",
      "13783 Training Loss: tensor(0.0369)\n",
      "13784 Training Loss: tensor(0.0365)\n",
      "13785 Training Loss: tensor(0.0365)\n",
      "13786 Training Loss: tensor(0.0368)\n",
      "13787 Training Loss: tensor(0.0366)\n",
      "13788 Training Loss: tensor(0.0364)\n",
      "13789 Training Loss: tensor(0.0365)\n",
      "13790 Training Loss: tensor(0.0366)\n",
      "13791 Training Loss: tensor(0.0364)\n",
      "13792 Training Loss: tensor(0.0366)\n",
      "13793 Training Loss: tensor(0.0365)\n",
      "13794 Training Loss: tensor(0.0365)\n",
      "13795 Training Loss: tensor(0.0365)\n",
      "13796 Training Loss: tensor(0.0367)\n",
      "13797 Training Loss: tensor(0.0368)\n",
      "13798 Training Loss: tensor(0.0365)\n",
      "13799 Training Loss: tensor(0.0365)\n",
      "13800 Training Loss: tensor(0.0366)\n",
      "13801 Training Loss: tensor(0.0366)\n",
      "13802 Training Loss: tensor(0.0366)\n",
      "13803 Training Loss: tensor(0.0365)\n",
      "13804 Training Loss: tensor(0.0364)\n",
      "13805 Training Loss: tensor(0.0364)\n",
      "13806 Training Loss: tensor(0.0366)\n",
      "13807 Training Loss: tensor(0.0364)\n",
      "13808 Training Loss: tensor(0.0367)\n",
      "13809 Training Loss: tensor(0.0365)\n",
      "13810 Training Loss: tensor(0.0366)\n",
      "13811 Training Loss: tensor(0.0365)\n",
      "13812 Training Loss: tensor(0.0366)\n",
      "13813 Training Loss: tensor(0.0367)\n",
      "13814 Training Loss: tensor(0.0367)\n",
      "13815 Training Loss: tensor(0.0368)\n",
      "13816 Training Loss: tensor(0.0364)\n",
      "13817 Training Loss: tensor(0.0365)\n",
      "13818 Training Loss: tensor(0.0365)\n",
      "13819 Training Loss: tensor(0.0368)\n",
      "13820 Training Loss: tensor(0.0365)\n",
      "13821 Training Loss: tensor(0.0365)\n",
      "13822 Training Loss: tensor(0.0365)\n",
      "13823 Training Loss: tensor(0.0365)\n",
      "13824 Training Loss: tensor(0.0365)\n",
      "13825 Training Loss: tensor(0.0367)\n",
      "13826 Training Loss: tensor(0.0366)\n",
      "13827 Training Loss: tensor(0.0365)\n",
      "13828 Training Loss: tensor(0.0365)\n",
      "13829 Training Loss: tensor(0.0367)\n",
      "13830 Training Loss: tensor(0.0364)\n",
      "13831 Training Loss: tensor(0.0365)\n",
      "13832 Training Loss: tensor(0.0364)\n",
      "13833 Training Loss: tensor(0.0365)\n",
      "13834 Training Loss: tensor(0.0366)\n",
      "13835 Training Loss: tensor(0.0365)\n",
      "13836 Training Loss: tensor(0.0365)\n",
      "13837 Training Loss: tensor(0.0365)\n",
      "13838 Training Loss: tensor(0.0365)\n",
      "13839 Training Loss: tensor(0.0364)\n",
      "13840 Training Loss: tensor(0.0365)\n",
      "13841 Training Loss: tensor(0.0364)\n",
      "13842 Training Loss: tensor(0.0367)\n",
      "13843 Training Loss: tensor(0.0365)\n",
      "13844 Training Loss: tensor(0.0364)\n",
      "13845 Training Loss: tensor(0.0364)\n",
      "13846 Training Loss: tensor(0.0365)\n",
      "13847 Training Loss: tensor(0.0364)\n",
      "13848 Training Loss: tensor(0.0369)\n",
      "13849 Training Loss: tensor(0.0365)\n",
      "13850 Training Loss: tensor(0.0366)\n",
      "13851 Training Loss: tensor(0.0367)\n",
      "13852 Training Loss: tensor(0.0364)\n",
      "13853 Training Loss: tensor(0.0365)\n",
      "13854 Training Loss: tensor(0.0364)\n",
      "13855 Training Loss: tensor(0.0365)\n",
      "13856 Training Loss: tensor(0.0365)\n",
      "13857 Training Loss: tensor(0.0365)\n",
      "13858 Training Loss: tensor(0.0365)\n",
      "13859 Training Loss: tensor(0.0364)\n",
      "13860 Training Loss: tensor(0.0365)\n",
      "13861 Training Loss: tensor(0.0365)\n",
      "13862 Training Loss: tensor(0.0365)\n",
      "13863 Training Loss: tensor(0.0366)\n",
      "13864 Training Loss: tensor(0.0365)\n",
      "13865 Training Loss: tensor(0.0365)\n",
      "13866 Training Loss: tensor(0.0363)\n",
      "13867 Training Loss: tensor(0.0365)\n",
      "13868 Training Loss: tensor(0.0366)\n",
      "13869 Training Loss: tensor(0.0363)\n",
      "13870 Training Loss: tensor(0.0364)\n",
      "13871 Training Loss: tensor(0.0364)\n",
      "13872 Training Loss: tensor(0.0365)\n",
      "13873 Training Loss: tensor(0.0364)\n",
      "13874 Training Loss: tensor(0.0365)\n",
      "13875 Training Loss: tensor(0.0364)\n",
      "13876 Training Loss: tensor(0.0365)\n",
      "13877 Training Loss: tensor(0.0365)\n",
      "13878 Training Loss: tensor(0.0363)\n",
      "13879 Training Loss: tensor(0.0366)\n",
      "13880 Training Loss: tensor(0.0363)\n",
      "13881 Training Loss: tensor(0.0365)\n",
      "13882 Training Loss: tensor(0.0363)\n",
      "13883 Training Loss: tensor(0.0362)\n",
      "13884 Training Loss: tensor(0.0367)\n",
      "13885 Training Loss: tensor(0.0364)\n",
      "13886 Training Loss: tensor(0.0364)\n",
      "13887 Training Loss: tensor(0.0365)\n",
      "13888 Training Loss: tensor(0.0365)\n",
      "13889 Training Loss: tensor(0.0362)\n",
      "13890 Training Loss: tensor(0.0365)\n",
      "13891 Training Loss: tensor(0.0365)\n",
      "13892 Training Loss: tensor(0.0364)\n",
      "13893 Training Loss: tensor(0.0363)\n",
      "13894 Training Loss: tensor(0.0366)\n",
      "13895 Training Loss: tensor(0.0363)\n",
      "13896 Training Loss: tensor(0.0364)\n",
      "13897 Training Loss: tensor(0.0367)\n",
      "13898 Training Loss: tensor(0.0365)\n",
      "13899 Training Loss: tensor(0.0364)\n",
      "13900 Training Loss: tensor(0.0363)\n",
      "13901 Training Loss: tensor(0.0363)\n",
      "13902 Training Loss: tensor(0.0362)\n",
      "13903 Training Loss: tensor(0.0364)\n",
      "13904 Training Loss: tensor(0.0365)\n",
      "13905 Training Loss: tensor(0.0364)\n",
      "13906 Training Loss: tensor(0.0364)\n",
      "13907 Training Loss: tensor(0.0365)\n",
      "13908 Training Loss: tensor(0.0364)\n",
      "13909 Training Loss: tensor(0.0363)\n",
      "13910 Training Loss: tensor(0.0364)\n",
      "13911 Training Loss: tensor(0.0365)\n",
      "13912 Training Loss: tensor(0.0365)\n",
      "13913 Training Loss: tensor(0.0364)\n",
      "13914 Training Loss: tensor(0.0365)\n",
      "13915 Training Loss: tensor(0.0365)\n",
      "13916 Training Loss: tensor(0.0364)\n",
      "13917 Training Loss: tensor(0.0363)\n",
      "13918 Training Loss: tensor(0.0363)\n",
      "13919 Training Loss: tensor(0.0365)\n",
      "13920 Training Loss: tensor(0.0363)\n",
      "13921 Training Loss: tensor(0.0362)\n",
      "13922 Training Loss: tensor(0.0363)\n",
      "13923 Training Loss: tensor(0.0363)\n",
      "13924 Training Loss: tensor(0.0363)\n",
      "13925 Training Loss: tensor(0.0365)\n",
      "13926 Training Loss: tensor(0.0364)\n",
      "13927 Training Loss: tensor(0.0364)\n",
      "13928 Training Loss: tensor(0.0363)\n",
      "13929 Training Loss: tensor(0.0364)\n",
      "13930 Training Loss: tensor(0.0363)\n",
      "13931 Training Loss: tensor(0.0363)\n",
      "13932 Training Loss: tensor(0.0361)\n",
      "13933 Training Loss: tensor(0.0363)\n",
      "13934 Training Loss: tensor(0.0363)\n",
      "13935 Training Loss: tensor(0.0364)\n",
      "13936 Training Loss: tensor(0.0363)\n",
      "13937 Training Loss: tensor(0.0366)\n",
      "13938 Training Loss: tensor(0.0364)\n",
      "13939 Training Loss: tensor(0.0362)\n",
      "13940 Training Loss: tensor(0.0368)\n",
      "13941 Training Loss: tensor(0.0363)\n",
      "13942 Training Loss: tensor(0.0366)\n",
      "13943 Training Loss: tensor(0.0363)\n",
      "13944 Training Loss: tensor(0.0362)\n",
      "13945 Training Loss: tensor(0.0364)\n",
      "13946 Training Loss: tensor(0.0364)\n",
      "13947 Training Loss: tensor(0.0362)\n",
      "13948 Training Loss: tensor(0.0363)\n",
      "13949 Training Loss: tensor(0.0363)\n",
      "13950 Training Loss: tensor(0.0363)\n",
      "13951 Training Loss: tensor(0.0363)\n",
      "13952 Training Loss: tensor(0.0362)\n",
      "13953 Training Loss: tensor(0.0362)\n",
      "13954 Training Loss: tensor(0.0364)\n",
      "13955 Training Loss: tensor(0.0362)\n",
      "13956 Training Loss: tensor(0.0363)\n",
      "13957 Training Loss: tensor(0.0363)\n",
      "13958 Training Loss: tensor(0.0363)\n",
      "13959 Training Loss: tensor(0.0362)\n",
      "13960 Training Loss: tensor(0.0361)\n",
      "13961 Training Loss: tensor(0.0362)\n",
      "13962 Training Loss: tensor(0.0364)\n",
      "13963 Training Loss: tensor(0.0360)\n",
      "13964 Training Loss: tensor(0.0363)\n",
      "13965 Training Loss: tensor(0.0364)\n",
      "13966 Training Loss: tensor(0.0361)\n",
      "13967 Training Loss: tensor(0.0363)\n",
      "13968 Training Loss: tensor(0.0363)\n",
      "13969 Training Loss: tensor(0.0361)\n",
      "13970 Training Loss: tensor(0.0362)\n",
      "13971 Training Loss: tensor(0.0363)\n",
      "13972 Training Loss: tensor(0.0362)\n",
      "13973 Training Loss: tensor(0.0365)\n",
      "13974 Training Loss: tensor(0.0362)\n",
      "13975 Training Loss: tensor(0.0362)\n",
      "13976 Training Loss: tensor(0.0363)\n",
      "13977 Training Loss: tensor(0.0364)\n",
      "13978 Training Loss: tensor(0.0362)\n",
      "13979 Training Loss: tensor(0.0361)\n",
      "13980 Training Loss: tensor(0.0362)\n",
      "13981 Training Loss: tensor(0.0362)\n",
      "13982 Training Loss: tensor(0.0364)\n",
      "13983 Training Loss: tensor(0.0364)\n",
      "13984 Training Loss: tensor(0.0361)\n",
      "13985 Training Loss: tensor(0.0367)\n",
      "13986 Training Loss: tensor(0.0361)\n",
      "13987 Training Loss: tensor(0.0361)\n",
      "13988 Training Loss: tensor(0.0364)\n",
      "13989 Training Loss: tensor(0.0365)\n",
      "13990 Training Loss: tensor(0.0360)\n",
      "13991 Training Loss: tensor(0.0362)\n",
      "13992 Training Loss: tensor(0.0363)\n",
      "13993 Training Loss: tensor(0.0361)\n",
      "13994 Training Loss: tensor(0.0362)\n",
      "13995 Training Loss: tensor(0.0362)\n",
      "13996 Training Loss: tensor(0.0363)\n",
      "13997 Training Loss: tensor(0.0362)\n",
      "13998 Training Loss: tensor(0.0362)\n",
      "13999 Training Loss: tensor(0.0363)\n",
      "14000 Training Loss: tensor(0.0361)\n",
      "14001 Training Loss: tensor(0.0363)\n",
      "14002 Training Loss: tensor(0.0363)\n",
      "14003 Training Loss: tensor(0.0364)\n",
      "14004 Training Loss: tensor(0.0362)\n",
      "14005 Training Loss: tensor(0.0362)\n",
      "14006 Training Loss: tensor(0.0363)\n",
      "14007 Training Loss: tensor(0.0362)\n",
      "14008 Training Loss: tensor(0.0362)\n",
      "14009 Training Loss: tensor(0.0363)\n",
      "14010 Training Loss: tensor(0.0361)\n",
      "14011 Training Loss: tensor(0.0364)\n",
      "14012 Training Loss: tensor(0.0362)\n",
      "14013 Training Loss: tensor(0.0362)\n",
      "14014 Training Loss: tensor(0.0363)\n",
      "14015 Training Loss: tensor(0.0363)\n",
      "14016 Training Loss: tensor(0.0362)\n",
      "14017 Training Loss: tensor(0.0361)\n",
      "14018 Training Loss: tensor(0.0363)\n",
      "14019 Training Loss: tensor(0.0362)\n",
      "14020 Training Loss: tensor(0.0365)\n",
      "14021 Training Loss: tensor(0.0363)\n",
      "14022 Training Loss: tensor(0.0362)\n",
      "14023 Training Loss: tensor(0.0363)\n",
      "14024 Training Loss: tensor(0.0362)\n",
      "14025 Training Loss: tensor(0.0361)\n",
      "14026 Training Loss: tensor(0.0362)\n",
      "14027 Training Loss: tensor(0.0362)\n",
      "14028 Training Loss: tensor(0.0362)\n",
      "14029 Training Loss: tensor(0.0361)\n",
      "14030 Training Loss: tensor(0.0363)\n",
      "14031 Training Loss: tensor(0.0363)\n",
      "14032 Training Loss: tensor(0.0363)\n",
      "14033 Training Loss: tensor(0.0363)\n",
      "14034 Training Loss: tensor(0.0362)\n",
      "14035 Training Loss: tensor(0.0360)\n",
      "14036 Training Loss: tensor(0.0360)\n",
      "14037 Training Loss: tensor(0.0361)\n",
      "14038 Training Loss: tensor(0.0363)\n",
      "14039 Training Loss: tensor(0.0360)\n",
      "14040 Training Loss: tensor(0.0365)\n",
      "14041 Training Loss: tensor(0.0362)\n",
      "14042 Training Loss: tensor(0.0363)\n",
      "14043 Training Loss: tensor(0.0360)\n",
      "14044 Training Loss: tensor(0.0361)\n",
      "14045 Training Loss: tensor(0.0360)\n",
      "14046 Training Loss: tensor(0.0361)\n",
      "14047 Training Loss: tensor(0.0359)\n",
      "14048 Training Loss: tensor(0.0361)\n",
      "14049 Training Loss: tensor(0.0361)\n",
      "14050 Training Loss: tensor(0.0360)\n",
      "14051 Training Loss: tensor(0.0365)\n",
      "14052 Training Loss: tensor(0.0362)\n",
      "14053 Training Loss: tensor(0.0360)\n",
      "14054 Training Loss: tensor(0.0363)\n",
      "14055 Training Loss: tensor(0.0362)\n",
      "14056 Training Loss: tensor(0.0360)\n",
      "14057 Training Loss: tensor(0.0360)\n",
      "14058 Training Loss: tensor(0.0359)\n",
      "14059 Training Loss: tensor(0.0364)\n",
      "14060 Training Loss: tensor(0.0363)\n",
      "14061 Training Loss: tensor(0.0362)\n",
      "14062 Training Loss: tensor(0.0361)\n",
      "14063 Training Loss: tensor(0.0361)\n",
      "14064 Training Loss: tensor(0.0361)\n",
      "14065 Training Loss: tensor(0.0362)\n",
      "14066 Training Loss: tensor(0.0365)\n",
      "14067 Training Loss: tensor(0.0363)\n",
      "14068 Training Loss: tensor(0.0361)\n",
      "14069 Training Loss: tensor(0.0360)\n",
      "14070 Training Loss: tensor(0.0360)\n",
      "14071 Training Loss: tensor(0.0361)\n",
      "14072 Training Loss: tensor(0.0361)\n",
      "14073 Training Loss: tensor(0.0362)\n",
      "14074 Training Loss: tensor(0.0361)\n",
      "14075 Training Loss: tensor(0.0359)\n",
      "14076 Training Loss: tensor(0.0360)\n",
      "14077 Training Loss: tensor(0.0361)\n",
      "14078 Training Loss: tensor(0.0363)\n",
      "14079 Training Loss: tensor(0.0361)\n",
      "14080 Training Loss: tensor(0.0360)\n",
      "14081 Training Loss: tensor(0.0360)\n",
      "14082 Training Loss: tensor(0.0362)\n",
      "14083 Training Loss: tensor(0.0360)\n",
      "14084 Training Loss: tensor(0.0360)\n",
      "14085 Training Loss: tensor(0.0360)\n",
      "14086 Training Loss: tensor(0.0360)\n",
      "14087 Training Loss: tensor(0.0364)\n",
      "14088 Training Loss: tensor(0.0359)\n",
      "14089 Training Loss: tensor(0.0359)\n",
      "14090 Training Loss: tensor(0.0359)\n",
      "14091 Training Loss: tensor(0.0361)\n",
      "14092 Training Loss: tensor(0.0361)\n",
      "14093 Training Loss: tensor(0.0363)\n",
      "14094 Training Loss: tensor(0.0361)\n",
      "14095 Training Loss: tensor(0.0361)\n",
      "14096 Training Loss: tensor(0.0362)\n",
      "14097 Training Loss: tensor(0.0360)\n",
      "14098 Training Loss: tensor(0.0362)\n",
      "14099 Training Loss: tensor(0.0361)\n",
      "14100 Training Loss: tensor(0.0360)\n",
      "14101 Training Loss: tensor(0.0362)\n",
      "14102 Training Loss: tensor(0.0361)\n",
      "14103 Training Loss: tensor(0.0361)\n",
      "14104 Training Loss: tensor(0.0362)\n",
      "14105 Training Loss: tensor(0.0362)\n",
      "14106 Training Loss: tensor(0.0360)\n",
      "14107 Training Loss: tensor(0.0361)\n",
      "14108 Training Loss: tensor(0.0358)\n",
      "14109 Training Loss: tensor(0.0361)\n",
      "14110 Training Loss: tensor(0.0360)\n",
      "14111 Training Loss: tensor(0.0360)\n",
      "14112 Training Loss: tensor(0.0359)\n",
      "14113 Training Loss: tensor(0.0360)\n",
      "14114 Training Loss: tensor(0.0361)\n",
      "14115 Training Loss: tensor(0.0359)\n",
      "14116 Training Loss: tensor(0.0364)\n",
      "14117 Training Loss: tensor(0.0359)\n",
      "14118 Training Loss: tensor(0.0360)\n",
      "14119 Training Loss: tensor(0.0361)\n",
      "14120 Training Loss: tensor(0.0361)\n",
      "14121 Training Loss: tensor(0.0360)\n",
      "14122 Training Loss: tensor(0.0359)\n",
      "14123 Training Loss: tensor(0.0359)\n",
      "14124 Training Loss: tensor(0.0358)\n",
      "14125 Training Loss: tensor(0.0363)\n",
      "14126 Training Loss: tensor(0.0360)\n",
      "14127 Training Loss: tensor(0.0360)\n",
      "14128 Training Loss: tensor(0.0360)\n",
      "14129 Training Loss: tensor(0.0359)\n",
      "14130 Training Loss: tensor(0.0362)\n",
      "14131 Training Loss: tensor(0.0361)\n",
      "14132 Training Loss: tensor(0.0360)\n",
      "14133 Training Loss: tensor(0.0361)\n",
      "14134 Training Loss: tensor(0.0359)\n",
      "14135 Training Loss: tensor(0.0360)\n",
      "14136 Training Loss: tensor(0.0359)\n",
      "14137 Training Loss: tensor(0.0359)\n",
      "14138 Training Loss: tensor(0.0361)\n",
      "14139 Training Loss: tensor(0.0358)\n",
      "14140 Training Loss: tensor(0.0360)\n",
      "14141 Training Loss: tensor(0.0363)\n",
      "14142 Training Loss: tensor(0.0360)\n",
      "14143 Training Loss: tensor(0.0360)\n",
      "14144 Training Loss: tensor(0.0360)\n",
      "14145 Training Loss: tensor(0.0359)\n",
      "14146 Training Loss: tensor(0.0361)\n",
      "14147 Training Loss: tensor(0.0360)\n",
      "14148 Training Loss: tensor(0.0362)\n",
      "14149 Training Loss: tensor(0.0360)\n",
      "14150 Training Loss: tensor(0.0359)\n",
      "14151 Training Loss: tensor(0.0359)\n",
      "14152 Training Loss: tensor(0.0360)\n",
      "14153 Training Loss: tensor(0.0359)\n",
      "14154 Training Loss: tensor(0.0359)\n",
      "14155 Training Loss: tensor(0.0359)\n",
      "14156 Training Loss: tensor(0.0363)\n",
      "14157 Training Loss: tensor(0.0360)\n",
      "14158 Training Loss: tensor(0.0361)\n",
      "14159 Training Loss: tensor(0.0360)\n",
      "14160 Training Loss: tensor(0.0359)\n",
      "14161 Training Loss: tensor(0.0361)\n",
      "14162 Training Loss: tensor(0.0361)\n",
      "14163 Training Loss: tensor(0.0361)\n",
      "14164 Training Loss: tensor(0.0358)\n",
      "14165 Training Loss: tensor(0.0360)\n",
      "14166 Training Loss: tensor(0.0359)\n",
      "14167 Training Loss: tensor(0.0360)\n",
      "14168 Training Loss: tensor(0.0358)\n",
      "14169 Training Loss: tensor(0.0359)\n",
      "14170 Training Loss: tensor(0.0360)\n",
      "14171 Training Loss: tensor(0.0359)\n",
      "14172 Training Loss: tensor(0.0360)\n",
      "14173 Training Loss: tensor(0.0359)\n",
      "14174 Training Loss: tensor(0.0359)\n",
      "14175 Training Loss: tensor(0.0358)\n",
      "14176 Training Loss: tensor(0.0359)\n",
      "14177 Training Loss: tensor(0.0361)\n",
      "14178 Training Loss: tensor(0.0360)\n",
      "14179 Training Loss: tensor(0.0358)\n",
      "14180 Training Loss: tensor(0.0358)\n",
      "14181 Training Loss: tensor(0.0359)\n",
      "14182 Training Loss: tensor(0.0360)\n",
      "14183 Training Loss: tensor(0.0359)\n",
      "14184 Training Loss: tensor(0.0359)\n",
      "14185 Training Loss: tensor(0.0360)\n",
      "14186 Training Loss: tensor(0.0359)\n",
      "14187 Training Loss: tensor(0.0358)\n",
      "14188 Training Loss: tensor(0.0358)\n",
      "14189 Training Loss: tensor(0.0360)\n",
      "14190 Training Loss: tensor(0.0358)\n",
      "14191 Training Loss: tensor(0.0359)\n",
      "14192 Training Loss: tensor(0.0361)\n",
      "14193 Training Loss: tensor(0.0359)\n",
      "14194 Training Loss: tensor(0.0358)\n",
      "14195 Training Loss: tensor(0.0359)\n",
      "14196 Training Loss: tensor(0.0361)\n",
      "14197 Training Loss: tensor(0.0359)\n",
      "14198 Training Loss: tensor(0.0359)\n",
      "14199 Training Loss: tensor(0.0359)\n",
      "14200 Training Loss: tensor(0.0360)\n",
      "14201 Training Loss: tensor(0.0359)\n",
      "14202 Training Loss: tensor(0.0358)\n",
      "14203 Training Loss: tensor(0.0358)\n",
      "14204 Training Loss: tensor(0.0363)\n",
      "14205 Training Loss: tensor(0.0358)\n",
      "14206 Training Loss: tensor(0.0359)\n",
      "14207 Training Loss: tensor(0.0359)\n",
      "14208 Training Loss: tensor(0.0357)\n",
      "14209 Training Loss: tensor(0.0359)\n",
      "14210 Training Loss: tensor(0.0359)\n",
      "14211 Training Loss: tensor(0.0360)\n",
      "14212 Training Loss: tensor(0.0360)\n",
      "14213 Training Loss: tensor(0.0358)\n",
      "14214 Training Loss: tensor(0.0359)\n",
      "14215 Training Loss: tensor(0.0358)\n",
      "14216 Training Loss: tensor(0.0359)\n",
      "14217 Training Loss: tensor(0.0358)\n",
      "14218 Training Loss: tensor(0.0359)\n",
      "14219 Training Loss: tensor(0.0361)\n",
      "14220 Training Loss: tensor(0.0360)\n",
      "14221 Training Loss: tensor(0.0358)\n",
      "14222 Training Loss: tensor(0.0358)\n",
      "14223 Training Loss: tensor(0.0357)\n",
      "14224 Training Loss: tensor(0.0357)\n",
      "14225 Training Loss: tensor(0.0360)\n",
      "14226 Training Loss: tensor(0.0359)\n",
      "14227 Training Loss: tensor(0.0361)\n",
      "14228 Training Loss: tensor(0.0359)\n",
      "14229 Training Loss: tensor(0.0358)\n",
      "14230 Training Loss: tensor(0.0356)\n",
      "14231 Training Loss: tensor(0.0358)\n",
      "14232 Training Loss: tensor(0.0360)\n",
      "14233 Training Loss: tensor(0.0357)\n",
      "14234 Training Loss: tensor(0.0357)\n",
      "14235 Training Loss: tensor(0.0361)\n",
      "14236 Training Loss: tensor(0.0359)\n",
      "14237 Training Loss: tensor(0.0359)\n",
      "14238 Training Loss: tensor(0.0358)\n",
      "14239 Training Loss: tensor(0.0358)\n",
      "14240 Training Loss: tensor(0.0357)\n",
      "14241 Training Loss: tensor(0.0357)\n",
      "14242 Training Loss: tensor(0.0360)\n",
      "14243 Training Loss: tensor(0.0358)\n",
      "14244 Training Loss: tensor(0.0359)\n",
      "14245 Training Loss: tensor(0.0359)\n",
      "14246 Training Loss: tensor(0.0357)\n",
      "14247 Training Loss: tensor(0.0358)\n",
      "14248 Training Loss: tensor(0.0357)\n",
      "14249 Training Loss: tensor(0.0359)\n",
      "14250 Training Loss: tensor(0.0359)\n",
      "14251 Training Loss: tensor(0.0359)\n",
      "14252 Training Loss: tensor(0.0358)\n",
      "14253 Training Loss: tensor(0.0358)\n",
      "14254 Training Loss: tensor(0.0359)\n",
      "14255 Training Loss: tensor(0.0358)\n",
      "14256 Training Loss: tensor(0.0359)\n",
      "14257 Training Loss: tensor(0.0358)\n",
      "14258 Training Loss: tensor(0.0359)\n",
      "14259 Training Loss: tensor(0.0357)\n",
      "14260 Training Loss: tensor(0.0358)\n",
      "14261 Training Loss: tensor(0.0359)\n",
      "14262 Training Loss: tensor(0.0359)\n",
      "14263 Training Loss: tensor(0.0358)\n",
      "14264 Training Loss: tensor(0.0360)\n",
      "14265 Training Loss: tensor(0.0360)\n",
      "14266 Training Loss: tensor(0.0357)\n",
      "14267 Training Loss: tensor(0.0357)\n",
      "14268 Training Loss: tensor(0.0356)\n",
      "14269 Training Loss: tensor(0.0358)\n",
      "14270 Training Loss: tensor(0.0357)\n",
      "14271 Training Loss: tensor(0.0358)\n",
      "14272 Training Loss: tensor(0.0359)\n",
      "14273 Training Loss: tensor(0.0358)\n",
      "14274 Training Loss: tensor(0.0358)\n",
      "14275 Training Loss: tensor(0.0358)\n",
      "14276 Training Loss: tensor(0.0359)\n",
      "14277 Training Loss: tensor(0.0358)\n",
      "14278 Training Loss: tensor(0.0359)\n",
      "14279 Training Loss: tensor(0.0358)\n",
      "14280 Training Loss: tensor(0.0357)\n",
      "14281 Training Loss: tensor(0.0359)\n",
      "14282 Training Loss: tensor(0.0356)\n",
      "14283 Training Loss: tensor(0.0356)\n",
      "14284 Training Loss: tensor(0.0360)\n",
      "14285 Training Loss: tensor(0.0357)\n",
      "14286 Training Loss: tensor(0.0356)\n",
      "14287 Training Loss: tensor(0.0356)\n",
      "14288 Training Loss: tensor(0.0357)\n",
      "14289 Training Loss: tensor(0.0358)\n",
      "14290 Training Loss: tensor(0.0356)\n",
      "14291 Training Loss: tensor(0.0362)\n",
      "14292 Training Loss: tensor(0.0358)\n",
      "14293 Training Loss: tensor(0.0357)\n",
      "14294 Training Loss: tensor(0.0358)\n",
      "14295 Training Loss: tensor(0.0358)\n",
      "14296 Training Loss: tensor(0.0361)\n",
      "14297 Training Loss: tensor(0.0357)\n",
      "14298 Training Loss: tensor(0.0356)\n",
      "14299 Training Loss: tensor(0.0360)\n",
      "14300 Training Loss: tensor(0.0357)\n",
      "14301 Training Loss: tensor(0.0358)\n",
      "14302 Training Loss: tensor(0.0358)\n",
      "14303 Training Loss: tensor(0.0356)\n",
      "14304 Training Loss: tensor(0.0358)\n",
      "14305 Training Loss: tensor(0.0355)\n",
      "14306 Training Loss: tensor(0.0358)\n",
      "14307 Training Loss: tensor(0.0356)\n",
      "14308 Training Loss: tensor(0.0357)\n",
      "14309 Training Loss: tensor(0.0356)\n",
      "14310 Training Loss: tensor(0.0357)\n",
      "14311 Training Loss: tensor(0.0357)\n",
      "14312 Training Loss: tensor(0.0361)\n",
      "14313 Training Loss: tensor(0.0355)\n",
      "14314 Training Loss: tensor(0.0355)\n",
      "14315 Training Loss: tensor(0.0358)\n",
      "14316 Training Loss: tensor(0.0358)\n",
      "14317 Training Loss: tensor(0.0357)\n",
      "14318 Training Loss: tensor(0.0358)\n",
      "14319 Training Loss: tensor(0.0361)\n",
      "14320 Training Loss: tensor(0.0359)\n",
      "14321 Training Loss: tensor(0.0358)\n",
      "14322 Training Loss: tensor(0.0358)\n",
      "14323 Training Loss: tensor(0.0356)\n",
      "14324 Training Loss: tensor(0.0360)\n",
      "14325 Training Loss: tensor(0.0357)\n",
      "14326 Training Loss: tensor(0.0356)\n",
      "14327 Training Loss: tensor(0.0358)\n",
      "14328 Training Loss: tensor(0.0356)\n",
      "14329 Training Loss: tensor(0.0360)\n",
      "14330 Training Loss: tensor(0.0357)\n",
      "14331 Training Loss: tensor(0.0356)\n",
      "14332 Training Loss: tensor(0.0357)\n",
      "14333 Training Loss: tensor(0.0355)\n",
      "14334 Training Loss: tensor(0.0356)\n",
      "14335 Training Loss: tensor(0.0356)\n",
      "14336 Training Loss: tensor(0.0357)\n",
      "14337 Training Loss: tensor(0.0355)\n",
      "14338 Training Loss: tensor(0.0358)\n",
      "14339 Training Loss: tensor(0.0355)\n",
      "14340 Training Loss: tensor(0.0357)\n",
      "14341 Training Loss: tensor(0.0357)\n",
      "14342 Training Loss: tensor(0.0357)\n",
      "14343 Training Loss: tensor(0.0357)\n",
      "14344 Training Loss: tensor(0.0356)\n",
      "14345 Training Loss: tensor(0.0358)\n",
      "14346 Training Loss: tensor(0.0357)\n",
      "14347 Training Loss: tensor(0.0358)\n",
      "14348 Training Loss: tensor(0.0361)\n",
      "14349 Training Loss: tensor(0.0357)\n",
      "14350 Training Loss: tensor(0.0358)\n",
      "14351 Training Loss: tensor(0.0356)\n",
      "14352 Training Loss: tensor(0.0355)\n",
      "14353 Training Loss: tensor(0.0358)\n",
      "14354 Training Loss: tensor(0.0358)\n",
      "14355 Training Loss: tensor(0.0359)\n",
      "14356 Training Loss: tensor(0.0356)\n",
      "14357 Training Loss: tensor(0.0356)\n",
      "14358 Training Loss: tensor(0.0357)\n",
      "14359 Training Loss: tensor(0.0355)\n",
      "14360 Training Loss: tensor(0.0356)\n",
      "14361 Training Loss: tensor(0.0357)\n",
      "14362 Training Loss: tensor(0.0356)\n",
      "14363 Training Loss: tensor(0.0357)\n",
      "14364 Training Loss: tensor(0.0357)\n",
      "14365 Training Loss: tensor(0.0356)\n",
      "14366 Training Loss: tensor(0.0356)\n",
      "14367 Training Loss: tensor(0.0355)\n",
      "14368 Training Loss: tensor(0.0356)\n",
      "14369 Training Loss: tensor(0.0356)\n",
      "14370 Training Loss: tensor(0.0355)\n",
      "14371 Training Loss: tensor(0.0354)\n",
      "14372 Training Loss: tensor(0.0355)\n",
      "14373 Training Loss: tensor(0.0356)\n",
      "14374 Training Loss: tensor(0.0358)\n",
      "14375 Training Loss: tensor(0.0356)\n",
      "14376 Training Loss: tensor(0.0358)\n",
      "14377 Training Loss: tensor(0.0356)\n",
      "14378 Training Loss: tensor(0.0356)\n",
      "14379 Training Loss: tensor(0.0355)\n",
      "14380 Training Loss: tensor(0.0355)\n",
      "14381 Training Loss: tensor(0.0354)\n",
      "14382 Training Loss: tensor(0.0356)\n",
      "14383 Training Loss: tensor(0.0357)\n",
      "14384 Training Loss: tensor(0.0355)\n",
      "14385 Training Loss: tensor(0.0356)\n",
      "14386 Training Loss: tensor(0.0356)\n",
      "14387 Training Loss: tensor(0.0358)\n",
      "14388 Training Loss: tensor(0.0354)\n",
      "14389 Training Loss: tensor(0.0355)\n",
      "14390 Training Loss: tensor(0.0355)\n",
      "14391 Training Loss: tensor(0.0354)\n",
      "14392 Training Loss: tensor(0.0355)\n",
      "14393 Training Loss: tensor(0.0355)\n",
      "14394 Training Loss: tensor(0.0358)\n",
      "14395 Training Loss: tensor(0.0357)\n",
      "14396 Training Loss: tensor(0.0355)\n",
      "14397 Training Loss: tensor(0.0355)\n",
      "14398 Training Loss: tensor(0.0355)\n",
      "14399 Training Loss: tensor(0.0354)\n",
      "14400 Training Loss: tensor(0.0354)\n",
      "14401 Training Loss: tensor(0.0355)\n",
      "14402 Training Loss: tensor(0.0354)\n",
      "14403 Training Loss: tensor(0.0355)\n",
      "14404 Training Loss: tensor(0.0356)\n",
      "14405 Training Loss: tensor(0.0355)\n",
      "14406 Training Loss: tensor(0.0360)\n",
      "14407 Training Loss: tensor(0.0361)\n",
      "14408 Training Loss: tensor(0.0359)\n",
      "14409 Training Loss: tensor(0.0356)\n",
      "14410 Training Loss: tensor(0.0356)\n",
      "14411 Training Loss: tensor(0.0355)\n",
      "14412 Training Loss: tensor(0.0356)\n",
      "14413 Training Loss: tensor(0.0357)\n",
      "14414 Training Loss: tensor(0.0355)\n",
      "14415 Training Loss: tensor(0.0355)\n",
      "14416 Training Loss: tensor(0.0356)\n",
      "14417 Training Loss: tensor(0.0356)\n",
      "14418 Training Loss: tensor(0.0357)\n",
      "14419 Training Loss: tensor(0.0355)\n",
      "14420 Training Loss: tensor(0.0355)\n",
      "14421 Training Loss: tensor(0.0355)\n",
      "14422 Training Loss: tensor(0.0354)\n",
      "14423 Training Loss: tensor(0.0355)\n",
      "14424 Training Loss: tensor(0.0355)\n",
      "14425 Training Loss: tensor(0.0354)\n",
      "14426 Training Loss: tensor(0.0355)\n",
      "14427 Training Loss: tensor(0.0355)\n",
      "14428 Training Loss: tensor(0.0358)\n",
      "14429 Training Loss: tensor(0.0356)\n",
      "14430 Training Loss: tensor(0.0355)\n",
      "14431 Training Loss: tensor(0.0355)\n",
      "14432 Training Loss: tensor(0.0354)\n",
      "14433 Training Loss: tensor(0.0355)\n",
      "14434 Training Loss: tensor(0.0357)\n",
      "14435 Training Loss: tensor(0.0355)\n",
      "14436 Training Loss: tensor(0.0355)\n",
      "14437 Training Loss: tensor(0.0354)\n",
      "14438 Training Loss: tensor(0.0355)\n",
      "14439 Training Loss: tensor(0.0355)\n",
      "14440 Training Loss: tensor(0.0354)\n",
      "14441 Training Loss: tensor(0.0357)\n",
      "14442 Training Loss: tensor(0.0354)\n",
      "14443 Training Loss: tensor(0.0354)\n",
      "14444 Training Loss: tensor(0.0355)\n",
      "14445 Training Loss: tensor(0.0355)\n",
      "14446 Training Loss: tensor(0.0354)\n",
      "14447 Training Loss: tensor(0.0356)\n",
      "14448 Training Loss: tensor(0.0357)\n",
      "14449 Training Loss: tensor(0.0354)\n",
      "14450 Training Loss: tensor(0.0355)\n",
      "14451 Training Loss: tensor(0.0355)\n",
      "14452 Training Loss: tensor(0.0356)\n",
      "14453 Training Loss: tensor(0.0355)\n",
      "14454 Training Loss: tensor(0.0355)\n",
      "14455 Training Loss: tensor(0.0353)\n",
      "14456 Training Loss: tensor(0.0355)\n",
      "14457 Training Loss: tensor(0.0354)\n",
      "14458 Training Loss: tensor(0.0353)\n",
      "14459 Training Loss: tensor(0.0355)\n",
      "14460 Training Loss: tensor(0.0355)\n",
      "14461 Training Loss: tensor(0.0354)\n",
      "14462 Training Loss: tensor(0.0359)\n",
      "14463 Training Loss: tensor(0.0356)\n",
      "14464 Training Loss: tensor(0.0353)\n",
      "14465 Training Loss: tensor(0.0354)\n",
      "14466 Training Loss: tensor(0.0356)\n",
      "14467 Training Loss: tensor(0.0356)\n",
      "14468 Training Loss: tensor(0.0353)\n",
      "14469 Training Loss: tensor(0.0354)\n",
      "14470 Training Loss: tensor(0.0354)\n",
      "14471 Training Loss: tensor(0.0356)\n",
      "14472 Training Loss: tensor(0.0356)\n",
      "14473 Training Loss: tensor(0.0357)\n",
      "14474 Training Loss: tensor(0.0355)\n",
      "14475 Training Loss: tensor(0.0354)\n",
      "14476 Training Loss: tensor(0.0354)\n",
      "14477 Training Loss: tensor(0.0355)\n",
      "14478 Training Loss: tensor(0.0355)\n",
      "14479 Training Loss: tensor(0.0355)\n",
      "14480 Training Loss: tensor(0.0354)\n",
      "14481 Training Loss: tensor(0.0356)\n",
      "14482 Training Loss: tensor(0.0355)\n",
      "14483 Training Loss: tensor(0.0356)\n",
      "14484 Training Loss: tensor(0.0354)\n",
      "14485 Training Loss: tensor(0.0356)\n",
      "14486 Training Loss: tensor(0.0354)\n",
      "14487 Training Loss: tensor(0.0355)\n",
      "14488 Training Loss: tensor(0.0353)\n",
      "14489 Training Loss: tensor(0.0353)\n",
      "14490 Training Loss: tensor(0.0353)\n",
      "14491 Training Loss: tensor(0.0353)\n",
      "14492 Training Loss: tensor(0.0352)\n",
      "14493 Training Loss: tensor(0.0354)\n",
      "14494 Training Loss: tensor(0.0354)\n",
      "14495 Training Loss: tensor(0.0356)\n",
      "14496 Training Loss: tensor(0.0354)\n",
      "14497 Training Loss: tensor(0.0354)\n",
      "14498 Training Loss: tensor(0.0353)\n",
      "14499 Training Loss: tensor(0.0353)\n",
      "14500 Training Loss: tensor(0.0354)\n",
      "14501 Training Loss: tensor(0.0353)\n",
      "14502 Training Loss: tensor(0.0354)\n",
      "14503 Training Loss: tensor(0.0355)\n",
      "14504 Training Loss: tensor(0.0353)\n",
      "14505 Training Loss: tensor(0.0353)\n",
      "14506 Training Loss: tensor(0.0357)\n",
      "14507 Training Loss: tensor(0.0355)\n",
      "14508 Training Loss: tensor(0.0352)\n",
      "14509 Training Loss: tensor(0.0355)\n",
      "14510 Training Loss: tensor(0.0354)\n",
      "14511 Training Loss: tensor(0.0353)\n",
      "14512 Training Loss: tensor(0.0354)\n",
      "14513 Training Loss: tensor(0.0355)\n",
      "14514 Training Loss: tensor(0.0357)\n",
      "14515 Training Loss: tensor(0.0354)\n",
      "14516 Training Loss: tensor(0.0355)\n",
      "14517 Training Loss: tensor(0.0354)\n",
      "14518 Training Loss: tensor(0.0353)\n",
      "14519 Training Loss: tensor(0.0352)\n",
      "14520 Training Loss: tensor(0.0354)\n",
      "14521 Training Loss: tensor(0.0354)\n",
      "14522 Training Loss: tensor(0.0352)\n",
      "14523 Training Loss: tensor(0.0353)\n",
      "14524 Training Loss: tensor(0.0353)\n",
      "14525 Training Loss: tensor(0.0356)\n",
      "14526 Training Loss: tensor(0.0353)\n",
      "14527 Training Loss: tensor(0.0352)\n",
      "14528 Training Loss: tensor(0.0352)\n",
      "14529 Training Loss: tensor(0.0354)\n",
      "14530 Training Loss: tensor(0.0355)\n",
      "14531 Training Loss: tensor(0.0353)\n",
      "14532 Training Loss: tensor(0.0354)\n",
      "14533 Training Loss: tensor(0.0355)\n",
      "14534 Training Loss: tensor(0.0352)\n",
      "14535 Training Loss: tensor(0.0352)\n",
      "14536 Training Loss: tensor(0.0355)\n",
      "14537 Training Loss: tensor(0.0352)\n",
      "14538 Training Loss: tensor(0.0354)\n",
      "14539 Training Loss: tensor(0.0354)\n",
      "14540 Training Loss: tensor(0.0356)\n",
      "14541 Training Loss: tensor(0.0355)\n",
      "14542 Training Loss: tensor(0.0354)\n",
      "14543 Training Loss: tensor(0.0354)\n",
      "14544 Training Loss: tensor(0.0353)\n",
      "14545 Training Loss: tensor(0.0353)\n",
      "14546 Training Loss: tensor(0.0352)\n",
      "14547 Training Loss: tensor(0.0353)\n",
      "14548 Training Loss: tensor(0.0355)\n",
      "14549 Training Loss: tensor(0.0354)\n",
      "14550 Training Loss: tensor(0.0353)\n",
      "14551 Training Loss: tensor(0.0353)\n",
      "14552 Training Loss: tensor(0.0353)\n",
      "14553 Training Loss: tensor(0.0353)\n",
      "14554 Training Loss: tensor(0.0353)\n",
      "14555 Training Loss: tensor(0.0355)\n",
      "14556 Training Loss: tensor(0.0355)\n",
      "14557 Training Loss: tensor(0.0355)\n",
      "14558 Training Loss: tensor(0.0355)\n",
      "14559 Training Loss: tensor(0.0353)\n",
      "14560 Training Loss: tensor(0.0354)\n",
      "14561 Training Loss: tensor(0.0353)\n",
      "14562 Training Loss: tensor(0.0354)\n",
      "14563 Training Loss: tensor(0.0353)\n",
      "14564 Training Loss: tensor(0.0351)\n",
      "14565 Training Loss: tensor(0.0354)\n",
      "14566 Training Loss: tensor(0.0355)\n",
      "14567 Training Loss: tensor(0.0354)\n",
      "14568 Training Loss: tensor(0.0355)\n",
      "14569 Training Loss: tensor(0.0352)\n",
      "14570 Training Loss: tensor(0.0354)\n",
      "14571 Training Loss: tensor(0.0353)\n",
      "14572 Training Loss: tensor(0.0351)\n",
      "14573 Training Loss: tensor(0.0352)\n",
      "14574 Training Loss: tensor(0.0353)\n",
      "14575 Training Loss: tensor(0.0356)\n",
      "14576 Training Loss: tensor(0.0352)\n",
      "14577 Training Loss: tensor(0.0358)\n",
      "14578 Training Loss: tensor(0.0352)\n",
      "14579 Training Loss: tensor(0.0353)\n",
      "14580 Training Loss: tensor(0.0352)\n",
      "14581 Training Loss: tensor(0.0352)\n",
      "14582 Training Loss: tensor(0.0351)\n",
      "14583 Training Loss: tensor(0.0352)\n",
      "14584 Training Loss: tensor(0.0352)\n",
      "14585 Training Loss: tensor(0.0356)\n",
      "14586 Training Loss: tensor(0.0354)\n",
      "14587 Training Loss: tensor(0.0352)\n",
      "14588 Training Loss: tensor(0.0352)\n",
      "14589 Training Loss: tensor(0.0354)\n",
      "14590 Training Loss: tensor(0.0352)\n",
      "14591 Training Loss: tensor(0.0352)\n",
      "14592 Training Loss: tensor(0.0352)\n",
      "14593 Training Loss: tensor(0.0353)\n",
      "14594 Training Loss: tensor(0.0355)\n",
      "14595 Training Loss: tensor(0.0351)\n",
      "14596 Training Loss: tensor(0.0353)\n",
      "14597 Training Loss: tensor(0.0352)\n",
      "14598 Training Loss: tensor(0.0354)\n",
      "14599 Training Loss: tensor(0.0352)\n",
      "14600 Training Loss: tensor(0.0356)\n",
      "14601 Training Loss: tensor(0.0351)\n",
      "14602 Training Loss: tensor(0.0352)\n",
      "14603 Training Loss: tensor(0.0355)\n",
      "14604 Training Loss: tensor(0.0352)\n",
      "14605 Training Loss: tensor(0.0352)\n",
      "14606 Training Loss: tensor(0.0352)\n",
      "14607 Training Loss: tensor(0.0356)\n",
      "14608 Training Loss: tensor(0.0354)\n",
      "14609 Training Loss: tensor(0.0353)\n",
      "14610 Training Loss: tensor(0.0352)\n",
      "14611 Training Loss: tensor(0.0353)\n",
      "14612 Training Loss: tensor(0.0351)\n",
      "14613 Training Loss: tensor(0.0355)\n",
      "14614 Training Loss: tensor(0.0352)\n",
      "14615 Training Loss: tensor(0.0355)\n",
      "14616 Training Loss: tensor(0.0353)\n",
      "14617 Training Loss: tensor(0.0351)\n",
      "14618 Training Loss: tensor(0.0352)\n",
      "14619 Training Loss: tensor(0.0350)\n",
      "14620 Training Loss: tensor(0.0351)\n",
      "14621 Training Loss: tensor(0.0355)\n",
      "14622 Training Loss: tensor(0.0352)\n",
      "14623 Training Loss: tensor(0.0351)\n",
      "14624 Training Loss: tensor(0.0351)\n",
      "14625 Training Loss: tensor(0.0352)\n",
      "14626 Training Loss: tensor(0.0352)\n",
      "14627 Training Loss: tensor(0.0351)\n",
      "14628 Training Loss: tensor(0.0354)\n",
      "14629 Training Loss: tensor(0.0353)\n",
      "14630 Training Loss: tensor(0.0354)\n",
      "14631 Training Loss: tensor(0.0351)\n",
      "14632 Training Loss: tensor(0.0352)\n",
      "14633 Training Loss: tensor(0.0352)\n",
      "14634 Training Loss: tensor(0.0353)\n",
      "14635 Training Loss: tensor(0.0352)\n",
      "14636 Training Loss: tensor(0.0354)\n",
      "14637 Training Loss: tensor(0.0352)\n",
      "14638 Training Loss: tensor(0.0354)\n",
      "14639 Training Loss: tensor(0.0353)\n",
      "14640 Training Loss: tensor(0.0351)\n",
      "14641 Training Loss: tensor(0.0354)\n",
      "14642 Training Loss: tensor(0.0351)\n",
      "14643 Training Loss: tensor(0.0352)\n",
      "14644 Training Loss: tensor(0.0352)\n",
      "14645 Training Loss: tensor(0.0352)\n",
      "14646 Training Loss: tensor(0.0352)\n",
      "14647 Training Loss: tensor(0.0350)\n",
      "14648 Training Loss: tensor(0.0350)\n",
      "14649 Training Loss: tensor(0.0352)\n",
      "14650 Training Loss: tensor(0.0355)\n",
      "14651 Training Loss: tensor(0.0353)\n",
      "14652 Training Loss: tensor(0.0356)\n",
      "14653 Training Loss: tensor(0.0351)\n",
      "14654 Training Loss: tensor(0.0351)\n",
      "14655 Training Loss: tensor(0.0352)\n",
      "14656 Training Loss: tensor(0.0352)\n",
      "14657 Training Loss: tensor(0.0351)\n",
      "14658 Training Loss: tensor(0.0350)\n",
      "14659 Training Loss: tensor(0.0351)\n",
      "14660 Training Loss: tensor(0.0350)\n",
      "14661 Training Loss: tensor(0.0354)\n",
      "14662 Training Loss: tensor(0.0351)\n",
      "14663 Training Loss: tensor(0.0352)\n",
      "14664 Training Loss: tensor(0.0351)\n",
      "14665 Training Loss: tensor(0.0351)\n",
      "14666 Training Loss: tensor(0.0351)\n",
      "14667 Training Loss: tensor(0.0350)\n",
      "14668 Training Loss: tensor(0.0350)\n",
      "14669 Training Loss: tensor(0.0359)\n",
      "14670 Training Loss: tensor(0.0352)\n",
      "14671 Training Loss: tensor(0.0350)\n",
      "14672 Training Loss: tensor(0.0351)\n",
      "14673 Training Loss: tensor(0.0352)\n",
      "14674 Training Loss: tensor(0.0351)\n",
      "14675 Training Loss: tensor(0.0351)\n",
      "14676 Training Loss: tensor(0.0350)\n",
      "14677 Training Loss: tensor(0.0351)\n",
      "14678 Training Loss: tensor(0.0352)\n",
      "14679 Training Loss: tensor(0.0351)\n",
      "14680 Training Loss: tensor(0.0351)\n",
      "14681 Training Loss: tensor(0.0352)\n",
      "14682 Training Loss: tensor(0.0350)\n",
      "14683 Training Loss: tensor(0.0352)\n",
      "14684 Training Loss: tensor(0.0352)\n",
      "14685 Training Loss: tensor(0.0350)\n",
      "14686 Training Loss: tensor(0.0351)\n",
      "14687 Training Loss: tensor(0.0357)\n",
      "14688 Training Loss: tensor(0.0351)\n",
      "14689 Training Loss: tensor(0.0354)\n",
      "14690 Training Loss: tensor(0.0351)\n",
      "14691 Training Loss: tensor(0.0352)\n",
      "14692 Training Loss: tensor(0.0350)\n",
      "14693 Training Loss: tensor(0.0351)\n",
      "14694 Training Loss: tensor(0.0352)\n",
      "14695 Training Loss: tensor(0.0352)\n",
      "14696 Training Loss: tensor(0.0352)\n",
      "14697 Training Loss: tensor(0.0351)\n",
      "14698 Training Loss: tensor(0.0351)\n",
      "14699 Training Loss: tensor(0.0352)\n",
      "14700 Training Loss: tensor(0.0350)\n",
      "14701 Training Loss: tensor(0.0351)\n",
      "14702 Training Loss: tensor(0.0351)\n",
      "14703 Training Loss: tensor(0.0351)\n",
      "14704 Training Loss: tensor(0.0353)\n",
      "14705 Training Loss: tensor(0.0349)\n",
      "14706 Training Loss: tensor(0.0351)\n",
      "14707 Training Loss: tensor(0.0352)\n",
      "14708 Training Loss: tensor(0.0352)\n",
      "14709 Training Loss: tensor(0.0349)\n",
      "14710 Training Loss: tensor(0.0350)\n",
      "14711 Training Loss: tensor(0.0352)\n",
      "14712 Training Loss: tensor(0.0353)\n",
      "14713 Training Loss: tensor(0.0350)\n",
      "14714 Training Loss: tensor(0.0350)\n",
      "14715 Training Loss: tensor(0.0352)\n",
      "14716 Training Loss: tensor(0.0353)\n",
      "14717 Training Loss: tensor(0.0351)\n",
      "14718 Training Loss: tensor(0.0350)\n",
      "14719 Training Loss: tensor(0.0351)\n",
      "14720 Training Loss: tensor(0.0350)\n",
      "14721 Training Loss: tensor(0.0350)\n",
      "14722 Training Loss: tensor(0.0352)\n",
      "14723 Training Loss: tensor(0.0351)\n",
      "14724 Training Loss: tensor(0.0350)\n",
      "14725 Training Loss: tensor(0.0352)\n",
      "14726 Training Loss: tensor(0.0351)\n",
      "14727 Training Loss: tensor(0.0351)\n",
      "14728 Training Loss: tensor(0.0352)\n",
      "14729 Training Loss: tensor(0.0352)\n",
      "14730 Training Loss: tensor(0.0350)\n",
      "14731 Training Loss: tensor(0.0353)\n",
      "14732 Training Loss: tensor(0.0349)\n",
      "14733 Training Loss: tensor(0.0350)\n",
      "14734 Training Loss: tensor(0.0352)\n",
      "14735 Training Loss: tensor(0.0350)\n",
      "14736 Training Loss: tensor(0.0349)\n",
      "14737 Training Loss: tensor(0.0351)\n",
      "14738 Training Loss: tensor(0.0350)\n",
      "14739 Training Loss: tensor(0.0350)\n",
      "14740 Training Loss: tensor(0.0349)\n",
      "14741 Training Loss: tensor(0.0350)\n",
      "14742 Training Loss: tensor(0.0350)\n",
      "14743 Training Loss: tensor(0.0350)\n",
      "14744 Training Loss: tensor(0.0352)\n",
      "14745 Training Loss: tensor(0.0350)\n",
      "14746 Training Loss: tensor(0.0351)\n",
      "14747 Training Loss: tensor(0.0350)\n",
      "14748 Training Loss: tensor(0.0353)\n",
      "14749 Training Loss: tensor(0.0349)\n",
      "14750 Training Loss: tensor(0.0351)\n",
      "14751 Training Loss: tensor(0.0352)\n",
      "14752 Training Loss: tensor(0.0350)\n",
      "14753 Training Loss: tensor(0.0351)\n",
      "14754 Training Loss: tensor(0.0350)\n",
      "14755 Training Loss: tensor(0.0351)\n",
      "14756 Training Loss: tensor(0.0349)\n",
      "14757 Training Loss: tensor(0.0349)\n",
      "14758 Training Loss: tensor(0.0350)\n",
      "14759 Training Loss: tensor(0.0349)\n",
      "14760 Training Loss: tensor(0.0350)\n",
      "14761 Training Loss: tensor(0.0350)\n",
      "14762 Training Loss: tensor(0.0353)\n",
      "14763 Training Loss: tensor(0.0349)\n",
      "14764 Training Loss: tensor(0.0349)\n",
      "14765 Training Loss: tensor(0.0350)\n",
      "14766 Training Loss: tensor(0.0353)\n",
      "14767 Training Loss: tensor(0.0351)\n",
      "14768 Training Loss: tensor(0.0348)\n",
      "14769 Training Loss: tensor(0.0350)\n",
      "14770 Training Loss: tensor(0.0350)\n",
      "14771 Training Loss: tensor(0.0348)\n",
      "14772 Training Loss: tensor(0.0350)\n",
      "14773 Training Loss: tensor(0.0353)\n",
      "14774 Training Loss: tensor(0.0349)\n",
      "14775 Training Loss: tensor(0.0349)\n",
      "14776 Training Loss: tensor(0.0351)\n",
      "14777 Training Loss: tensor(0.0350)\n",
      "14778 Training Loss: tensor(0.0351)\n",
      "14779 Training Loss: tensor(0.0351)\n",
      "14780 Training Loss: tensor(0.0349)\n",
      "14781 Training Loss: tensor(0.0348)\n",
      "14782 Training Loss: tensor(0.0349)\n",
      "14783 Training Loss: tensor(0.0350)\n",
      "14784 Training Loss: tensor(0.0351)\n",
      "14785 Training Loss: tensor(0.0348)\n",
      "14786 Training Loss: tensor(0.0350)\n",
      "14787 Training Loss: tensor(0.0349)\n",
      "14788 Training Loss: tensor(0.0348)\n",
      "14789 Training Loss: tensor(0.0348)\n",
      "14790 Training Loss: tensor(0.0348)\n",
      "14791 Training Loss: tensor(0.0350)\n",
      "14792 Training Loss: tensor(0.0352)\n",
      "14793 Training Loss: tensor(0.0350)\n",
      "14794 Training Loss: tensor(0.0349)\n",
      "14795 Training Loss: tensor(0.0348)\n",
      "14796 Training Loss: tensor(0.0352)\n",
      "14797 Training Loss: tensor(0.0350)\n",
      "14798 Training Loss: tensor(0.0348)\n",
      "14799 Training Loss: tensor(0.0348)\n",
      "14800 Training Loss: tensor(0.0350)\n",
      "14801 Training Loss: tensor(0.0351)\n",
      "14802 Training Loss: tensor(0.0350)\n",
      "14803 Training Loss: tensor(0.0353)\n",
      "14804 Training Loss: tensor(0.0349)\n",
      "14805 Training Loss: tensor(0.0353)\n",
      "14806 Training Loss: tensor(0.0349)\n",
      "14807 Training Loss: tensor(0.0349)\n",
      "14808 Training Loss: tensor(0.0352)\n",
      "14809 Training Loss: tensor(0.0350)\n",
      "14810 Training Loss: tensor(0.0349)\n",
      "14811 Training Loss: tensor(0.0349)\n",
      "14812 Training Loss: tensor(0.0348)\n",
      "14813 Training Loss: tensor(0.0349)\n",
      "14814 Training Loss: tensor(0.0348)\n",
      "14815 Training Loss: tensor(0.0352)\n",
      "14816 Training Loss: tensor(0.0348)\n",
      "14817 Training Loss: tensor(0.0351)\n",
      "14818 Training Loss: tensor(0.0351)\n",
      "14819 Training Loss: tensor(0.0347)\n",
      "14820 Training Loss: tensor(0.0349)\n",
      "14821 Training Loss: tensor(0.0350)\n",
      "14822 Training Loss: tensor(0.0348)\n",
      "14823 Training Loss: tensor(0.0348)\n",
      "14824 Training Loss: tensor(0.0349)\n",
      "14825 Training Loss: tensor(0.0349)\n",
      "14826 Training Loss: tensor(0.0348)\n",
      "14827 Training Loss: tensor(0.0352)\n",
      "14828 Training Loss: tensor(0.0348)\n",
      "14829 Training Loss: tensor(0.0352)\n",
      "14830 Training Loss: tensor(0.0349)\n",
      "14831 Training Loss: tensor(0.0349)\n",
      "14832 Training Loss: tensor(0.0351)\n",
      "14833 Training Loss: tensor(0.0349)\n",
      "14834 Training Loss: tensor(0.0349)\n",
      "14835 Training Loss: tensor(0.0349)\n",
      "14836 Training Loss: tensor(0.0350)\n",
      "14837 Training Loss: tensor(0.0349)\n",
      "14838 Training Loss: tensor(0.0350)\n",
      "14839 Training Loss: tensor(0.0349)\n",
      "14840 Training Loss: tensor(0.0350)\n",
      "14841 Training Loss: tensor(0.0348)\n",
      "14842 Training Loss: tensor(0.0350)\n",
      "14843 Training Loss: tensor(0.0350)\n",
      "14844 Training Loss: tensor(0.0350)\n",
      "14845 Training Loss: tensor(0.0348)\n",
      "14846 Training Loss: tensor(0.0348)\n",
      "14847 Training Loss: tensor(0.0352)\n",
      "14848 Training Loss: tensor(0.0350)\n",
      "14849 Training Loss: tensor(0.0350)\n",
      "14850 Training Loss: tensor(0.0350)\n",
      "14851 Training Loss: tensor(0.0347)\n",
      "14852 Training Loss: tensor(0.0349)\n",
      "14853 Training Loss: tensor(0.0349)\n",
      "14854 Training Loss: tensor(0.0353)\n",
      "14855 Training Loss: tensor(0.0349)\n",
      "14856 Training Loss: tensor(0.0347)\n",
      "14857 Training Loss: tensor(0.0349)\n",
      "14858 Training Loss: tensor(0.0347)\n",
      "14859 Training Loss: tensor(0.0347)\n",
      "14860 Training Loss: tensor(0.0348)\n",
      "14861 Training Loss: tensor(0.0348)\n",
      "14862 Training Loss: tensor(0.0348)\n",
      "14863 Training Loss: tensor(0.0350)\n",
      "14864 Training Loss: tensor(0.0348)\n",
      "14865 Training Loss: tensor(0.0349)\n",
      "14866 Training Loss: tensor(0.0348)\n",
      "14867 Training Loss: tensor(0.0351)\n",
      "14868 Training Loss: tensor(0.0349)\n",
      "14869 Training Loss: tensor(0.0348)\n",
      "14870 Training Loss: tensor(0.0349)\n",
      "14871 Training Loss: tensor(0.0348)\n",
      "14872 Training Loss: tensor(0.0350)\n",
      "14873 Training Loss: tensor(0.0347)\n",
      "14874 Training Loss: tensor(0.0349)\n",
      "14875 Training Loss: tensor(0.0348)\n",
      "14876 Training Loss: tensor(0.0350)\n",
      "14877 Training Loss: tensor(0.0353)\n",
      "14878 Training Loss: tensor(0.0349)\n",
      "14879 Training Loss: tensor(0.0347)\n",
      "14880 Training Loss: tensor(0.0348)\n",
      "14881 Training Loss: tensor(0.0349)\n",
      "14882 Training Loss: tensor(0.0347)\n",
      "14883 Training Loss: tensor(0.0348)\n",
      "14884 Training Loss: tensor(0.0349)\n",
      "14885 Training Loss: tensor(0.0349)\n",
      "14886 Training Loss: tensor(0.0348)\n",
      "14887 Training Loss: tensor(0.0348)\n",
      "14888 Training Loss: tensor(0.0348)\n",
      "14889 Training Loss: tensor(0.0348)\n",
      "14890 Training Loss: tensor(0.0346)\n",
      "14891 Training Loss: tensor(0.0347)\n",
      "14892 Training Loss: tensor(0.0348)\n",
      "14893 Training Loss: tensor(0.0351)\n",
      "14894 Training Loss: tensor(0.0351)\n",
      "14895 Training Loss: tensor(0.0348)\n",
      "14896 Training Loss: tensor(0.0349)\n",
      "14897 Training Loss: tensor(0.0348)\n",
      "14898 Training Loss: tensor(0.0350)\n",
      "14899 Training Loss: tensor(0.0349)\n",
      "14900 Training Loss: tensor(0.0349)\n",
      "14901 Training Loss: tensor(0.0348)\n",
      "14902 Training Loss: tensor(0.0348)\n",
      "14903 Training Loss: tensor(0.0347)\n",
      "14904 Training Loss: tensor(0.0348)\n",
      "14905 Training Loss: tensor(0.0349)\n",
      "14906 Training Loss: tensor(0.0349)\n",
      "14907 Training Loss: tensor(0.0348)\n",
      "14908 Training Loss: tensor(0.0349)\n",
      "14909 Training Loss: tensor(0.0348)\n",
      "14910 Training Loss: tensor(0.0348)\n",
      "14911 Training Loss: tensor(0.0347)\n",
      "14912 Training Loss: tensor(0.0346)\n",
      "14913 Training Loss: tensor(0.0346)\n",
      "14914 Training Loss: tensor(0.0347)\n",
      "14915 Training Loss: tensor(0.0350)\n",
      "14916 Training Loss: tensor(0.0348)\n",
      "14917 Training Loss: tensor(0.0348)\n",
      "14918 Training Loss: tensor(0.0347)\n",
      "14919 Training Loss: tensor(0.0350)\n",
      "14920 Training Loss: tensor(0.0347)\n",
      "14921 Training Loss: tensor(0.0346)\n",
      "14922 Training Loss: tensor(0.0347)\n",
      "14923 Training Loss: tensor(0.0349)\n",
      "14924 Training Loss: tensor(0.0351)\n",
      "14925 Training Loss: tensor(0.0350)\n",
      "14926 Training Loss: tensor(0.0348)\n",
      "14927 Training Loss: tensor(0.0347)\n",
      "14928 Training Loss: tensor(0.0346)\n",
      "14929 Training Loss: tensor(0.0346)\n",
      "14930 Training Loss: tensor(0.0348)\n",
      "14931 Training Loss: tensor(0.0350)\n",
      "14932 Training Loss: tensor(0.0347)\n",
      "14933 Training Loss: tensor(0.0348)\n",
      "14934 Training Loss: tensor(0.0348)\n",
      "14935 Training Loss: tensor(0.0349)\n",
      "14936 Training Loss: tensor(0.0346)\n",
      "14937 Training Loss: tensor(0.0347)\n",
      "14938 Training Loss: tensor(0.0347)\n",
      "14939 Training Loss: tensor(0.0348)\n",
      "14940 Training Loss: tensor(0.0348)\n",
      "14941 Training Loss: tensor(0.0350)\n",
      "14942 Training Loss: tensor(0.0347)\n",
      "14943 Training Loss: tensor(0.0350)\n",
      "14944 Training Loss: tensor(0.0348)\n",
      "14945 Training Loss: tensor(0.0348)\n",
      "14946 Training Loss: tensor(0.0345)\n",
      "14947 Training Loss: tensor(0.0347)\n",
      "14948 Training Loss: tensor(0.0348)\n",
      "14949 Training Loss: tensor(0.0348)\n",
      "14950 Training Loss: tensor(0.0348)\n",
      "14951 Training Loss: tensor(0.0351)\n",
      "14952 Training Loss: tensor(0.0348)\n",
      "14953 Training Loss: tensor(0.0347)\n",
      "14954 Training Loss: tensor(0.0346)\n",
      "14955 Training Loss: tensor(0.0348)\n",
      "14956 Training Loss: tensor(0.0349)\n",
      "14957 Training Loss: tensor(0.0349)\n",
      "14958 Training Loss: tensor(0.0349)\n",
      "14959 Training Loss: tensor(0.0347)\n",
      "14960 Training Loss: tensor(0.0349)\n",
      "14961 Training Loss: tensor(0.0346)\n",
      "14962 Training Loss: tensor(0.0347)\n",
      "14963 Training Loss: tensor(0.0347)\n",
      "14964 Training Loss: tensor(0.0347)\n",
      "14965 Training Loss: tensor(0.0347)\n",
      "14966 Training Loss: tensor(0.0349)\n",
      "14967 Training Loss: tensor(0.0345)\n",
      "14968 Training Loss: tensor(0.0347)\n",
      "14969 Training Loss: tensor(0.0347)\n",
      "14970 Training Loss: tensor(0.0348)\n",
      "14971 Training Loss: tensor(0.0346)\n",
      "14972 Training Loss: tensor(0.0346)\n",
      "14973 Training Loss: tensor(0.0347)\n",
      "14974 Training Loss: tensor(0.0347)\n",
      "14975 Training Loss: tensor(0.0346)\n",
      "14976 Training Loss: tensor(0.0345)\n",
      "14977 Training Loss: tensor(0.0349)\n",
      "14978 Training Loss: tensor(0.0352)\n",
      "14979 Training Loss: tensor(0.0346)\n",
      "14980 Training Loss: tensor(0.0347)\n",
      "14981 Training Loss: tensor(0.0347)\n",
      "14982 Training Loss: tensor(0.0348)\n",
      "14983 Training Loss: tensor(0.0348)\n",
      "14984 Training Loss: tensor(0.0346)\n",
      "14985 Training Loss: tensor(0.0349)\n",
      "14986 Training Loss: tensor(0.0349)\n",
      "14987 Training Loss: tensor(0.0347)\n",
      "14988 Training Loss: tensor(0.0347)\n",
      "14989 Training Loss: tensor(0.0348)\n",
      "14990 Training Loss: tensor(0.0348)\n",
      "14991 Training Loss: tensor(0.0347)\n",
      "14992 Training Loss: tensor(0.0346)\n",
      "14993 Training Loss: tensor(0.0352)\n",
      "14994 Training Loss: tensor(0.0352)\n",
      "14995 Training Loss: tensor(0.0345)\n",
      "14996 Training Loss: tensor(0.0346)\n",
      "14997 Training Loss: tensor(0.0346)\n",
      "14998 Training Loss: tensor(0.0347)\n",
      "14999 Training Loss: tensor(0.0349)\n",
      "15000 Training Loss: tensor(0.0348)\n",
      "15001 Training Loss: tensor(0.0346)\n",
      "15002 Training Loss: tensor(0.0345)\n",
      "15003 Training Loss: tensor(0.0346)\n",
      "15004 Training Loss: tensor(0.0347)\n",
      "15005 Training Loss: tensor(0.0348)\n",
      "15006 Training Loss: tensor(0.0346)\n",
      "15007 Training Loss: tensor(0.0346)\n",
      "15008 Training Loss: tensor(0.0348)\n",
      "15009 Training Loss: tensor(0.0346)\n",
      "15010 Training Loss: tensor(0.0349)\n",
      "15011 Training Loss: tensor(0.0347)\n",
      "15012 Training Loss: tensor(0.0346)\n",
      "15013 Training Loss: tensor(0.0347)\n",
      "15014 Training Loss: tensor(0.0346)\n",
      "15015 Training Loss: tensor(0.0346)\n",
      "15016 Training Loss: tensor(0.0346)\n",
      "15017 Training Loss: tensor(0.0345)\n",
      "15018 Training Loss: tensor(0.0347)\n",
      "15019 Training Loss: tensor(0.0346)\n",
      "15020 Training Loss: tensor(0.0344)\n",
      "15021 Training Loss: tensor(0.0346)\n",
      "15022 Training Loss: tensor(0.0347)\n",
      "15023 Training Loss: tensor(0.0345)\n",
      "15024 Training Loss: tensor(0.0348)\n",
      "15025 Training Loss: tensor(0.0346)\n",
      "15026 Training Loss: tensor(0.0347)\n",
      "15027 Training Loss: tensor(0.0346)\n",
      "15028 Training Loss: tensor(0.0350)\n",
      "15029 Training Loss: tensor(0.0347)\n",
      "15030 Training Loss: tensor(0.0348)\n",
      "15031 Training Loss: tensor(0.0346)\n",
      "15032 Training Loss: tensor(0.0346)\n",
      "15033 Training Loss: tensor(0.0347)\n",
      "15034 Training Loss: tensor(0.0346)\n",
      "15035 Training Loss: tensor(0.0346)\n",
      "15036 Training Loss: tensor(0.0347)\n",
      "15037 Training Loss: tensor(0.0346)\n",
      "15038 Training Loss: tensor(0.0345)\n",
      "15039 Training Loss: tensor(0.0345)\n",
      "15040 Training Loss: tensor(0.0347)\n",
      "15041 Training Loss: tensor(0.0351)\n",
      "15042 Training Loss: tensor(0.0347)\n",
      "15043 Training Loss: tensor(0.0346)\n",
      "15044 Training Loss: tensor(0.0347)\n",
      "15045 Training Loss: tensor(0.0346)\n",
      "15046 Training Loss: tensor(0.0346)\n",
      "15047 Training Loss: tensor(0.0344)\n",
      "15048 Training Loss: tensor(0.0345)\n",
      "15049 Training Loss: tensor(0.0348)\n",
      "15050 Training Loss: tensor(0.0346)\n",
      "15051 Training Loss: tensor(0.0349)\n",
      "15052 Training Loss: tensor(0.0348)\n",
      "15053 Training Loss: tensor(0.0345)\n",
      "15054 Training Loss: tensor(0.0345)\n",
      "15055 Training Loss: tensor(0.0345)\n",
      "15056 Training Loss: tensor(0.0346)\n",
      "15057 Training Loss: tensor(0.0347)\n",
      "15058 Training Loss: tensor(0.0346)\n",
      "15059 Training Loss: tensor(0.0346)\n",
      "15060 Training Loss: tensor(0.0344)\n",
      "15061 Training Loss: tensor(0.0346)\n",
      "15062 Training Loss: tensor(0.0349)\n",
      "15063 Training Loss: tensor(0.0348)\n",
      "15064 Training Loss: tensor(0.0349)\n",
      "15065 Training Loss: tensor(0.0346)\n",
      "15066 Training Loss: tensor(0.0348)\n",
      "15067 Training Loss: tensor(0.0348)\n",
      "15068 Training Loss: tensor(0.0348)\n",
      "15069 Training Loss: tensor(0.0346)\n",
      "15070 Training Loss: tensor(0.0347)\n",
      "15071 Training Loss: tensor(0.0347)\n",
      "15072 Training Loss: tensor(0.0348)\n",
      "15073 Training Loss: tensor(0.0347)\n",
      "15074 Training Loss: tensor(0.0346)\n",
      "15075 Training Loss: tensor(0.0349)\n",
      "15076 Training Loss: tensor(0.0345)\n",
      "15077 Training Loss: tensor(0.0347)\n",
      "15078 Training Loss: tensor(0.0348)\n",
      "15079 Training Loss: tensor(0.0346)\n",
      "15080 Training Loss: tensor(0.0346)\n",
      "15081 Training Loss: tensor(0.0347)\n",
      "15082 Training Loss: tensor(0.0345)\n",
      "15083 Training Loss: tensor(0.0345)\n",
      "15084 Training Loss: tensor(0.0345)\n",
      "15085 Training Loss: tensor(0.0347)\n",
      "15086 Training Loss: tensor(0.0345)\n",
      "15087 Training Loss: tensor(0.0346)\n",
      "15088 Training Loss: tensor(0.0346)\n",
      "15089 Training Loss: tensor(0.0346)\n",
      "15090 Training Loss: tensor(0.0347)\n",
      "15091 Training Loss: tensor(0.0348)\n",
      "15092 Training Loss: tensor(0.0344)\n",
      "15093 Training Loss: tensor(0.0346)\n",
      "15094 Training Loss: tensor(0.0345)\n",
      "15095 Training Loss: tensor(0.0346)\n",
      "15096 Training Loss: tensor(0.0344)\n",
      "15097 Training Loss: tensor(0.0348)\n",
      "15098 Training Loss: tensor(0.0344)\n",
      "15099 Training Loss: tensor(0.0348)\n",
      "15100 Training Loss: tensor(0.0348)\n",
      "15101 Training Loss: tensor(0.0344)\n",
      "15102 Training Loss: tensor(0.0347)\n",
      "15103 Training Loss: tensor(0.0344)\n",
      "15104 Training Loss: tensor(0.0345)\n",
      "15105 Training Loss: tensor(0.0344)\n",
      "15106 Training Loss: tensor(0.0350)\n",
      "15107 Training Loss: tensor(0.0346)\n",
      "15108 Training Loss: tensor(0.0346)\n",
      "15109 Training Loss: tensor(0.0346)\n",
      "15110 Training Loss: tensor(0.0344)\n",
      "15111 Training Loss: tensor(0.0346)\n",
      "15112 Training Loss: tensor(0.0346)\n",
      "15113 Training Loss: tensor(0.0344)\n",
      "15114 Training Loss: tensor(0.0345)\n",
      "15115 Training Loss: tensor(0.0345)\n",
      "15116 Training Loss: tensor(0.0347)\n",
      "15117 Training Loss: tensor(0.0345)\n",
      "15118 Training Loss: tensor(0.0347)\n",
      "15119 Training Loss: tensor(0.0347)\n",
      "15120 Training Loss: tensor(0.0347)\n",
      "15121 Training Loss: tensor(0.0344)\n",
      "15122 Training Loss: tensor(0.0345)\n",
      "15123 Training Loss: tensor(0.0347)\n",
      "15124 Training Loss: tensor(0.0344)\n",
      "15125 Training Loss: tensor(0.0346)\n",
      "15126 Training Loss: tensor(0.0344)\n",
      "15127 Training Loss: tensor(0.0344)\n",
      "15128 Training Loss: tensor(0.0345)\n",
      "15129 Training Loss: tensor(0.0347)\n",
      "15130 Training Loss: tensor(0.0347)\n",
      "15131 Training Loss: tensor(0.0349)\n",
      "15132 Training Loss: tensor(0.0345)\n",
      "15133 Training Loss: tensor(0.0345)\n",
      "15134 Training Loss: tensor(0.0346)\n",
      "15135 Training Loss: tensor(0.0343)\n",
      "15136 Training Loss: tensor(0.0344)\n",
      "15137 Training Loss: tensor(0.0347)\n",
      "15138 Training Loss: tensor(0.0346)\n",
      "15139 Training Loss: tensor(0.0347)\n",
      "15140 Training Loss: tensor(0.0345)\n",
      "15141 Training Loss: tensor(0.0345)\n",
      "15142 Training Loss: tensor(0.0344)\n",
      "15143 Training Loss: tensor(0.0345)\n",
      "15144 Training Loss: tensor(0.0344)\n",
      "15145 Training Loss: tensor(0.0347)\n",
      "15146 Training Loss: tensor(0.0344)\n",
      "15147 Training Loss: tensor(0.0345)\n",
      "15148 Training Loss: tensor(0.0348)\n",
      "15149 Training Loss: tensor(0.0346)\n",
      "15150 Training Loss: tensor(0.0347)\n",
      "15151 Training Loss: tensor(0.0345)\n",
      "15152 Training Loss: tensor(0.0347)\n",
      "15153 Training Loss: tensor(0.0347)\n",
      "15154 Training Loss: tensor(0.0346)\n",
      "15155 Training Loss: tensor(0.0345)\n",
      "15156 Training Loss: tensor(0.0345)\n",
      "15157 Training Loss: tensor(0.0346)\n",
      "15158 Training Loss: tensor(0.0347)\n",
      "15159 Training Loss: tensor(0.0346)\n",
      "15160 Training Loss: tensor(0.0345)\n",
      "15161 Training Loss: tensor(0.0344)\n",
      "15162 Training Loss: tensor(0.0345)\n",
      "15163 Training Loss: tensor(0.0343)\n",
      "15164 Training Loss: tensor(0.0344)\n",
      "15165 Training Loss: tensor(0.0343)\n",
      "15166 Training Loss: tensor(0.0346)\n",
      "15167 Training Loss: tensor(0.0344)\n",
      "15168 Training Loss: tensor(0.0344)\n",
      "15169 Training Loss: tensor(0.0344)\n",
      "15170 Training Loss: tensor(0.0344)\n",
      "15171 Training Loss: tensor(0.0345)\n",
      "15172 Training Loss: tensor(0.0346)\n",
      "15173 Training Loss: tensor(0.0347)\n",
      "15174 Training Loss: tensor(0.0348)\n",
      "15175 Training Loss: tensor(0.0348)\n",
      "15176 Training Loss: tensor(0.0345)\n",
      "15177 Training Loss: tensor(0.0344)\n",
      "15178 Training Loss: tensor(0.0346)\n",
      "15179 Training Loss: tensor(0.0347)\n",
      "15180 Training Loss: tensor(0.0345)\n",
      "15181 Training Loss: tensor(0.0344)\n",
      "15182 Training Loss: tensor(0.0347)\n",
      "15183 Training Loss: tensor(0.0345)\n",
      "15184 Training Loss: tensor(0.0344)\n",
      "15185 Training Loss: tensor(0.0346)\n",
      "15186 Training Loss: tensor(0.0345)\n",
      "15187 Training Loss: tensor(0.0345)\n",
      "15188 Training Loss: tensor(0.0344)\n",
      "15189 Training Loss: tensor(0.0347)\n",
      "15190 Training Loss: tensor(0.0344)\n",
      "15191 Training Loss: tensor(0.0344)\n",
      "15192 Training Loss: tensor(0.0344)\n",
      "15193 Training Loss: tensor(0.0346)\n",
      "15194 Training Loss: tensor(0.0346)\n",
      "15195 Training Loss: tensor(0.0344)\n",
      "15196 Training Loss: tensor(0.0345)\n",
      "15197 Training Loss: tensor(0.0347)\n",
      "15198 Training Loss: tensor(0.0347)\n",
      "15199 Training Loss: tensor(0.0344)\n",
      "15200 Training Loss: tensor(0.0345)\n",
      "15201 Training Loss: tensor(0.0344)\n",
      "15202 Training Loss: tensor(0.0344)\n",
      "15203 Training Loss: tensor(0.0344)\n",
      "15204 Training Loss: tensor(0.0344)\n",
      "15205 Training Loss: tensor(0.0342)\n",
      "15206 Training Loss: tensor(0.0344)\n",
      "15207 Training Loss: tensor(0.0345)\n",
      "15208 Training Loss: tensor(0.0345)\n",
      "15209 Training Loss: tensor(0.0344)\n",
      "15210 Training Loss: tensor(0.0344)\n",
      "15211 Training Loss: tensor(0.0345)\n",
      "15212 Training Loss: tensor(0.0345)\n",
      "15213 Training Loss: tensor(0.0343)\n",
      "15214 Training Loss: tensor(0.0344)\n",
      "15215 Training Loss: tensor(0.0344)\n",
      "15216 Training Loss: tensor(0.0344)\n",
      "15217 Training Loss: tensor(0.0347)\n",
      "15218 Training Loss: tensor(0.0343)\n",
      "15219 Training Loss: tensor(0.0346)\n",
      "15220 Training Loss: tensor(0.0343)\n",
      "15221 Training Loss: tensor(0.0344)\n",
      "15222 Training Loss: tensor(0.0346)\n",
      "15223 Training Loss: tensor(0.0345)\n",
      "15224 Training Loss: tensor(0.0343)\n",
      "15225 Training Loss: tensor(0.0344)\n",
      "15226 Training Loss: tensor(0.0347)\n",
      "15227 Training Loss: tensor(0.0344)\n",
      "15228 Training Loss: tensor(0.0346)\n",
      "15229 Training Loss: tensor(0.0344)\n",
      "15230 Training Loss: tensor(0.0343)\n",
      "15231 Training Loss: tensor(0.0346)\n",
      "15232 Training Loss: tensor(0.0346)\n",
      "15233 Training Loss: tensor(0.0343)\n",
      "15234 Training Loss: tensor(0.0346)\n",
      "15235 Training Loss: tensor(0.0346)\n",
      "15236 Training Loss: tensor(0.0344)\n",
      "15237 Training Loss: tensor(0.0344)\n",
      "15238 Training Loss: tensor(0.0344)\n",
      "15239 Training Loss: tensor(0.0344)\n",
      "15240 Training Loss: tensor(0.0344)\n",
      "15241 Training Loss: tensor(0.0343)\n",
      "15242 Training Loss: tensor(0.0343)\n",
      "15243 Training Loss: tensor(0.0343)\n",
      "15244 Training Loss: tensor(0.0345)\n",
      "15245 Training Loss: tensor(0.0344)\n",
      "15246 Training Loss: tensor(0.0345)\n",
      "15247 Training Loss: tensor(0.0345)\n",
      "15248 Training Loss: tensor(0.0343)\n",
      "15249 Training Loss: tensor(0.0345)\n",
      "15250 Training Loss: tensor(0.0348)\n",
      "15251 Training Loss: tensor(0.0346)\n",
      "15252 Training Loss: tensor(0.0344)\n",
      "15253 Training Loss: tensor(0.0342)\n",
      "15254 Training Loss: tensor(0.0343)\n",
      "15255 Training Loss: tensor(0.0343)\n",
      "15256 Training Loss: tensor(0.0344)\n",
      "15257 Training Loss: tensor(0.0343)\n",
      "15258 Training Loss: tensor(0.0344)\n",
      "15259 Training Loss: tensor(0.0344)\n",
      "15260 Training Loss: tensor(0.0345)\n",
      "15261 Training Loss: tensor(0.0347)\n",
      "15262 Training Loss: tensor(0.0344)\n",
      "15263 Training Loss: tensor(0.0344)\n",
      "15264 Training Loss: tensor(0.0344)\n",
      "15265 Training Loss: tensor(0.0342)\n",
      "15266 Training Loss: tensor(0.0342)\n",
      "15267 Training Loss: tensor(0.0346)\n",
      "15268 Training Loss: tensor(0.0344)\n",
      "15269 Training Loss: tensor(0.0344)\n",
      "15270 Training Loss: tensor(0.0343)\n",
      "15271 Training Loss: tensor(0.0343)\n",
      "15272 Training Loss: tensor(0.0343)\n",
      "15273 Training Loss: tensor(0.0343)\n",
      "15274 Training Loss: tensor(0.0344)\n",
      "15275 Training Loss: tensor(0.0342)\n",
      "15276 Training Loss: tensor(0.0345)\n",
      "15277 Training Loss: tensor(0.0343)\n",
      "15278 Training Loss: tensor(0.0343)\n",
      "15279 Training Loss: tensor(0.0343)\n",
      "15280 Training Loss: tensor(0.0343)\n",
      "15281 Training Loss: tensor(0.0341)\n",
      "15282 Training Loss: tensor(0.0342)\n",
      "15283 Training Loss: tensor(0.0342)\n",
      "15284 Training Loss: tensor(0.0343)\n",
      "15285 Training Loss: tensor(0.0342)\n",
      "15286 Training Loss: tensor(0.0343)\n",
      "15287 Training Loss: tensor(0.0347)\n",
      "15288 Training Loss: tensor(0.0346)\n",
      "15289 Training Loss: tensor(0.0346)\n",
      "15290 Training Loss: tensor(0.0343)\n",
      "15291 Training Loss: tensor(0.0343)\n",
      "15292 Training Loss: tensor(0.0343)\n",
      "15293 Training Loss: tensor(0.0345)\n",
      "15294 Training Loss: tensor(0.0344)\n",
      "15295 Training Loss: tensor(0.0345)\n",
      "15296 Training Loss: tensor(0.0343)\n",
      "15297 Training Loss: tensor(0.0350)\n",
      "15298 Training Loss: tensor(0.0345)\n",
      "15299 Training Loss: tensor(0.0344)\n",
      "15300 Training Loss: tensor(0.0343)\n",
      "15301 Training Loss: tensor(0.0345)\n",
      "15302 Training Loss: tensor(0.0343)\n",
      "15303 Training Loss: tensor(0.0343)\n",
      "15304 Training Loss: tensor(0.0343)\n",
      "15305 Training Loss: tensor(0.0343)\n",
      "15306 Training Loss: tensor(0.0344)\n",
      "15307 Training Loss: tensor(0.0346)\n",
      "15308 Training Loss: tensor(0.0343)\n",
      "15309 Training Loss: tensor(0.0344)\n",
      "15310 Training Loss: tensor(0.0343)\n",
      "15311 Training Loss: tensor(0.0348)\n",
      "15312 Training Loss: tensor(0.0344)\n",
      "15313 Training Loss: tensor(0.0344)\n",
      "15314 Training Loss: tensor(0.0344)\n",
      "15315 Training Loss: tensor(0.0344)\n",
      "15316 Training Loss: tensor(0.0343)\n",
      "15317 Training Loss: tensor(0.0345)\n",
      "15318 Training Loss: tensor(0.0343)\n",
      "15319 Training Loss: tensor(0.0345)\n",
      "15320 Training Loss: tensor(0.0344)\n",
      "15321 Training Loss: tensor(0.0344)\n",
      "15322 Training Loss: tensor(0.0344)\n",
      "15323 Training Loss: tensor(0.0342)\n",
      "15324 Training Loss: tensor(0.0342)\n",
      "15325 Training Loss: tensor(0.0345)\n",
      "15326 Training Loss: tensor(0.0343)\n",
      "15327 Training Loss: tensor(0.0343)\n",
      "15328 Training Loss: tensor(0.0343)\n",
      "15329 Training Loss: tensor(0.0345)\n",
      "15330 Training Loss: tensor(0.0344)\n",
      "15331 Training Loss: tensor(0.0343)\n",
      "15332 Training Loss: tensor(0.0344)\n",
      "15333 Training Loss: tensor(0.0346)\n",
      "15334 Training Loss: tensor(0.0344)\n",
      "15335 Training Loss: tensor(0.0345)\n",
      "15336 Training Loss: tensor(0.0341)\n",
      "15337 Training Loss: tensor(0.0343)\n",
      "15338 Training Loss: tensor(0.0344)\n",
      "15339 Training Loss: tensor(0.0344)\n",
      "15340 Training Loss: tensor(0.0342)\n",
      "15341 Training Loss: tensor(0.0344)\n",
      "15342 Training Loss: tensor(0.0345)\n",
      "15343 Training Loss: tensor(0.0345)\n",
      "15344 Training Loss: tensor(0.0343)\n",
      "15345 Training Loss: tensor(0.0343)\n",
      "15346 Training Loss: tensor(0.0346)\n",
      "15347 Training Loss: tensor(0.0343)\n",
      "15348 Training Loss: tensor(0.0345)\n",
      "15349 Training Loss: tensor(0.0343)\n",
      "15350 Training Loss: tensor(0.0343)\n",
      "15351 Training Loss: tensor(0.0347)\n",
      "15352 Training Loss: tensor(0.0342)\n",
      "15353 Training Loss: tensor(0.0343)\n",
      "15354 Training Loss: tensor(0.0344)\n",
      "15355 Training Loss: tensor(0.0343)\n",
      "15356 Training Loss: tensor(0.0344)\n",
      "15357 Training Loss: tensor(0.0343)\n",
      "15358 Training Loss: tensor(0.0341)\n",
      "15359 Training Loss: tensor(0.0345)\n",
      "15360 Training Loss: tensor(0.0343)\n",
      "15361 Training Loss: tensor(0.0343)\n",
      "15362 Training Loss: tensor(0.0344)\n",
      "15363 Training Loss: tensor(0.0343)\n",
      "15364 Training Loss: tensor(0.0343)\n",
      "15365 Training Loss: tensor(0.0342)\n",
      "15366 Training Loss: tensor(0.0345)\n",
      "15367 Training Loss: tensor(0.0342)\n",
      "15368 Training Loss: tensor(0.0342)\n",
      "15369 Training Loss: tensor(0.0342)\n",
      "15370 Training Loss: tensor(0.0342)\n",
      "15371 Training Loss: tensor(0.0343)\n",
      "15372 Training Loss: tensor(0.0347)\n",
      "15373 Training Loss: tensor(0.0341)\n",
      "15374 Training Loss: tensor(0.0344)\n",
      "15375 Training Loss: tensor(0.0342)\n",
      "15376 Training Loss: tensor(0.0345)\n",
      "15377 Training Loss: tensor(0.0342)\n",
      "15378 Training Loss: tensor(0.0344)\n",
      "15379 Training Loss: tensor(0.0342)\n",
      "15380 Training Loss: tensor(0.0342)\n",
      "15381 Training Loss: tensor(0.0342)\n",
      "15382 Training Loss: tensor(0.0342)\n",
      "15383 Training Loss: tensor(0.0342)\n",
      "15384 Training Loss: tensor(0.0343)\n",
      "15385 Training Loss: tensor(0.0344)\n",
      "15386 Training Loss: tensor(0.0341)\n",
      "15387 Training Loss: tensor(0.0344)\n",
      "15388 Training Loss: tensor(0.0343)\n",
      "15389 Training Loss: tensor(0.0344)\n",
      "15390 Training Loss: tensor(0.0341)\n",
      "15391 Training Loss: tensor(0.0346)\n",
      "15392 Training Loss: tensor(0.0345)\n",
      "15393 Training Loss: tensor(0.0341)\n",
      "15394 Training Loss: tensor(0.0343)\n",
      "15395 Training Loss: tensor(0.0344)\n",
      "15396 Training Loss: tensor(0.0343)\n",
      "15397 Training Loss: tensor(0.0346)\n",
      "15398 Training Loss: tensor(0.0343)\n",
      "15399 Training Loss: tensor(0.0343)\n",
      "15400 Training Loss: tensor(0.0344)\n",
      "15401 Training Loss: tensor(0.0343)\n",
      "15402 Training Loss: tensor(0.0345)\n",
      "15403 Training Loss: tensor(0.0344)\n",
      "15404 Training Loss: tensor(0.0342)\n",
      "15405 Training Loss: tensor(0.0342)\n",
      "15406 Training Loss: tensor(0.0342)\n",
      "15407 Training Loss: tensor(0.0344)\n",
      "15408 Training Loss: tensor(0.0342)\n",
      "15409 Training Loss: tensor(0.0342)\n",
      "15410 Training Loss: tensor(0.0341)\n",
      "15411 Training Loss: tensor(0.0342)\n",
      "15412 Training Loss: tensor(0.0343)\n",
      "15413 Training Loss: tensor(0.0341)\n",
      "15414 Training Loss: tensor(0.0344)\n",
      "15415 Training Loss: tensor(0.0341)\n",
      "15416 Training Loss: tensor(0.0343)\n",
      "15417 Training Loss: tensor(0.0344)\n",
      "15418 Training Loss: tensor(0.0343)\n",
      "15419 Training Loss: tensor(0.0341)\n",
      "15420 Training Loss: tensor(0.0341)\n",
      "15421 Training Loss: tensor(0.0344)\n",
      "15422 Training Loss: tensor(0.0342)\n",
      "15423 Training Loss: tensor(0.0341)\n",
      "15424 Training Loss: tensor(0.0341)\n",
      "15425 Training Loss: tensor(0.0343)\n",
      "15426 Training Loss: tensor(0.0342)\n",
      "15427 Training Loss: tensor(0.0343)\n",
      "15428 Training Loss: tensor(0.0342)\n",
      "15429 Training Loss: tensor(0.0343)\n",
      "15430 Training Loss: tensor(0.0342)\n",
      "15431 Training Loss: tensor(0.0342)\n",
      "15432 Training Loss: tensor(0.0343)\n",
      "15433 Training Loss: tensor(0.0343)\n",
      "15434 Training Loss: tensor(0.0341)\n",
      "15435 Training Loss: tensor(0.0342)\n",
      "15436 Training Loss: tensor(0.0343)\n",
      "15437 Training Loss: tensor(0.0342)\n",
      "15438 Training Loss: tensor(0.0346)\n",
      "15439 Training Loss: tensor(0.0342)\n",
      "15440 Training Loss: tensor(0.0344)\n",
      "15441 Training Loss: tensor(0.0345)\n",
      "15442 Training Loss: tensor(0.0343)\n",
      "15443 Training Loss: tensor(0.0341)\n",
      "15444 Training Loss: tensor(0.0343)\n",
      "15445 Training Loss: tensor(0.0342)\n",
      "15446 Training Loss: tensor(0.0341)\n",
      "15447 Training Loss: tensor(0.0342)\n",
      "15448 Training Loss: tensor(0.0341)\n",
      "15449 Training Loss: tensor(0.0342)\n",
      "15450 Training Loss: tensor(0.0346)\n",
      "15451 Training Loss: tensor(0.0343)\n",
      "15452 Training Loss: tensor(0.0344)\n",
      "15453 Training Loss: tensor(0.0344)\n",
      "15454 Training Loss: tensor(0.0342)\n",
      "15455 Training Loss: tensor(0.0345)\n",
      "15456 Training Loss: tensor(0.0341)\n",
      "15457 Training Loss: tensor(0.0342)\n",
      "15458 Training Loss: tensor(0.0342)\n",
      "15459 Training Loss: tensor(0.0343)\n",
      "15460 Training Loss: tensor(0.0344)\n",
      "15461 Training Loss: tensor(0.0343)\n",
      "15462 Training Loss: tensor(0.0342)\n",
      "15463 Training Loss: tensor(0.0341)\n",
      "15464 Training Loss: tensor(0.0342)\n",
      "15465 Training Loss: tensor(0.0341)\n",
      "15466 Training Loss: tensor(0.0341)\n",
      "15467 Training Loss: tensor(0.0344)\n",
      "15468 Training Loss: tensor(0.0342)\n",
      "15469 Training Loss: tensor(0.0342)\n",
      "15470 Training Loss: tensor(0.0342)\n",
      "15471 Training Loss: tensor(0.0343)\n",
      "15472 Training Loss: tensor(0.0344)\n",
      "15473 Training Loss: tensor(0.0342)\n",
      "15474 Training Loss: tensor(0.0343)\n",
      "15475 Training Loss: tensor(0.0341)\n",
      "15476 Training Loss: tensor(0.0341)\n",
      "15477 Training Loss: tensor(0.0341)\n",
      "15478 Training Loss: tensor(0.0344)\n",
      "15479 Training Loss: tensor(0.0344)\n",
      "15480 Training Loss: tensor(0.0345)\n",
      "15481 Training Loss: tensor(0.0344)\n",
      "15482 Training Loss: tensor(0.0342)\n",
      "15483 Training Loss: tensor(0.0344)\n",
      "15484 Training Loss: tensor(0.0342)\n",
      "15485 Training Loss: tensor(0.0342)\n",
      "15486 Training Loss: tensor(0.0344)\n",
      "15487 Training Loss: tensor(0.0342)\n",
      "15488 Training Loss: tensor(0.0342)\n",
      "15489 Training Loss: tensor(0.0341)\n",
      "15490 Training Loss: tensor(0.0344)\n",
      "15491 Training Loss: tensor(0.0341)\n",
      "15492 Training Loss: tensor(0.0343)\n",
      "15493 Training Loss: tensor(0.0341)\n",
      "15494 Training Loss: tensor(0.0342)\n",
      "15495 Training Loss: tensor(0.0344)\n",
      "15496 Training Loss: tensor(0.0341)\n",
      "15497 Training Loss: tensor(0.0343)\n",
      "15498 Training Loss: tensor(0.0341)\n",
      "15499 Training Loss: tensor(0.0342)\n",
      "15500 Training Loss: tensor(0.0343)\n",
      "15501 Training Loss: tensor(0.0341)\n",
      "15502 Training Loss: tensor(0.0339)\n",
      "15503 Training Loss: tensor(0.0340)\n",
      "15504 Training Loss: tensor(0.0344)\n",
      "15505 Training Loss: tensor(0.0342)\n",
      "15506 Training Loss: tensor(0.0341)\n",
      "15507 Training Loss: tensor(0.0343)\n",
      "15508 Training Loss: tensor(0.0340)\n",
      "15509 Training Loss: tensor(0.0341)\n",
      "15510 Training Loss: tensor(0.0344)\n",
      "15511 Training Loss: tensor(0.0342)\n",
      "15512 Training Loss: tensor(0.0342)\n",
      "15513 Training Loss: tensor(0.0344)\n",
      "15514 Training Loss: tensor(0.0341)\n",
      "15515 Training Loss: tensor(0.0342)\n",
      "15516 Training Loss: tensor(0.0341)\n",
      "15517 Training Loss: tensor(0.0342)\n",
      "15518 Training Loss: tensor(0.0341)\n",
      "15519 Training Loss: tensor(0.0341)\n",
      "15520 Training Loss: tensor(0.0343)\n",
      "15521 Training Loss: tensor(0.0342)\n",
      "15522 Training Loss: tensor(0.0342)\n",
      "15523 Training Loss: tensor(0.0343)\n",
      "15524 Training Loss: tensor(0.0342)\n",
      "15525 Training Loss: tensor(0.0343)\n",
      "15526 Training Loss: tensor(0.0343)\n",
      "15527 Training Loss: tensor(0.0343)\n",
      "15528 Training Loss: tensor(0.0341)\n",
      "15529 Training Loss: tensor(0.0344)\n",
      "15530 Training Loss: tensor(0.0342)\n",
      "15531 Training Loss: tensor(0.0341)\n",
      "15532 Training Loss: tensor(0.0342)\n",
      "15533 Training Loss: tensor(0.0342)\n",
      "15534 Training Loss: tensor(0.0340)\n",
      "15535 Training Loss: tensor(0.0341)\n",
      "15536 Training Loss: tensor(0.0342)\n",
      "15537 Training Loss: tensor(0.0341)\n",
      "15538 Training Loss: tensor(0.0341)\n",
      "15539 Training Loss: tensor(0.0340)\n",
      "15540 Training Loss: tensor(0.0340)\n",
      "15541 Training Loss: tensor(0.0340)\n",
      "15542 Training Loss: tensor(0.0339)\n",
      "15543 Training Loss: tensor(0.0344)\n",
      "15544 Training Loss: tensor(0.0344)\n",
      "15545 Training Loss: tensor(0.0343)\n",
      "15546 Training Loss: tensor(0.0342)\n",
      "15547 Training Loss: tensor(0.0344)\n",
      "15548 Training Loss: tensor(0.0340)\n",
      "15549 Training Loss: tensor(0.0342)\n",
      "15550 Training Loss: tensor(0.0343)\n",
      "15551 Training Loss: tensor(0.0342)\n",
      "15552 Training Loss: tensor(0.0341)\n",
      "15553 Training Loss: tensor(0.0340)\n",
      "15554 Training Loss: tensor(0.0342)\n",
      "15555 Training Loss: tensor(0.0341)\n",
      "15556 Training Loss: tensor(0.0341)\n",
      "15557 Training Loss: tensor(0.0340)\n",
      "15558 Training Loss: tensor(0.0339)\n",
      "15559 Training Loss: tensor(0.0339)\n",
      "15560 Training Loss: tensor(0.0341)\n",
      "15561 Training Loss: tensor(0.0342)\n",
      "15562 Training Loss: tensor(0.0340)\n",
      "15563 Training Loss: tensor(0.0340)\n",
      "15564 Training Loss: tensor(0.0343)\n",
      "15565 Training Loss: tensor(0.0343)\n",
      "15566 Training Loss: tensor(0.0340)\n",
      "15567 Training Loss: tensor(0.0341)\n",
      "15568 Training Loss: tensor(0.0340)\n",
      "15569 Training Loss: tensor(0.0342)\n",
      "15570 Training Loss: tensor(0.0342)\n",
      "15571 Training Loss: tensor(0.0340)\n",
      "15572 Training Loss: tensor(0.0343)\n",
      "15573 Training Loss: tensor(0.0341)\n",
      "15574 Training Loss: tensor(0.0341)\n",
      "15575 Training Loss: tensor(0.0342)\n",
      "15576 Training Loss: tensor(0.0343)\n",
      "15577 Training Loss: tensor(0.0340)\n",
      "15578 Training Loss: tensor(0.0341)\n",
      "15579 Training Loss: tensor(0.0341)\n",
      "15580 Training Loss: tensor(0.0342)\n",
      "15581 Training Loss: tensor(0.0342)\n",
      "15582 Training Loss: tensor(0.0342)\n",
      "15583 Training Loss: tensor(0.0340)\n",
      "15584 Training Loss: tensor(0.0342)\n",
      "15585 Training Loss: tensor(0.0340)\n",
      "15586 Training Loss: tensor(0.0341)\n",
      "15587 Training Loss: tensor(0.0340)\n",
      "15588 Training Loss: tensor(0.0340)\n",
      "15589 Training Loss: tensor(0.0339)\n",
      "15590 Training Loss: tensor(0.0340)\n",
      "15591 Training Loss: tensor(0.0340)\n",
      "15592 Training Loss: tensor(0.0342)\n",
      "15593 Training Loss: tensor(0.0342)\n",
      "15594 Training Loss: tensor(0.0339)\n",
      "15595 Training Loss: tensor(0.0340)\n",
      "15596 Training Loss: tensor(0.0341)\n",
      "15597 Training Loss: tensor(0.0342)\n",
      "15598 Training Loss: tensor(0.0343)\n",
      "15599 Training Loss: tensor(0.0341)\n",
      "15600 Training Loss: tensor(0.0342)\n",
      "15601 Training Loss: tensor(0.0342)\n",
      "15602 Training Loss: tensor(0.0340)\n",
      "15603 Training Loss: tensor(0.0341)\n",
      "15604 Training Loss: tensor(0.0344)\n",
      "15605 Training Loss: tensor(0.0341)\n",
      "15606 Training Loss: tensor(0.0340)\n",
      "15607 Training Loss: tensor(0.0341)\n",
      "15608 Training Loss: tensor(0.0340)\n",
      "15609 Training Loss: tensor(0.0340)\n",
      "15610 Training Loss: tensor(0.0340)\n",
      "15611 Training Loss: tensor(0.0339)\n",
      "15612 Training Loss: tensor(0.0340)\n",
      "15613 Training Loss: tensor(0.0340)\n",
      "15614 Training Loss: tensor(0.0342)\n",
      "15615 Training Loss: tensor(0.0339)\n",
      "15616 Training Loss: tensor(0.0341)\n",
      "15617 Training Loss: tensor(0.0340)\n",
      "15618 Training Loss: tensor(0.0342)\n",
      "15619 Training Loss: tensor(0.0342)\n",
      "15620 Training Loss: tensor(0.0341)\n",
      "15621 Training Loss: tensor(0.0341)\n",
      "15622 Training Loss: tensor(0.0342)\n",
      "15623 Training Loss: tensor(0.0341)\n",
      "15624 Training Loss: tensor(0.0343)\n",
      "15625 Training Loss: tensor(0.0340)\n",
      "15626 Training Loss: tensor(0.0340)\n",
      "15627 Training Loss: tensor(0.0342)\n",
      "15628 Training Loss: tensor(0.0340)\n",
      "15629 Training Loss: tensor(0.0341)\n",
      "15630 Training Loss: tensor(0.0341)\n",
      "15631 Training Loss: tensor(0.0339)\n",
      "15632 Training Loss: tensor(0.0341)\n",
      "15633 Training Loss: tensor(0.0341)\n",
      "15634 Training Loss: tensor(0.0341)\n",
      "15635 Training Loss: tensor(0.0339)\n",
      "15636 Training Loss: tensor(0.0340)\n",
      "15637 Training Loss: tensor(0.0340)\n",
      "15638 Training Loss: tensor(0.0341)\n",
      "15639 Training Loss: tensor(0.0340)\n",
      "15640 Training Loss: tensor(0.0341)\n",
      "15641 Training Loss: tensor(0.0342)\n",
      "15642 Training Loss: tensor(0.0340)\n",
      "15643 Training Loss: tensor(0.0340)\n",
      "15644 Training Loss: tensor(0.0341)\n",
      "15645 Training Loss: tensor(0.0341)\n",
      "15646 Training Loss: tensor(0.0341)\n",
      "15647 Training Loss: tensor(0.0340)\n",
      "15648 Training Loss: tensor(0.0338)\n",
      "15649 Training Loss: tensor(0.0341)\n",
      "15650 Training Loss: tensor(0.0340)\n",
      "15651 Training Loss: tensor(0.0340)\n",
      "15652 Training Loss: tensor(0.0340)\n",
      "15653 Training Loss: tensor(0.0340)\n",
      "15654 Training Loss: tensor(0.0340)\n",
      "15655 Training Loss: tensor(0.0339)\n",
      "15656 Training Loss: tensor(0.0339)\n",
      "15657 Training Loss: tensor(0.0341)\n",
      "15658 Training Loss: tensor(0.0340)\n",
      "15659 Training Loss: tensor(0.0341)\n",
      "15660 Training Loss: tensor(0.0340)\n",
      "15661 Training Loss: tensor(0.0340)\n",
      "15662 Training Loss: tensor(0.0339)\n",
      "15663 Training Loss: tensor(0.0339)\n",
      "15664 Training Loss: tensor(0.0341)\n",
      "15665 Training Loss: tensor(0.0340)\n",
      "15666 Training Loss: tensor(0.0340)\n",
      "15667 Training Loss: tensor(0.0339)\n",
      "15668 Training Loss: tensor(0.0339)\n",
      "15669 Training Loss: tensor(0.0339)\n",
      "15670 Training Loss: tensor(0.0341)\n",
      "15671 Training Loss: tensor(0.0339)\n",
      "15672 Training Loss: tensor(0.0340)\n",
      "15673 Training Loss: tensor(0.0340)\n",
      "15674 Training Loss: tensor(0.0340)\n",
      "15675 Training Loss: tensor(0.0338)\n",
      "15676 Training Loss: tensor(0.0341)\n",
      "15677 Training Loss: tensor(0.0342)\n",
      "15678 Training Loss: tensor(0.0340)\n",
      "15679 Training Loss: tensor(0.0339)\n",
      "15680 Training Loss: tensor(0.0340)\n",
      "15681 Training Loss: tensor(0.0339)\n",
      "15682 Training Loss: tensor(0.0340)\n",
      "15683 Training Loss: tensor(0.0339)\n",
      "15684 Training Loss: tensor(0.0339)\n",
      "15685 Training Loss: tensor(0.0339)\n",
      "15686 Training Loss: tensor(0.0341)\n",
      "15687 Training Loss: tensor(0.0341)\n",
      "15688 Training Loss: tensor(0.0342)\n",
      "15689 Training Loss: tensor(0.0345)\n",
      "15690 Training Loss: tensor(0.0338)\n",
      "15691 Training Loss: tensor(0.0340)\n",
      "15692 Training Loss: tensor(0.0342)\n",
      "15693 Training Loss: tensor(0.0340)\n",
      "15694 Training Loss: tensor(0.0342)\n",
      "15695 Training Loss: tensor(0.0340)\n",
      "15696 Training Loss: tensor(0.0340)\n",
      "15697 Training Loss: tensor(0.0340)\n",
      "15698 Training Loss: tensor(0.0341)\n",
      "15699 Training Loss: tensor(0.0339)\n",
      "15700 Training Loss: tensor(0.0340)\n",
      "15701 Training Loss: tensor(0.0340)\n",
      "15702 Training Loss: tensor(0.0340)\n",
      "15703 Training Loss: tensor(0.0339)\n",
      "15704 Training Loss: tensor(0.0340)\n",
      "15705 Training Loss: tensor(0.0339)\n",
      "15706 Training Loss: tensor(0.0341)\n",
      "15707 Training Loss: tensor(0.0342)\n",
      "15708 Training Loss: tensor(0.0342)\n",
      "15709 Training Loss: tensor(0.0340)\n",
      "15710 Training Loss: tensor(0.0340)\n",
      "15711 Training Loss: tensor(0.0341)\n",
      "15712 Training Loss: tensor(0.0343)\n",
      "15713 Training Loss: tensor(0.0338)\n",
      "15714 Training Loss: tensor(0.0338)\n",
      "15715 Training Loss: tensor(0.0343)\n",
      "15716 Training Loss: tensor(0.0338)\n",
      "15717 Training Loss: tensor(0.0339)\n",
      "15718 Training Loss: tensor(0.0342)\n",
      "15719 Training Loss: tensor(0.0340)\n",
      "15720 Training Loss: tensor(0.0339)\n",
      "15721 Training Loss: tensor(0.0341)\n",
      "15722 Training Loss: tensor(0.0341)\n",
      "15723 Training Loss: tensor(0.0341)\n",
      "15724 Training Loss: tensor(0.0340)\n",
      "15725 Training Loss: tensor(0.0339)\n",
      "15726 Training Loss: tensor(0.0339)\n",
      "15727 Training Loss: tensor(0.0338)\n",
      "15728 Training Loss: tensor(0.0341)\n",
      "15729 Training Loss: tensor(0.0341)\n",
      "15730 Training Loss: tensor(0.0339)\n",
      "15731 Training Loss: tensor(0.0341)\n",
      "15732 Training Loss: tensor(0.0339)\n",
      "15733 Training Loss: tensor(0.0342)\n",
      "15734 Training Loss: tensor(0.0338)\n",
      "15735 Training Loss: tensor(0.0340)\n",
      "15736 Training Loss: tensor(0.0340)\n",
      "15737 Training Loss: tensor(0.0341)\n",
      "15738 Training Loss: tensor(0.0341)\n",
      "15739 Training Loss: tensor(0.0340)\n",
      "15740 Training Loss: tensor(0.0340)\n",
      "15741 Training Loss: tensor(0.0341)\n",
      "15742 Training Loss: tensor(0.0340)\n",
      "15743 Training Loss: tensor(0.0340)\n",
      "15744 Training Loss: tensor(0.0340)\n",
      "15745 Training Loss: tensor(0.0338)\n",
      "15746 Training Loss: tensor(0.0339)\n",
      "15747 Training Loss: tensor(0.0341)\n",
      "15748 Training Loss: tensor(0.0340)\n",
      "15749 Training Loss: tensor(0.0340)\n",
      "15750 Training Loss: tensor(0.0338)\n",
      "15751 Training Loss: tensor(0.0338)\n",
      "15752 Training Loss: tensor(0.0339)\n",
      "15753 Training Loss: tensor(0.0341)\n",
      "15754 Training Loss: tensor(0.0339)\n",
      "15755 Training Loss: tensor(0.0341)\n",
      "15756 Training Loss: tensor(0.0338)\n",
      "15757 Training Loss: tensor(0.0343)\n",
      "15758 Training Loss: tensor(0.0340)\n",
      "15759 Training Loss: tensor(0.0339)\n",
      "15760 Training Loss: tensor(0.0338)\n",
      "15761 Training Loss: tensor(0.0341)\n",
      "15762 Training Loss: tensor(0.0338)\n",
      "15763 Training Loss: tensor(0.0342)\n",
      "15764 Training Loss: tensor(0.0340)\n",
      "15765 Training Loss: tensor(0.0339)\n",
      "15766 Training Loss: tensor(0.0337)\n",
      "15767 Training Loss: tensor(0.0341)\n",
      "15768 Training Loss: tensor(0.0340)\n",
      "15769 Training Loss: tensor(0.0338)\n",
      "15770 Training Loss: tensor(0.0340)\n",
      "15771 Training Loss: tensor(0.0340)\n",
      "15772 Training Loss: tensor(0.0340)\n",
      "15773 Training Loss: tensor(0.0337)\n",
      "15774 Training Loss: tensor(0.0342)\n",
      "15775 Training Loss: tensor(0.0341)\n",
      "15776 Training Loss: tensor(0.0339)\n",
      "15777 Training Loss: tensor(0.0337)\n",
      "15778 Training Loss: tensor(0.0339)\n",
      "15779 Training Loss: tensor(0.0339)\n",
      "15780 Training Loss: tensor(0.0339)\n",
      "15781 Training Loss: tensor(0.0338)\n",
      "15782 Training Loss: tensor(0.0340)\n",
      "15783 Training Loss: tensor(0.0338)\n",
      "15784 Training Loss: tensor(0.0338)\n",
      "15785 Training Loss: tensor(0.0339)\n",
      "15786 Training Loss: tensor(0.0340)\n",
      "15787 Training Loss: tensor(0.0339)\n",
      "15788 Training Loss: tensor(0.0338)\n",
      "15789 Training Loss: tensor(0.0339)\n",
      "15790 Training Loss: tensor(0.0340)\n",
      "15791 Training Loss: tensor(0.0340)\n",
      "15792 Training Loss: tensor(0.0337)\n",
      "15793 Training Loss: tensor(0.0338)\n",
      "15794 Training Loss: tensor(0.0338)\n",
      "15795 Training Loss: tensor(0.0338)\n",
      "15796 Training Loss: tensor(0.0337)\n",
      "15797 Training Loss: tensor(0.0340)\n",
      "15798 Training Loss: tensor(0.0339)\n",
      "15799 Training Loss: tensor(0.0339)\n",
      "15800 Training Loss: tensor(0.0338)\n",
      "15801 Training Loss: tensor(0.0339)\n",
      "15802 Training Loss: tensor(0.0339)\n",
      "15803 Training Loss: tensor(0.0340)\n",
      "15804 Training Loss: tensor(0.0340)\n",
      "15805 Training Loss: tensor(0.0342)\n",
      "15806 Training Loss: tensor(0.0341)\n",
      "15807 Training Loss: tensor(0.0340)\n",
      "15808 Training Loss: tensor(0.0339)\n",
      "15809 Training Loss: tensor(0.0340)\n",
      "15810 Training Loss: tensor(0.0340)\n",
      "15811 Training Loss: tensor(0.0340)\n",
      "15812 Training Loss: tensor(0.0342)\n",
      "15813 Training Loss: tensor(0.0339)\n",
      "15814 Training Loss: tensor(0.0342)\n",
      "15815 Training Loss: tensor(0.0339)\n",
      "15816 Training Loss: tensor(0.0339)\n",
      "15817 Training Loss: tensor(0.0341)\n",
      "15818 Training Loss: tensor(0.0340)\n",
      "15819 Training Loss: tensor(0.0339)\n",
      "15820 Training Loss: tensor(0.0340)\n",
      "15821 Training Loss: tensor(0.0340)\n",
      "15822 Training Loss: tensor(0.0339)\n",
      "15823 Training Loss: tensor(0.0339)\n",
      "15824 Training Loss: tensor(0.0342)\n",
      "15825 Training Loss: tensor(0.0339)\n",
      "15826 Training Loss: tensor(0.0337)\n",
      "15827 Training Loss: tensor(0.0341)\n",
      "15828 Training Loss: tensor(0.0340)\n",
      "15829 Training Loss: tensor(0.0339)\n",
      "15830 Training Loss: tensor(0.0337)\n",
      "15831 Training Loss: tensor(0.0340)\n",
      "15832 Training Loss: tensor(0.0339)\n",
      "15833 Training Loss: tensor(0.0337)\n",
      "15834 Training Loss: tensor(0.0338)\n",
      "15835 Training Loss: tensor(0.0342)\n",
      "15836 Training Loss: tensor(0.0338)\n",
      "15837 Training Loss: tensor(0.0339)\n",
      "15838 Training Loss: tensor(0.0339)\n",
      "15839 Training Loss: tensor(0.0340)\n",
      "15840 Training Loss: tensor(0.0343)\n",
      "15841 Training Loss: tensor(0.0340)\n",
      "15842 Training Loss: tensor(0.0339)\n",
      "15843 Training Loss: tensor(0.0337)\n",
      "15844 Training Loss: tensor(0.0338)\n",
      "15845 Training Loss: tensor(0.0339)\n",
      "15846 Training Loss: tensor(0.0339)\n",
      "15847 Training Loss: tensor(0.0339)\n",
      "15848 Training Loss: tensor(0.0339)\n",
      "15849 Training Loss: tensor(0.0339)\n",
      "15850 Training Loss: tensor(0.0338)\n",
      "15851 Training Loss: tensor(0.0341)\n",
      "15852 Training Loss: tensor(0.0340)\n",
      "15853 Training Loss: tensor(0.0339)\n",
      "15854 Training Loss: tensor(0.0338)\n",
      "15855 Training Loss: tensor(0.0340)\n",
      "15856 Training Loss: tensor(0.0338)\n",
      "15857 Training Loss: tensor(0.0339)\n",
      "15858 Training Loss: tensor(0.0339)\n",
      "15859 Training Loss: tensor(0.0337)\n",
      "15860 Training Loss: tensor(0.0339)\n",
      "15861 Training Loss: tensor(0.0339)\n",
      "15862 Training Loss: tensor(0.0338)\n",
      "15863 Training Loss: tensor(0.0339)\n",
      "15864 Training Loss: tensor(0.0338)\n",
      "15865 Training Loss: tensor(0.0341)\n",
      "15866 Training Loss: tensor(0.0341)\n",
      "15867 Training Loss: tensor(0.0338)\n",
      "15868 Training Loss: tensor(0.0340)\n",
      "15869 Training Loss: tensor(0.0338)\n",
      "15870 Training Loss: tensor(0.0338)\n",
      "15871 Training Loss: tensor(0.0339)\n",
      "15872 Training Loss: tensor(0.0339)\n",
      "15873 Training Loss: tensor(0.0340)\n",
      "15874 Training Loss: tensor(0.0339)\n",
      "15875 Training Loss: tensor(0.0340)\n",
      "15876 Training Loss: tensor(0.0339)\n",
      "15877 Training Loss: tensor(0.0338)\n",
      "15878 Training Loss: tensor(0.0338)\n",
      "15879 Training Loss: tensor(0.0337)\n",
      "15880 Training Loss: tensor(0.0337)\n",
      "15881 Training Loss: tensor(0.0338)\n",
      "15882 Training Loss: tensor(0.0340)\n",
      "15883 Training Loss: tensor(0.0338)\n",
      "15884 Training Loss: tensor(0.0340)\n",
      "15885 Training Loss: tensor(0.0340)\n",
      "15886 Training Loss: tensor(0.0340)\n",
      "15887 Training Loss: tensor(0.0339)\n",
      "15888 Training Loss: tensor(0.0337)\n",
      "15889 Training Loss: tensor(0.0338)\n",
      "15890 Training Loss: tensor(0.0338)\n",
      "15891 Training Loss: tensor(0.0337)\n",
      "15892 Training Loss: tensor(0.0340)\n",
      "15893 Training Loss: tensor(0.0338)\n",
      "15894 Training Loss: tensor(0.0337)\n",
      "15895 Training Loss: tensor(0.0340)\n",
      "15896 Training Loss: tensor(0.0338)\n",
      "15897 Training Loss: tensor(0.0339)\n",
      "15898 Training Loss: tensor(0.0339)\n",
      "15899 Training Loss: tensor(0.0339)\n",
      "15900 Training Loss: tensor(0.0341)\n",
      "15901 Training Loss: tensor(0.0339)\n",
      "15902 Training Loss: tensor(0.0338)\n",
      "15903 Training Loss: tensor(0.0338)\n",
      "15904 Training Loss: tensor(0.0339)\n",
      "15905 Training Loss: tensor(0.0340)\n",
      "15906 Training Loss: tensor(0.0340)\n",
      "15907 Training Loss: tensor(0.0339)\n",
      "15908 Training Loss: tensor(0.0338)\n",
      "15909 Training Loss: tensor(0.0340)\n",
      "15910 Training Loss: tensor(0.0337)\n",
      "15911 Training Loss: tensor(0.0338)\n",
      "15912 Training Loss: tensor(0.0338)\n",
      "15913 Training Loss: tensor(0.0337)\n",
      "15914 Training Loss: tensor(0.0339)\n",
      "15915 Training Loss: tensor(0.0340)\n",
      "15916 Training Loss: tensor(0.0338)\n",
      "15917 Training Loss: tensor(0.0340)\n",
      "15918 Training Loss: tensor(0.0338)\n",
      "15919 Training Loss: tensor(0.0339)\n",
      "15920 Training Loss: tensor(0.0339)\n",
      "15921 Training Loss: tensor(0.0340)\n",
      "15922 Training Loss: tensor(0.0338)\n",
      "15923 Training Loss: tensor(0.0339)\n",
      "15924 Training Loss: tensor(0.0339)\n",
      "15925 Training Loss: tensor(0.0339)\n",
      "15926 Training Loss: tensor(0.0338)\n",
      "15927 Training Loss: tensor(0.0338)\n",
      "15928 Training Loss: tensor(0.0337)\n",
      "15929 Training Loss: tensor(0.0337)\n",
      "15930 Training Loss: tensor(0.0338)\n",
      "15931 Training Loss: tensor(0.0338)\n",
      "15932 Training Loss: tensor(0.0340)\n",
      "15933 Training Loss: tensor(0.0339)\n",
      "15934 Training Loss: tensor(0.0339)\n",
      "15935 Training Loss: tensor(0.0338)\n",
      "15936 Training Loss: tensor(0.0338)\n",
      "15937 Training Loss: tensor(0.0339)\n",
      "15938 Training Loss: tensor(0.0335)\n",
      "15939 Training Loss: tensor(0.0337)\n",
      "15940 Training Loss: tensor(0.0337)\n",
      "15941 Training Loss: tensor(0.0338)\n",
      "15942 Training Loss: tensor(0.0338)\n",
      "15943 Training Loss: tensor(0.0336)\n",
      "15944 Training Loss: tensor(0.0342)\n",
      "15945 Training Loss: tensor(0.0339)\n",
      "15946 Training Loss: tensor(0.0339)\n",
      "15947 Training Loss: tensor(0.0339)\n",
      "15948 Training Loss: tensor(0.0339)\n",
      "15949 Training Loss: tensor(0.0339)\n",
      "15950 Training Loss: tensor(0.0337)\n",
      "15951 Training Loss: tensor(0.0339)\n",
      "15952 Training Loss: tensor(0.0339)\n",
      "15953 Training Loss: tensor(0.0337)\n",
      "15954 Training Loss: tensor(0.0338)\n",
      "15955 Training Loss: tensor(0.0339)\n",
      "15956 Training Loss: tensor(0.0340)\n",
      "15957 Training Loss: tensor(0.0340)\n",
      "15958 Training Loss: tensor(0.0337)\n",
      "15959 Training Loss: tensor(0.0338)\n",
      "15960 Training Loss: tensor(0.0337)\n",
      "15961 Training Loss: tensor(0.0338)\n",
      "15962 Training Loss: tensor(0.0338)\n",
      "15963 Training Loss: tensor(0.0340)\n",
      "15964 Training Loss: tensor(0.0339)\n",
      "15965 Training Loss: tensor(0.0338)\n",
      "15966 Training Loss: tensor(0.0337)\n",
      "15967 Training Loss: tensor(0.0338)\n",
      "15968 Training Loss: tensor(0.0339)\n",
      "15969 Training Loss: tensor(0.0338)\n",
      "15970 Training Loss: tensor(0.0338)\n",
      "15971 Training Loss: tensor(0.0340)\n",
      "15972 Training Loss: tensor(0.0338)\n",
      "15973 Training Loss: tensor(0.0337)\n",
      "15974 Training Loss: tensor(0.0337)\n",
      "15975 Training Loss: tensor(0.0338)\n",
      "15976 Training Loss: tensor(0.0339)\n",
      "15977 Training Loss: tensor(0.0337)\n",
      "15978 Training Loss: tensor(0.0340)\n",
      "15979 Training Loss: tensor(0.0340)\n",
      "15980 Training Loss: tensor(0.0338)\n",
      "15981 Training Loss: tensor(0.0337)\n",
      "15982 Training Loss: tensor(0.0340)\n",
      "15983 Training Loss: tensor(0.0339)\n",
      "15984 Training Loss: tensor(0.0338)\n",
      "15985 Training Loss: tensor(0.0338)\n",
      "15986 Training Loss: tensor(0.0338)\n",
      "15987 Training Loss: tensor(0.0338)\n",
      "15988 Training Loss: tensor(0.0338)\n",
      "15989 Training Loss: tensor(0.0339)\n",
      "15990 Training Loss: tensor(0.0338)\n",
      "15991 Training Loss: tensor(0.0338)\n",
      "15992 Training Loss: tensor(0.0340)\n",
      "15993 Training Loss: tensor(0.0336)\n",
      "15994 Training Loss: tensor(0.0338)\n",
      "15995 Training Loss: tensor(0.0340)\n",
      "15996 Training Loss: tensor(0.0338)\n",
      "15997 Training Loss: tensor(0.0339)\n",
      "15998 Training Loss: tensor(0.0337)\n",
      "15999 Training Loss: tensor(0.0338)\n",
      "16000 Training Loss: tensor(0.0339)\n",
      "16001 Training Loss: tensor(0.0339)\n",
      "16002 Training Loss: tensor(0.0338)\n",
      "16003 Training Loss: tensor(0.0338)\n",
      "16004 Training Loss: tensor(0.0339)\n",
      "16005 Training Loss: tensor(0.0340)\n",
      "16006 Training Loss: tensor(0.0337)\n",
      "16007 Training Loss: tensor(0.0337)\n",
      "16008 Training Loss: tensor(0.0337)\n",
      "16009 Training Loss: tensor(0.0336)\n",
      "16010 Training Loss: tensor(0.0337)\n",
      "16011 Training Loss: tensor(0.0340)\n",
      "16012 Training Loss: tensor(0.0337)\n",
      "16013 Training Loss: tensor(0.0337)\n",
      "16014 Training Loss: tensor(0.0338)\n",
      "16015 Training Loss: tensor(0.0337)\n",
      "16016 Training Loss: tensor(0.0341)\n",
      "16017 Training Loss: tensor(0.0337)\n",
      "16018 Training Loss: tensor(0.0338)\n",
      "16019 Training Loss: tensor(0.0338)\n",
      "16020 Training Loss: tensor(0.0337)\n",
      "16021 Training Loss: tensor(0.0338)\n",
      "16022 Training Loss: tensor(0.0337)\n",
      "16023 Training Loss: tensor(0.0336)\n",
      "16024 Training Loss: tensor(0.0338)\n",
      "16025 Training Loss: tensor(0.0338)\n",
      "16026 Training Loss: tensor(0.0336)\n",
      "16027 Training Loss: tensor(0.0337)\n",
      "16028 Training Loss: tensor(0.0338)\n",
      "16029 Training Loss: tensor(0.0337)\n",
      "16030 Training Loss: tensor(0.0337)\n",
      "16031 Training Loss: tensor(0.0339)\n",
      "16032 Training Loss: tensor(0.0338)\n",
      "16033 Training Loss: tensor(0.0339)\n",
      "16034 Training Loss: tensor(0.0339)\n",
      "16035 Training Loss: tensor(0.0338)\n",
      "16036 Training Loss: tensor(0.0338)\n",
      "16037 Training Loss: tensor(0.0339)\n",
      "16038 Training Loss: tensor(0.0336)\n",
      "16039 Training Loss: tensor(0.0337)\n",
      "16040 Training Loss: tensor(0.0336)\n",
      "16041 Training Loss: tensor(0.0338)\n",
      "16042 Training Loss: tensor(0.0339)\n",
      "16043 Training Loss: tensor(0.0337)\n",
      "16044 Training Loss: tensor(0.0336)\n",
      "16045 Training Loss: tensor(0.0337)\n",
      "16046 Training Loss: tensor(0.0337)\n",
      "16047 Training Loss: tensor(0.0336)\n",
      "16048 Training Loss: tensor(0.0337)\n",
      "16049 Training Loss: tensor(0.0337)\n",
      "16050 Training Loss: tensor(0.0339)\n",
      "16051 Training Loss: tensor(0.0336)\n",
      "16052 Training Loss: tensor(0.0337)\n",
      "16053 Training Loss: tensor(0.0338)\n",
      "16054 Training Loss: tensor(0.0338)\n",
      "16055 Training Loss: tensor(0.0337)\n",
      "16056 Training Loss: tensor(0.0336)\n",
      "16057 Training Loss: tensor(0.0336)\n",
      "16058 Training Loss: tensor(0.0338)\n",
      "16059 Training Loss: tensor(0.0342)\n",
      "16060 Training Loss: tensor(0.0336)\n",
      "16061 Training Loss: tensor(0.0338)\n",
      "16062 Training Loss: tensor(0.0338)\n",
      "16063 Training Loss: tensor(0.0335)\n",
      "16064 Training Loss: tensor(0.0337)\n",
      "16065 Training Loss: tensor(0.0337)\n",
      "16066 Training Loss: tensor(0.0337)\n",
      "16067 Training Loss: tensor(0.0335)\n",
      "16068 Training Loss: tensor(0.0337)\n",
      "16069 Training Loss: tensor(0.0337)\n",
      "16070 Training Loss: tensor(0.0337)\n",
      "16071 Training Loss: tensor(0.0336)\n",
      "16072 Training Loss: tensor(0.0336)\n",
      "16073 Training Loss: tensor(0.0341)\n",
      "16074 Training Loss: tensor(0.0336)\n",
      "16075 Training Loss: tensor(0.0339)\n",
      "16076 Training Loss: tensor(0.0338)\n",
      "16077 Training Loss: tensor(0.0337)\n",
      "16078 Training Loss: tensor(0.0337)\n",
      "16079 Training Loss: tensor(0.0335)\n",
      "16080 Training Loss: tensor(0.0337)\n",
      "16081 Training Loss: tensor(0.0337)\n",
      "16082 Training Loss: tensor(0.0336)\n",
      "16083 Training Loss: tensor(0.0337)\n",
      "16084 Training Loss: tensor(0.0339)\n",
      "16085 Training Loss: tensor(0.0337)\n",
      "16086 Training Loss: tensor(0.0336)\n",
      "16087 Training Loss: tensor(0.0339)\n",
      "16088 Training Loss: tensor(0.0336)\n",
      "16089 Training Loss: tensor(0.0338)\n",
      "16090 Training Loss: tensor(0.0337)\n",
      "16091 Training Loss: tensor(0.0336)\n",
      "16092 Training Loss: tensor(0.0335)\n",
      "16093 Training Loss: tensor(0.0338)\n",
      "16094 Training Loss: tensor(0.0336)\n",
      "16095 Training Loss: tensor(0.0337)\n",
      "16096 Training Loss: tensor(0.0337)\n",
      "16097 Training Loss: tensor(0.0338)\n",
      "16098 Training Loss: tensor(0.0339)\n",
      "16099 Training Loss: tensor(0.0336)\n",
      "16100 Training Loss: tensor(0.0337)\n",
      "16101 Training Loss: tensor(0.0338)\n",
      "16102 Training Loss: tensor(0.0337)\n",
      "16103 Training Loss: tensor(0.0336)\n",
      "16104 Training Loss: tensor(0.0336)\n",
      "16105 Training Loss: tensor(0.0337)\n",
      "16106 Training Loss: tensor(0.0339)\n",
      "16107 Training Loss: tensor(0.0339)\n",
      "16108 Training Loss: tensor(0.0337)\n",
      "16109 Training Loss: tensor(0.0336)\n",
      "16110 Training Loss: tensor(0.0338)\n",
      "16111 Training Loss: tensor(0.0338)\n",
      "16112 Training Loss: tensor(0.0338)\n",
      "16113 Training Loss: tensor(0.0339)\n",
      "16114 Training Loss: tensor(0.0338)\n",
      "16115 Training Loss: tensor(0.0339)\n",
      "16116 Training Loss: tensor(0.0337)\n",
      "16117 Training Loss: tensor(0.0339)\n",
      "16118 Training Loss: tensor(0.0337)\n",
      "16119 Training Loss: tensor(0.0337)\n",
      "16120 Training Loss: tensor(0.0337)\n",
      "16121 Training Loss: tensor(0.0338)\n",
      "16122 Training Loss: tensor(0.0336)\n",
      "16123 Training Loss: tensor(0.0336)\n",
      "16124 Training Loss: tensor(0.0336)\n",
      "16125 Training Loss: tensor(0.0337)\n",
      "16126 Training Loss: tensor(0.0336)\n",
      "16127 Training Loss: tensor(0.0337)\n",
      "16128 Training Loss: tensor(0.0335)\n",
      "16129 Training Loss: tensor(0.0337)\n",
      "16130 Training Loss: tensor(0.0338)\n",
      "16131 Training Loss: tensor(0.0341)\n",
      "16132 Training Loss: tensor(0.0336)\n",
      "16133 Training Loss: tensor(0.0338)\n",
      "16134 Training Loss: tensor(0.0336)\n",
      "16135 Training Loss: tensor(0.0336)\n",
      "16136 Training Loss: tensor(0.0337)\n",
      "16137 Training Loss: tensor(0.0336)\n",
      "16138 Training Loss: tensor(0.0338)\n",
      "16139 Training Loss: tensor(0.0336)\n",
      "16140 Training Loss: tensor(0.0338)\n",
      "16141 Training Loss: tensor(0.0337)\n",
      "16142 Training Loss: tensor(0.0337)\n",
      "16143 Training Loss: tensor(0.0338)\n",
      "16144 Training Loss: tensor(0.0338)\n",
      "16145 Training Loss: tensor(0.0335)\n",
      "16146 Training Loss: tensor(0.0336)\n",
      "16147 Training Loss: tensor(0.0335)\n",
      "16148 Training Loss: tensor(0.0337)\n",
      "16149 Training Loss: tensor(0.0336)\n",
      "16150 Training Loss: tensor(0.0338)\n",
      "16151 Training Loss: tensor(0.0336)\n",
      "16152 Training Loss: tensor(0.0336)\n",
      "16153 Training Loss: tensor(0.0335)\n",
      "16154 Training Loss: tensor(0.0336)\n",
      "16155 Training Loss: tensor(0.0335)\n",
      "16156 Training Loss: tensor(0.0335)\n",
      "16157 Training Loss: tensor(0.0335)\n",
      "16158 Training Loss: tensor(0.0341)\n",
      "16159 Training Loss: tensor(0.0336)\n",
      "16160 Training Loss: tensor(0.0335)\n",
      "16161 Training Loss: tensor(0.0335)\n",
      "16162 Training Loss: tensor(0.0334)\n",
      "16163 Training Loss: tensor(0.0337)\n",
      "16164 Training Loss: tensor(0.0338)\n",
      "16165 Training Loss: tensor(0.0336)\n",
      "16166 Training Loss: tensor(0.0335)\n",
      "16167 Training Loss: tensor(0.0340)\n",
      "16168 Training Loss: tensor(0.0335)\n",
      "16169 Training Loss: tensor(0.0335)\n",
      "16170 Training Loss: tensor(0.0337)\n",
      "16171 Training Loss: tensor(0.0336)\n",
      "16172 Training Loss: tensor(0.0336)\n",
      "16173 Training Loss: tensor(0.0337)\n",
      "16174 Training Loss: tensor(0.0335)\n",
      "16175 Training Loss: tensor(0.0337)\n",
      "16176 Training Loss: tensor(0.0336)\n",
      "16177 Training Loss: tensor(0.0337)\n",
      "16178 Training Loss: tensor(0.0336)\n",
      "16179 Training Loss: tensor(0.0336)\n",
      "16180 Training Loss: tensor(0.0336)\n",
      "16181 Training Loss: tensor(0.0335)\n",
      "16182 Training Loss: tensor(0.0335)\n",
      "16183 Training Loss: tensor(0.0337)\n",
      "16184 Training Loss: tensor(0.0336)\n",
      "16185 Training Loss: tensor(0.0337)\n",
      "16186 Training Loss: tensor(0.0336)\n",
      "16187 Training Loss: tensor(0.0336)\n",
      "16188 Training Loss: tensor(0.0335)\n",
      "16189 Training Loss: tensor(0.0336)\n",
      "16190 Training Loss: tensor(0.0336)\n",
      "16191 Training Loss: tensor(0.0337)\n",
      "16192 Training Loss: tensor(0.0339)\n",
      "16193 Training Loss: tensor(0.0335)\n",
      "16194 Training Loss: tensor(0.0336)\n",
      "16195 Training Loss: tensor(0.0335)\n",
      "16196 Training Loss: tensor(0.0337)\n",
      "16197 Training Loss: tensor(0.0336)\n",
      "16198 Training Loss: tensor(0.0338)\n",
      "16199 Training Loss: tensor(0.0336)\n",
      "16200 Training Loss: tensor(0.0336)\n",
      "16201 Training Loss: tensor(0.0337)\n",
      "16202 Training Loss: tensor(0.0338)\n",
      "16203 Training Loss: tensor(0.0335)\n",
      "16204 Training Loss: tensor(0.0335)\n",
      "16205 Training Loss: tensor(0.0336)\n",
      "16206 Training Loss: tensor(0.0336)\n",
      "16207 Training Loss: tensor(0.0337)\n",
      "16208 Training Loss: tensor(0.0338)\n",
      "16209 Training Loss: tensor(0.0337)\n",
      "16210 Training Loss: tensor(0.0336)\n",
      "16211 Training Loss: tensor(0.0335)\n",
      "16212 Training Loss: tensor(0.0336)\n",
      "16213 Training Loss: tensor(0.0335)\n",
      "16214 Training Loss: tensor(0.0335)\n",
      "16215 Training Loss: tensor(0.0336)\n",
      "16216 Training Loss: tensor(0.0336)\n",
      "16217 Training Loss: tensor(0.0334)\n",
      "16218 Training Loss: tensor(0.0339)\n",
      "16219 Training Loss: tensor(0.0336)\n",
      "16220 Training Loss: tensor(0.0339)\n",
      "16221 Training Loss: tensor(0.0336)\n",
      "16222 Training Loss: tensor(0.0339)\n",
      "16223 Training Loss: tensor(0.0336)\n",
      "16224 Training Loss: tensor(0.0336)\n",
      "16225 Training Loss: tensor(0.0339)\n",
      "16226 Training Loss: tensor(0.0336)\n",
      "16227 Training Loss: tensor(0.0336)\n",
      "16228 Training Loss: tensor(0.0335)\n",
      "16229 Training Loss: tensor(0.0334)\n",
      "16230 Training Loss: tensor(0.0337)\n",
      "16231 Training Loss: tensor(0.0336)\n",
      "16232 Training Loss: tensor(0.0335)\n",
      "16233 Training Loss: tensor(0.0336)\n",
      "16234 Training Loss: tensor(0.0336)\n",
      "16235 Training Loss: tensor(0.0335)\n",
      "16236 Training Loss: tensor(0.0337)\n",
      "16237 Training Loss: tensor(0.0336)\n",
      "16238 Training Loss: tensor(0.0336)\n",
      "16239 Training Loss: tensor(0.0335)\n",
      "16240 Training Loss: tensor(0.0335)\n",
      "16241 Training Loss: tensor(0.0336)\n",
      "16242 Training Loss: tensor(0.0336)\n",
      "16243 Training Loss: tensor(0.0336)\n",
      "16244 Training Loss: tensor(0.0336)\n",
      "16245 Training Loss: tensor(0.0335)\n",
      "16246 Training Loss: tensor(0.0337)\n",
      "16247 Training Loss: tensor(0.0334)\n",
      "16248 Training Loss: tensor(0.0338)\n",
      "16249 Training Loss: tensor(0.0336)\n",
      "16250 Training Loss: tensor(0.0334)\n",
      "16251 Training Loss: tensor(0.0336)\n",
      "16252 Training Loss: tensor(0.0337)\n",
      "16253 Training Loss: tensor(0.0337)\n",
      "16254 Training Loss: tensor(0.0335)\n",
      "16255 Training Loss: tensor(0.0335)\n",
      "16256 Training Loss: tensor(0.0336)\n",
      "16257 Training Loss: tensor(0.0335)\n",
      "16258 Training Loss: tensor(0.0335)\n",
      "16259 Training Loss: tensor(0.0334)\n",
      "16260 Training Loss: tensor(0.0335)\n",
      "16261 Training Loss: tensor(0.0338)\n",
      "16262 Training Loss: tensor(0.0335)\n",
      "16263 Training Loss: tensor(0.0335)\n",
      "16264 Training Loss: tensor(0.0336)\n",
      "16265 Training Loss: tensor(0.0335)\n",
      "16266 Training Loss: tensor(0.0334)\n",
      "16267 Training Loss: tensor(0.0335)\n",
      "16268 Training Loss: tensor(0.0335)\n",
      "16269 Training Loss: tensor(0.0338)\n",
      "16270 Training Loss: tensor(0.0336)\n",
      "16271 Training Loss: tensor(0.0335)\n",
      "16272 Training Loss: tensor(0.0337)\n",
      "16273 Training Loss: tensor(0.0336)\n",
      "16274 Training Loss: tensor(0.0337)\n",
      "16275 Training Loss: tensor(0.0334)\n",
      "16276 Training Loss: tensor(0.0335)\n",
      "16277 Training Loss: tensor(0.0336)\n",
      "16278 Training Loss: tensor(0.0336)\n",
      "16279 Training Loss: tensor(0.0338)\n",
      "16280 Training Loss: tensor(0.0337)\n",
      "16281 Training Loss: tensor(0.0336)\n",
      "16282 Training Loss: tensor(0.0336)\n",
      "16283 Training Loss: tensor(0.0336)\n",
      "16284 Training Loss: tensor(0.0335)\n",
      "16285 Training Loss: tensor(0.0335)\n",
      "16286 Training Loss: tensor(0.0336)\n",
      "16287 Training Loss: tensor(0.0336)\n",
      "16288 Training Loss: tensor(0.0335)\n",
      "16289 Training Loss: tensor(0.0335)\n",
      "16290 Training Loss: tensor(0.0335)\n",
      "16291 Training Loss: tensor(0.0337)\n",
      "16292 Training Loss: tensor(0.0336)\n",
      "16293 Training Loss: tensor(0.0335)\n",
      "16294 Training Loss: tensor(0.0338)\n",
      "16295 Training Loss: tensor(0.0334)\n",
      "16296 Training Loss: tensor(0.0335)\n",
      "16297 Training Loss: tensor(0.0335)\n",
      "16298 Training Loss: tensor(0.0334)\n",
      "16299 Training Loss: tensor(0.0336)\n",
      "16300 Training Loss: tensor(0.0336)\n",
      "16301 Training Loss: tensor(0.0334)\n",
      "16302 Training Loss: tensor(0.0335)\n",
      "16303 Training Loss: tensor(0.0336)\n",
      "16304 Training Loss: tensor(0.0336)\n",
      "16305 Training Loss: tensor(0.0335)\n",
      "16306 Training Loss: tensor(0.0334)\n",
      "16307 Training Loss: tensor(0.0335)\n",
      "16308 Training Loss: tensor(0.0335)\n",
      "16309 Training Loss: tensor(0.0335)\n",
      "16310 Training Loss: tensor(0.0336)\n",
      "16311 Training Loss: tensor(0.0335)\n",
      "16312 Training Loss: tensor(0.0335)\n",
      "16313 Training Loss: tensor(0.0336)\n",
      "16314 Training Loss: tensor(0.0335)\n",
      "16315 Training Loss: tensor(0.0335)\n",
      "16316 Training Loss: tensor(0.0337)\n",
      "16317 Training Loss: tensor(0.0334)\n",
      "16318 Training Loss: tensor(0.0334)\n",
      "16319 Training Loss: tensor(0.0335)\n",
      "16320 Training Loss: tensor(0.0335)\n",
      "16321 Training Loss: tensor(0.0336)\n",
      "16322 Training Loss: tensor(0.0335)\n",
      "16323 Training Loss: tensor(0.0336)\n",
      "16324 Training Loss: tensor(0.0336)\n",
      "16325 Training Loss: tensor(0.0333)\n",
      "16326 Training Loss: tensor(0.0335)\n",
      "16327 Training Loss: tensor(0.0335)\n",
      "16328 Training Loss: tensor(0.0336)\n",
      "16329 Training Loss: tensor(0.0337)\n",
      "16330 Training Loss: tensor(0.0334)\n",
      "16331 Training Loss: tensor(0.0335)\n",
      "16332 Training Loss: tensor(0.0335)\n",
      "16333 Training Loss: tensor(0.0337)\n",
      "16334 Training Loss: tensor(0.0336)\n",
      "16335 Training Loss: tensor(0.0336)\n",
      "16336 Training Loss: tensor(0.0334)\n",
      "16337 Training Loss: tensor(0.0333)\n",
      "16338 Training Loss: tensor(0.0334)\n",
      "16339 Training Loss: tensor(0.0335)\n",
      "16340 Training Loss: tensor(0.0335)\n",
      "16341 Training Loss: tensor(0.0336)\n",
      "16342 Training Loss: tensor(0.0337)\n",
      "16343 Training Loss: tensor(0.0335)\n",
      "16344 Training Loss: tensor(0.0337)\n",
      "16345 Training Loss: tensor(0.0337)\n",
      "16346 Training Loss: tensor(0.0335)\n",
      "16347 Training Loss: tensor(0.0335)\n",
      "16348 Training Loss: tensor(0.0334)\n",
      "16349 Training Loss: tensor(0.0335)\n",
      "16350 Training Loss: tensor(0.0337)\n",
      "16351 Training Loss: tensor(0.0335)\n",
      "16352 Training Loss: tensor(0.0334)\n",
      "16353 Training Loss: tensor(0.0335)\n",
      "16354 Training Loss: tensor(0.0335)\n",
      "16355 Training Loss: tensor(0.0334)\n",
      "16356 Training Loss: tensor(0.0338)\n",
      "16357 Training Loss: tensor(0.0336)\n",
      "16358 Training Loss: tensor(0.0334)\n",
      "16359 Training Loss: tensor(0.0336)\n",
      "16360 Training Loss: tensor(0.0335)\n",
      "16361 Training Loss: tensor(0.0335)\n",
      "16362 Training Loss: tensor(0.0335)\n",
      "16363 Training Loss: tensor(0.0335)\n",
      "16364 Training Loss: tensor(0.0335)\n",
      "16365 Training Loss: tensor(0.0335)\n",
      "16366 Training Loss: tensor(0.0335)\n",
      "16367 Training Loss: tensor(0.0335)\n",
      "16368 Training Loss: tensor(0.0336)\n",
      "16369 Training Loss: tensor(0.0337)\n",
      "16370 Training Loss: tensor(0.0338)\n",
      "16371 Training Loss: tensor(0.0336)\n",
      "16372 Training Loss: tensor(0.0335)\n",
      "16373 Training Loss: tensor(0.0336)\n",
      "16374 Training Loss: tensor(0.0335)\n",
      "16375 Training Loss: tensor(0.0335)\n",
      "16376 Training Loss: tensor(0.0337)\n",
      "16377 Training Loss: tensor(0.0335)\n",
      "16378 Training Loss: tensor(0.0335)\n",
      "16379 Training Loss: tensor(0.0336)\n",
      "16380 Training Loss: tensor(0.0335)\n",
      "16381 Training Loss: tensor(0.0333)\n",
      "16382 Training Loss: tensor(0.0334)\n",
      "16383 Training Loss: tensor(0.0336)\n",
      "16384 Training Loss: tensor(0.0334)\n",
      "16385 Training Loss: tensor(0.0338)\n",
      "16386 Training Loss: tensor(0.0336)\n",
      "16387 Training Loss: tensor(0.0337)\n",
      "16388 Training Loss: tensor(0.0335)\n",
      "16389 Training Loss: tensor(0.0335)\n",
      "16390 Training Loss: tensor(0.0335)\n",
      "16391 Training Loss: tensor(0.0334)\n",
      "16392 Training Loss: tensor(0.0336)\n",
      "16393 Training Loss: tensor(0.0336)\n",
      "16394 Training Loss: tensor(0.0335)\n",
      "16395 Training Loss: tensor(0.0337)\n",
      "16396 Training Loss: tensor(0.0334)\n",
      "16397 Training Loss: tensor(0.0335)\n",
      "16398 Training Loss: tensor(0.0334)\n",
      "16399 Training Loss: tensor(0.0333)\n",
      "16400 Training Loss: tensor(0.0336)\n",
      "16401 Training Loss: tensor(0.0335)\n",
      "16402 Training Loss: tensor(0.0335)\n",
      "16403 Training Loss: tensor(0.0334)\n",
      "16404 Training Loss: tensor(0.0337)\n",
      "16405 Training Loss: tensor(0.0335)\n",
      "16406 Training Loss: tensor(0.0335)\n",
      "16407 Training Loss: tensor(0.0336)\n",
      "16408 Training Loss: tensor(0.0334)\n",
      "16409 Training Loss: tensor(0.0336)\n",
      "16410 Training Loss: tensor(0.0337)\n",
      "16411 Training Loss: tensor(0.0334)\n",
      "16412 Training Loss: tensor(0.0335)\n",
      "16413 Training Loss: tensor(0.0335)\n",
      "16414 Training Loss: tensor(0.0335)\n",
      "16415 Training Loss: tensor(0.0335)\n",
      "16416 Training Loss: tensor(0.0334)\n",
      "16417 Training Loss: tensor(0.0337)\n",
      "16418 Training Loss: tensor(0.0336)\n",
      "16419 Training Loss: tensor(0.0336)\n",
      "16420 Training Loss: tensor(0.0336)\n",
      "16421 Training Loss: tensor(0.0335)\n",
      "16422 Training Loss: tensor(0.0334)\n",
      "16423 Training Loss: tensor(0.0334)\n",
      "16424 Training Loss: tensor(0.0336)\n",
      "16425 Training Loss: tensor(0.0335)\n",
      "16426 Training Loss: tensor(0.0336)\n",
      "16427 Training Loss: tensor(0.0335)\n",
      "16428 Training Loss: tensor(0.0335)\n",
      "16429 Training Loss: tensor(0.0334)\n",
      "16430 Training Loss: tensor(0.0335)\n",
      "16431 Training Loss: tensor(0.0334)\n",
      "16432 Training Loss: tensor(0.0337)\n",
      "16433 Training Loss: tensor(0.0334)\n",
      "16434 Training Loss: tensor(0.0333)\n",
      "16435 Training Loss: tensor(0.0334)\n",
      "16436 Training Loss: tensor(0.0334)\n",
      "16437 Training Loss: tensor(0.0334)\n",
      "16438 Training Loss: tensor(0.0334)\n",
      "16439 Training Loss: tensor(0.0337)\n",
      "16440 Training Loss: tensor(0.0334)\n",
      "16441 Training Loss: tensor(0.0336)\n",
      "16442 Training Loss: tensor(0.0334)\n",
      "16443 Training Loss: tensor(0.0335)\n",
      "16444 Training Loss: tensor(0.0335)\n",
      "16445 Training Loss: tensor(0.0335)\n",
      "16446 Training Loss: tensor(0.0335)\n",
      "16447 Training Loss: tensor(0.0334)\n",
      "16448 Training Loss: tensor(0.0335)\n",
      "16449 Training Loss: tensor(0.0334)\n",
      "16450 Training Loss: tensor(0.0335)\n",
      "16451 Training Loss: tensor(0.0335)\n",
      "16452 Training Loss: tensor(0.0335)\n",
      "16453 Training Loss: tensor(0.0336)\n",
      "16454 Training Loss: tensor(0.0334)\n",
      "16455 Training Loss: tensor(0.0333)\n",
      "16456 Training Loss: tensor(0.0335)\n",
      "16457 Training Loss: tensor(0.0333)\n",
      "16458 Training Loss: tensor(0.0333)\n",
      "16459 Training Loss: tensor(0.0335)\n",
      "16460 Training Loss: tensor(0.0335)\n",
      "16461 Training Loss: tensor(0.0335)\n",
      "16462 Training Loss: tensor(0.0335)\n",
      "16463 Training Loss: tensor(0.0335)\n",
      "16464 Training Loss: tensor(0.0334)\n",
      "16465 Training Loss: tensor(0.0335)\n",
      "16466 Training Loss: tensor(0.0334)\n",
      "16467 Training Loss: tensor(0.0336)\n",
      "16468 Training Loss: tensor(0.0334)\n",
      "16469 Training Loss: tensor(0.0334)\n",
      "16470 Training Loss: tensor(0.0334)\n",
      "16471 Training Loss: tensor(0.0334)\n",
      "16472 Training Loss: tensor(0.0335)\n",
      "16473 Training Loss: tensor(0.0335)\n",
      "16474 Training Loss: tensor(0.0333)\n",
      "16475 Training Loss: tensor(0.0334)\n",
      "16476 Training Loss: tensor(0.0334)\n",
      "16477 Training Loss: tensor(0.0334)\n",
      "16478 Training Loss: tensor(0.0335)\n",
      "16479 Training Loss: tensor(0.0335)\n",
      "16480 Training Loss: tensor(0.0333)\n",
      "16481 Training Loss: tensor(0.0334)\n",
      "16482 Training Loss: tensor(0.0335)\n",
      "16483 Training Loss: tensor(0.0334)\n",
      "16484 Training Loss: tensor(0.0333)\n",
      "16485 Training Loss: tensor(0.0338)\n",
      "16486 Training Loss: tensor(0.0334)\n",
      "16487 Training Loss: tensor(0.0334)\n",
      "16488 Training Loss: tensor(0.0334)\n",
      "16489 Training Loss: tensor(0.0335)\n",
      "16490 Training Loss: tensor(0.0334)\n",
      "16491 Training Loss: tensor(0.0333)\n",
      "16492 Training Loss: tensor(0.0333)\n",
      "16493 Training Loss: tensor(0.0336)\n",
      "16494 Training Loss: tensor(0.0333)\n",
      "16495 Training Loss: tensor(0.0334)\n",
      "16496 Training Loss: tensor(0.0334)\n",
      "16497 Training Loss: tensor(0.0335)\n",
      "16498 Training Loss: tensor(0.0334)\n",
      "16499 Training Loss: tensor(0.0333)\n",
      "16500 Training Loss: tensor(0.0334)\n",
      "16501 Training Loss: tensor(0.0333)\n",
      "16502 Training Loss: tensor(0.0334)\n",
      "16503 Training Loss: tensor(0.0335)\n",
      "16504 Training Loss: tensor(0.0333)\n",
      "16505 Training Loss: tensor(0.0334)\n",
      "16506 Training Loss: tensor(0.0334)\n",
      "16507 Training Loss: tensor(0.0333)\n",
      "16508 Training Loss: tensor(0.0333)\n",
      "16509 Training Loss: tensor(0.0334)\n",
      "16510 Training Loss: tensor(0.0333)\n",
      "16511 Training Loss: tensor(0.0333)\n",
      "16512 Training Loss: tensor(0.0335)\n",
      "16513 Training Loss: tensor(0.0335)\n",
      "16514 Training Loss: tensor(0.0335)\n",
      "16515 Training Loss: tensor(0.0335)\n",
      "16516 Training Loss: tensor(0.0334)\n",
      "16517 Training Loss: tensor(0.0334)\n",
      "16518 Training Loss: tensor(0.0335)\n",
      "16519 Training Loss: tensor(0.0334)\n",
      "16520 Training Loss: tensor(0.0334)\n",
      "16521 Training Loss: tensor(0.0336)\n",
      "16522 Training Loss: tensor(0.0334)\n",
      "16523 Training Loss: tensor(0.0335)\n",
      "16524 Training Loss: tensor(0.0335)\n",
      "16525 Training Loss: tensor(0.0335)\n",
      "16526 Training Loss: tensor(0.0335)\n",
      "16527 Training Loss: tensor(0.0335)\n",
      "16528 Training Loss: tensor(0.0334)\n",
      "16529 Training Loss: tensor(0.0333)\n",
      "16530 Training Loss: tensor(0.0334)\n",
      "16531 Training Loss: tensor(0.0333)\n",
      "16532 Training Loss: tensor(0.0334)\n",
      "16533 Training Loss: tensor(0.0335)\n",
      "16534 Training Loss: tensor(0.0333)\n",
      "16535 Training Loss: tensor(0.0333)\n",
      "16536 Training Loss: tensor(0.0333)\n",
      "16537 Training Loss: tensor(0.0335)\n",
      "16538 Training Loss: tensor(0.0334)\n",
      "16539 Training Loss: tensor(0.0334)\n",
      "16540 Training Loss: tensor(0.0335)\n",
      "16541 Training Loss: tensor(0.0336)\n",
      "16542 Training Loss: tensor(0.0334)\n",
      "16543 Training Loss: tensor(0.0335)\n",
      "16544 Training Loss: tensor(0.0334)\n",
      "16545 Training Loss: tensor(0.0332)\n",
      "16546 Training Loss: tensor(0.0333)\n",
      "16547 Training Loss: tensor(0.0336)\n",
      "16548 Training Loss: tensor(0.0335)\n",
      "16549 Training Loss: tensor(0.0333)\n",
      "16550 Training Loss: tensor(0.0336)\n",
      "16551 Training Loss: tensor(0.0336)\n",
      "16552 Training Loss: tensor(0.0335)\n",
      "16553 Training Loss: tensor(0.0333)\n",
      "16554 Training Loss: tensor(0.0333)\n",
      "16555 Training Loss: tensor(0.0335)\n",
      "16556 Training Loss: tensor(0.0336)\n",
      "16557 Training Loss: tensor(0.0333)\n",
      "16558 Training Loss: tensor(0.0335)\n",
      "16559 Training Loss: tensor(0.0333)\n",
      "16560 Training Loss: tensor(0.0334)\n",
      "16561 Training Loss: tensor(0.0333)\n",
      "16562 Training Loss: tensor(0.0334)\n",
      "16563 Training Loss: tensor(0.0333)\n",
      "16564 Training Loss: tensor(0.0334)\n",
      "16565 Training Loss: tensor(0.0334)\n",
      "16566 Training Loss: tensor(0.0333)\n",
      "16567 Training Loss: tensor(0.0333)\n",
      "16568 Training Loss: tensor(0.0332)\n",
      "16569 Training Loss: tensor(0.0334)\n",
      "16570 Training Loss: tensor(0.0334)\n",
      "16571 Training Loss: tensor(0.0333)\n",
      "16572 Training Loss: tensor(0.0332)\n",
      "16573 Training Loss: tensor(0.0336)\n",
      "16574 Training Loss: tensor(0.0334)\n",
      "16575 Training Loss: tensor(0.0333)\n",
      "16576 Training Loss: tensor(0.0333)\n",
      "16577 Training Loss: tensor(0.0333)\n",
      "16578 Training Loss: tensor(0.0333)\n",
      "16579 Training Loss: tensor(0.0333)\n",
      "16580 Training Loss: tensor(0.0333)\n",
      "16581 Training Loss: tensor(0.0332)\n",
      "16582 Training Loss: tensor(0.0334)\n",
      "16583 Training Loss: tensor(0.0334)\n",
      "16584 Training Loss: tensor(0.0335)\n",
      "16585 Training Loss: tensor(0.0334)\n",
      "16586 Training Loss: tensor(0.0334)\n",
      "16587 Training Loss: tensor(0.0334)\n",
      "16588 Training Loss: tensor(0.0333)\n",
      "16589 Training Loss: tensor(0.0334)\n",
      "16590 Training Loss: tensor(0.0333)\n",
      "16591 Training Loss: tensor(0.0334)\n",
      "16592 Training Loss: tensor(0.0333)\n",
      "16593 Training Loss: tensor(0.0334)\n",
      "16594 Training Loss: tensor(0.0334)\n",
      "16595 Training Loss: tensor(0.0333)\n",
      "16596 Training Loss: tensor(0.0336)\n",
      "16597 Training Loss: tensor(0.0334)\n",
      "16598 Training Loss: tensor(0.0333)\n",
      "16599 Training Loss: tensor(0.0332)\n",
      "16600 Training Loss: tensor(0.0333)\n",
      "16601 Training Loss: tensor(0.0333)\n",
      "16602 Training Loss: tensor(0.0334)\n",
      "16603 Training Loss: tensor(0.0334)\n",
      "16604 Training Loss: tensor(0.0333)\n",
      "16605 Training Loss: tensor(0.0334)\n",
      "16606 Training Loss: tensor(0.0334)\n",
      "16607 Training Loss: tensor(0.0334)\n",
      "16608 Training Loss: tensor(0.0336)\n",
      "16609 Training Loss: tensor(0.0334)\n",
      "16610 Training Loss: tensor(0.0333)\n",
      "16611 Training Loss: tensor(0.0333)\n",
      "16612 Training Loss: tensor(0.0334)\n",
      "16613 Training Loss: tensor(0.0333)\n",
      "16614 Training Loss: tensor(0.0332)\n",
      "16615 Training Loss: tensor(0.0333)\n",
      "16616 Training Loss: tensor(0.0334)\n",
      "16617 Training Loss: tensor(0.0333)\n",
      "16618 Training Loss: tensor(0.0332)\n",
      "16619 Training Loss: tensor(0.0333)\n",
      "16620 Training Loss: tensor(0.0333)\n",
      "16621 Training Loss: tensor(0.0335)\n",
      "16622 Training Loss: tensor(0.0333)\n",
      "16623 Training Loss: tensor(0.0334)\n",
      "16624 Training Loss: tensor(0.0332)\n",
      "16625 Training Loss: tensor(0.0334)\n",
      "16626 Training Loss: tensor(0.0333)\n",
      "16627 Training Loss: tensor(0.0333)\n",
      "16628 Training Loss: tensor(0.0333)\n",
      "16629 Training Loss: tensor(0.0335)\n",
      "16630 Training Loss: tensor(0.0333)\n",
      "16631 Training Loss: tensor(0.0333)\n",
      "16632 Training Loss: tensor(0.0334)\n",
      "16633 Training Loss: tensor(0.0335)\n",
      "16634 Training Loss: tensor(0.0333)\n",
      "16635 Training Loss: tensor(0.0332)\n",
      "16636 Training Loss: tensor(0.0333)\n",
      "16637 Training Loss: tensor(0.0335)\n",
      "16638 Training Loss: tensor(0.0333)\n",
      "16639 Training Loss: tensor(0.0333)\n",
      "16640 Training Loss: tensor(0.0333)\n",
      "16641 Training Loss: tensor(0.0333)\n",
      "16642 Training Loss: tensor(0.0332)\n",
      "16643 Training Loss: tensor(0.0334)\n",
      "16644 Training Loss: tensor(0.0333)\n",
      "16645 Training Loss: tensor(0.0333)\n",
      "16646 Training Loss: tensor(0.0333)\n",
      "16647 Training Loss: tensor(0.0333)\n",
      "16648 Training Loss: tensor(0.0333)\n",
      "16649 Training Loss: tensor(0.0333)\n",
      "16650 Training Loss: tensor(0.0334)\n",
      "16651 Training Loss: tensor(0.0333)\n",
      "16652 Training Loss: tensor(0.0332)\n",
      "16653 Training Loss: tensor(0.0337)\n",
      "16654 Training Loss: tensor(0.0334)\n",
      "16655 Training Loss: tensor(0.0334)\n",
      "16656 Training Loss: tensor(0.0333)\n",
      "16657 Training Loss: tensor(0.0334)\n",
      "16658 Training Loss: tensor(0.0332)\n",
      "16659 Training Loss: tensor(0.0333)\n",
      "16660 Training Loss: tensor(0.0334)\n",
      "16661 Training Loss: tensor(0.0333)\n",
      "16662 Training Loss: tensor(0.0333)\n",
      "16663 Training Loss: tensor(0.0332)\n",
      "16664 Training Loss: tensor(0.0332)\n",
      "16665 Training Loss: tensor(0.0333)\n",
      "16666 Training Loss: tensor(0.0332)\n",
      "16667 Training Loss: tensor(0.0333)\n",
      "16668 Training Loss: tensor(0.0337)\n",
      "16669 Training Loss: tensor(0.0332)\n",
      "16670 Training Loss: tensor(0.0332)\n",
      "16671 Training Loss: tensor(0.0333)\n",
      "16672 Training Loss: tensor(0.0332)\n",
      "16673 Training Loss: tensor(0.0331)\n",
      "16674 Training Loss: tensor(0.0335)\n",
      "16675 Training Loss: tensor(0.0334)\n",
      "16676 Training Loss: tensor(0.0332)\n",
      "16677 Training Loss: tensor(0.0334)\n",
      "16678 Training Loss: tensor(0.0331)\n",
      "16679 Training Loss: tensor(0.0334)\n",
      "16680 Training Loss: tensor(0.0333)\n",
      "16681 Training Loss: tensor(0.0333)\n",
      "16682 Training Loss: tensor(0.0332)\n",
      "16683 Training Loss: tensor(0.0334)\n",
      "16684 Training Loss: tensor(0.0332)\n",
      "16685 Training Loss: tensor(0.0333)\n",
      "16686 Training Loss: tensor(0.0334)\n",
      "16687 Training Loss: tensor(0.0332)\n",
      "16688 Training Loss: tensor(0.0332)\n",
      "16689 Training Loss: tensor(0.0331)\n",
      "16690 Training Loss: tensor(0.0333)\n",
      "16691 Training Loss: tensor(0.0333)\n",
      "16692 Training Loss: tensor(0.0334)\n",
      "16693 Training Loss: tensor(0.0333)\n",
      "16694 Training Loss: tensor(0.0333)\n",
      "16695 Training Loss: tensor(0.0333)\n",
      "16696 Training Loss: tensor(0.0332)\n",
      "16697 Training Loss: tensor(0.0333)\n",
      "16698 Training Loss: tensor(0.0336)\n",
      "16699 Training Loss: tensor(0.0333)\n",
      "16700 Training Loss: tensor(0.0333)\n",
      "16701 Training Loss: tensor(0.0332)\n",
      "16702 Training Loss: tensor(0.0332)\n",
      "16703 Training Loss: tensor(0.0334)\n",
      "16704 Training Loss: tensor(0.0334)\n",
      "16705 Training Loss: tensor(0.0331)\n",
      "16706 Training Loss: tensor(0.0332)\n",
      "16707 Training Loss: tensor(0.0333)\n",
      "16708 Training Loss: tensor(0.0332)\n",
      "16709 Training Loss: tensor(0.0334)\n",
      "16710 Training Loss: tensor(0.0332)\n",
      "16711 Training Loss: tensor(0.0333)\n",
      "16712 Training Loss: tensor(0.0332)\n",
      "16713 Training Loss: tensor(0.0334)\n",
      "16714 Training Loss: tensor(0.0333)\n",
      "16715 Training Loss: tensor(0.0333)\n",
      "16716 Training Loss: tensor(0.0331)\n",
      "16717 Training Loss: tensor(0.0334)\n",
      "16718 Training Loss: tensor(0.0332)\n",
      "16719 Training Loss: tensor(0.0332)\n",
      "16720 Training Loss: tensor(0.0332)\n",
      "16721 Training Loss: tensor(0.0333)\n",
      "16722 Training Loss: tensor(0.0334)\n",
      "16723 Training Loss: tensor(0.0334)\n",
      "16724 Training Loss: tensor(0.0332)\n",
      "16725 Training Loss: tensor(0.0335)\n",
      "16726 Training Loss: tensor(0.0334)\n",
      "16727 Training Loss: tensor(0.0333)\n",
      "16728 Training Loss: tensor(0.0333)\n",
      "16729 Training Loss: tensor(0.0332)\n",
      "16730 Training Loss: tensor(0.0333)\n",
      "16731 Training Loss: tensor(0.0332)\n",
      "16732 Training Loss: tensor(0.0331)\n",
      "16733 Training Loss: tensor(0.0332)\n",
      "16734 Training Loss: tensor(0.0333)\n",
      "16735 Training Loss: tensor(0.0333)\n",
      "16736 Training Loss: tensor(0.0335)\n",
      "16737 Training Loss: tensor(0.0333)\n",
      "16738 Training Loss: tensor(0.0335)\n",
      "16739 Training Loss: tensor(0.0335)\n",
      "16740 Training Loss: tensor(0.0333)\n",
      "16741 Training Loss: tensor(0.0333)\n",
      "16742 Training Loss: tensor(0.0332)\n",
      "16743 Training Loss: tensor(0.0332)\n",
      "16744 Training Loss: tensor(0.0332)\n",
      "16745 Training Loss: tensor(0.0333)\n",
      "16746 Training Loss: tensor(0.0332)\n",
      "16747 Training Loss: tensor(0.0334)\n",
      "16748 Training Loss: tensor(0.0331)\n",
      "16749 Training Loss: tensor(0.0334)\n",
      "16750 Training Loss: tensor(0.0332)\n",
      "16751 Training Loss: tensor(0.0331)\n",
      "16752 Training Loss: tensor(0.0334)\n",
      "16753 Training Loss: tensor(0.0333)\n",
      "16754 Training Loss: tensor(0.0332)\n",
      "16755 Training Loss: tensor(0.0331)\n",
      "16756 Training Loss: tensor(0.0331)\n",
      "16757 Training Loss: tensor(0.0332)\n",
      "16758 Training Loss: tensor(0.0333)\n",
      "16759 Training Loss: tensor(0.0332)\n",
      "16760 Training Loss: tensor(0.0332)\n",
      "16761 Training Loss: tensor(0.0332)\n",
      "16762 Training Loss: tensor(0.0335)\n",
      "16763 Training Loss: tensor(0.0335)\n",
      "16764 Training Loss: tensor(0.0332)\n",
      "16765 Training Loss: tensor(0.0333)\n",
      "16766 Training Loss: tensor(0.0334)\n",
      "16767 Training Loss: tensor(0.0333)\n",
      "16768 Training Loss: tensor(0.0332)\n",
      "16769 Training Loss: tensor(0.0335)\n",
      "16770 Training Loss: tensor(0.0334)\n",
      "16771 Training Loss: tensor(0.0333)\n",
      "16772 Training Loss: tensor(0.0333)\n",
      "16773 Training Loss: tensor(0.0333)\n",
      "16774 Training Loss: tensor(0.0333)\n",
      "16775 Training Loss: tensor(0.0332)\n",
      "16776 Training Loss: tensor(0.0332)\n",
      "16777 Training Loss: tensor(0.0332)\n",
      "16778 Training Loss: tensor(0.0333)\n",
      "16779 Training Loss: tensor(0.0335)\n",
      "16780 Training Loss: tensor(0.0333)\n",
      "16781 Training Loss: tensor(0.0334)\n",
      "16782 Training Loss: tensor(0.0333)\n",
      "16783 Training Loss: tensor(0.0333)\n",
      "16784 Training Loss: tensor(0.0333)\n",
      "16785 Training Loss: tensor(0.0332)\n",
      "16786 Training Loss: tensor(0.0332)\n",
      "16787 Training Loss: tensor(0.0334)\n",
      "16788 Training Loss: tensor(0.0332)\n",
      "16789 Training Loss: tensor(0.0332)\n",
      "16790 Training Loss: tensor(0.0333)\n",
      "16791 Training Loss: tensor(0.0332)\n",
      "16792 Training Loss: tensor(0.0332)\n",
      "16793 Training Loss: tensor(0.0332)\n",
      "16794 Training Loss: tensor(0.0332)\n",
      "16795 Training Loss: tensor(0.0332)\n",
      "16796 Training Loss: tensor(0.0333)\n",
      "16797 Training Loss: tensor(0.0333)\n",
      "16798 Training Loss: tensor(0.0333)\n",
      "16799 Training Loss: tensor(0.0332)\n",
      "16800 Training Loss: tensor(0.0332)\n",
      "16801 Training Loss: tensor(0.0333)\n",
      "16802 Training Loss: tensor(0.0334)\n",
      "16803 Training Loss: tensor(0.0333)\n",
      "16804 Training Loss: tensor(0.0333)\n",
      "16805 Training Loss: tensor(0.0332)\n",
      "16806 Training Loss: tensor(0.0332)\n",
      "16807 Training Loss: tensor(0.0332)\n",
      "16808 Training Loss: tensor(0.0333)\n",
      "16809 Training Loss: tensor(0.0333)\n",
      "16810 Training Loss: tensor(0.0333)\n",
      "16811 Training Loss: tensor(0.0334)\n",
      "16812 Training Loss: tensor(0.0332)\n",
      "16813 Training Loss: tensor(0.0332)\n",
      "16814 Training Loss: tensor(0.0332)\n",
      "16815 Training Loss: tensor(0.0331)\n",
      "16816 Training Loss: tensor(0.0333)\n",
      "16817 Training Loss: tensor(0.0331)\n",
      "16818 Training Loss: tensor(0.0332)\n",
      "16819 Training Loss: tensor(0.0333)\n",
      "16820 Training Loss: tensor(0.0330)\n",
      "16821 Training Loss: tensor(0.0331)\n",
      "16822 Training Loss: tensor(0.0333)\n",
      "16823 Training Loss: tensor(0.0332)\n",
      "16824 Training Loss: tensor(0.0331)\n",
      "16825 Training Loss: tensor(0.0331)\n",
      "16826 Training Loss: tensor(0.0331)\n",
      "16827 Training Loss: tensor(0.0332)\n",
      "16828 Training Loss: tensor(0.0333)\n",
      "16829 Training Loss: tensor(0.0336)\n",
      "16830 Training Loss: tensor(0.0331)\n",
      "16831 Training Loss: tensor(0.0332)\n",
      "16832 Training Loss: tensor(0.0332)\n",
      "16833 Training Loss: tensor(0.0334)\n",
      "16834 Training Loss: tensor(0.0333)\n",
      "16835 Training Loss: tensor(0.0332)\n",
      "16836 Training Loss: tensor(0.0335)\n",
      "16837 Training Loss: tensor(0.0332)\n",
      "16838 Training Loss: tensor(0.0332)\n",
      "16839 Training Loss: tensor(0.0331)\n",
      "16840 Training Loss: tensor(0.0332)\n",
      "16841 Training Loss: tensor(0.0332)\n",
      "16842 Training Loss: tensor(0.0333)\n",
      "16843 Training Loss: tensor(0.0331)\n",
      "16844 Training Loss: tensor(0.0331)\n",
      "16845 Training Loss: tensor(0.0332)\n",
      "16846 Training Loss: tensor(0.0333)\n",
      "16847 Training Loss: tensor(0.0333)\n",
      "16848 Training Loss: tensor(0.0332)\n",
      "16849 Training Loss: tensor(0.0332)\n",
      "16850 Training Loss: tensor(0.0332)\n",
      "16851 Training Loss: tensor(0.0333)\n",
      "16852 Training Loss: tensor(0.0331)\n",
      "16853 Training Loss: tensor(0.0332)\n",
      "16854 Training Loss: tensor(0.0335)\n",
      "16855 Training Loss: tensor(0.0331)\n",
      "16856 Training Loss: tensor(0.0331)\n",
      "16857 Training Loss: tensor(0.0332)\n",
      "16858 Training Loss: tensor(0.0332)\n",
      "16859 Training Loss: tensor(0.0334)\n",
      "16860 Training Loss: tensor(0.0334)\n",
      "16861 Training Loss: tensor(0.0332)\n",
      "16862 Training Loss: tensor(0.0332)\n",
      "16863 Training Loss: tensor(0.0332)\n",
      "16864 Training Loss: tensor(0.0333)\n",
      "16865 Training Loss: tensor(0.0331)\n",
      "16866 Training Loss: tensor(0.0333)\n",
      "16867 Training Loss: tensor(0.0332)\n",
      "16868 Training Loss: tensor(0.0331)\n",
      "16869 Training Loss: tensor(0.0333)\n",
      "16870 Training Loss: tensor(0.0331)\n",
      "16871 Training Loss: tensor(0.0330)\n",
      "16872 Training Loss: tensor(0.0331)\n",
      "16873 Training Loss: tensor(0.0332)\n",
      "16874 Training Loss: tensor(0.0331)\n",
      "16875 Training Loss: tensor(0.0334)\n",
      "16876 Training Loss: tensor(0.0335)\n",
      "16877 Training Loss: tensor(0.0331)\n",
      "16878 Training Loss: tensor(0.0331)\n",
      "16879 Training Loss: tensor(0.0332)\n",
      "16880 Training Loss: tensor(0.0330)\n",
      "16881 Training Loss: tensor(0.0330)\n",
      "16882 Training Loss: tensor(0.0334)\n",
      "16883 Training Loss: tensor(0.0330)\n",
      "16884 Training Loss: tensor(0.0330)\n",
      "16885 Training Loss: tensor(0.0333)\n",
      "16886 Training Loss: tensor(0.0331)\n",
      "16887 Training Loss: tensor(0.0330)\n",
      "16888 Training Loss: tensor(0.0333)\n",
      "16889 Training Loss: tensor(0.0332)\n",
      "16890 Training Loss: tensor(0.0331)\n",
      "16891 Training Loss: tensor(0.0330)\n",
      "16892 Training Loss: tensor(0.0333)\n",
      "16893 Training Loss: tensor(0.0331)\n",
      "16894 Training Loss: tensor(0.0332)\n",
      "16895 Training Loss: tensor(0.0331)\n",
      "16896 Training Loss: tensor(0.0333)\n",
      "16897 Training Loss: tensor(0.0332)\n",
      "16898 Training Loss: tensor(0.0333)\n",
      "16899 Training Loss: tensor(0.0332)\n",
      "16900 Training Loss: tensor(0.0333)\n",
      "16901 Training Loss: tensor(0.0334)\n",
      "16902 Training Loss: tensor(0.0334)\n",
      "16903 Training Loss: tensor(0.0333)\n",
      "16904 Training Loss: tensor(0.0331)\n",
      "16905 Training Loss: tensor(0.0332)\n",
      "16906 Training Loss: tensor(0.0331)\n",
      "16907 Training Loss: tensor(0.0331)\n",
      "16908 Training Loss: tensor(0.0331)\n",
      "16909 Training Loss: tensor(0.0331)\n",
      "16910 Training Loss: tensor(0.0332)\n",
      "16911 Training Loss: tensor(0.0331)\n",
      "16912 Training Loss: tensor(0.0333)\n",
      "16913 Training Loss: tensor(0.0330)\n",
      "16914 Training Loss: tensor(0.0333)\n",
      "16915 Training Loss: tensor(0.0331)\n",
      "16916 Training Loss: tensor(0.0331)\n",
      "16917 Training Loss: tensor(0.0333)\n",
      "16918 Training Loss: tensor(0.0330)\n",
      "16919 Training Loss: tensor(0.0330)\n",
      "16920 Training Loss: tensor(0.0332)\n",
      "16921 Training Loss: tensor(0.0330)\n",
      "16922 Training Loss: tensor(0.0333)\n",
      "16923 Training Loss: tensor(0.0330)\n",
      "16924 Training Loss: tensor(0.0330)\n",
      "16925 Training Loss: tensor(0.0331)\n",
      "16926 Training Loss: tensor(0.0331)\n",
      "16927 Training Loss: tensor(0.0331)\n",
      "16928 Training Loss: tensor(0.0330)\n",
      "16929 Training Loss: tensor(0.0331)\n",
      "16930 Training Loss: tensor(0.0331)\n",
      "16931 Training Loss: tensor(0.0333)\n",
      "16932 Training Loss: tensor(0.0332)\n",
      "16933 Training Loss: tensor(0.0331)\n",
      "16934 Training Loss: tensor(0.0331)\n",
      "16935 Training Loss: tensor(0.0333)\n",
      "16936 Training Loss: tensor(0.0331)\n",
      "16937 Training Loss: tensor(0.0331)\n",
      "16938 Training Loss: tensor(0.0331)\n",
      "16939 Training Loss: tensor(0.0333)\n",
      "16940 Training Loss: tensor(0.0332)\n",
      "16941 Training Loss: tensor(0.0331)\n",
      "16942 Training Loss: tensor(0.0333)\n",
      "16943 Training Loss: tensor(0.0330)\n",
      "16944 Training Loss: tensor(0.0331)\n",
      "16945 Training Loss: tensor(0.0331)\n",
      "16946 Training Loss: tensor(0.0331)\n",
      "16947 Training Loss: tensor(0.0330)\n",
      "16948 Training Loss: tensor(0.0333)\n",
      "16949 Training Loss: tensor(0.0332)\n",
      "16950 Training Loss: tensor(0.0333)\n",
      "16951 Training Loss: tensor(0.0331)\n",
      "16952 Training Loss: tensor(0.0331)\n",
      "16953 Training Loss: tensor(0.0331)\n",
      "16954 Training Loss: tensor(0.0331)\n",
      "16955 Training Loss: tensor(0.0330)\n",
      "16956 Training Loss: tensor(0.0330)\n",
      "16957 Training Loss: tensor(0.0332)\n",
      "16958 Training Loss: tensor(0.0334)\n",
      "16959 Training Loss: tensor(0.0331)\n",
      "16960 Training Loss: tensor(0.0332)\n",
      "16961 Training Loss: tensor(0.0333)\n",
      "16962 Training Loss: tensor(0.0331)\n",
      "16963 Training Loss: tensor(0.0332)\n",
      "16964 Training Loss: tensor(0.0330)\n",
      "16965 Training Loss: tensor(0.0331)\n",
      "16966 Training Loss: tensor(0.0331)\n",
      "16967 Training Loss: tensor(0.0331)\n",
      "16968 Training Loss: tensor(0.0332)\n",
      "16969 Training Loss: tensor(0.0331)\n",
      "16970 Training Loss: tensor(0.0330)\n",
      "16971 Training Loss: tensor(0.0331)\n",
      "16972 Training Loss: tensor(0.0333)\n",
      "16973 Training Loss: tensor(0.0331)\n",
      "16974 Training Loss: tensor(0.0330)\n",
      "16975 Training Loss: tensor(0.0331)\n",
      "16976 Training Loss: tensor(0.0334)\n",
      "16977 Training Loss: tensor(0.0331)\n",
      "16978 Training Loss: tensor(0.0333)\n",
      "16979 Training Loss: tensor(0.0332)\n",
      "16980 Training Loss: tensor(0.0332)\n",
      "16981 Training Loss: tensor(0.0332)\n",
      "16982 Training Loss: tensor(0.0330)\n",
      "16983 Training Loss: tensor(0.0332)\n",
      "16984 Training Loss: tensor(0.0332)\n",
      "16985 Training Loss: tensor(0.0331)\n",
      "16986 Training Loss: tensor(0.0330)\n",
      "16987 Training Loss: tensor(0.0331)\n",
      "16988 Training Loss: tensor(0.0332)\n",
      "16989 Training Loss: tensor(0.0330)\n",
      "16990 Training Loss: tensor(0.0333)\n",
      "16991 Training Loss: tensor(0.0333)\n",
      "16992 Training Loss: tensor(0.0333)\n",
      "16993 Training Loss: tensor(0.0331)\n",
      "16994 Training Loss: tensor(0.0331)\n",
      "16995 Training Loss: tensor(0.0331)\n",
      "16996 Training Loss: tensor(0.0331)\n",
      "16997 Training Loss: tensor(0.0332)\n",
      "16998 Training Loss: tensor(0.0331)\n",
      "16999 Training Loss: tensor(0.0332)\n",
      "17000 Training Loss: tensor(0.0332)\n",
      "17001 Training Loss: tensor(0.0332)\n",
      "17002 Training Loss: tensor(0.0331)\n",
      "17003 Training Loss: tensor(0.0331)\n",
      "17004 Training Loss: tensor(0.0331)\n",
      "17005 Training Loss: tensor(0.0329)\n",
      "17006 Training Loss: tensor(0.0330)\n",
      "17007 Training Loss: tensor(0.0331)\n",
      "17008 Training Loss: tensor(0.0331)\n",
      "17009 Training Loss: tensor(0.0331)\n",
      "17010 Training Loss: tensor(0.0331)\n",
      "17011 Training Loss: tensor(0.0331)\n",
      "17012 Training Loss: tensor(0.0333)\n",
      "17013 Training Loss: tensor(0.0332)\n",
      "17014 Training Loss: tensor(0.0330)\n",
      "17015 Training Loss: tensor(0.0331)\n",
      "17016 Training Loss: tensor(0.0331)\n",
      "17017 Training Loss: tensor(0.0331)\n",
      "17018 Training Loss: tensor(0.0333)\n",
      "17019 Training Loss: tensor(0.0332)\n",
      "17020 Training Loss: tensor(0.0331)\n",
      "17021 Training Loss: tensor(0.0332)\n",
      "17022 Training Loss: tensor(0.0333)\n",
      "17023 Training Loss: tensor(0.0332)\n",
      "17024 Training Loss: tensor(0.0331)\n",
      "17025 Training Loss: tensor(0.0330)\n",
      "17026 Training Loss: tensor(0.0333)\n",
      "17027 Training Loss: tensor(0.0331)\n",
      "17028 Training Loss: tensor(0.0331)\n",
      "17029 Training Loss: tensor(0.0332)\n",
      "17030 Training Loss: tensor(0.0332)\n",
      "17031 Training Loss: tensor(0.0331)\n",
      "17032 Training Loss: tensor(0.0332)\n",
      "17033 Training Loss: tensor(0.0331)\n",
      "17034 Training Loss: tensor(0.0331)\n",
      "17035 Training Loss: tensor(0.0329)\n",
      "17036 Training Loss: tensor(0.0331)\n",
      "17037 Training Loss: tensor(0.0331)\n",
      "17038 Training Loss: tensor(0.0333)\n",
      "17039 Training Loss: tensor(0.0331)\n",
      "17040 Training Loss: tensor(0.0330)\n",
      "17041 Training Loss: tensor(0.0331)\n",
      "17042 Training Loss: tensor(0.0331)\n",
      "17043 Training Loss: tensor(0.0331)\n",
      "17044 Training Loss: tensor(0.0329)\n",
      "17045 Training Loss: tensor(0.0330)\n",
      "17046 Training Loss: tensor(0.0330)\n",
      "17047 Training Loss: tensor(0.0330)\n",
      "17048 Training Loss: tensor(0.0330)\n",
      "17049 Training Loss: tensor(0.0329)\n",
      "17050 Training Loss: tensor(0.0332)\n",
      "17051 Training Loss: tensor(0.0331)\n",
      "17052 Training Loss: tensor(0.0331)\n",
      "17053 Training Loss: tensor(0.0330)\n",
      "17054 Training Loss: tensor(0.0330)\n",
      "17055 Training Loss: tensor(0.0332)\n",
      "17056 Training Loss: tensor(0.0330)\n",
      "17057 Training Loss: tensor(0.0331)\n",
      "17058 Training Loss: tensor(0.0330)\n",
      "17059 Training Loss: tensor(0.0331)\n",
      "17060 Training Loss: tensor(0.0331)\n",
      "17061 Training Loss: tensor(0.0331)\n",
      "17062 Training Loss: tensor(0.0330)\n",
      "17063 Training Loss: tensor(0.0331)\n",
      "17064 Training Loss: tensor(0.0330)\n",
      "17065 Training Loss: tensor(0.0329)\n",
      "17066 Training Loss: tensor(0.0329)\n",
      "17067 Training Loss: tensor(0.0333)\n",
      "17068 Training Loss: tensor(0.0329)\n",
      "17069 Training Loss: tensor(0.0332)\n",
      "17070 Training Loss: tensor(0.0331)\n",
      "17071 Training Loss: tensor(0.0331)\n",
      "17072 Training Loss: tensor(0.0331)\n",
      "17073 Training Loss: tensor(0.0331)\n",
      "17074 Training Loss: tensor(0.0332)\n",
      "17075 Training Loss: tensor(0.0330)\n",
      "17076 Training Loss: tensor(0.0334)\n",
      "17077 Training Loss: tensor(0.0329)\n",
      "17078 Training Loss: tensor(0.0330)\n",
      "17079 Training Loss: tensor(0.0329)\n",
      "17080 Training Loss: tensor(0.0330)\n",
      "17081 Training Loss: tensor(0.0332)\n",
      "17082 Training Loss: tensor(0.0330)\n",
      "17083 Training Loss: tensor(0.0332)\n",
      "17084 Training Loss: tensor(0.0330)\n",
      "17085 Training Loss: tensor(0.0333)\n",
      "17086 Training Loss: tensor(0.0331)\n",
      "17087 Training Loss: tensor(0.0330)\n",
      "17088 Training Loss: tensor(0.0330)\n",
      "17089 Training Loss: tensor(0.0332)\n",
      "17090 Training Loss: tensor(0.0330)\n",
      "17091 Training Loss: tensor(0.0330)\n",
      "17092 Training Loss: tensor(0.0335)\n",
      "17093 Training Loss: tensor(0.0332)\n",
      "17094 Training Loss: tensor(0.0332)\n",
      "17095 Training Loss: tensor(0.0331)\n",
      "17096 Training Loss: tensor(0.0331)\n",
      "17097 Training Loss: tensor(0.0332)\n",
      "17098 Training Loss: tensor(0.0331)\n",
      "17099 Training Loss: tensor(0.0330)\n",
      "17100 Training Loss: tensor(0.0330)\n",
      "17101 Training Loss: tensor(0.0330)\n",
      "17102 Training Loss: tensor(0.0330)\n",
      "17103 Training Loss: tensor(0.0332)\n",
      "17104 Training Loss: tensor(0.0330)\n",
      "17105 Training Loss: tensor(0.0330)\n",
      "17106 Training Loss: tensor(0.0331)\n",
      "17107 Training Loss: tensor(0.0332)\n",
      "17108 Training Loss: tensor(0.0330)\n",
      "17109 Training Loss: tensor(0.0331)\n",
      "17110 Training Loss: tensor(0.0332)\n",
      "17111 Training Loss: tensor(0.0330)\n",
      "17112 Training Loss: tensor(0.0331)\n",
      "17113 Training Loss: tensor(0.0331)\n",
      "17114 Training Loss: tensor(0.0330)\n",
      "17115 Training Loss: tensor(0.0329)\n",
      "17116 Training Loss: tensor(0.0329)\n",
      "17117 Training Loss: tensor(0.0330)\n",
      "17118 Training Loss: tensor(0.0331)\n",
      "17119 Training Loss: tensor(0.0330)\n",
      "17120 Training Loss: tensor(0.0330)\n",
      "17121 Training Loss: tensor(0.0332)\n",
      "17122 Training Loss: tensor(0.0333)\n",
      "17123 Training Loss: tensor(0.0331)\n",
      "17124 Training Loss: tensor(0.0331)\n",
      "17125 Training Loss: tensor(0.0333)\n",
      "17126 Training Loss: tensor(0.0330)\n",
      "17127 Training Loss: tensor(0.0330)\n",
      "17128 Training Loss: tensor(0.0330)\n",
      "17129 Training Loss: tensor(0.0331)\n",
      "17130 Training Loss: tensor(0.0331)\n",
      "17131 Training Loss: tensor(0.0330)\n",
      "17132 Training Loss: tensor(0.0334)\n",
      "17133 Training Loss: tensor(0.0330)\n",
      "17134 Training Loss: tensor(0.0331)\n",
      "17135 Training Loss: tensor(0.0331)\n",
      "17136 Training Loss: tensor(0.0329)\n",
      "17137 Training Loss: tensor(0.0332)\n",
      "17138 Training Loss: tensor(0.0331)\n",
      "17139 Training Loss: tensor(0.0331)\n",
      "17140 Training Loss: tensor(0.0330)\n",
      "17141 Training Loss: tensor(0.0331)\n",
      "17142 Training Loss: tensor(0.0332)\n",
      "17143 Training Loss: tensor(0.0330)\n",
      "17144 Training Loss: tensor(0.0331)\n",
      "17145 Training Loss: tensor(0.0331)\n",
      "17146 Training Loss: tensor(0.0332)\n",
      "17147 Training Loss: tensor(0.0331)\n",
      "17148 Training Loss: tensor(0.0330)\n",
      "17149 Training Loss: tensor(0.0332)\n",
      "17150 Training Loss: tensor(0.0331)\n",
      "17151 Training Loss: tensor(0.0330)\n",
      "17152 Training Loss: tensor(0.0329)\n",
      "17153 Training Loss: tensor(0.0330)\n",
      "17154 Training Loss: tensor(0.0329)\n",
      "17155 Training Loss: tensor(0.0331)\n",
      "17156 Training Loss: tensor(0.0330)\n",
      "17157 Training Loss: tensor(0.0331)\n",
      "17158 Training Loss: tensor(0.0330)\n",
      "17159 Training Loss: tensor(0.0331)\n",
      "17160 Training Loss: tensor(0.0331)\n",
      "17161 Training Loss: tensor(0.0330)\n",
      "17162 Training Loss: tensor(0.0329)\n",
      "17163 Training Loss: tensor(0.0332)\n",
      "17164 Training Loss: tensor(0.0331)\n",
      "17165 Training Loss: tensor(0.0331)\n",
      "17166 Training Loss: tensor(0.0329)\n",
      "17167 Training Loss: tensor(0.0331)\n",
      "17168 Training Loss: tensor(0.0330)\n",
      "17169 Training Loss: tensor(0.0332)\n",
      "17170 Training Loss: tensor(0.0331)\n",
      "17171 Training Loss: tensor(0.0332)\n",
      "17172 Training Loss: tensor(0.0331)\n",
      "17173 Training Loss: tensor(0.0330)\n",
      "17174 Training Loss: tensor(0.0330)\n",
      "17175 Training Loss: tensor(0.0331)\n",
      "17176 Training Loss: tensor(0.0330)\n",
      "17177 Training Loss: tensor(0.0330)\n",
      "17178 Training Loss: tensor(0.0331)\n",
      "17179 Training Loss: tensor(0.0330)\n",
      "17180 Training Loss: tensor(0.0330)\n",
      "17181 Training Loss: tensor(0.0329)\n",
      "17182 Training Loss: tensor(0.0331)\n",
      "17183 Training Loss: tensor(0.0330)\n",
      "17184 Training Loss: tensor(0.0329)\n",
      "17185 Training Loss: tensor(0.0330)\n",
      "17186 Training Loss: tensor(0.0330)\n",
      "17187 Training Loss: tensor(0.0330)\n",
      "17188 Training Loss: tensor(0.0329)\n",
      "17189 Training Loss: tensor(0.0329)\n",
      "17190 Training Loss: tensor(0.0330)\n",
      "17191 Training Loss: tensor(0.0331)\n",
      "17192 Training Loss: tensor(0.0330)\n",
      "17193 Training Loss: tensor(0.0329)\n",
      "17194 Training Loss: tensor(0.0330)\n",
      "17195 Training Loss: tensor(0.0331)\n",
      "17196 Training Loss: tensor(0.0333)\n",
      "17197 Training Loss: tensor(0.0330)\n",
      "17198 Training Loss: tensor(0.0329)\n",
      "17199 Training Loss: tensor(0.0329)\n",
      "17200 Training Loss: tensor(0.0330)\n",
      "17201 Training Loss: tensor(0.0328)\n",
      "17202 Training Loss: tensor(0.0330)\n",
      "17203 Training Loss: tensor(0.0329)\n",
      "17204 Training Loss: tensor(0.0330)\n",
      "17205 Training Loss: tensor(0.0329)\n",
      "17206 Training Loss: tensor(0.0329)\n",
      "17207 Training Loss: tensor(0.0330)\n",
      "17208 Training Loss: tensor(0.0330)\n",
      "17209 Training Loss: tensor(0.0330)\n",
      "17210 Training Loss: tensor(0.0331)\n",
      "17211 Training Loss: tensor(0.0329)\n",
      "17212 Training Loss: tensor(0.0330)\n",
      "17213 Training Loss: tensor(0.0330)\n",
      "17214 Training Loss: tensor(0.0330)\n",
      "17215 Training Loss: tensor(0.0330)\n",
      "17216 Training Loss: tensor(0.0330)\n",
      "17217 Training Loss: tensor(0.0330)\n",
      "17218 Training Loss: tensor(0.0329)\n",
      "17219 Training Loss: tensor(0.0330)\n",
      "17220 Training Loss: tensor(0.0330)\n",
      "17221 Training Loss: tensor(0.0330)\n",
      "17222 Training Loss: tensor(0.0331)\n",
      "17223 Training Loss: tensor(0.0330)\n",
      "17224 Training Loss: tensor(0.0329)\n",
      "17225 Training Loss: tensor(0.0330)\n",
      "17226 Training Loss: tensor(0.0330)\n",
      "17227 Training Loss: tensor(0.0328)\n",
      "17228 Training Loss: tensor(0.0331)\n",
      "17229 Training Loss: tensor(0.0330)\n",
      "17230 Training Loss: tensor(0.0329)\n",
      "17231 Training Loss: tensor(0.0331)\n",
      "17232 Training Loss: tensor(0.0329)\n",
      "17233 Training Loss: tensor(0.0329)\n",
      "17234 Training Loss: tensor(0.0330)\n",
      "17235 Training Loss: tensor(0.0330)\n",
      "17236 Training Loss: tensor(0.0329)\n",
      "17237 Training Loss: tensor(0.0331)\n",
      "17238 Training Loss: tensor(0.0329)\n",
      "17239 Training Loss: tensor(0.0331)\n",
      "17240 Training Loss: tensor(0.0330)\n",
      "17241 Training Loss: tensor(0.0330)\n",
      "17242 Training Loss: tensor(0.0330)\n",
      "17243 Training Loss: tensor(0.0332)\n",
      "17244 Training Loss: tensor(0.0330)\n",
      "17245 Training Loss: tensor(0.0329)\n",
      "17246 Training Loss: tensor(0.0330)\n",
      "17247 Training Loss: tensor(0.0330)\n",
      "17248 Training Loss: tensor(0.0328)\n",
      "17249 Training Loss: tensor(0.0331)\n",
      "17250 Training Loss: tensor(0.0329)\n",
      "17251 Training Loss: tensor(0.0329)\n",
      "17252 Training Loss: tensor(0.0330)\n",
      "17253 Training Loss: tensor(0.0329)\n",
      "17254 Training Loss: tensor(0.0330)\n",
      "17255 Training Loss: tensor(0.0329)\n",
      "17256 Training Loss: tensor(0.0329)\n",
      "17257 Training Loss: tensor(0.0330)\n",
      "17258 Training Loss: tensor(0.0330)\n",
      "17259 Training Loss: tensor(0.0332)\n",
      "17260 Training Loss: tensor(0.0329)\n",
      "17261 Training Loss: tensor(0.0330)\n",
      "17262 Training Loss: tensor(0.0329)\n",
      "17263 Training Loss: tensor(0.0329)\n",
      "17264 Training Loss: tensor(0.0331)\n",
      "17265 Training Loss: tensor(0.0329)\n",
      "17266 Training Loss: tensor(0.0329)\n",
      "17267 Training Loss: tensor(0.0329)\n",
      "17268 Training Loss: tensor(0.0329)\n",
      "17269 Training Loss: tensor(0.0329)\n",
      "17270 Training Loss: tensor(0.0328)\n",
      "17271 Training Loss: tensor(0.0330)\n",
      "17272 Training Loss: tensor(0.0331)\n",
      "17273 Training Loss: tensor(0.0329)\n",
      "17274 Training Loss: tensor(0.0331)\n",
      "17275 Training Loss: tensor(0.0329)\n",
      "17276 Training Loss: tensor(0.0329)\n",
      "17277 Training Loss: tensor(0.0330)\n",
      "17278 Training Loss: tensor(0.0328)\n",
      "17279 Training Loss: tensor(0.0329)\n",
      "17280 Training Loss: tensor(0.0330)\n",
      "17281 Training Loss: tensor(0.0330)\n",
      "17282 Training Loss: tensor(0.0328)\n",
      "17283 Training Loss: tensor(0.0329)\n",
      "17284 Training Loss: tensor(0.0329)\n",
      "17285 Training Loss: tensor(0.0331)\n",
      "17286 Training Loss: tensor(0.0329)\n",
      "17287 Training Loss: tensor(0.0329)\n",
      "17288 Training Loss: tensor(0.0328)\n",
      "17289 Training Loss: tensor(0.0330)\n",
      "17290 Training Loss: tensor(0.0329)\n",
      "17291 Training Loss: tensor(0.0328)\n",
      "17292 Training Loss: tensor(0.0331)\n",
      "17293 Training Loss: tensor(0.0329)\n",
      "17294 Training Loss: tensor(0.0329)\n",
      "17295 Training Loss: tensor(0.0328)\n",
      "17296 Training Loss: tensor(0.0329)\n",
      "17297 Training Loss: tensor(0.0330)\n",
      "17298 Training Loss: tensor(0.0329)\n",
      "17299 Training Loss: tensor(0.0329)\n",
      "17300 Training Loss: tensor(0.0330)\n",
      "17301 Training Loss: tensor(0.0330)\n",
      "17302 Training Loss: tensor(0.0328)\n",
      "17303 Training Loss: tensor(0.0329)\n",
      "17304 Training Loss: tensor(0.0329)\n",
      "17305 Training Loss: tensor(0.0330)\n",
      "17306 Training Loss: tensor(0.0330)\n",
      "17307 Training Loss: tensor(0.0329)\n",
      "17308 Training Loss: tensor(0.0330)\n",
      "17309 Training Loss: tensor(0.0330)\n",
      "17310 Training Loss: tensor(0.0329)\n",
      "17311 Training Loss: tensor(0.0330)\n",
      "17312 Training Loss: tensor(0.0328)\n",
      "17313 Training Loss: tensor(0.0330)\n",
      "17314 Training Loss: tensor(0.0329)\n",
      "17315 Training Loss: tensor(0.0329)\n",
      "17316 Training Loss: tensor(0.0331)\n",
      "17317 Training Loss: tensor(0.0329)\n",
      "17318 Training Loss: tensor(0.0330)\n",
      "17319 Training Loss: tensor(0.0329)\n",
      "17320 Training Loss: tensor(0.0328)\n",
      "17321 Training Loss: tensor(0.0329)\n",
      "17322 Training Loss: tensor(0.0330)\n",
      "17323 Training Loss: tensor(0.0329)\n",
      "17324 Training Loss: tensor(0.0329)\n",
      "17325 Training Loss: tensor(0.0328)\n",
      "17326 Training Loss: tensor(0.0328)\n",
      "17327 Training Loss: tensor(0.0330)\n",
      "17328 Training Loss: tensor(0.0328)\n",
      "17329 Training Loss: tensor(0.0330)\n",
      "17330 Training Loss: tensor(0.0328)\n",
      "17331 Training Loss: tensor(0.0330)\n",
      "17332 Training Loss: tensor(0.0329)\n",
      "17333 Training Loss: tensor(0.0329)\n",
      "17334 Training Loss: tensor(0.0328)\n",
      "17335 Training Loss: tensor(0.0329)\n",
      "17336 Training Loss: tensor(0.0329)\n",
      "17337 Training Loss: tensor(0.0331)\n",
      "17338 Training Loss: tensor(0.0330)\n",
      "17339 Training Loss: tensor(0.0330)\n",
      "17340 Training Loss: tensor(0.0330)\n",
      "17341 Training Loss: tensor(0.0329)\n",
      "17342 Training Loss: tensor(0.0329)\n",
      "17343 Training Loss: tensor(0.0329)\n",
      "17344 Training Loss: tensor(0.0329)\n",
      "17345 Training Loss: tensor(0.0329)\n",
      "17346 Training Loss: tensor(0.0329)\n",
      "17347 Training Loss: tensor(0.0329)\n",
      "17348 Training Loss: tensor(0.0329)\n",
      "17349 Training Loss: tensor(0.0328)\n",
      "17350 Training Loss: tensor(0.0327)\n",
      "17351 Training Loss: tensor(0.0329)\n",
      "17352 Training Loss: tensor(0.0329)\n",
      "17353 Training Loss: tensor(0.0328)\n",
      "17354 Training Loss: tensor(0.0329)\n",
      "17355 Training Loss: tensor(0.0331)\n",
      "17356 Training Loss: tensor(0.0328)\n",
      "17357 Training Loss: tensor(0.0330)\n",
      "17358 Training Loss: tensor(0.0329)\n",
      "17359 Training Loss: tensor(0.0330)\n",
      "17360 Training Loss: tensor(0.0331)\n",
      "17361 Training Loss: tensor(0.0329)\n",
      "17362 Training Loss: tensor(0.0329)\n",
      "17363 Training Loss: tensor(0.0329)\n",
      "17364 Training Loss: tensor(0.0328)\n",
      "17365 Training Loss: tensor(0.0332)\n",
      "17366 Training Loss: tensor(0.0327)\n",
      "17367 Training Loss: tensor(0.0330)\n",
      "17368 Training Loss: tensor(0.0331)\n",
      "17369 Training Loss: tensor(0.0329)\n",
      "17370 Training Loss: tensor(0.0330)\n",
      "17371 Training Loss: tensor(0.0329)\n",
      "17372 Training Loss: tensor(0.0329)\n",
      "17373 Training Loss: tensor(0.0329)\n",
      "17374 Training Loss: tensor(0.0329)\n",
      "17375 Training Loss: tensor(0.0328)\n",
      "17376 Training Loss: tensor(0.0329)\n",
      "17377 Training Loss: tensor(0.0330)\n",
      "17378 Training Loss: tensor(0.0329)\n",
      "17379 Training Loss: tensor(0.0329)\n",
      "17380 Training Loss: tensor(0.0331)\n",
      "17381 Training Loss: tensor(0.0329)\n",
      "17382 Training Loss: tensor(0.0330)\n",
      "17383 Training Loss: tensor(0.0329)\n",
      "17384 Training Loss: tensor(0.0328)\n",
      "17385 Training Loss: tensor(0.0328)\n",
      "17386 Training Loss: tensor(0.0330)\n",
      "17387 Training Loss: tensor(0.0329)\n",
      "17388 Training Loss: tensor(0.0328)\n",
      "17389 Training Loss: tensor(0.0329)\n",
      "17390 Training Loss: tensor(0.0329)\n",
      "17391 Training Loss: tensor(0.0329)\n",
      "17392 Training Loss: tensor(0.0328)\n",
      "17393 Training Loss: tensor(0.0328)\n",
      "17394 Training Loss: tensor(0.0329)\n",
      "17395 Training Loss: tensor(0.0328)\n",
      "17396 Training Loss: tensor(0.0329)\n",
      "17397 Training Loss: tensor(0.0329)\n",
      "17398 Training Loss: tensor(0.0328)\n",
      "17399 Training Loss: tensor(0.0331)\n",
      "17400 Training Loss: tensor(0.0330)\n",
      "17401 Training Loss: tensor(0.0330)\n",
      "17402 Training Loss: tensor(0.0330)\n",
      "17403 Training Loss: tensor(0.0329)\n",
      "17404 Training Loss: tensor(0.0328)\n",
      "17405 Training Loss: tensor(0.0329)\n",
      "17406 Training Loss: tensor(0.0329)\n",
      "17407 Training Loss: tensor(0.0329)\n",
      "17408 Training Loss: tensor(0.0329)\n",
      "17409 Training Loss: tensor(0.0329)\n",
      "17410 Training Loss: tensor(0.0328)\n",
      "17411 Training Loss: tensor(0.0329)\n",
      "17412 Training Loss: tensor(0.0329)\n",
      "17413 Training Loss: tensor(0.0331)\n",
      "17414 Training Loss: tensor(0.0328)\n",
      "17415 Training Loss: tensor(0.0328)\n",
      "17416 Training Loss: tensor(0.0326)\n",
      "17417 Training Loss: tensor(0.0329)\n",
      "17418 Training Loss: tensor(0.0329)\n",
      "17419 Training Loss: tensor(0.0328)\n",
      "17420 Training Loss: tensor(0.0328)\n",
      "17421 Training Loss: tensor(0.0330)\n",
      "17422 Training Loss: tensor(0.0327)\n",
      "17423 Training Loss: tensor(0.0330)\n",
      "17424 Training Loss: tensor(0.0328)\n",
      "17425 Training Loss: tensor(0.0329)\n",
      "17426 Training Loss: tensor(0.0330)\n",
      "17427 Training Loss: tensor(0.0329)\n",
      "17428 Training Loss: tensor(0.0328)\n",
      "17429 Training Loss: tensor(0.0329)\n",
      "17430 Training Loss: tensor(0.0329)\n",
      "17431 Training Loss: tensor(0.0328)\n",
      "17432 Training Loss: tensor(0.0328)\n",
      "17433 Training Loss: tensor(0.0330)\n",
      "17434 Training Loss: tensor(0.0329)\n",
      "17435 Training Loss: tensor(0.0329)\n",
      "17436 Training Loss: tensor(0.0328)\n",
      "17437 Training Loss: tensor(0.0328)\n",
      "17438 Training Loss: tensor(0.0329)\n",
      "17439 Training Loss: tensor(0.0330)\n",
      "17440 Training Loss: tensor(0.0328)\n",
      "17441 Training Loss: tensor(0.0328)\n",
      "17442 Training Loss: tensor(0.0327)\n",
      "17443 Training Loss: tensor(0.0328)\n",
      "17444 Training Loss: tensor(0.0330)\n",
      "17445 Training Loss: tensor(0.0328)\n",
      "17446 Training Loss: tensor(0.0327)\n",
      "17447 Training Loss: tensor(0.0329)\n",
      "17448 Training Loss: tensor(0.0329)\n",
      "17449 Training Loss: tensor(0.0329)\n",
      "17450 Training Loss: tensor(0.0328)\n",
      "17451 Training Loss: tensor(0.0329)\n",
      "17452 Training Loss: tensor(0.0328)\n",
      "17453 Training Loss: tensor(0.0327)\n",
      "17454 Training Loss: tensor(0.0329)\n",
      "17455 Training Loss: tensor(0.0331)\n",
      "17456 Training Loss: tensor(0.0329)\n",
      "17457 Training Loss: tensor(0.0329)\n",
      "17458 Training Loss: tensor(0.0329)\n",
      "17459 Training Loss: tensor(0.0329)\n",
      "17460 Training Loss: tensor(0.0328)\n",
      "17461 Training Loss: tensor(0.0329)\n",
      "17462 Training Loss: tensor(0.0328)\n",
      "17463 Training Loss: tensor(0.0328)\n",
      "17464 Training Loss: tensor(0.0329)\n",
      "17465 Training Loss: tensor(0.0327)\n",
      "17466 Training Loss: tensor(0.0330)\n",
      "17467 Training Loss: tensor(0.0329)\n",
      "17468 Training Loss: tensor(0.0329)\n",
      "17469 Training Loss: tensor(0.0328)\n",
      "17470 Training Loss: tensor(0.0329)\n",
      "17471 Training Loss: tensor(0.0327)\n",
      "17472 Training Loss: tensor(0.0329)\n",
      "17473 Training Loss: tensor(0.0327)\n",
      "17474 Training Loss: tensor(0.0330)\n",
      "17475 Training Loss: tensor(0.0328)\n",
      "17476 Training Loss: tensor(0.0328)\n",
      "17477 Training Loss: tensor(0.0330)\n",
      "17478 Training Loss: tensor(0.0327)\n",
      "17479 Training Loss: tensor(0.0329)\n",
      "17480 Training Loss: tensor(0.0330)\n",
      "17481 Training Loss: tensor(0.0330)\n",
      "17482 Training Loss: tensor(0.0328)\n",
      "17483 Training Loss: tensor(0.0329)\n",
      "17484 Training Loss: tensor(0.0329)\n",
      "17485 Training Loss: tensor(0.0328)\n",
      "17486 Training Loss: tensor(0.0330)\n",
      "17487 Training Loss: tensor(0.0329)\n",
      "17488 Training Loss: tensor(0.0329)\n",
      "17489 Training Loss: tensor(0.0328)\n",
      "17490 Training Loss: tensor(0.0328)\n",
      "17491 Training Loss: tensor(0.0329)\n",
      "17492 Training Loss: tensor(0.0329)\n",
      "17493 Training Loss: tensor(0.0328)\n",
      "17494 Training Loss: tensor(0.0329)\n",
      "17495 Training Loss: tensor(0.0327)\n",
      "17496 Training Loss: tensor(0.0327)\n",
      "17497 Training Loss: tensor(0.0328)\n",
      "17498 Training Loss: tensor(0.0329)\n",
      "17499 Training Loss: tensor(0.0327)\n",
      "17500 Training Loss: tensor(0.0328)\n",
      "17501 Training Loss: tensor(0.0329)\n",
      "17502 Training Loss: tensor(0.0327)\n",
      "17503 Training Loss: tensor(0.0330)\n",
      "17504 Training Loss: tensor(0.0328)\n",
      "17505 Training Loss: tensor(0.0329)\n",
      "17506 Training Loss: tensor(0.0330)\n",
      "17507 Training Loss: tensor(0.0327)\n",
      "17508 Training Loss: tensor(0.0329)\n",
      "17509 Training Loss: tensor(0.0329)\n",
      "17510 Training Loss: tensor(0.0327)\n",
      "17511 Training Loss: tensor(0.0328)\n",
      "17512 Training Loss: tensor(0.0328)\n",
      "17513 Training Loss: tensor(0.0327)\n",
      "17514 Training Loss: tensor(0.0329)\n",
      "17515 Training Loss: tensor(0.0330)\n",
      "17516 Training Loss: tensor(0.0330)\n",
      "17517 Training Loss: tensor(0.0328)\n",
      "17518 Training Loss: tensor(0.0328)\n",
      "17519 Training Loss: tensor(0.0327)\n",
      "17520 Training Loss: tensor(0.0328)\n",
      "17521 Training Loss: tensor(0.0328)\n",
      "17522 Training Loss: tensor(0.0328)\n",
      "17523 Training Loss: tensor(0.0329)\n",
      "17524 Training Loss: tensor(0.0327)\n",
      "17525 Training Loss: tensor(0.0328)\n",
      "17526 Training Loss: tensor(0.0328)\n",
      "17527 Training Loss: tensor(0.0327)\n",
      "17528 Training Loss: tensor(0.0328)\n",
      "17529 Training Loss: tensor(0.0332)\n",
      "17530 Training Loss: tensor(0.0329)\n",
      "17531 Training Loss: tensor(0.0328)\n",
      "17532 Training Loss: tensor(0.0330)\n",
      "17533 Training Loss: tensor(0.0328)\n",
      "17534 Training Loss: tensor(0.0331)\n",
      "17535 Training Loss: tensor(0.0332)\n",
      "17536 Training Loss: tensor(0.0328)\n",
      "17537 Training Loss: tensor(0.0329)\n",
      "17538 Training Loss: tensor(0.0328)\n",
      "17539 Training Loss: tensor(0.0329)\n",
      "17540 Training Loss: tensor(0.0328)\n",
      "17541 Training Loss: tensor(0.0328)\n",
      "17542 Training Loss: tensor(0.0328)\n",
      "17543 Training Loss: tensor(0.0328)\n",
      "17544 Training Loss: tensor(0.0328)\n",
      "17545 Training Loss: tensor(0.0329)\n",
      "17546 Training Loss: tensor(0.0329)\n",
      "17547 Training Loss: tensor(0.0328)\n",
      "17548 Training Loss: tensor(0.0328)\n",
      "17549 Training Loss: tensor(0.0328)\n",
      "17550 Training Loss: tensor(0.0327)\n",
      "17551 Training Loss: tensor(0.0327)\n",
      "17552 Training Loss: tensor(0.0327)\n",
      "17553 Training Loss: tensor(0.0329)\n",
      "17554 Training Loss: tensor(0.0329)\n",
      "17555 Training Loss: tensor(0.0328)\n",
      "17556 Training Loss: tensor(0.0327)\n",
      "17557 Training Loss: tensor(0.0329)\n",
      "17558 Training Loss: tensor(0.0331)\n",
      "17559 Training Loss: tensor(0.0327)\n",
      "17560 Training Loss: tensor(0.0327)\n",
      "17561 Training Loss: tensor(0.0328)\n",
      "17562 Training Loss: tensor(0.0329)\n",
      "17563 Training Loss: tensor(0.0328)\n",
      "17564 Training Loss: tensor(0.0328)\n",
      "17565 Training Loss: tensor(0.0327)\n",
      "17566 Training Loss: tensor(0.0328)\n",
      "17567 Training Loss: tensor(0.0329)\n",
      "17568 Training Loss: tensor(0.0328)\n",
      "17569 Training Loss: tensor(0.0329)\n",
      "17570 Training Loss: tensor(0.0328)\n",
      "17571 Training Loss: tensor(0.0328)\n",
      "17572 Training Loss: tensor(0.0328)\n",
      "17573 Training Loss: tensor(0.0327)\n",
      "17574 Training Loss: tensor(0.0327)\n",
      "17575 Training Loss: tensor(0.0327)\n",
      "17576 Training Loss: tensor(0.0328)\n",
      "17577 Training Loss: tensor(0.0327)\n",
      "17578 Training Loss: tensor(0.0326)\n",
      "17579 Training Loss: tensor(0.0327)\n",
      "17580 Training Loss: tensor(0.0327)\n",
      "17581 Training Loss: tensor(0.0327)\n",
      "17582 Training Loss: tensor(0.0327)\n",
      "17583 Training Loss: tensor(0.0327)\n",
      "17584 Training Loss: tensor(0.0329)\n",
      "17585 Training Loss: tensor(0.0327)\n",
      "17586 Training Loss: tensor(0.0327)\n",
      "17587 Training Loss: tensor(0.0329)\n",
      "17588 Training Loss: tensor(0.0327)\n",
      "17589 Training Loss: tensor(0.0327)\n",
      "17590 Training Loss: tensor(0.0329)\n",
      "17591 Training Loss: tensor(0.0329)\n",
      "17592 Training Loss: tensor(0.0328)\n",
      "17593 Training Loss: tensor(0.0329)\n",
      "17594 Training Loss: tensor(0.0328)\n",
      "17595 Training Loss: tensor(0.0326)\n",
      "17596 Training Loss: tensor(0.0327)\n",
      "17597 Training Loss: tensor(0.0328)\n",
      "17598 Training Loss: tensor(0.0327)\n",
      "17599 Training Loss: tensor(0.0329)\n",
      "17600 Training Loss: tensor(0.0326)\n",
      "17601 Training Loss: tensor(0.0329)\n",
      "17602 Training Loss: tensor(0.0329)\n",
      "17603 Training Loss: tensor(0.0328)\n",
      "17604 Training Loss: tensor(0.0327)\n",
      "17605 Training Loss: tensor(0.0329)\n",
      "17606 Training Loss: tensor(0.0327)\n",
      "17607 Training Loss: tensor(0.0327)\n",
      "17608 Training Loss: tensor(0.0330)\n",
      "17609 Training Loss: tensor(0.0327)\n",
      "17610 Training Loss: tensor(0.0328)\n",
      "17611 Training Loss: tensor(0.0328)\n",
      "17612 Training Loss: tensor(0.0327)\n",
      "17613 Training Loss: tensor(0.0328)\n",
      "17614 Training Loss: tensor(0.0328)\n",
      "17615 Training Loss: tensor(0.0328)\n",
      "17616 Training Loss: tensor(0.0328)\n",
      "17617 Training Loss: tensor(0.0329)\n",
      "17618 Training Loss: tensor(0.0329)\n",
      "17619 Training Loss: tensor(0.0327)\n",
      "17620 Training Loss: tensor(0.0330)\n",
      "17621 Training Loss: tensor(0.0328)\n",
      "17622 Training Loss: tensor(0.0327)\n",
      "17623 Training Loss: tensor(0.0327)\n",
      "17624 Training Loss: tensor(0.0327)\n",
      "17625 Training Loss: tensor(0.0327)\n",
      "17626 Training Loss: tensor(0.0328)\n",
      "17627 Training Loss: tensor(0.0328)\n",
      "17628 Training Loss: tensor(0.0327)\n",
      "17629 Training Loss: tensor(0.0326)\n",
      "17630 Training Loss: tensor(0.0326)\n",
      "17631 Training Loss: tensor(0.0328)\n",
      "17632 Training Loss: tensor(0.0326)\n",
      "17633 Training Loss: tensor(0.0327)\n",
      "17634 Training Loss: tensor(0.0329)\n",
      "17635 Training Loss: tensor(0.0328)\n",
      "17636 Training Loss: tensor(0.0329)\n",
      "17637 Training Loss: tensor(0.0327)\n",
      "17638 Training Loss: tensor(0.0328)\n",
      "17639 Training Loss: tensor(0.0327)\n",
      "17640 Training Loss: tensor(0.0327)\n",
      "17641 Training Loss: tensor(0.0328)\n",
      "17642 Training Loss: tensor(0.0327)\n",
      "17643 Training Loss: tensor(0.0327)\n",
      "17644 Training Loss: tensor(0.0327)\n",
      "17645 Training Loss: tensor(0.0328)\n",
      "17646 Training Loss: tensor(0.0328)\n",
      "17647 Training Loss: tensor(0.0327)\n",
      "17648 Training Loss: tensor(0.0327)\n",
      "17649 Training Loss: tensor(0.0328)\n",
      "17650 Training Loss: tensor(0.0329)\n",
      "17651 Training Loss: tensor(0.0328)\n",
      "17652 Training Loss: tensor(0.0330)\n",
      "17653 Training Loss: tensor(0.0327)\n",
      "17654 Training Loss: tensor(0.0327)\n",
      "17655 Training Loss: tensor(0.0328)\n",
      "17656 Training Loss: tensor(0.0328)\n",
      "17657 Training Loss: tensor(0.0329)\n",
      "17658 Training Loss: tensor(0.0327)\n",
      "17659 Training Loss: tensor(0.0327)\n",
      "17660 Training Loss: tensor(0.0329)\n",
      "17661 Training Loss: tensor(0.0329)\n",
      "17662 Training Loss: tensor(0.0329)\n",
      "17663 Training Loss: tensor(0.0329)\n",
      "17664 Training Loss: tensor(0.0328)\n",
      "17665 Training Loss: tensor(0.0327)\n",
      "17666 Training Loss: tensor(0.0329)\n",
      "17667 Training Loss: tensor(0.0329)\n",
      "17668 Training Loss: tensor(0.0327)\n",
      "17669 Training Loss: tensor(0.0327)\n",
      "17670 Training Loss: tensor(0.0330)\n",
      "17671 Training Loss: tensor(0.0328)\n",
      "17672 Training Loss: tensor(0.0327)\n",
      "17673 Training Loss: tensor(0.0328)\n",
      "17674 Training Loss: tensor(0.0328)\n",
      "17675 Training Loss: tensor(0.0328)\n",
      "17676 Training Loss: tensor(0.0327)\n",
      "17677 Training Loss: tensor(0.0326)\n",
      "17678 Training Loss: tensor(0.0327)\n",
      "17679 Training Loss: tensor(0.0326)\n",
      "17680 Training Loss: tensor(0.0327)\n",
      "17681 Training Loss: tensor(0.0327)\n",
      "17682 Training Loss: tensor(0.0327)\n",
      "17683 Training Loss: tensor(0.0328)\n",
      "17684 Training Loss: tensor(0.0329)\n",
      "17685 Training Loss: tensor(0.0328)\n",
      "17686 Training Loss: tensor(0.0327)\n",
      "17687 Training Loss: tensor(0.0328)\n",
      "17688 Training Loss: tensor(0.0327)\n",
      "17689 Training Loss: tensor(0.0327)\n",
      "17690 Training Loss: tensor(0.0327)\n",
      "17691 Training Loss: tensor(0.0327)\n",
      "17692 Training Loss: tensor(0.0328)\n",
      "17693 Training Loss: tensor(0.0329)\n",
      "17694 Training Loss: tensor(0.0331)\n",
      "17695 Training Loss: tensor(0.0329)\n",
      "17696 Training Loss: tensor(0.0327)\n",
      "17697 Training Loss: tensor(0.0329)\n",
      "17698 Training Loss: tensor(0.0328)\n",
      "17699 Training Loss: tensor(0.0328)\n",
      "17700 Training Loss: tensor(0.0327)\n",
      "17701 Training Loss: tensor(0.0327)\n",
      "17702 Training Loss: tensor(0.0327)\n",
      "17703 Training Loss: tensor(0.0328)\n",
      "17704 Training Loss: tensor(0.0327)\n",
      "17705 Training Loss: tensor(0.0328)\n",
      "17706 Training Loss: tensor(0.0327)\n",
      "17707 Training Loss: tensor(0.0328)\n",
      "17708 Training Loss: tensor(0.0331)\n",
      "17709 Training Loss: tensor(0.0326)\n",
      "17710 Training Loss: tensor(0.0327)\n",
      "17711 Training Loss: tensor(0.0327)\n",
      "17712 Training Loss: tensor(0.0326)\n",
      "17713 Training Loss: tensor(0.0327)\n",
      "17714 Training Loss: tensor(0.0328)\n",
      "17715 Training Loss: tensor(0.0327)\n",
      "17716 Training Loss: tensor(0.0326)\n",
      "17717 Training Loss: tensor(0.0328)\n",
      "17718 Training Loss: tensor(0.0325)\n",
      "17719 Training Loss: tensor(0.0326)\n",
      "17720 Training Loss: tensor(0.0328)\n",
      "17721 Training Loss: tensor(0.0329)\n",
      "17722 Training Loss: tensor(0.0328)\n",
      "17723 Training Loss: tensor(0.0327)\n",
      "17724 Training Loss: tensor(0.0326)\n",
      "17725 Training Loss: tensor(0.0327)\n",
      "17726 Training Loss: tensor(0.0328)\n",
      "17727 Training Loss: tensor(0.0329)\n",
      "17728 Training Loss: tensor(0.0329)\n",
      "17729 Training Loss: tensor(0.0329)\n",
      "17730 Training Loss: tensor(0.0328)\n",
      "17731 Training Loss: tensor(0.0327)\n",
      "17732 Training Loss: tensor(0.0327)\n",
      "17733 Training Loss: tensor(0.0329)\n",
      "17734 Training Loss: tensor(0.0327)\n",
      "17735 Training Loss: tensor(0.0328)\n",
      "17736 Training Loss: tensor(0.0327)\n",
      "17737 Training Loss: tensor(0.0329)\n",
      "17738 Training Loss: tensor(0.0327)\n",
      "17739 Training Loss: tensor(0.0328)\n",
      "17740 Training Loss: tensor(0.0327)\n",
      "17741 Training Loss: tensor(0.0327)\n",
      "17742 Training Loss: tensor(0.0328)\n",
      "17743 Training Loss: tensor(0.0327)\n",
      "17744 Training Loss: tensor(0.0327)\n",
      "17745 Training Loss: tensor(0.0328)\n",
      "17746 Training Loss: tensor(0.0328)\n",
      "17747 Training Loss: tensor(0.0328)\n",
      "17748 Training Loss: tensor(0.0327)\n",
      "17749 Training Loss: tensor(0.0327)\n",
      "17750 Training Loss: tensor(0.0329)\n",
      "17751 Training Loss: tensor(0.0327)\n",
      "17752 Training Loss: tensor(0.0326)\n",
      "17753 Training Loss: tensor(0.0327)\n",
      "17754 Training Loss: tensor(0.0327)\n",
      "17755 Training Loss: tensor(0.0326)\n",
      "17756 Training Loss: tensor(0.0327)\n",
      "17757 Training Loss: tensor(0.0326)\n",
      "17758 Training Loss: tensor(0.0326)\n",
      "17759 Training Loss: tensor(0.0326)\n",
      "17760 Training Loss: tensor(0.0326)\n",
      "17761 Training Loss: tensor(0.0327)\n",
      "17762 Training Loss: tensor(0.0328)\n",
      "17763 Training Loss: tensor(0.0326)\n",
      "17764 Training Loss: tensor(0.0326)\n",
      "17765 Training Loss: tensor(0.0326)\n",
      "17766 Training Loss: tensor(0.0328)\n",
      "17767 Training Loss: tensor(0.0327)\n",
      "17768 Training Loss: tensor(0.0328)\n",
      "17769 Training Loss: tensor(0.0326)\n",
      "17770 Training Loss: tensor(0.0327)\n",
      "17771 Training Loss: tensor(0.0329)\n",
      "17772 Training Loss: tensor(0.0327)\n",
      "17773 Training Loss: tensor(0.0327)\n",
      "17774 Training Loss: tensor(0.0326)\n",
      "17775 Training Loss: tensor(0.0329)\n",
      "17776 Training Loss: tensor(0.0327)\n",
      "17777 Training Loss: tensor(0.0325)\n",
      "17778 Training Loss: tensor(0.0328)\n",
      "17779 Training Loss: tensor(0.0327)\n",
      "17780 Training Loss: tensor(0.0326)\n",
      "17781 Training Loss: tensor(0.0327)\n",
      "17782 Training Loss: tensor(0.0326)\n",
      "17783 Training Loss: tensor(0.0328)\n",
      "17784 Training Loss: tensor(0.0328)\n",
      "17785 Training Loss: tensor(0.0326)\n",
      "17786 Training Loss: tensor(0.0326)\n",
      "17787 Training Loss: tensor(0.0327)\n",
      "17788 Training Loss: tensor(0.0326)\n",
      "17789 Training Loss: tensor(0.0328)\n",
      "17790 Training Loss: tensor(0.0327)\n",
      "17791 Training Loss: tensor(0.0327)\n",
      "17792 Training Loss: tensor(0.0326)\n",
      "17793 Training Loss: tensor(0.0327)\n",
      "17794 Training Loss: tensor(0.0327)\n",
      "17795 Training Loss: tensor(0.0326)\n",
      "17796 Training Loss: tensor(0.0327)\n",
      "17797 Training Loss: tensor(0.0325)\n",
      "17798 Training Loss: tensor(0.0326)\n",
      "17799 Training Loss: tensor(0.0326)\n",
      "17800 Training Loss: tensor(0.0327)\n",
      "17801 Training Loss: tensor(0.0327)\n",
      "17802 Training Loss: tensor(0.0327)\n",
      "17803 Training Loss: tensor(0.0327)\n",
      "17804 Training Loss: tensor(0.0328)\n",
      "17805 Training Loss: tensor(0.0327)\n",
      "17806 Training Loss: tensor(0.0328)\n",
      "17807 Training Loss: tensor(0.0326)\n",
      "17808 Training Loss: tensor(0.0326)\n",
      "17809 Training Loss: tensor(0.0328)\n",
      "17810 Training Loss: tensor(0.0328)\n",
      "17811 Training Loss: tensor(0.0327)\n",
      "17812 Training Loss: tensor(0.0328)\n",
      "17813 Training Loss: tensor(0.0327)\n",
      "17814 Training Loss: tensor(0.0327)\n",
      "17815 Training Loss: tensor(0.0327)\n",
      "17816 Training Loss: tensor(0.0327)\n",
      "17817 Training Loss: tensor(0.0326)\n",
      "17818 Training Loss: tensor(0.0326)\n",
      "17819 Training Loss: tensor(0.0327)\n",
      "17820 Training Loss: tensor(0.0326)\n",
      "17821 Training Loss: tensor(0.0325)\n",
      "17822 Training Loss: tensor(0.0327)\n",
      "17823 Training Loss: tensor(0.0327)\n",
      "17824 Training Loss: tensor(0.0327)\n",
      "17825 Training Loss: tensor(0.0326)\n",
      "17826 Training Loss: tensor(0.0327)\n",
      "17827 Training Loss: tensor(0.0325)\n",
      "17828 Training Loss: tensor(0.0327)\n",
      "17829 Training Loss: tensor(0.0327)\n",
      "17830 Training Loss: tensor(0.0326)\n",
      "17831 Training Loss: tensor(0.0328)\n",
      "17832 Training Loss: tensor(0.0327)\n",
      "17833 Training Loss: tensor(0.0326)\n",
      "17834 Training Loss: tensor(0.0330)\n",
      "17835 Training Loss: tensor(0.0327)\n",
      "17836 Training Loss: tensor(0.0327)\n",
      "17837 Training Loss: tensor(0.0327)\n",
      "17838 Training Loss: tensor(0.0328)\n",
      "17839 Training Loss: tensor(0.0326)\n",
      "17840 Training Loss: tensor(0.0328)\n",
      "17841 Training Loss: tensor(0.0326)\n",
      "17842 Training Loss: tensor(0.0327)\n",
      "17843 Training Loss: tensor(0.0325)\n",
      "17844 Training Loss: tensor(0.0326)\n",
      "17845 Training Loss: tensor(0.0326)\n",
      "17846 Training Loss: tensor(0.0327)\n",
      "17847 Training Loss: tensor(0.0326)\n",
      "17848 Training Loss: tensor(0.0326)\n",
      "17849 Training Loss: tensor(0.0325)\n",
      "17850 Training Loss: tensor(0.0327)\n",
      "17851 Training Loss: tensor(0.0330)\n",
      "17852 Training Loss: tensor(0.0326)\n",
      "17853 Training Loss: tensor(0.0326)\n",
      "17854 Training Loss: tensor(0.0327)\n",
      "17855 Training Loss: tensor(0.0326)\n",
      "17856 Training Loss: tensor(0.0327)\n",
      "17857 Training Loss: tensor(0.0328)\n",
      "17858 Training Loss: tensor(0.0324)\n",
      "17859 Training Loss: tensor(0.0326)\n",
      "17860 Training Loss: tensor(0.0326)\n",
      "17861 Training Loss: tensor(0.0326)\n",
      "17862 Training Loss: tensor(0.0326)\n",
      "17863 Training Loss: tensor(0.0325)\n",
      "17864 Training Loss: tensor(0.0326)\n",
      "17865 Training Loss: tensor(0.0328)\n",
      "17866 Training Loss: tensor(0.0327)\n",
      "17867 Training Loss: tensor(0.0325)\n",
      "17868 Training Loss: tensor(0.0326)\n",
      "17869 Training Loss: tensor(0.0325)\n",
      "17870 Training Loss: tensor(0.0327)\n",
      "17871 Training Loss: tensor(0.0326)\n",
      "17872 Training Loss: tensor(0.0329)\n",
      "17873 Training Loss: tensor(0.0326)\n",
      "17874 Training Loss: tensor(0.0326)\n",
      "17875 Training Loss: tensor(0.0328)\n",
      "17876 Training Loss: tensor(0.0327)\n",
      "17877 Training Loss: tensor(0.0327)\n",
      "17878 Training Loss: tensor(0.0325)\n",
      "17879 Training Loss: tensor(0.0326)\n",
      "17880 Training Loss: tensor(0.0328)\n",
      "17881 Training Loss: tensor(0.0327)\n",
      "17882 Training Loss: tensor(0.0326)\n",
      "17883 Training Loss: tensor(0.0325)\n",
      "17884 Training Loss: tensor(0.0326)\n",
      "17885 Training Loss: tensor(0.0327)\n",
      "17886 Training Loss: tensor(0.0327)\n",
      "17887 Training Loss: tensor(0.0326)\n",
      "17888 Training Loss: tensor(0.0326)\n",
      "17889 Training Loss: tensor(0.0328)\n",
      "17890 Training Loss: tensor(0.0326)\n",
      "17891 Training Loss: tensor(0.0326)\n",
      "17892 Training Loss: tensor(0.0329)\n",
      "17893 Training Loss: tensor(0.0327)\n",
      "17894 Training Loss: tensor(0.0330)\n",
      "17895 Training Loss: tensor(0.0326)\n",
      "17896 Training Loss: tensor(0.0327)\n",
      "17897 Training Loss: tensor(0.0327)\n",
      "17898 Training Loss: tensor(0.0326)\n",
      "17899 Training Loss: tensor(0.0326)\n",
      "17900 Training Loss: tensor(0.0327)\n",
      "17901 Training Loss: tensor(0.0328)\n",
      "17902 Training Loss: tensor(0.0329)\n",
      "17903 Training Loss: tensor(0.0326)\n",
      "17904 Training Loss: tensor(0.0327)\n",
      "17905 Training Loss: tensor(0.0327)\n",
      "17906 Training Loss: tensor(0.0327)\n",
      "17907 Training Loss: tensor(0.0328)\n",
      "17908 Training Loss: tensor(0.0326)\n",
      "17909 Training Loss: tensor(0.0325)\n",
      "17910 Training Loss: tensor(0.0327)\n",
      "17911 Training Loss: tensor(0.0326)\n",
      "17912 Training Loss: tensor(0.0328)\n",
      "17913 Training Loss: tensor(0.0326)\n",
      "17914 Training Loss: tensor(0.0326)\n",
      "17915 Training Loss: tensor(0.0329)\n",
      "17916 Training Loss: tensor(0.0326)\n",
      "17917 Training Loss: tensor(0.0326)\n",
      "17918 Training Loss: tensor(0.0326)\n",
      "17919 Training Loss: tensor(0.0327)\n",
      "17920 Training Loss: tensor(0.0327)\n",
      "17921 Training Loss: tensor(0.0328)\n",
      "17922 Training Loss: tensor(0.0325)\n",
      "17923 Training Loss: tensor(0.0326)\n",
      "17924 Training Loss: tensor(0.0329)\n",
      "17925 Training Loss: tensor(0.0326)\n",
      "17926 Training Loss: tensor(0.0326)\n",
      "17927 Training Loss: tensor(0.0327)\n",
      "17928 Training Loss: tensor(0.0327)\n",
      "17929 Training Loss: tensor(0.0327)\n",
      "17930 Training Loss: tensor(0.0326)\n",
      "17931 Training Loss: tensor(0.0325)\n",
      "17932 Training Loss: tensor(0.0326)\n",
      "17933 Training Loss: tensor(0.0326)\n",
      "17934 Training Loss: tensor(0.0325)\n",
      "17935 Training Loss: tensor(0.0325)\n",
      "17936 Training Loss: tensor(0.0326)\n",
      "17937 Training Loss: tensor(0.0326)\n",
      "17938 Training Loss: tensor(0.0326)\n",
      "17939 Training Loss: tensor(0.0326)\n",
      "17940 Training Loss: tensor(0.0328)\n",
      "17941 Training Loss: tensor(0.0326)\n",
      "17942 Training Loss: tensor(0.0326)\n",
      "17943 Training Loss: tensor(0.0325)\n",
      "17944 Training Loss: tensor(0.0326)\n",
      "17945 Training Loss: tensor(0.0326)\n",
      "17946 Training Loss: tensor(0.0326)\n",
      "17947 Training Loss: tensor(0.0325)\n",
      "17948 Training Loss: tensor(0.0325)\n",
      "17949 Training Loss: tensor(0.0325)\n",
      "17950 Training Loss: tensor(0.0327)\n",
      "17951 Training Loss: tensor(0.0327)\n",
      "17952 Training Loss: tensor(0.0327)\n",
      "17953 Training Loss: tensor(0.0325)\n",
      "17954 Training Loss: tensor(0.0326)\n",
      "17955 Training Loss: tensor(0.0326)\n",
      "17956 Training Loss: tensor(0.0326)\n",
      "17957 Training Loss: tensor(0.0325)\n",
      "17958 Training Loss: tensor(0.0325)\n",
      "17959 Training Loss: tensor(0.0325)\n",
      "17960 Training Loss: tensor(0.0326)\n",
      "17961 Training Loss: tensor(0.0327)\n",
      "17962 Training Loss: tensor(0.0326)\n",
      "17963 Training Loss: tensor(0.0328)\n",
      "17964 Training Loss: tensor(0.0325)\n",
      "17965 Training Loss: tensor(0.0327)\n",
      "17966 Training Loss: tensor(0.0326)\n",
      "17967 Training Loss: tensor(0.0325)\n",
      "17968 Training Loss: tensor(0.0326)\n",
      "17969 Training Loss: tensor(0.0328)\n",
      "17970 Training Loss: tensor(0.0326)\n",
      "17971 Training Loss: tensor(0.0326)\n",
      "17972 Training Loss: tensor(0.0324)\n",
      "17973 Training Loss: tensor(0.0324)\n",
      "17974 Training Loss: tensor(0.0325)\n",
      "17975 Training Loss: tensor(0.0326)\n",
      "17976 Training Loss: tensor(0.0325)\n",
      "17977 Training Loss: tensor(0.0327)\n",
      "17978 Training Loss: tensor(0.0326)\n",
      "17979 Training Loss: tensor(0.0325)\n",
      "17980 Training Loss: tensor(0.0325)\n",
      "17981 Training Loss: tensor(0.0327)\n",
      "17982 Training Loss: tensor(0.0326)\n",
      "17983 Training Loss: tensor(0.0327)\n",
      "17984 Training Loss: tensor(0.0328)\n",
      "17985 Training Loss: tensor(0.0326)\n",
      "17986 Training Loss: tensor(0.0326)\n",
      "17987 Training Loss: tensor(0.0325)\n",
      "17988 Training Loss: tensor(0.0326)\n",
      "17989 Training Loss: tensor(0.0326)\n",
      "17990 Training Loss: tensor(0.0327)\n",
      "17991 Training Loss: tensor(0.0325)\n",
      "17992 Training Loss: tensor(0.0326)\n",
      "17993 Training Loss: tensor(0.0325)\n",
      "17994 Training Loss: tensor(0.0327)\n",
      "17995 Training Loss: tensor(0.0326)\n",
      "17996 Training Loss: tensor(0.0327)\n",
      "17997 Training Loss: tensor(0.0325)\n",
      "17998 Training Loss: tensor(0.0327)\n",
      "17999 Training Loss: tensor(0.0325)\n",
      "18000 Training Loss: tensor(0.0329)\n",
      "18001 Training Loss: tensor(0.0326)\n",
      "18002 Training Loss: tensor(0.0325)\n",
      "18003 Training Loss: tensor(0.0327)\n",
      "18004 Training Loss: tensor(0.0327)\n",
      "18005 Training Loss: tensor(0.0325)\n",
      "18006 Training Loss: tensor(0.0324)\n",
      "18007 Training Loss: tensor(0.0326)\n",
      "18008 Training Loss: tensor(0.0326)\n",
      "18009 Training Loss: tensor(0.0326)\n",
      "18010 Training Loss: tensor(0.0327)\n",
      "18011 Training Loss: tensor(0.0325)\n",
      "18012 Training Loss: tensor(0.0326)\n",
      "18013 Training Loss: tensor(0.0326)\n",
      "18014 Training Loss: tensor(0.0326)\n",
      "18015 Training Loss: tensor(0.0326)\n",
      "18016 Training Loss: tensor(0.0325)\n",
      "18017 Training Loss: tensor(0.0326)\n",
      "18018 Training Loss: tensor(0.0328)\n",
      "18019 Training Loss: tensor(0.0326)\n",
      "18020 Training Loss: tensor(0.0326)\n",
      "18021 Training Loss: tensor(0.0325)\n",
      "18022 Training Loss: tensor(0.0327)\n",
      "18023 Training Loss: tensor(0.0326)\n",
      "18024 Training Loss: tensor(0.0325)\n",
      "18025 Training Loss: tensor(0.0325)\n",
      "18026 Training Loss: tensor(0.0325)\n",
      "18027 Training Loss: tensor(0.0325)\n",
      "18028 Training Loss: tensor(0.0325)\n",
      "18029 Training Loss: tensor(0.0325)\n",
      "18030 Training Loss: tensor(0.0327)\n",
      "18031 Training Loss: tensor(0.0326)\n",
      "18032 Training Loss: tensor(0.0326)\n",
      "18033 Training Loss: tensor(0.0327)\n",
      "18034 Training Loss: tensor(0.0325)\n",
      "18035 Training Loss: tensor(0.0325)\n",
      "18036 Training Loss: tensor(0.0326)\n",
      "18037 Training Loss: tensor(0.0326)\n",
      "18038 Training Loss: tensor(0.0326)\n",
      "18039 Training Loss: tensor(0.0325)\n",
      "18040 Training Loss: tensor(0.0326)\n",
      "18041 Training Loss: tensor(0.0325)\n",
      "18042 Training Loss: tensor(0.0326)\n",
      "18043 Training Loss: tensor(0.0325)\n",
      "18044 Training Loss: tensor(0.0326)\n",
      "18045 Training Loss: tensor(0.0326)\n",
      "18046 Training Loss: tensor(0.0326)\n",
      "18047 Training Loss: tensor(0.0327)\n",
      "18048 Training Loss: tensor(0.0326)\n",
      "18049 Training Loss: tensor(0.0328)\n",
      "18050 Training Loss: tensor(0.0325)\n",
      "18051 Training Loss: tensor(0.0327)\n",
      "18052 Training Loss: tensor(0.0325)\n",
      "18053 Training Loss: tensor(0.0325)\n",
      "18054 Training Loss: tensor(0.0325)\n",
      "18055 Training Loss: tensor(0.0325)\n",
      "18056 Training Loss: tensor(0.0326)\n",
      "18057 Training Loss: tensor(0.0325)\n",
      "18058 Training Loss: tensor(0.0325)\n",
      "18059 Training Loss: tensor(0.0326)\n",
      "18060 Training Loss: tensor(0.0326)\n",
      "18061 Training Loss: tensor(0.0327)\n",
      "18062 Training Loss: tensor(0.0327)\n",
      "18063 Training Loss: tensor(0.0327)\n",
      "18064 Training Loss: tensor(0.0325)\n",
      "18065 Training Loss: tensor(0.0325)\n",
      "18066 Training Loss: tensor(0.0325)\n",
      "18067 Training Loss: tensor(0.0327)\n",
      "18068 Training Loss: tensor(0.0325)\n",
      "18069 Training Loss: tensor(0.0327)\n",
      "18070 Training Loss: tensor(0.0326)\n",
      "18071 Training Loss: tensor(0.0326)\n",
      "18072 Training Loss: tensor(0.0326)\n",
      "18073 Training Loss: tensor(0.0326)\n",
      "18074 Training Loss: tensor(0.0326)\n",
      "18075 Training Loss: tensor(0.0326)\n",
      "18076 Training Loss: tensor(0.0325)\n",
      "18077 Training Loss: tensor(0.0325)\n",
      "18078 Training Loss: tensor(0.0324)\n",
      "18079 Training Loss: tensor(0.0326)\n",
      "18080 Training Loss: tensor(0.0326)\n",
      "18081 Training Loss: tensor(0.0326)\n",
      "18082 Training Loss: tensor(0.0325)\n",
      "18083 Training Loss: tensor(0.0326)\n",
      "18084 Training Loss: tensor(0.0326)\n",
      "18085 Training Loss: tensor(0.0326)\n",
      "18086 Training Loss: tensor(0.0325)\n",
      "18087 Training Loss: tensor(0.0325)\n",
      "18088 Training Loss: tensor(0.0325)\n",
      "18089 Training Loss: tensor(0.0325)\n",
      "18090 Training Loss: tensor(0.0325)\n",
      "18091 Training Loss: tensor(0.0324)\n",
      "18092 Training Loss: tensor(0.0326)\n",
      "18093 Training Loss: tensor(0.0325)\n",
      "18094 Training Loss: tensor(0.0326)\n",
      "18095 Training Loss: tensor(0.0326)\n",
      "18096 Training Loss: tensor(0.0327)\n",
      "18097 Training Loss: tensor(0.0327)\n",
      "18098 Training Loss: tensor(0.0325)\n",
      "18099 Training Loss: tensor(0.0325)\n",
      "18100 Training Loss: tensor(0.0326)\n",
      "18101 Training Loss: tensor(0.0326)\n",
      "18102 Training Loss: tensor(0.0326)\n",
      "18103 Training Loss: tensor(0.0326)\n",
      "18104 Training Loss: tensor(0.0325)\n",
      "18105 Training Loss: tensor(0.0325)\n",
      "18106 Training Loss: tensor(0.0326)\n",
      "18107 Training Loss: tensor(0.0326)\n",
      "18108 Training Loss: tensor(0.0326)\n",
      "18109 Training Loss: tensor(0.0326)\n",
      "18110 Training Loss: tensor(0.0326)\n",
      "18111 Training Loss: tensor(0.0326)\n",
      "18112 Training Loss: tensor(0.0325)\n",
      "18113 Training Loss: tensor(0.0326)\n",
      "18114 Training Loss: tensor(0.0325)\n",
      "18115 Training Loss: tensor(0.0325)\n",
      "18116 Training Loss: tensor(0.0326)\n",
      "18117 Training Loss: tensor(0.0326)\n",
      "18118 Training Loss: tensor(0.0326)\n",
      "18119 Training Loss: tensor(0.0325)\n",
      "18120 Training Loss: tensor(0.0330)\n",
      "18121 Training Loss: tensor(0.0326)\n",
      "18122 Training Loss: tensor(0.0325)\n",
      "18123 Training Loss: tensor(0.0326)\n",
      "18124 Training Loss: tensor(0.0327)\n",
      "18125 Training Loss: tensor(0.0325)\n",
      "18126 Training Loss: tensor(0.0325)\n",
      "18127 Training Loss: tensor(0.0325)\n",
      "18128 Training Loss: tensor(0.0326)\n",
      "18129 Training Loss: tensor(0.0324)\n",
      "18130 Training Loss: tensor(0.0325)\n",
      "18131 Training Loss: tensor(0.0325)\n",
      "18132 Training Loss: tensor(0.0325)\n",
      "18133 Training Loss: tensor(0.0324)\n",
      "18134 Training Loss: tensor(0.0325)\n",
      "18135 Training Loss: tensor(0.0325)\n",
      "18136 Training Loss: tensor(0.0326)\n",
      "18137 Training Loss: tensor(0.0326)\n",
      "18138 Training Loss: tensor(0.0327)\n",
      "18139 Training Loss: tensor(0.0324)\n",
      "18140 Training Loss: tensor(0.0325)\n",
      "18141 Training Loss: tensor(0.0326)\n",
      "18142 Training Loss: tensor(0.0327)\n",
      "18143 Training Loss: tensor(0.0325)\n",
      "18144 Training Loss: tensor(0.0324)\n",
      "18145 Training Loss: tensor(0.0325)\n",
      "18146 Training Loss: tensor(0.0325)\n",
      "18147 Training Loss: tensor(0.0326)\n",
      "18148 Training Loss: tensor(0.0325)\n",
      "18149 Training Loss: tensor(0.0326)\n",
      "18150 Training Loss: tensor(0.0327)\n",
      "18151 Training Loss: tensor(0.0325)\n",
      "18152 Training Loss: tensor(0.0325)\n",
      "18153 Training Loss: tensor(0.0326)\n",
      "18154 Training Loss: tensor(0.0325)\n",
      "18155 Training Loss: tensor(0.0326)\n",
      "18156 Training Loss: tensor(0.0325)\n",
      "18157 Training Loss: tensor(0.0326)\n",
      "18158 Training Loss: tensor(0.0324)\n",
      "18159 Training Loss: tensor(0.0325)\n",
      "18160 Training Loss: tensor(0.0325)\n",
      "18161 Training Loss: tensor(0.0325)\n",
      "18162 Training Loss: tensor(0.0324)\n",
      "18163 Training Loss: tensor(0.0326)\n",
      "18164 Training Loss: tensor(0.0325)\n",
      "18165 Training Loss: tensor(0.0325)\n",
      "18166 Training Loss: tensor(0.0325)\n",
      "18167 Training Loss: tensor(0.0326)\n",
      "18168 Training Loss: tensor(0.0326)\n",
      "18169 Training Loss: tensor(0.0327)\n",
      "18170 Training Loss: tensor(0.0325)\n",
      "18171 Training Loss: tensor(0.0326)\n",
      "18172 Training Loss: tensor(0.0325)\n",
      "18173 Training Loss: tensor(0.0324)\n",
      "18174 Training Loss: tensor(0.0326)\n",
      "18175 Training Loss: tensor(0.0326)\n",
      "18176 Training Loss: tensor(0.0325)\n",
      "18177 Training Loss: tensor(0.0326)\n",
      "18178 Training Loss: tensor(0.0325)\n",
      "18179 Training Loss: tensor(0.0325)\n",
      "18180 Training Loss: tensor(0.0325)\n",
      "18181 Training Loss: tensor(0.0326)\n",
      "18182 Training Loss: tensor(0.0326)\n",
      "18183 Training Loss: tensor(0.0326)\n",
      "18184 Training Loss: tensor(0.0325)\n",
      "18185 Training Loss: tensor(0.0325)\n",
      "18186 Training Loss: tensor(0.0326)\n",
      "18187 Training Loss: tensor(0.0325)\n",
      "18188 Training Loss: tensor(0.0324)\n",
      "18189 Training Loss: tensor(0.0325)\n",
      "18190 Training Loss: tensor(0.0325)\n",
      "18191 Training Loss: tensor(0.0326)\n",
      "18192 Training Loss: tensor(0.0325)\n",
      "18193 Training Loss: tensor(0.0324)\n",
      "18194 Training Loss: tensor(0.0325)\n",
      "18195 Training Loss: tensor(0.0325)\n",
      "18196 Training Loss: tensor(0.0325)\n",
      "18197 Training Loss: tensor(0.0324)\n",
      "18198 Training Loss: tensor(0.0328)\n",
      "18199 Training Loss: tensor(0.0324)\n",
      "18200 Training Loss: tensor(0.0325)\n",
      "18201 Training Loss: tensor(0.0325)\n",
      "18202 Training Loss: tensor(0.0324)\n",
      "18203 Training Loss: tensor(0.0327)\n",
      "18204 Training Loss: tensor(0.0324)\n",
      "18205 Training Loss: tensor(0.0324)\n",
      "18206 Training Loss: tensor(0.0324)\n",
      "18207 Training Loss: tensor(0.0325)\n",
      "18208 Training Loss: tensor(0.0325)\n",
      "18209 Training Loss: tensor(0.0326)\n",
      "18210 Training Loss: tensor(0.0325)\n",
      "18211 Training Loss: tensor(0.0325)\n",
      "18212 Training Loss: tensor(0.0324)\n",
      "18213 Training Loss: tensor(0.0325)\n",
      "18214 Training Loss: tensor(0.0324)\n",
      "18215 Training Loss: tensor(0.0325)\n",
      "18216 Training Loss: tensor(0.0325)\n",
      "18217 Training Loss: tensor(0.0329)\n",
      "18218 Training Loss: tensor(0.0324)\n",
      "18219 Training Loss: tensor(0.0328)\n",
      "18220 Training Loss: tensor(0.0326)\n",
      "18221 Training Loss: tensor(0.0324)\n",
      "18222 Training Loss: tensor(0.0325)\n",
      "18223 Training Loss: tensor(0.0325)\n",
      "18224 Training Loss: tensor(0.0326)\n",
      "18225 Training Loss: tensor(0.0326)\n",
      "18226 Training Loss: tensor(0.0325)\n",
      "18227 Training Loss: tensor(0.0326)\n",
      "18228 Training Loss: tensor(0.0325)\n",
      "18229 Training Loss: tensor(0.0326)\n",
      "18230 Training Loss: tensor(0.0325)\n",
      "18231 Training Loss: tensor(0.0326)\n",
      "18232 Training Loss: tensor(0.0325)\n",
      "18233 Training Loss: tensor(0.0324)\n",
      "18234 Training Loss: tensor(0.0324)\n",
      "18235 Training Loss: tensor(0.0324)\n",
      "18236 Training Loss: tensor(0.0326)\n",
      "18237 Training Loss: tensor(0.0325)\n",
      "18238 Training Loss: tensor(0.0325)\n",
      "18239 Training Loss: tensor(0.0324)\n",
      "18240 Training Loss: tensor(0.0323)\n",
      "18241 Training Loss: tensor(0.0324)\n",
      "18242 Training Loss: tensor(0.0324)\n",
      "18243 Training Loss: tensor(0.0324)\n",
      "18244 Training Loss: tensor(0.0326)\n",
      "18245 Training Loss: tensor(0.0325)\n",
      "18246 Training Loss: tensor(0.0324)\n",
      "18247 Training Loss: tensor(0.0324)\n",
      "18248 Training Loss: tensor(0.0323)\n",
      "18249 Training Loss: tensor(0.0325)\n",
      "18250 Training Loss: tensor(0.0325)\n",
      "18251 Training Loss: tensor(0.0325)\n",
      "18252 Training Loss: tensor(0.0324)\n",
      "18253 Training Loss: tensor(0.0325)\n",
      "18254 Training Loss: tensor(0.0325)\n",
      "18255 Training Loss: tensor(0.0325)\n",
      "18256 Training Loss: tensor(0.0324)\n",
      "18257 Training Loss: tensor(0.0325)\n",
      "18258 Training Loss: tensor(0.0325)\n",
      "18259 Training Loss: tensor(0.0326)\n",
      "18260 Training Loss: tensor(0.0325)\n",
      "18261 Training Loss: tensor(0.0324)\n",
      "18262 Training Loss: tensor(0.0325)\n",
      "18263 Training Loss: tensor(0.0324)\n",
      "18264 Training Loss: tensor(0.0326)\n",
      "18265 Training Loss: tensor(0.0324)\n",
      "18266 Training Loss: tensor(0.0325)\n",
      "18267 Training Loss: tensor(0.0324)\n",
      "18268 Training Loss: tensor(0.0326)\n",
      "18269 Training Loss: tensor(0.0325)\n",
      "18270 Training Loss: tensor(0.0325)\n",
      "18271 Training Loss: tensor(0.0325)\n",
      "18272 Training Loss: tensor(0.0325)\n",
      "18273 Training Loss: tensor(0.0326)\n",
      "18274 Training Loss: tensor(0.0325)\n",
      "18275 Training Loss: tensor(0.0324)\n",
      "18276 Training Loss: tensor(0.0324)\n",
      "18277 Training Loss: tensor(0.0324)\n",
      "18278 Training Loss: tensor(0.0325)\n",
      "18279 Training Loss: tensor(0.0325)\n",
      "18280 Training Loss: tensor(0.0324)\n",
      "18281 Training Loss: tensor(0.0325)\n",
      "18282 Training Loss: tensor(0.0326)\n",
      "18283 Training Loss: tensor(0.0324)\n",
      "18284 Training Loss: tensor(0.0324)\n",
      "18285 Training Loss: tensor(0.0325)\n",
      "18286 Training Loss: tensor(0.0325)\n",
      "18287 Training Loss: tensor(0.0325)\n",
      "18288 Training Loss: tensor(0.0326)\n",
      "18289 Training Loss: tensor(0.0325)\n",
      "18290 Training Loss: tensor(0.0324)\n",
      "18291 Training Loss: tensor(0.0325)\n",
      "18292 Training Loss: tensor(0.0326)\n",
      "18293 Training Loss: tensor(0.0324)\n",
      "18294 Training Loss: tensor(0.0325)\n",
      "18295 Training Loss: tensor(0.0325)\n",
      "18296 Training Loss: tensor(0.0325)\n",
      "18297 Training Loss: tensor(0.0324)\n",
      "18298 Training Loss: tensor(0.0325)\n",
      "18299 Training Loss: tensor(0.0326)\n",
      "18300 Training Loss: tensor(0.0324)\n",
      "18301 Training Loss: tensor(0.0324)\n",
      "18302 Training Loss: tensor(0.0324)\n",
      "18303 Training Loss: tensor(0.0325)\n",
      "18304 Training Loss: tensor(0.0325)\n",
      "18305 Training Loss: tensor(0.0323)\n",
      "18306 Training Loss: tensor(0.0324)\n",
      "18307 Training Loss: tensor(0.0324)\n",
      "18308 Training Loss: tensor(0.0327)\n",
      "18309 Training Loss: tensor(0.0324)\n",
      "18310 Training Loss: tensor(0.0324)\n",
      "18311 Training Loss: tensor(0.0325)\n",
      "18312 Training Loss: tensor(0.0326)\n",
      "18313 Training Loss: tensor(0.0326)\n",
      "18314 Training Loss: tensor(0.0324)\n",
      "18315 Training Loss: tensor(0.0326)\n",
      "18316 Training Loss: tensor(0.0325)\n",
      "18317 Training Loss: tensor(0.0324)\n",
      "18318 Training Loss: tensor(0.0328)\n",
      "18319 Training Loss: tensor(0.0324)\n",
      "18320 Training Loss: tensor(0.0324)\n",
      "18321 Training Loss: tensor(0.0325)\n",
      "18322 Training Loss: tensor(0.0325)\n",
      "18323 Training Loss: tensor(0.0324)\n",
      "18324 Training Loss: tensor(0.0325)\n",
      "18325 Training Loss: tensor(0.0325)\n",
      "18326 Training Loss: tensor(0.0325)\n",
      "18327 Training Loss: tensor(0.0325)\n",
      "18328 Training Loss: tensor(0.0326)\n",
      "18329 Training Loss: tensor(0.0324)\n",
      "18330 Training Loss: tensor(0.0326)\n",
      "18331 Training Loss: tensor(0.0326)\n",
      "18332 Training Loss: tensor(0.0326)\n",
      "18333 Training Loss: tensor(0.0326)\n",
      "18334 Training Loss: tensor(0.0324)\n",
      "18335 Training Loss: tensor(0.0325)\n",
      "18336 Training Loss: tensor(0.0324)\n",
      "18337 Training Loss: tensor(0.0324)\n",
      "18338 Training Loss: tensor(0.0326)\n",
      "18339 Training Loss: tensor(0.0325)\n",
      "18340 Training Loss: tensor(0.0324)\n",
      "18341 Training Loss: tensor(0.0326)\n",
      "18342 Training Loss: tensor(0.0326)\n",
      "18343 Training Loss: tensor(0.0325)\n",
      "18344 Training Loss: tensor(0.0325)\n",
      "18345 Training Loss: tensor(0.0326)\n",
      "18346 Training Loss: tensor(0.0324)\n",
      "18347 Training Loss: tensor(0.0324)\n",
      "18348 Training Loss: tensor(0.0324)\n",
      "18349 Training Loss: tensor(0.0323)\n",
      "18350 Training Loss: tensor(0.0325)\n",
      "18351 Training Loss: tensor(0.0324)\n",
      "18352 Training Loss: tensor(0.0324)\n",
      "18353 Training Loss: tensor(0.0325)\n",
      "18354 Training Loss: tensor(0.0323)\n",
      "18355 Training Loss: tensor(0.0324)\n",
      "18356 Training Loss: tensor(0.0323)\n",
      "18357 Training Loss: tensor(0.0325)\n",
      "18358 Training Loss: tensor(0.0324)\n",
      "18359 Training Loss: tensor(0.0325)\n",
      "18360 Training Loss: tensor(0.0324)\n",
      "18361 Training Loss: tensor(0.0326)\n",
      "18362 Training Loss: tensor(0.0324)\n",
      "18363 Training Loss: tensor(0.0325)\n",
      "18364 Training Loss: tensor(0.0323)\n",
      "18365 Training Loss: tensor(0.0325)\n",
      "18366 Training Loss: tensor(0.0324)\n",
      "18367 Training Loss: tensor(0.0326)\n",
      "18368 Training Loss: tensor(0.0324)\n",
      "18369 Training Loss: tensor(0.0324)\n",
      "18370 Training Loss: tensor(0.0326)\n",
      "18371 Training Loss: tensor(0.0324)\n",
      "18372 Training Loss: tensor(0.0324)\n",
      "18373 Training Loss: tensor(0.0324)\n",
      "18374 Training Loss: tensor(0.0324)\n",
      "18375 Training Loss: tensor(0.0324)\n",
      "18376 Training Loss: tensor(0.0324)\n",
      "18377 Training Loss: tensor(0.0323)\n",
      "18378 Training Loss: tensor(0.0326)\n",
      "18379 Training Loss: tensor(0.0324)\n",
      "18380 Training Loss: tensor(0.0324)\n",
      "18381 Training Loss: tensor(0.0325)\n",
      "18382 Training Loss: tensor(0.0325)\n",
      "18383 Training Loss: tensor(0.0324)\n",
      "18384 Training Loss: tensor(0.0324)\n",
      "18385 Training Loss: tensor(0.0324)\n",
      "18386 Training Loss: tensor(0.0325)\n",
      "18387 Training Loss: tensor(0.0324)\n",
      "18388 Training Loss: tensor(0.0324)\n",
      "18389 Training Loss: tensor(0.0323)\n",
      "18390 Training Loss: tensor(0.0325)\n",
      "18391 Training Loss: tensor(0.0324)\n",
      "18392 Training Loss: tensor(0.0323)\n",
      "18393 Training Loss: tensor(0.0325)\n",
      "18394 Training Loss: tensor(0.0324)\n",
      "18395 Training Loss: tensor(0.0324)\n",
      "18396 Training Loss: tensor(0.0322)\n",
      "18397 Training Loss: tensor(0.0324)\n",
      "18398 Training Loss: tensor(0.0323)\n",
      "18399 Training Loss: tensor(0.0323)\n",
      "18400 Training Loss: tensor(0.0324)\n",
      "18401 Training Loss: tensor(0.0324)\n",
      "18402 Training Loss: tensor(0.0322)\n",
      "18403 Training Loss: tensor(0.0323)\n",
      "18404 Training Loss: tensor(0.0324)\n",
      "18405 Training Loss: tensor(0.0324)\n",
      "18406 Training Loss: tensor(0.0324)\n",
      "18407 Training Loss: tensor(0.0324)\n",
      "18408 Training Loss: tensor(0.0323)\n",
      "18409 Training Loss: tensor(0.0323)\n",
      "18410 Training Loss: tensor(0.0322)\n",
      "18411 Training Loss: tensor(0.0326)\n",
      "18412 Training Loss: tensor(0.0327)\n",
      "18413 Training Loss: tensor(0.0324)\n",
      "18414 Training Loss: tensor(0.0325)\n",
      "18415 Training Loss: tensor(0.0326)\n",
      "18416 Training Loss: tensor(0.0325)\n",
      "18417 Training Loss: tensor(0.0324)\n",
      "18418 Training Loss: tensor(0.0325)\n",
      "18419 Training Loss: tensor(0.0324)\n",
      "18420 Training Loss: tensor(0.0324)\n",
      "18421 Training Loss: tensor(0.0325)\n",
      "18422 Training Loss: tensor(0.0325)\n",
      "18423 Training Loss: tensor(0.0326)\n",
      "18424 Training Loss: tensor(0.0325)\n",
      "18425 Training Loss: tensor(0.0324)\n",
      "18426 Training Loss: tensor(0.0324)\n",
      "18427 Training Loss: tensor(0.0325)\n",
      "18428 Training Loss: tensor(0.0326)\n",
      "18429 Training Loss: tensor(0.0325)\n",
      "18430 Training Loss: tensor(0.0324)\n",
      "18431 Training Loss: tensor(0.0323)\n",
      "18432 Training Loss: tensor(0.0324)\n",
      "18433 Training Loss: tensor(0.0324)\n",
      "18434 Training Loss: tensor(0.0325)\n",
      "18435 Training Loss: tensor(0.0324)\n",
      "18436 Training Loss: tensor(0.0324)\n",
      "18437 Training Loss: tensor(0.0325)\n",
      "18438 Training Loss: tensor(0.0325)\n",
      "18439 Training Loss: tensor(0.0323)\n",
      "18440 Training Loss: tensor(0.0324)\n",
      "18441 Training Loss: tensor(0.0325)\n",
      "18442 Training Loss: tensor(0.0326)\n",
      "18443 Training Loss: tensor(0.0324)\n",
      "18444 Training Loss: tensor(0.0324)\n",
      "18445 Training Loss: tensor(0.0324)\n",
      "18446 Training Loss: tensor(0.0325)\n",
      "18447 Training Loss: tensor(0.0324)\n",
      "18448 Training Loss: tensor(0.0324)\n",
      "18449 Training Loss: tensor(0.0326)\n",
      "18450 Training Loss: tensor(0.0324)\n",
      "18451 Training Loss: tensor(0.0324)\n",
      "18452 Training Loss: tensor(0.0324)\n",
      "18453 Training Loss: tensor(0.0324)\n",
      "18454 Training Loss: tensor(0.0326)\n",
      "18455 Training Loss: tensor(0.0324)\n",
      "18456 Training Loss: tensor(0.0324)\n",
      "18457 Training Loss: tensor(0.0324)\n",
      "18458 Training Loss: tensor(0.0324)\n",
      "18459 Training Loss: tensor(0.0324)\n",
      "18460 Training Loss: tensor(0.0323)\n",
      "18461 Training Loss: tensor(0.0325)\n",
      "18462 Training Loss: tensor(0.0323)\n",
      "18463 Training Loss: tensor(0.0325)\n",
      "18464 Training Loss: tensor(0.0324)\n",
      "18465 Training Loss: tensor(0.0323)\n",
      "18466 Training Loss: tensor(0.0324)\n",
      "18467 Training Loss: tensor(0.0324)\n",
      "18468 Training Loss: tensor(0.0324)\n",
      "18469 Training Loss: tensor(0.0324)\n",
      "18470 Training Loss: tensor(0.0325)\n",
      "18471 Training Loss: tensor(0.0324)\n",
      "18472 Training Loss: tensor(0.0324)\n",
      "18473 Training Loss: tensor(0.0324)\n",
      "18474 Training Loss: tensor(0.0324)\n",
      "18475 Training Loss: tensor(0.0323)\n",
      "18476 Training Loss: tensor(0.0323)\n",
      "18477 Training Loss: tensor(0.0325)\n",
      "18478 Training Loss: tensor(0.0323)\n",
      "18479 Training Loss: tensor(0.0323)\n",
      "18480 Training Loss: tensor(0.0324)\n",
      "18481 Training Loss: tensor(0.0323)\n",
      "18482 Training Loss: tensor(0.0324)\n",
      "18483 Training Loss: tensor(0.0324)\n",
      "18484 Training Loss: tensor(0.0324)\n",
      "18485 Training Loss: tensor(0.0323)\n",
      "18486 Training Loss: tensor(0.0324)\n",
      "18487 Training Loss: tensor(0.0324)\n",
      "18488 Training Loss: tensor(0.0325)\n",
      "18489 Training Loss: tensor(0.0324)\n",
      "18490 Training Loss: tensor(0.0323)\n",
      "18491 Training Loss: tensor(0.0326)\n",
      "18492 Training Loss: tensor(0.0326)\n",
      "18493 Training Loss: tensor(0.0326)\n",
      "18494 Training Loss: tensor(0.0324)\n",
      "18495 Training Loss: tensor(0.0324)\n",
      "18496 Training Loss: tensor(0.0324)\n",
      "18497 Training Loss: tensor(0.0325)\n",
      "18498 Training Loss: tensor(0.0324)\n",
      "18499 Training Loss: tensor(0.0326)\n",
      "18500 Training Loss: tensor(0.0323)\n",
      "18501 Training Loss: tensor(0.0324)\n",
      "18502 Training Loss: tensor(0.0324)\n",
      "18503 Training Loss: tensor(0.0324)\n",
      "18504 Training Loss: tensor(0.0326)\n",
      "18505 Training Loss: tensor(0.0324)\n",
      "18506 Training Loss: tensor(0.0324)\n",
      "18507 Training Loss: tensor(0.0323)\n",
      "18508 Training Loss: tensor(0.0324)\n",
      "18509 Training Loss: tensor(0.0325)\n",
      "18510 Training Loss: tensor(0.0324)\n",
      "18511 Training Loss: tensor(0.0324)\n",
      "18512 Training Loss: tensor(0.0324)\n",
      "18513 Training Loss: tensor(0.0326)\n",
      "18514 Training Loss: tensor(0.0325)\n",
      "18515 Training Loss: tensor(0.0323)\n",
      "18516 Training Loss: tensor(0.0323)\n",
      "18517 Training Loss: tensor(0.0325)\n",
      "18518 Training Loss: tensor(0.0324)\n",
      "18519 Training Loss: tensor(0.0323)\n",
      "18520 Training Loss: tensor(0.0324)\n",
      "18521 Training Loss: tensor(0.0323)\n",
      "18522 Training Loss: tensor(0.0323)\n",
      "18523 Training Loss: tensor(0.0324)\n",
      "18524 Training Loss: tensor(0.0323)\n",
      "18525 Training Loss: tensor(0.0323)\n",
      "18526 Training Loss: tensor(0.0324)\n",
      "18527 Training Loss: tensor(0.0324)\n",
      "18528 Training Loss: tensor(0.0325)\n",
      "18529 Training Loss: tensor(0.0324)\n",
      "18530 Training Loss: tensor(0.0325)\n",
      "18531 Training Loss: tensor(0.0324)\n",
      "18532 Training Loss: tensor(0.0323)\n",
      "18533 Training Loss: tensor(0.0324)\n",
      "18534 Training Loss: tensor(0.0324)\n",
      "18535 Training Loss: tensor(0.0324)\n",
      "18536 Training Loss: tensor(0.0324)\n",
      "18537 Training Loss: tensor(0.0323)\n",
      "18538 Training Loss: tensor(0.0324)\n",
      "18539 Training Loss: tensor(0.0324)\n",
      "18540 Training Loss: tensor(0.0323)\n",
      "18541 Training Loss: tensor(0.0324)\n",
      "18542 Training Loss: tensor(0.0324)\n",
      "18543 Training Loss: tensor(0.0323)\n",
      "18544 Training Loss: tensor(0.0325)\n",
      "18545 Training Loss: tensor(0.0324)\n",
      "18546 Training Loss: tensor(0.0323)\n",
      "18547 Training Loss: tensor(0.0324)\n",
      "18548 Training Loss: tensor(0.0323)\n",
      "18549 Training Loss: tensor(0.0324)\n",
      "18550 Training Loss: tensor(0.0324)\n",
      "18551 Training Loss: tensor(0.0323)\n",
      "18552 Training Loss: tensor(0.0324)\n",
      "18553 Training Loss: tensor(0.0323)\n",
      "18554 Training Loss: tensor(0.0325)\n",
      "18555 Training Loss: tensor(0.0323)\n",
      "18556 Training Loss: tensor(0.0324)\n",
      "18557 Training Loss: tensor(0.0322)\n",
      "18558 Training Loss: tensor(0.0324)\n",
      "18559 Training Loss: tensor(0.0323)\n",
      "18560 Training Loss: tensor(0.0324)\n",
      "18561 Training Loss: tensor(0.0324)\n",
      "18562 Training Loss: tensor(0.0323)\n",
      "18563 Training Loss: tensor(0.0324)\n",
      "18564 Training Loss: tensor(0.0323)\n",
      "18565 Training Loss: tensor(0.0322)\n",
      "18566 Training Loss: tensor(0.0324)\n",
      "18567 Training Loss: tensor(0.0324)\n",
      "18568 Training Loss: tensor(0.0323)\n",
      "18569 Training Loss: tensor(0.0323)\n",
      "18570 Training Loss: tensor(0.0323)\n",
      "18571 Training Loss: tensor(0.0324)\n",
      "18572 Training Loss: tensor(0.0325)\n",
      "18573 Training Loss: tensor(0.0323)\n",
      "18574 Training Loss: tensor(0.0324)\n",
      "18575 Training Loss: tensor(0.0324)\n",
      "18576 Training Loss: tensor(0.0323)\n",
      "18577 Training Loss: tensor(0.0325)\n",
      "18578 Training Loss: tensor(0.0324)\n",
      "18579 Training Loss: tensor(0.0324)\n",
      "18580 Training Loss: tensor(0.0323)\n",
      "18581 Training Loss: tensor(0.0323)\n",
      "18582 Training Loss: tensor(0.0323)\n",
      "18583 Training Loss: tensor(0.0324)\n",
      "18584 Training Loss: tensor(0.0322)\n",
      "18585 Training Loss: tensor(0.0323)\n",
      "18586 Training Loss: tensor(0.0323)\n",
      "18587 Training Loss: tensor(0.0324)\n",
      "18588 Training Loss: tensor(0.0322)\n",
      "18589 Training Loss: tensor(0.0324)\n",
      "18590 Training Loss: tensor(0.0324)\n",
      "18591 Training Loss: tensor(0.0323)\n",
      "18592 Training Loss: tensor(0.0323)\n",
      "18593 Training Loss: tensor(0.0323)\n",
      "18594 Training Loss: tensor(0.0323)\n",
      "18595 Training Loss: tensor(0.0323)\n",
      "18596 Training Loss: tensor(0.0324)\n",
      "18597 Training Loss: tensor(0.0325)\n",
      "18598 Training Loss: tensor(0.0323)\n",
      "18599 Training Loss: tensor(0.0323)\n",
      "18600 Training Loss: tensor(0.0325)\n",
      "18601 Training Loss: tensor(0.0324)\n",
      "18602 Training Loss: tensor(0.0323)\n",
      "18603 Training Loss: tensor(0.0324)\n",
      "18604 Training Loss: tensor(0.0324)\n",
      "18605 Training Loss: tensor(0.0323)\n",
      "18606 Training Loss: tensor(0.0323)\n",
      "18607 Training Loss: tensor(0.0324)\n",
      "18608 Training Loss: tensor(0.0323)\n",
      "18609 Training Loss: tensor(0.0323)\n",
      "18610 Training Loss: tensor(0.0324)\n",
      "18611 Training Loss: tensor(0.0325)\n",
      "18612 Training Loss: tensor(0.0323)\n",
      "18613 Training Loss: tensor(0.0323)\n",
      "18614 Training Loss: tensor(0.0323)\n",
      "18615 Training Loss: tensor(0.0323)\n",
      "18616 Training Loss: tensor(0.0324)\n",
      "18617 Training Loss: tensor(0.0324)\n",
      "18618 Training Loss: tensor(0.0326)\n",
      "18619 Training Loss: tensor(0.0325)\n",
      "18620 Training Loss: tensor(0.0324)\n",
      "18621 Training Loss: tensor(0.0325)\n",
      "18622 Training Loss: tensor(0.0323)\n",
      "18623 Training Loss: tensor(0.0324)\n",
      "18624 Training Loss: tensor(0.0323)\n",
      "18625 Training Loss: tensor(0.0323)\n",
      "18626 Training Loss: tensor(0.0325)\n",
      "18627 Training Loss: tensor(0.0324)\n",
      "18628 Training Loss: tensor(0.0324)\n",
      "18629 Training Loss: tensor(0.0322)\n",
      "18630 Training Loss: tensor(0.0322)\n",
      "18631 Training Loss: tensor(0.0324)\n",
      "18632 Training Loss: tensor(0.0324)\n",
      "18633 Training Loss: tensor(0.0323)\n",
      "18634 Training Loss: tensor(0.0323)\n",
      "18635 Training Loss: tensor(0.0324)\n",
      "18636 Training Loss: tensor(0.0324)\n",
      "18637 Training Loss: tensor(0.0323)\n",
      "18638 Training Loss: tensor(0.0323)\n",
      "18639 Training Loss: tensor(0.0323)\n",
      "18640 Training Loss: tensor(0.0324)\n",
      "18641 Training Loss: tensor(0.0322)\n",
      "18642 Training Loss: tensor(0.0323)\n",
      "18643 Training Loss: tensor(0.0323)\n",
      "18644 Training Loss: tensor(0.0323)\n",
      "18645 Training Loss: tensor(0.0323)\n",
      "18646 Training Loss: tensor(0.0324)\n",
      "18647 Training Loss: tensor(0.0323)\n",
      "18648 Training Loss: tensor(0.0324)\n",
      "18649 Training Loss: tensor(0.0322)\n",
      "18650 Training Loss: tensor(0.0323)\n",
      "18651 Training Loss: tensor(0.0322)\n",
      "18652 Training Loss: tensor(0.0324)\n",
      "18653 Training Loss: tensor(0.0323)\n",
      "18654 Training Loss: tensor(0.0323)\n",
      "18655 Training Loss: tensor(0.0323)\n",
      "18656 Training Loss: tensor(0.0324)\n",
      "18657 Training Loss: tensor(0.0324)\n",
      "18658 Training Loss: tensor(0.0324)\n",
      "18659 Training Loss: tensor(0.0324)\n",
      "18660 Training Loss: tensor(0.0323)\n",
      "18661 Training Loss: tensor(0.0323)\n",
      "18662 Training Loss: tensor(0.0323)\n",
      "18663 Training Loss: tensor(0.0323)\n",
      "18664 Training Loss: tensor(0.0323)\n",
      "18665 Training Loss: tensor(0.0322)\n",
      "18666 Training Loss: tensor(0.0323)\n",
      "18667 Training Loss: tensor(0.0324)\n",
      "18668 Training Loss: tensor(0.0323)\n",
      "18669 Training Loss: tensor(0.0323)\n",
      "18670 Training Loss: tensor(0.0322)\n",
      "18671 Training Loss: tensor(0.0322)\n",
      "18672 Training Loss: tensor(0.0323)\n",
      "18673 Training Loss: tensor(0.0326)\n",
      "18674 Training Loss: tensor(0.0322)\n",
      "18675 Training Loss: tensor(0.0324)\n",
      "18676 Training Loss: tensor(0.0324)\n",
      "18677 Training Loss: tensor(0.0324)\n",
      "18678 Training Loss: tensor(0.0322)\n",
      "18679 Training Loss: tensor(0.0323)\n",
      "18680 Training Loss: tensor(0.0323)\n",
      "18681 Training Loss: tensor(0.0323)\n",
      "18682 Training Loss: tensor(0.0323)\n",
      "18683 Training Loss: tensor(0.0325)\n",
      "18684 Training Loss: tensor(0.0325)\n",
      "18685 Training Loss: tensor(0.0325)\n",
      "18686 Training Loss: tensor(0.0322)\n",
      "18687 Training Loss: tensor(0.0323)\n",
      "18688 Training Loss: tensor(0.0323)\n",
      "18689 Training Loss: tensor(0.0323)\n",
      "18690 Training Loss: tensor(0.0322)\n",
      "18691 Training Loss: tensor(0.0323)\n",
      "18692 Training Loss: tensor(0.0324)\n",
      "18693 Training Loss: tensor(0.0324)\n",
      "18694 Training Loss: tensor(0.0322)\n",
      "18695 Training Loss: tensor(0.0324)\n",
      "18696 Training Loss: tensor(0.0323)\n",
      "18697 Training Loss: tensor(0.0323)\n",
      "18698 Training Loss: tensor(0.0323)\n",
      "18699 Training Loss: tensor(0.0322)\n",
      "18700 Training Loss: tensor(0.0322)\n",
      "18701 Training Loss: tensor(0.0322)\n",
      "18702 Training Loss: tensor(0.0325)\n",
      "18703 Training Loss: tensor(0.0324)\n",
      "18704 Training Loss: tensor(0.0324)\n",
      "18705 Training Loss: tensor(0.0324)\n",
      "18706 Training Loss: tensor(0.0322)\n",
      "18707 Training Loss: tensor(0.0322)\n",
      "18708 Training Loss: tensor(0.0323)\n",
      "18709 Training Loss: tensor(0.0323)\n",
      "18710 Training Loss: tensor(0.0324)\n",
      "18711 Training Loss: tensor(0.0323)\n",
      "18712 Training Loss: tensor(0.0322)\n",
      "18713 Training Loss: tensor(0.0323)\n",
      "18714 Training Loss: tensor(0.0324)\n",
      "18715 Training Loss: tensor(0.0322)\n",
      "18716 Training Loss: tensor(0.0323)\n",
      "18717 Training Loss: tensor(0.0324)\n",
      "18718 Training Loss: tensor(0.0324)\n",
      "18719 Training Loss: tensor(0.0323)\n",
      "18720 Training Loss: tensor(0.0323)\n",
      "18721 Training Loss: tensor(0.0322)\n",
      "18722 Training Loss: tensor(0.0324)\n",
      "18723 Training Loss: tensor(0.0321)\n",
      "18724 Training Loss: tensor(0.0322)\n",
      "18725 Training Loss: tensor(0.0323)\n",
      "18726 Training Loss: tensor(0.0323)\n",
      "18727 Training Loss: tensor(0.0322)\n",
      "18728 Training Loss: tensor(0.0322)\n",
      "18729 Training Loss: tensor(0.0322)\n",
      "18730 Training Loss: tensor(0.0323)\n",
      "18731 Training Loss: tensor(0.0324)\n",
      "18732 Training Loss: tensor(0.0321)\n",
      "18733 Training Loss: tensor(0.0321)\n",
      "18734 Training Loss: tensor(0.0322)\n",
      "18735 Training Loss: tensor(0.0324)\n",
      "18736 Training Loss: tensor(0.0324)\n",
      "18737 Training Loss: tensor(0.0323)\n",
      "18738 Training Loss: tensor(0.0323)\n",
      "18739 Training Loss: tensor(0.0323)\n",
      "18740 Training Loss: tensor(0.0321)\n",
      "18741 Training Loss: tensor(0.0323)\n",
      "18742 Training Loss: tensor(0.0323)\n",
      "18743 Training Loss: tensor(0.0322)\n",
      "18744 Training Loss: tensor(0.0324)\n",
      "18745 Training Loss: tensor(0.0322)\n",
      "18746 Training Loss: tensor(0.0325)\n",
      "18747 Training Loss: tensor(0.0323)\n",
      "18748 Training Loss: tensor(0.0323)\n",
      "18749 Training Loss: tensor(0.0322)\n",
      "18750 Training Loss: tensor(0.0323)\n",
      "18751 Training Loss: tensor(0.0325)\n",
      "18752 Training Loss: tensor(0.0322)\n",
      "18753 Training Loss: tensor(0.0323)\n",
      "18754 Training Loss: tensor(0.0322)\n",
      "18755 Training Loss: tensor(0.0324)\n",
      "18756 Training Loss: tensor(0.0322)\n",
      "18757 Training Loss: tensor(0.0322)\n",
      "18758 Training Loss: tensor(0.0323)\n",
      "18759 Training Loss: tensor(0.0322)\n",
      "18760 Training Loss: tensor(0.0322)\n",
      "18761 Training Loss: tensor(0.0323)\n",
      "18762 Training Loss: tensor(0.0324)\n",
      "18763 Training Loss: tensor(0.0324)\n",
      "18764 Training Loss: tensor(0.0324)\n",
      "18765 Training Loss: tensor(0.0323)\n",
      "18766 Training Loss: tensor(0.0322)\n",
      "18767 Training Loss: tensor(0.0322)\n",
      "18768 Training Loss: tensor(0.0323)\n",
      "18769 Training Loss: tensor(0.0323)\n",
      "18770 Training Loss: tensor(0.0325)\n",
      "18771 Training Loss: tensor(0.0323)\n",
      "18772 Training Loss: tensor(0.0322)\n",
      "18773 Training Loss: tensor(0.0323)\n",
      "18774 Training Loss: tensor(0.0322)\n",
      "18775 Training Loss: tensor(0.0323)\n",
      "18776 Training Loss: tensor(0.0323)\n",
      "18777 Training Loss: tensor(0.0323)\n",
      "18778 Training Loss: tensor(0.0323)\n",
      "18779 Training Loss: tensor(0.0322)\n",
      "18780 Training Loss: tensor(0.0322)\n",
      "18781 Training Loss: tensor(0.0323)\n",
      "18782 Training Loss: tensor(0.0323)\n",
      "18783 Training Loss: tensor(0.0323)\n",
      "18784 Training Loss: tensor(0.0322)\n",
      "18785 Training Loss: tensor(0.0324)\n",
      "18786 Training Loss: tensor(0.0322)\n",
      "18787 Training Loss: tensor(0.0325)\n",
      "18788 Training Loss: tensor(0.0324)\n",
      "18789 Training Loss: tensor(0.0322)\n",
      "18790 Training Loss: tensor(0.0323)\n",
      "18791 Training Loss: tensor(0.0324)\n",
      "18792 Training Loss: tensor(0.0323)\n",
      "18793 Training Loss: tensor(0.0323)\n",
      "18794 Training Loss: tensor(0.0322)\n",
      "18795 Training Loss: tensor(0.0321)\n",
      "18796 Training Loss: tensor(0.0322)\n",
      "18797 Training Loss: tensor(0.0323)\n",
      "18798 Training Loss: tensor(0.0323)\n",
      "18799 Training Loss: tensor(0.0323)\n",
      "18800 Training Loss: tensor(0.0323)\n",
      "18801 Training Loss: tensor(0.0324)\n",
      "18802 Training Loss: tensor(0.0322)\n",
      "18803 Training Loss: tensor(0.0323)\n",
      "18804 Training Loss: tensor(0.0324)\n",
      "18805 Training Loss: tensor(0.0322)\n",
      "18806 Training Loss: tensor(0.0322)\n",
      "18807 Training Loss: tensor(0.0323)\n",
      "18808 Training Loss: tensor(0.0322)\n",
      "18809 Training Loss: tensor(0.0323)\n",
      "18810 Training Loss: tensor(0.0323)\n",
      "18811 Training Loss: tensor(0.0322)\n",
      "18812 Training Loss: tensor(0.0323)\n",
      "18813 Training Loss: tensor(0.0321)\n",
      "18814 Training Loss: tensor(0.0324)\n",
      "18815 Training Loss: tensor(0.0322)\n",
      "18816 Training Loss: tensor(0.0322)\n",
      "18817 Training Loss: tensor(0.0322)\n",
      "18818 Training Loss: tensor(0.0322)\n",
      "18819 Training Loss: tensor(0.0323)\n",
      "18820 Training Loss: tensor(0.0323)\n",
      "18821 Training Loss: tensor(0.0322)\n",
      "18822 Training Loss: tensor(0.0323)\n",
      "18823 Training Loss: tensor(0.0323)\n",
      "18824 Training Loss: tensor(0.0322)\n",
      "18825 Training Loss: tensor(0.0323)\n",
      "18826 Training Loss: tensor(0.0324)\n",
      "18827 Training Loss: tensor(0.0323)\n",
      "18828 Training Loss: tensor(0.0322)\n",
      "18829 Training Loss: tensor(0.0323)\n",
      "18830 Training Loss: tensor(0.0323)\n",
      "18831 Training Loss: tensor(0.0325)\n",
      "18832 Training Loss: tensor(0.0323)\n",
      "18833 Training Loss: tensor(0.0323)\n",
      "18834 Training Loss: tensor(0.0323)\n",
      "18835 Training Loss: tensor(0.0323)\n",
      "18836 Training Loss: tensor(0.0322)\n",
      "18837 Training Loss: tensor(0.0324)\n",
      "18838 Training Loss: tensor(0.0322)\n",
      "18839 Training Loss: tensor(0.0322)\n",
      "18840 Training Loss: tensor(0.0323)\n",
      "18841 Training Loss: tensor(0.0322)\n",
      "18842 Training Loss: tensor(0.0323)\n",
      "18843 Training Loss: tensor(0.0322)\n",
      "18844 Training Loss: tensor(0.0322)\n",
      "18845 Training Loss: tensor(0.0322)\n",
      "18846 Training Loss: tensor(0.0322)\n",
      "18847 Training Loss: tensor(0.0322)\n",
      "18848 Training Loss: tensor(0.0322)\n",
      "18849 Training Loss: tensor(0.0322)\n",
      "18850 Training Loss: tensor(0.0323)\n",
      "18851 Training Loss: tensor(0.0321)\n",
      "18852 Training Loss: tensor(0.0322)\n",
      "18853 Training Loss: tensor(0.0323)\n",
      "18854 Training Loss: tensor(0.0323)\n",
      "18855 Training Loss: tensor(0.0324)\n",
      "18856 Training Loss: tensor(0.0324)\n",
      "18857 Training Loss: tensor(0.0323)\n",
      "18858 Training Loss: tensor(0.0322)\n",
      "18859 Training Loss: tensor(0.0322)\n",
      "18860 Training Loss: tensor(0.0323)\n",
      "18861 Training Loss: tensor(0.0322)\n",
      "18862 Training Loss: tensor(0.0321)\n",
      "18863 Training Loss: tensor(0.0322)\n",
      "18864 Training Loss: tensor(0.0323)\n",
      "18865 Training Loss: tensor(0.0322)\n",
      "18866 Training Loss: tensor(0.0323)\n",
      "18867 Training Loss: tensor(0.0321)\n",
      "18868 Training Loss: tensor(0.0322)\n",
      "18869 Training Loss: tensor(0.0323)\n",
      "18870 Training Loss: tensor(0.0323)\n",
      "18871 Training Loss: tensor(0.0323)\n",
      "18872 Training Loss: tensor(0.0322)\n",
      "18873 Training Loss: tensor(0.0321)\n",
      "18874 Training Loss: tensor(0.0323)\n",
      "18875 Training Loss: tensor(0.0322)\n",
      "18876 Training Loss: tensor(0.0321)\n",
      "18877 Training Loss: tensor(0.0323)\n",
      "18878 Training Loss: tensor(0.0322)\n",
      "18879 Training Loss: tensor(0.0322)\n",
      "18880 Training Loss: tensor(0.0323)\n",
      "18881 Training Loss: tensor(0.0323)\n",
      "18882 Training Loss: tensor(0.0323)\n",
      "18883 Training Loss: tensor(0.0324)\n",
      "18884 Training Loss: tensor(0.0323)\n",
      "18885 Training Loss: tensor(0.0322)\n",
      "18886 Training Loss: tensor(0.0321)\n",
      "18887 Training Loss: tensor(0.0322)\n",
      "18888 Training Loss: tensor(0.0323)\n",
      "18889 Training Loss: tensor(0.0322)\n",
      "18890 Training Loss: tensor(0.0323)\n",
      "18891 Training Loss: tensor(0.0321)\n",
      "18892 Training Loss: tensor(0.0322)\n",
      "18893 Training Loss: tensor(0.0323)\n",
      "18894 Training Loss: tensor(0.0323)\n",
      "18895 Training Loss: tensor(0.0323)\n",
      "18896 Training Loss: tensor(0.0322)\n",
      "18897 Training Loss: tensor(0.0323)\n",
      "18898 Training Loss: tensor(0.0323)\n",
      "18899 Training Loss: tensor(0.0322)\n",
      "18900 Training Loss: tensor(0.0323)\n",
      "18901 Training Loss: tensor(0.0322)\n",
      "18902 Training Loss: tensor(0.0322)\n",
      "18903 Training Loss: tensor(0.0322)\n",
      "18904 Training Loss: tensor(0.0323)\n",
      "18905 Training Loss: tensor(0.0322)\n",
      "18906 Training Loss: tensor(0.0322)\n",
      "18907 Training Loss: tensor(0.0322)\n",
      "18908 Training Loss: tensor(0.0323)\n",
      "18909 Training Loss: tensor(0.0323)\n",
      "18910 Training Loss: tensor(0.0322)\n",
      "18911 Training Loss: tensor(0.0324)\n",
      "18912 Training Loss: tensor(0.0322)\n",
      "18913 Training Loss: tensor(0.0324)\n",
      "18914 Training Loss: tensor(0.0321)\n",
      "18915 Training Loss: tensor(0.0323)\n",
      "18916 Training Loss: tensor(0.0321)\n",
      "18917 Training Loss: tensor(0.0322)\n",
      "18918 Training Loss: tensor(0.0322)\n",
      "18919 Training Loss: tensor(0.0323)\n",
      "18920 Training Loss: tensor(0.0323)\n",
      "18921 Training Loss: tensor(0.0321)\n",
      "18922 Training Loss: tensor(0.0322)\n",
      "18923 Training Loss: tensor(0.0323)\n",
      "18924 Training Loss: tensor(0.0324)\n",
      "18925 Training Loss: tensor(0.0322)\n",
      "18926 Training Loss: tensor(0.0323)\n",
      "18927 Training Loss: tensor(0.0322)\n",
      "18928 Training Loss: tensor(0.0322)\n",
      "18929 Training Loss: tensor(0.0322)\n",
      "18930 Training Loss: tensor(0.0324)\n",
      "18931 Training Loss: tensor(0.0323)\n",
      "18932 Training Loss: tensor(0.0323)\n",
      "18933 Training Loss: tensor(0.0323)\n",
      "18934 Training Loss: tensor(0.0322)\n",
      "18935 Training Loss: tensor(0.0323)\n",
      "18936 Training Loss: tensor(0.0321)\n",
      "18937 Training Loss: tensor(0.0323)\n",
      "18938 Training Loss: tensor(0.0322)\n",
      "18939 Training Loss: tensor(0.0322)\n",
      "18940 Training Loss: tensor(0.0322)\n",
      "18941 Training Loss: tensor(0.0322)\n",
      "18942 Training Loss: tensor(0.0323)\n",
      "18943 Training Loss: tensor(0.0323)\n",
      "18944 Training Loss: tensor(0.0322)\n",
      "18945 Training Loss: tensor(0.0324)\n",
      "18946 Training Loss: tensor(0.0323)\n",
      "18947 Training Loss: tensor(0.0322)\n",
      "18948 Training Loss: tensor(0.0324)\n",
      "18949 Training Loss: tensor(0.0323)\n",
      "18950 Training Loss: tensor(0.0323)\n",
      "18951 Training Loss: tensor(0.0322)\n",
      "18952 Training Loss: tensor(0.0321)\n",
      "18953 Training Loss: tensor(0.0321)\n",
      "18954 Training Loss: tensor(0.0322)\n",
      "18955 Training Loss: tensor(0.0322)\n",
      "18956 Training Loss: tensor(0.0324)\n",
      "18957 Training Loss: tensor(0.0322)\n",
      "18958 Training Loss: tensor(0.0322)\n",
      "18959 Training Loss: tensor(0.0324)\n",
      "18960 Training Loss: tensor(0.0322)\n",
      "18961 Training Loss: tensor(0.0323)\n",
      "18962 Training Loss: tensor(0.0322)\n",
      "18963 Training Loss: tensor(0.0323)\n",
      "18964 Training Loss: tensor(0.0323)\n",
      "18965 Training Loss: tensor(0.0322)\n",
      "18966 Training Loss: tensor(0.0321)\n",
      "18967 Training Loss: tensor(0.0322)\n",
      "18968 Training Loss: tensor(0.0322)\n",
      "18969 Training Loss: tensor(0.0322)\n",
      "18970 Training Loss: tensor(0.0323)\n",
      "18971 Training Loss: tensor(0.0322)\n",
      "18972 Training Loss: tensor(0.0321)\n",
      "18973 Training Loss: tensor(0.0322)\n",
      "18974 Training Loss: tensor(0.0322)\n",
      "18975 Training Loss: tensor(0.0322)\n",
      "18976 Training Loss: tensor(0.0322)\n",
      "18977 Training Loss: tensor(0.0321)\n",
      "18978 Training Loss: tensor(0.0321)\n",
      "18979 Training Loss: tensor(0.0323)\n",
      "18980 Training Loss: tensor(0.0324)\n",
      "18981 Training Loss: tensor(0.0321)\n",
      "18982 Training Loss: tensor(0.0322)\n",
      "18983 Training Loss: tensor(0.0321)\n",
      "18984 Training Loss: tensor(0.0322)\n",
      "18985 Training Loss: tensor(0.0321)\n",
      "18986 Training Loss: tensor(0.0322)\n",
      "18987 Training Loss: tensor(0.0323)\n",
      "18988 Training Loss: tensor(0.0322)\n",
      "18989 Training Loss: tensor(0.0321)\n",
      "18990 Training Loss: tensor(0.0325)\n",
      "18991 Training Loss: tensor(0.0322)\n",
      "18992 Training Loss: tensor(0.0321)\n",
      "18993 Training Loss: tensor(0.0324)\n",
      "18994 Training Loss: tensor(0.0322)\n",
      "18995 Training Loss: tensor(0.0323)\n",
      "18996 Training Loss: tensor(0.0322)\n",
      "18997 Training Loss: tensor(0.0321)\n",
      "18998 Training Loss: tensor(0.0322)\n",
      "18999 Training Loss: tensor(0.0322)\n",
      "19000 Training Loss: tensor(0.0322)\n",
      "19001 Training Loss: tensor(0.0323)\n",
      "19002 Training Loss: tensor(0.0321)\n",
      "19003 Training Loss: tensor(0.0323)\n",
      "19004 Training Loss: tensor(0.0321)\n",
      "19005 Training Loss: tensor(0.0321)\n",
      "19006 Training Loss: tensor(0.0323)\n",
      "19007 Training Loss: tensor(0.0322)\n",
      "19008 Training Loss: tensor(0.0324)\n",
      "19009 Training Loss: tensor(0.0323)\n",
      "19010 Training Loss: tensor(0.0321)\n",
      "19011 Training Loss: tensor(0.0321)\n",
      "19012 Training Loss: tensor(0.0320)\n",
      "19013 Training Loss: tensor(0.0322)\n",
      "19014 Training Loss: tensor(0.0321)\n",
      "19015 Training Loss: tensor(0.0323)\n",
      "19016 Training Loss: tensor(0.0322)\n",
      "19017 Training Loss: tensor(0.0322)\n",
      "19018 Training Loss: tensor(0.0323)\n",
      "19019 Training Loss: tensor(0.0321)\n",
      "19020 Training Loss: tensor(0.0321)\n",
      "19021 Training Loss: tensor(0.0321)\n",
      "19022 Training Loss: tensor(0.0322)\n",
      "19023 Training Loss: tensor(0.0322)\n",
      "19024 Training Loss: tensor(0.0321)\n",
      "19025 Training Loss: tensor(0.0322)\n",
      "19026 Training Loss: tensor(0.0323)\n",
      "19027 Training Loss: tensor(0.0321)\n",
      "19028 Training Loss: tensor(0.0321)\n",
      "19029 Training Loss: tensor(0.0322)\n",
      "19030 Training Loss: tensor(0.0322)\n",
      "19031 Training Loss: tensor(0.0322)\n",
      "19032 Training Loss: tensor(0.0322)\n",
      "19033 Training Loss: tensor(0.0323)\n",
      "19034 Training Loss: tensor(0.0321)\n",
      "19035 Training Loss: tensor(0.0321)\n",
      "19036 Training Loss: tensor(0.0322)\n",
      "19037 Training Loss: tensor(0.0323)\n",
      "19038 Training Loss: tensor(0.0321)\n",
      "19039 Training Loss: tensor(0.0323)\n",
      "19040 Training Loss: tensor(0.0321)\n",
      "19041 Training Loss: tensor(0.0322)\n",
      "19042 Training Loss: tensor(0.0321)\n",
      "19043 Training Loss: tensor(0.0322)\n",
      "19044 Training Loss: tensor(0.0321)\n",
      "19045 Training Loss: tensor(0.0322)\n",
      "19046 Training Loss: tensor(0.0321)\n",
      "19047 Training Loss: tensor(0.0322)\n",
      "19048 Training Loss: tensor(0.0323)\n",
      "19049 Training Loss: tensor(0.0322)\n",
      "19050 Training Loss: tensor(0.0323)\n",
      "19051 Training Loss: tensor(0.0322)\n",
      "19052 Training Loss: tensor(0.0322)\n",
      "19053 Training Loss: tensor(0.0323)\n",
      "19054 Training Loss: tensor(0.0324)\n",
      "19055 Training Loss: tensor(0.0322)\n",
      "19056 Training Loss: tensor(0.0322)\n",
      "19057 Training Loss: tensor(0.0322)\n",
      "19058 Training Loss: tensor(0.0323)\n",
      "19059 Training Loss: tensor(0.0321)\n",
      "19060 Training Loss: tensor(0.0321)\n",
      "19061 Training Loss: tensor(0.0321)\n",
      "19062 Training Loss: tensor(0.0321)\n",
      "19063 Training Loss: tensor(0.0321)\n",
      "19064 Training Loss: tensor(0.0323)\n",
      "19065 Training Loss: tensor(0.0322)\n",
      "19066 Training Loss: tensor(0.0322)\n",
      "19067 Training Loss: tensor(0.0322)\n",
      "19068 Training Loss: tensor(0.0321)\n",
      "19069 Training Loss: tensor(0.0321)\n",
      "19070 Training Loss: tensor(0.0322)\n",
      "19071 Training Loss: tensor(0.0322)\n",
      "19072 Training Loss: tensor(0.0322)\n",
      "19073 Training Loss: tensor(0.0322)\n",
      "19074 Training Loss: tensor(0.0323)\n",
      "19075 Training Loss: tensor(0.0322)\n",
      "19076 Training Loss: tensor(0.0323)\n",
      "19077 Training Loss: tensor(0.0323)\n",
      "19078 Training Loss: tensor(0.0321)\n",
      "19079 Training Loss: tensor(0.0325)\n",
      "19080 Training Loss: tensor(0.0323)\n",
      "19081 Training Loss: tensor(0.0321)\n",
      "19082 Training Loss: tensor(0.0321)\n",
      "19083 Training Loss: tensor(0.0322)\n",
      "19084 Training Loss: tensor(0.0323)\n",
      "19085 Training Loss: tensor(0.0322)\n",
      "19086 Training Loss: tensor(0.0322)\n",
      "19087 Training Loss: tensor(0.0322)\n",
      "19088 Training Loss: tensor(0.0322)\n",
      "19089 Training Loss: tensor(0.0321)\n",
      "19090 Training Loss: tensor(0.0322)\n",
      "19091 Training Loss: tensor(0.0322)\n",
      "19092 Training Loss: tensor(0.0322)\n",
      "19093 Training Loss: tensor(0.0322)\n",
      "19094 Training Loss: tensor(0.0321)\n",
      "19095 Training Loss: tensor(0.0320)\n",
      "19096 Training Loss: tensor(0.0322)\n",
      "19097 Training Loss: tensor(0.0322)\n",
      "19098 Training Loss: tensor(0.0322)\n",
      "19099 Training Loss: tensor(0.0321)\n",
      "19100 Training Loss: tensor(0.0320)\n",
      "19101 Training Loss: tensor(0.0323)\n",
      "19102 Training Loss: tensor(0.0321)\n",
      "19103 Training Loss: tensor(0.0323)\n",
      "19104 Training Loss: tensor(0.0321)\n",
      "19105 Training Loss: tensor(0.0322)\n",
      "19106 Training Loss: tensor(0.0322)\n",
      "19107 Training Loss: tensor(0.0321)\n",
      "19108 Training Loss: tensor(0.0321)\n",
      "19109 Training Loss: tensor(0.0321)\n",
      "19110 Training Loss: tensor(0.0322)\n",
      "19111 Training Loss: tensor(0.0323)\n",
      "19112 Training Loss: tensor(0.0322)\n",
      "19113 Training Loss: tensor(0.0321)\n",
      "19114 Training Loss: tensor(0.0321)\n",
      "19115 Training Loss: tensor(0.0322)\n",
      "19116 Training Loss: tensor(0.0323)\n",
      "19117 Training Loss: tensor(0.0321)\n",
      "19118 Training Loss: tensor(0.0320)\n",
      "19119 Training Loss: tensor(0.0322)\n",
      "19120 Training Loss: tensor(0.0322)\n",
      "19121 Training Loss: tensor(0.0321)\n",
      "19122 Training Loss: tensor(0.0321)\n",
      "19123 Training Loss: tensor(0.0321)\n",
      "19124 Training Loss: tensor(0.0321)\n",
      "19125 Training Loss: tensor(0.0322)\n",
      "19126 Training Loss: tensor(0.0321)\n",
      "19127 Training Loss: tensor(0.0320)\n",
      "19128 Training Loss: tensor(0.0322)\n",
      "19129 Training Loss: tensor(0.0321)\n",
      "19130 Training Loss: tensor(0.0321)\n",
      "19131 Training Loss: tensor(0.0321)\n",
      "19132 Training Loss: tensor(0.0322)\n",
      "19133 Training Loss: tensor(0.0321)\n",
      "19134 Training Loss: tensor(0.0322)\n",
      "19135 Training Loss: tensor(0.0322)\n",
      "19136 Training Loss: tensor(0.0321)\n",
      "19137 Training Loss: tensor(0.0321)\n",
      "19138 Training Loss: tensor(0.0321)\n",
      "19139 Training Loss: tensor(0.0321)\n",
      "19140 Training Loss: tensor(0.0324)\n",
      "19141 Training Loss: tensor(0.0321)\n",
      "19142 Training Loss: tensor(0.0326)\n",
      "19143 Training Loss: tensor(0.0321)\n",
      "19144 Training Loss: tensor(0.0320)\n",
      "19145 Training Loss: tensor(0.0321)\n",
      "19146 Training Loss: tensor(0.0322)\n",
      "19147 Training Loss: tensor(0.0321)\n",
      "19148 Training Loss: tensor(0.0321)\n",
      "19149 Training Loss: tensor(0.0321)\n",
      "19150 Training Loss: tensor(0.0322)\n",
      "19151 Training Loss: tensor(0.0322)\n",
      "19152 Training Loss: tensor(0.0322)\n",
      "19153 Training Loss: tensor(0.0321)\n",
      "19154 Training Loss: tensor(0.0321)\n",
      "19155 Training Loss: tensor(0.0321)\n",
      "19156 Training Loss: tensor(0.0321)\n",
      "19157 Training Loss: tensor(0.0321)\n",
      "19158 Training Loss: tensor(0.0321)\n",
      "19159 Training Loss: tensor(0.0322)\n",
      "19160 Training Loss: tensor(0.0320)\n",
      "19161 Training Loss: tensor(0.0322)\n",
      "19162 Training Loss: tensor(0.0323)\n",
      "19163 Training Loss: tensor(0.0320)\n",
      "19164 Training Loss: tensor(0.0321)\n",
      "19165 Training Loss: tensor(0.0321)\n",
      "19166 Training Loss: tensor(0.0321)\n",
      "19167 Training Loss: tensor(0.0321)\n",
      "19168 Training Loss: tensor(0.0321)\n",
      "19169 Training Loss: tensor(0.0321)\n",
      "19170 Training Loss: tensor(0.0321)\n",
      "19171 Training Loss: tensor(0.0322)\n",
      "19172 Training Loss: tensor(0.0321)\n",
      "19173 Training Loss: tensor(0.0320)\n",
      "19174 Training Loss: tensor(0.0321)\n",
      "19175 Training Loss: tensor(0.0322)\n",
      "19176 Training Loss: tensor(0.0322)\n",
      "19177 Training Loss: tensor(0.0321)\n",
      "19178 Training Loss: tensor(0.0321)\n",
      "19179 Training Loss: tensor(0.0322)\n",
      "19180 Training Loss: tensor(0.0321)\n",
      "19181 Training Loss: tensor(0.0322)\n",
      "19182 Training Loss: tensor(0.0322)\n",
      "19183 Training Loss: tensor(0.0321)\n",
      "19184 Training Loss: tensor(0.0321)\n",
      "19185 Training Loss: tensor(0.0321)\n",
      "19186 Training Loss: tensor(0.0321)\n",
      "19187 Training Loss: tensor(0.0322)\n",
      "19188 Training Loss: tensor(0.0322)\n",
      "19189 Training Loss: tensor(0.0322)\n",
      "19190 Training Loss: tensor(0.0321)\n",
      "19191 Training Loss: tensor(0.0321)\n",
      "19192 Training Loss: tensor(0.0323)\n",
      "19193 Training Loss: tensor(0.0321)\n",
      "19194 Training Loss: tensor(0.0321)\n",
      "19195 Training Loss: tensor(0.0321)\n",
      "19196 Training Loss: tensor(0.0324)\n",
      "19197 Training Loss: tensor(0.0322)\n",
      "19198 Training Loss: tensor(0.0321)\n",
      "19199 Training Loss: tensor(0.0321)\n",
      "19200 Training Loss: tensor(0.0322)\n",
      "19201 Training Loss: tensor(0.0321)\n",
      "19202 Training Loss: tensor(0.0320)\n",
      "19203 Training Loss: tensor(0.0322)\n",
      "19204 Training Loss: tensor(0.0321)\n",
      "19205 Training Loss: tensor(0.0321)\n",
      "19206 Training Loss: tensor(0.0321)\n",
      "19207 Training Loss: tensor(0.0320)\n",
      "19208 Training Loss: tensor(0.0323)\n",
      "19209 Training Loss: tensor(0.0321)\n",
      "19210 Training Loss: tensor(0.0321)\n",
      "19211 Training Loss: tensor(0.0322)\n",
      "19212 Training Loss: tensor(0.0323)\n",
      "19213 Training Loss: tensor(0.0323)\n",
      "19214 Training Loss: tensor(0.0323)\n",
      "19215 Training Loss: tensor(0.0321)\n",
      "19216 Training Loss: tensor(0.0321)\n",
      "19217 Training Loss: tensor(0.0322)\n",
      "19218 Training Loss: tensor(0.0320)\n",
      "19219 Training Loss: tensor(0.0322)\n",
      "19220 Training Loss: tensor(0.0320)\n",
      "19221 Training Loss: tensor(0.0321)\n",
      "19222 Training Loss: tensor(0.0323)\n",
      "19223 Training Loss: tensor(0.0323)\n",
      "19224 Training Loss: tensor(0.0321)\n",
      "19225 Training Loss: tensor(0.0322)\n",
      "19226 Training Loss: tensor(0.0322)\n",
      "19227 Training Loss: tensor(0.0320)\n",
      "19228 Training Loss: tensor(0.0322)\n",
      "19229 Training Loss: tensor(0.0322)\n",
      "19230 Training Loss: tensor(0.0322)\n",
      "19231 Training Loss: tensor(0.0321)\n",
      "19232 Training Loss: tensor(0.0321)\n",
      "19233 Training Loss: tensor(0.0320)\n",
      "19234 Training Loss: tensor(0.0321)\n",
      "19235 Training Loss: tensor(0.0320)\n",
      "19236 Training Loss: tensor(0.0322)\n",
      "19237 Training Loss: tensor(0.0321)\n",
      "19238 Training Loss: tensor(0.0322)\n",
      "19239 Training Loss: tensor(0.0321)\n",
      "19240 Training Loss: tensor(0.0322)\n",
      "19241 Training Loss: tensor(0.0322)\n",
      "19242 Training Loss: tensor(0.0322)\n",
      "19243 Training Loss: tensor(0.0322)\n",
      "19244 Training Loss: tensor(0.0322)\n",
      "19245 Training Loss: tensor(0.0322)\n",
      "19246 Training Loss: tensor(0.0323)\n",
      "19247 Training Loss: tensor(0.0321)\n",
      "19248 Training Loss: tensor(0.0322)\n",
      "19249 Training Loss: tensor(0.0322)\n",
      "19250 Training Loss: tensor(0.0322)\n",
      "19251 Training Loss: tensor(0.0322)\n",
      "19252 Training Loss: tensor(0.0321)\n",
      "19253 Training Loss: tensor(0.0323)\n",
      "19254 Training Loss: tensor(0.0321)\n",
      "19255 Training Loss: tensor(0.0321)\n",
      "19256 Training Loss: tensor(0.0321)\n",
      "19257 Training Loss: tensor(0.0320)\n",
      "19258 Training Loss: tensor(0.0321)\n",
      "19259 Training Loss: tensor(0.0321)\n",
      "19260 Training Loss: tensor(0.0321)\n",
      "19261 Training Loss: tensor(0.0322)\n",
      "19262 Training Loss: tensor(0.0321)\n",
      "19263 Training Loss: tensor(0.0321)\n",
      "19264 Training Loss: tensor(0.0321)\n",
      "19265 Training Loss: tensor(0.0322)\n",
      "19266 Training Loss: tensor(0.0322)\n",
      "19267 Training Loss: tensor(0.0321)\n",
      "19268 Training Loss: tensor(0.0323)\n",
      "19269 Training Loss: tensor(0.0322)\n",
      "19270 Training Loss: tensor(0.0320)\n",
      "19271 Training Loss: tensor(0.0321)\n",
      "19272 Training Loss: tensor(0.0320)\n",
      "19273 Training Loss: tensor(0.0321)\n",
      "19274 Training Loss: tensor(0.0320)\n",
      "19275 Training Loss: tensor(0.0323)\n",
      "19276 Training Loss: tensor(0.0323)\n",
      "19277 Training Loss: tensor(0.0323)\n",
      "19278 Training Loss: tensor(0.0321)\n",
      "19279 Training Loss: tensor(0.0321)\n",
      "19280 Training Loss: tensor(0.0321)\n",
      "19281 Training Loss: tensor(0.0321)\n",
      "19282 Training Loss: tensor(0.0322)\n",
      "19283 Training Loss: tensor(0.0321)\n",
      "19284 Training Loss: tensor(0.0321)\n",
      "19285 Training Loss: tensor(0.0321)\n",
      "19286 Training Loss: tensor(0.0320)\n",
      "19287 Training Loss: tensor(0.0322)\n",
      "19288 Training Loss: tensor(0.0321)\n",
      "19289 Training Loss: tensor(0.0321)\n",
      "19290 Training Loss: tensor(0.0321)\n",
      "19291 Training Loss: tensor(0.0321)\n",
      "19292 Training Loss: tensor(0.0324)\n",
      "19293 Training Loss: tensor(0.0321)\n",
      "19294 Training Loss: tensor(0.0321)\n",
      "19295 Training Loss: tensor(0.0320)\n",
      "19296 Training Loss: tensor(0.0322)\n",
      "19297 Training Loss: tensor(0.0320)\n",
      "19298 Training Loss: tensor(0.0321)\n",
      "19299 Training Loss: tensor(0.0321)\n",
      "19300 Training Loss: tensor(0.0321)\n",
      "19301 Training Loss: tensor(0.0321)\n",
      "19302 Training Loss: tensor(0.0321)\n",
      "19303 Training Loss: tensor(0.0321)\n",
      "19304 Training Loss: tensor(0.0321)\n",
      "19305 Training Loss: tensor(0.0320)\n",
      "19306 Training Loss: tensor(0.0321)\n",
      "19307 Training Loss: tensor(0.0321)\n",
      "19308 Training Loss: tensor(0.0320)\n",
      "19309 Training Loss: tensor(0.0321)\n",
      "19310 Training Loss: tensor(0.0321)\n",
      "19311 Training Loss: tensor(0.0322)\n",
      "19312 Training Loss: tensor(0.0321)\n",
      "19313 Training Loss: tensor(0.0322)\n",
      "19314 Training Loss: tensor(0.0320)\n",
      "19315 Training Loss: tensor(0.0322)\n",
      "19316 Training Loss: tensor(0.0321)\n",
      "19317 Training Loss: tensor(0.0320)\n",
      "19318 Training Loss: tensor(0.0323)\n",
      "19319 Training Loss: tensor(0.0320)\n",
      "19320 Training Loss: tensor(0.0321)\n",
      "19321 Training Loss: tensor(0.0321)\n",
      "19322 Training Loss: tensor(0.0320)\n",
      "19323 Training Loss: tensor(0.0321)\n",
      "19324 Training Loss: tensor(0.0321)\n",
      "19325 Training Loss: tensor(0.0322)\n",
      "19326 Training Loss: tensor(0.0321)\n",
      "19327 Training Loss: tensor(0.0320)\n",
      "19328 Training Loss: tensor(0.0322)\n",
      "19329 Training Loss: tensor(0.0320)\n",
      "19330 Training Loss: tensor(0.0321)\n",
      "19331 Training Loss: tensor(0.0319)\n",
      "19332 Training Loss: tensor(0.0322)\n",
      "19333 Training Loss: tensor(0.0320)\n",
      "19334 Training Loss: tensor(0.0321)\n",
      "19335 Training Loss: tensor(0.0321)\n",
      "19336 Training Loss: tensor(0.0323)\n",
      "19337 Training Loss: tensor(0.0321)\n",
      "19338 Training Loss: tensor(0.0322)\n",
      "19339 Training Loss: tensor(0.0321)\n",
      "19340 Training Loss: tensor(0.0320)\n",
      "19341 Training Loss: tensor(0.0320)\n",
      "19342 Training Loss: tensor(0.0320)\n",
      "19343 Training Loss: tensor(0.0320)\n",
      "19344 Training Loss: tensor(0.0321)\n",
      "19345 Training Loss: tensor(0.0320)\n",
      "19346 Training Loss: tensor(0.0321)\n",
      "19347 Training Loss: tensor(0.0322)\n",
      "19348 Training Loss: tensor(0.0321)\n",
      "19349 Training Loss: tensor(0.0320)\n",
      "19350 Training Loss: tensor(0.0320)\n",
      "19351 Training Loss: tensor(0.0321)\n",
      "19352 Training Loss: tensor(0.0320)\n",
      "19353 Training Loss: tensor(0.0321)\n",
      "19354 Training Loss: tensor(0.0320)\n",
      "19355 Training Loss: tensor(0.0321)\n",
      "19356 Training Loss: tensor(0.0321)\n",
      "19357 Training Loss: tensor(0.0322)\n",
      "19358 Training Loss: tensor(0.0320)\n",
      "19359 Training Loss: tensor(0.0320)\n",
      "19360 Training Loss: tensor(0.0321)\n",
      "19361 Training Loss: tensor(0.0320)\n",
      "19362 Training Loss: tensor(0.0321)\n",
      "19363 Training Loss: tensor(0.0320)\n",
      "19364 Training Loss: tensor(0.0322)\n",
      "19365 Training Loss: tensor(0.0320)\n",
      "19366 Training Loss: tensor(0.0321)\n",
      "19367 Training Loss: tensor(0.0321)\n",
      "19368 Training Loss: tensor(0.0320)\n",
      "19369 Training Loss: tensor(0.0321)\n",
      "19370 Training Loss: tensor(0.0322)\n",
      "19371 Training Loss: tensor(0.0320)\n",
      "19372 Training Loss: tensor(0.0322)\n",
      "19373 Training Loss: tensor(0.0321)\n",
      "19374 Training Loss: tensor(0.0320)\n",
      "19375 Training Loss: tensor(0.0321)\n",
      "19376 Training Loss: tensor(0.0321)\n",
      "19377 Training Loss: tensor(0.0322)\n",
      "19378 Training Loss: tensor(0.0323)\n",
      "19379 Training Loss: tensor(0.0320)\n",
      "19380 Training Loss: tensor(0.0320)\n",
      "19381 Training Loss: tensor(0.0322)\n",
      "19382 Training Loss: tensor(0.0319)\n",
      "19383 Training Loss: tensor(0.0320)\n",
      "19384 Training Loss: tensor(0.0322)\n",
      "19385 Training Loss: tensor(0.0321)\n",
      "19386 Training Loss: tensor(0.0321)\n",
      "19387 Training Loss: tensor(0.0322)\n",
      "19388 Training Loss: tensor(0.0321)\n",
      "19389 Training Loss: tensor(0.0320)\n",
      "19390 Training Loss: tensor(0.0321)\n",
      "19391 Training Loss: tensor(0.0321)\n",
      "19392 Training Loss: tensor(0.0321)\n",
      "19393 Training Loss: tensor(0.0321)\n",
      "19394 Training Loss: tensor(0.0321)\n",
      "19395 Training Loss: tensor(0.0320)\n",
      "19396 Training Loss: tensor(0.0321)\n",
      "19397 Training Loss: tensor(0.0321)\n",
      "19398 Training Loss: tensor(0.0321)\n",
      "19399 Training Loss: tensor(0.0321)\n",
      "19400 Training Loss: tensor(0.0321)\n",
      "19401 Training Loss: tensor(0.0320)\n",
      "19402 Training Loss: tensor(0.0321)\n",
      "19403 Training Loss: tensor(0.0320)\n",
      "19404 Training Loss: tensor(0.0322)\n",
      "19405 Training Loss: tensor(0.0321)\n",
      "19406 Training Loss: tensor(0.0320)\n",
      "19407 Training Loss: tensor(0.0322)\n",
      "19408 Training Loss: tensor(0.0321)\n",
      "19409 Training Loss: tensor(0.0321)\n",
      "19410 Training Loss: tensor(0.0320)\n",
      "19411 Training Loss: tensor(0.0321)\n",
      "19412 Training Loss: tensor(0.0321)\n",
      "19413 Training Loss: tensor(0.0320)\n",
      "19414 Training Loss: tensor(0.0321)\n",
      "19415 Training Loss: tensor(0.0322)\n",
      "19416 Training Loss: tensor(0.0321)\n",
      "19417 Training Loss: tensor(0.0320)\n",
      "19418 Training Loss: tensor(0.0321)\n",
      "19419 Training Loss: tensor(0.0321)\n",
      "19420 Training Loss: tensor(0.0321)\n",
      "19421 Training Loss: tensor(0.0320)\n",
      "19422 Training Loss: tensor(0.0321)\n",
      "19423 Training Loss: tensor(0.0321)\n",
      "19424 Training Loss: tensor(0.0319)\n",
      "19425 Training Loss: tensor(0.0320)\n",
      "19426 Training Loss: tensor(0.0320)\n",
      "19427 Training Loss: tensor(0.0323)\n",
      "19428 Training Loss: tensor(0.0320)\n",
      "19429 Training Loss: tensor(0.0321)\n",
      "19430 Training Loss: tensor(0.0323)\n",
      "19431 Training Loss: tensor(0.0321)\n",
      "19432 Training Loss: tensor(0.0319)\n",
      "19433 Training Loss: tensor(0.0321)\n",
      "19434 Training Loss: tensor(0.0320)\n",
      "19435 Training Loss: tensor(0.0320)\n",
      "19436 Training Loss: tensor(0.0321)\n",
      "19437 Training Loss: tensor(0.0322)\n",
      "19438 Training Loss: tensor(0.0322)\n",
      "19439 Training Loss: tensor(0.0320)\n",
      "19440 Training Loss: tensor(0.0320)\n",
      "19441 Training Loss: tensor(0.0322)\n",
      "19442 Training Loss: tensor(0.0320)\n",
      "19443 Training Loss: tensor(0.0320)\n",
      "19444 Training Loss: tensor(0.0321)\n",
      "19445 Training Loss: tensor(0.0321)\n",
      "19446 Training Loss: tensor(0.0321)\n",
      "19447 Training Loss: tensor(0.0321)\n",
      "19448 Training Loss: tensor(0.0321)\n",
      "19449 Training Loss: tensor(0.0321)\n",
      "19450 Training Loss: tensor(0.0321)\n",
      "19451 Training Loss: tensor(0.0321)\n",
      "19452 Training Loss: tensor(0.0320)\n",
      "19453 Training Loss: tensor(0.0321)\n",
      "19454 Training Loss: tensor(0.0322)\n",
      "19455 Training Loss: tensor(0.0320)\n",
      "19456 Training Loss: tensor(0.0321)\n",
      "19457 Training Loss: tensor(0.0320)\n",
      "19458 Training Loss: tensor(0.0320)\n",
      "19459 Training Loss: tensor(0.0321)\n",
      "19460 Training Loss: tensor(0.0321)\n",
      "19461 Training Loss: tensor(0.0322)\n",
      "19462 Training Loss: tensor(0.0321)\n",
      "19463 Training Loss: tensor(0.0322)\n",
      "19464 Training Loss: tensor(0.0323)\n",
      "19465 Training Loss: tensor(0.0322)\n",
      "19466 Training Loss: tensor(0.0321)\n",
      "19467 Training Loss: tensor(0.0320)\n",
      "19468 Training Loss: tensor(0.0320)\n",
      "19469 Training Loss: tensor(0.0321)\n",
      "19470 Training Loss: tensor(0.0320)\n",
      "19471 Training Loss: tensor(0.0320)\n",
      "19472 Training Loss: tensor(0.0320)\n",
      "19473 Training Loss: tensor(0.0320)\n",
      "19474 Training Loss: tensor(0.0321)\n",
      "19475 Training Loss: tensor(0.0320)\n",
      "19476 Training Loss: tensor(0.0321)\n",
      "19477 Training Loss: tensor(0.0320)\n",
      "19478 Training Loss: tensor(0.0321)\n",
      "19479 Training Loss: tensor(0.0320)\n",
      "19480 Training Loss: tensor(0.0320)\n",
      "19481 Training Loss: tensor(0.0319)\n",
      "19482 Training Loss: tensor(0.0319)\n",
      "19483 Training Loss: tensor(0.0322)\n",
      "19484 Training Loss: tensor(0.0319)\n",
      "19485 Training Loss: tensor(0.0321)\n",
      "19486 Training Loss: tensor(0.0319)\n",
      "19487 Training Loss: tensor(0.0320)\n",
      "19488 Training Loss: tensor(0.0322)\n",
      "19489 Training Loss: tensor(0.0320)\n",
      "19490 Training Loss: tensor(0.0320)\n",
      "19491 Training Loss: tensor(0.0321)\n",
      "19492 Training Loss: tensor(0.0321)\n",
      "19493 Training Loss: tensor(0.0320)\n",
      "19494 Training Loss: tensor(0.0321)\n",
      "19495 Training Loss: tensor(0.0321)\n",
      "19496 Training Loss: tensor(0.0321)\n",
      "19497 Training Loss: tensor(0.0320)\n",
      "19498 Training Loss: tensor(0.0319)\n",
      "19499 Training Loss: tensor(0.0320)\n",
      "19500 Training Loss: tensor(0.0320)\n",
      "19501 Training Loss: tensor(0.0320)\n",
      "19502 Training Loss: tensor(0.0320)\n",
      "19503 Training Loss: tensor(0.0320)\n",
      "19504 Training Loss: tensor(0.0320)\n",
      "19505 Training Loss: tensor(0.0321)\n",
      "19506 Training Loss: tensor(0.0320)\n",
      "19507 Training Loss: tensor(0.0321)\n",
      "19508 Training Loss: tensor(0.0319)\n",
      "19509 Training Loss: tensor(0.0319)\n",
      "19510 Training Loss: tensor(0.0321)\n",
      "19511 Training Loss: tensor(0.0320)\n",
      "19512 Training Loss: tensor(0.0323)\n",
      "19513 Training Loss: tensor(0.0321)\n",
      "19514 Training Loss: tensor(0.0320)\n",
      "19515 Training Loss: tensor(0.0322)\n",
      "19516 Training Loss: tensor(0.0320)\n",
      "19517 Training Loss: tensor(0.0321)\n",
      "19518 Training Loss: tensor(0.0320)\n",
      "19519 Training Loss: tensor(0.0321)\n",
      "19520 Training Loss: tensor(0.0321)\n",
      "19521 Training Loss: tensor(0.0320)\n",
      "19522 Training Loss: tensor(0.0321)\n",
      "19523 Training Loss: tensor(0.0321)\n",
      "19524 Training Loss: tensor(0.0319)\n",
      "19525 Training Loss: tensor(0.0320)\n",
      "19526 Training Loss: tensor(0.0320)\n",
      "19527 Training Loss: tensor(0.0320)\n",
      "19528 Training Loss: tensor(0.0320)\n",
      "19529 Training Loss: tensor(0.0320)\n",
      "19530 Training Loss: tensor(0.0320)\n",
      "19531 Training Loss: tensor(0.0322)\n",
      "19532 Training Loss: tensor(0.0320)\n",
      "19533 Training Loss: tensor(0.0320)\n",
      "19534 Training Loss: tensor(0.0321)\n",
      "19535 Training Loss: tensor(0.0319)\n",
      "19536 Training Loss: tensor(0.0322)\n",
      "19537 Training Loss: tensor(0.0320)\n",
      "19538 Training Loss: tensor(0.0319)\n",
      "19539 Training Loss: tensor(0.0320)\n",
      "19540 Training Loss: tensor(0.0320)\n",
      "19541 Training Loss: tensor(0.0320)\n",
      "19542 Training Loss: tensor(0.0323)\n",
      "19543 Training Loss: tensor(0.0319)\n",
      "19544 Training Loss: tensor(0.0320)\n",
      "19545 Training Loss: tensor(0.0319)\n",
      "19546 Training Loss: tensor(0.0320)\n",
      "19547 Training Loss: tensor(0.0321)\n",
      "19548 Training Loss: tensor(0.0319)\n",
      "19549 Training Loss: tensor(0.0323)\n",
      "19550 Training Loss: tensor(0.0320)\n",
      "19551 Training Loss: tensor(0.0320)\n",
      "19552 Training Loss: tensor(0.0322)\n",
      "19553 Training Loss: tensor(0.0321)\n",
      "19554 Training Loss: tensor(0.0320)\n",
      "19555 Training Loss: tensor(0.0320)\n",
      "19556 Training Loss: tensor(0.0320)\n",
      "19557 Training Loss: tensor(0.0320)\n",
      "19558 Training Loss: tensor(0.0320)\n",
      "19559 Training Loss: tensor(0.0321)\n",
      "19560 Training Loss: tensor(0.0320)\n",
      "19561 Training Loss: tensor(0.0320)\n",
      "19562 Training Loss: tensor(0.0321)\n",
      "19563 Training Loss: tensor(0.0321)\n",
      "19564 Training Loss: tensor(0.0321)\n",
      "19565 Training Loss: tensor(0.0322)\n",
      "19566 Training Loss: tensor(0.0321)\n",
      "19567 Training Loss: tensor(0.0321)\n",
      "19568 Training Loss: tensor(0.0320)\n",
      "19569 Training Loss: tensor(0.0320)\n",
      "19570 Training Loss: tensor(0.0320)\n",
      "19571 Training Loss: tensor(0.0320)\n",
      "19572 Training Loss: tensor(0.0321)\n",
      "19573 Training Loss: tensor(0.0321)\n",
      "19574 Training Loss: tensor(0.0320)\n",
      "19575 Training Loss: tensor(0.0319)\n",
      "19576 Training Loss: tensor(0.0320)\n",
      "19577 Training Loss: tensor(0.0319)\n",
      "19578 Training Loss: tensor(0.0321)\n",
      "19579 Training Loss: tensor(0.0320)\n",
      "19580 Training Loss: tensor(0.0320)\n",
      "19581 Training Loss: tensor(0.0320)\n",
      "19582 Training Loss: tensor(0.0320)\n",
      "19583 Training Loss: tensor(0.0321)\n",
      "19584 Training Loss: tensor(0.0319)\n",
      "19585 Training Loss: tensor(0.0320)\n",
      "19586 Training Loss: tensor(0.0321)\n",
      "19587 Training Loss: tensor(0.0322)\n",
      "19588 Training Loss: tensor(0.0322)\n",
      "19589 Training Loss: tensor(0.0319)\n",
      "19590 Training Loss: tensor(0.0320)\n",
      "19591 Training Loss: tensor(0.0321)\n",
      "19592 Training Loss: tensor(0.0320)\n",
      "19593 Training Loss: tensor(0.0320)\n",
      "19594 Training Loss: tensor(0.0321)\n",
      "19595 Training Loss: tensor(0.0322)\n",
      "19596 Training Loss: tensor(0.0320)\n",
      "19597 Training Loss: tensor(0.0321)\n",
      "19598 Training Loss: tensor(0.0320)\n",
      "19599 Training Loss: tensor(0.0321)\n",
      "19600 Training Loss: tensor(0.0320)\n",
      "19601 Training Loss: tensor(0.0319)\n",
      "19602 Training Loss: tensor(0.0320)\n",
      "19603 Training Loss: tensor(0.0319)\n",
      "19604 Training Loss: tensor(0.0320)\n",
      "19605 Training Loss: tensor(0.0320)\n",
      "19606 Training Loss: tensor(0.0320)\n",
      "19607 Training Loss: tensor(0.0321)\n",
      "19608 Training Loss: tensor(0.0320)\n",
      "19609 Training Loss: tensor(0.0320)\n",
      "19610 Training Loss: tensor(0.0322)\n",
      "19611 Training Loss: tensor(0.0321)\n",
      "19612 Training Loss: tensor(0.0321)\n",
      "19613 Training Loss: tensor(0.0320)\n",
      "19614 Training Loss: tensor(0.0320)\n",
      "19615 Training Loss: tensor(0.0320)\n",
      "19616 Training Loss: tensor(0.0320)\n",
      "19617 Training Loss: tensor(0.0321)\n",
      "19618 Training Loss: tensor(0.0321)\n",
      "19619 Training Loss: tensor(0.0320)\n",
      "19620 Training Loss: tensor(0.0320)\n",
      "19621 Training Loss: tensor(0.0319)\n",
      "19622 Training Loss: tensor(0.0320)\n",
      "19623 Training Loss: tensor(0.0319)\n",
      "19624 Training Loss: tensor(0.0321)\n",
      "19625 Training Loss: tensor(0.0321)\n",
      "19626 Training Loss: tensor(0.0320)\n",
      "19627 Training Loss: tensor(0.0320)\n",
      "19628 Training Loss: tensor(0.0321)\n",
      "19629 Training Loss: tensor(0.0322)\n",
      "19630 Training Loss: tensor(0.0321)\n",
      "19631 Training Loss: tensor(0.0320)\n",
      "19632 Training Loss: tensor(0.0320)\n",
      "19633 Training Loss: tensor(0.0320)\n",
      "19634 Training Loss: tensor(0.0319)\n",
      "19635 Training Loss: tensor(0.0319)\n",
      "19636 Training Loss: tensor(0.0320)\n",
      "19637 Training Loss: tensor(0.0320)\n",
      "19638 Training Loss: tensor(0.0320)\n",
      "19639 Training Loss: tensor(0.0321)\n",
      "19640 Training Loss: tensor(0.0320)\n",
      "19641 Training Loss: tensor(0.0319)\n",
      "19642 Training Loss: tensor(0.0319)\n",
      "19643 Training Loss: tensor(0.0321)\n",
      "19644 Training Loss: tensor(0.0322)\n",
      "19645 Training Loss: tensor(0.0320)\n",
      "19646 Training Loss: tensor(0.0321)\n",
      "19647 Training Loss: tensor(0.0321)\n",
      "19648 Training Loss: tensor(0.0319)\n",
      "19649 Training Loss: tensor(0.0321)\n",
      "19650 Training Loss: tensor(0.0320)\n",
      "19651 Training Loss: tensor(0.0319)\n",
      "19652 Training Loss: tensor(0.0319)\n",
      "19653 Training Loss: tensor(0.0319)\n",
      "19654 Training Loss: tensor(0.0320)\n",
      "19655 Training Loss: tensor(0.0321)\n",
      "19656 Training Loss: tensor(0.0320)\n",
      "19657 Training Loss: tensor(0.0321)\n",
      "19658 Training Loss: tensor(0.0322)\n",
      "19659 Training Loss: tensor(0.0319)\n",
      "19660 Training Loss: tensor(0.0320)\n",
      "19661 Training Loss: tensor(0.0320)\n",
      "19662 Training Loss: tensor(0.0320)\n",
      "19663 Training Loss: tensor(0.0320)\n",
      "19664 Training Loss: tensor(0.0320)\n",
      "19665 Training Loss: tensor(0.0319)\n",
      "19666 Training Loss: tensor(0.0321)\n",
      "19667 Training Loss: tensor(0.0318)\n",
      "19668 Training Loss: tensor(0.0320)\n",
      "19669 Training Loss: tensor(0.0320)\n",
      "19670 Training Loss: tensor(0.0318)\n",
      "19671 Training Loss: tensor(0.0319)\n",
      "19672 Training Loss: tensor(0.0319)\n",
      "19673 Training Loss: tensor(0.0321)\n",
      "19674 Training Loss: tensor(0.0320)\n",
      "19675 Training Loss: tensor(0.0322)\n",
      "19676 Training Loss: tensor(0.0319)\n",
      "19677 Training Loss: tensor(0.0319)\n",
      "19678 Training Loss: tensor(0.0319)\n",
      "19679 Training Loss: tensor(0.0319)\n",
      "19680 Training Loss: tensor(0.0319)\n",
      "19681 Training Loss: tensor(0.0320)\n",
      "19682 Training Loss: tensor(0.0321)\n",
      "19683 Training Loss: tensor(0.0320)\n",
      "19684 Training Loss: tensor(0.0320)\n",
      "19685 Training Loss: tensor(0.0321)\n",
      "19686 Training Loss: tensor(0.0321)\n",
      "19687 Training Loss: tensor(0.0319)\n",
      "19688 Training Loss: tensor(0.0320)\n",
      "19689 Training Loss: tensor(0.0320)\n",
      "19690 Training Loss: tensor(0.0319)\n",
      "19691 Training Loss: tensor(0.0321)\n",
      "19692 Training Loss: tensor(0.0320)\n",
      "19693 Training Loss: tensor(0.0319)\n",
      "19694 Training Loss: tensor(0.0320)\n",
      "19695 Training Loss: tensor(0.0319)\n",
      "19696 Training Loss: tensor(0.0319)\n",
      "19697 Training Loss: tensor(0.0320)\n",
      "19698 Training Loss: tensor(0.0320)\n",
      "19699 Training Loss: tensor(0.0319)\n",
      "19700 Training Loss: tensor(0.0320)\n",
      "19701 Training Loss: tensor(0.0319)\n",
      "19702 Training Loss: tensor(0.0319)\n",
      "19703 Training Loss: tensor(0.0320)\n",
      "19704 Training Loss: tensor(0.0319)\n",
      "19705 Training Loss: tensor(0.0320)\n",
      "19706 Training Loss: tensor(0.0320)\n",
      "19707 Training Loss: tensor(0.0319)\n",
      "19708 Training Loss: tensor(0.0320)\n",
      "19709 Training Loss: tensor(0.0320)\n",
      "19710 Training Loss: tensor(0.0319)\n",
      "19711 Training Loss: tensor(0.0320)\n",
      "19712 Training Loss: tensor(0.0319)\n",
      "19713 Training Loss: tensor(0.0319)\n",
      "19714 Training Loss: tensor(0.0320)\n",
      "19715 Training Loss: tensor(0.0320)\n",
      "19716 Training Loss: tensor(0.0324)\n",
      "19717 Training Loss: tensor(0.0321)\n",
      "19718 Training Loss: tensor(0.0321)\n",
      "19719 Training Loss: tensor(0.0321)\n",
      "19720 Training Loss: tensor(0.0320)\n",
      "19721 Training Loss: tensor(0.0321)\n",
      "19722 Training Loss: tensor(0.0322)\n",
      "19723 Training Loss: tensor(0.0320)\n",
      "19724 Training Loss: tensor(0.0320)\n",
      "19725 Training Loss: tensor(0.0321)\n",
      "19726 Training Loss: tensor(0.0320)\n",
      "19727 Training Loss: tensor(0.0321)\n",
      "19728 Training Loss: tensor(0.0321)\n",
      "19729 Training Loss: tensor(0.0321)\n",
      "19730 Training Loss: tensor(0.0321)\n",
      "19731 Training Loss: tensor(0.0320)\n",
      "19732 Training Loss: tensor(0.0321)\n",
      "19733 Training Loss: tensor(0.0319)\n",
      "19734 Training Loss: tensor(0.0320)\n",
      "19735 Training Loss: tensor(0.0320)\n",
      "19736 Training Loss: tensor(0.0320)\n",
      "19737 Training Loss: tensor(0.0321)\n",
      "19738 Training Loss: tensor(0.0320)\n",
      "19739 Training Loss: tensor(0.0319)\n",
      "19740 Training Loss: tensor(0.0320)\n",
      "19741 Training Loss: tensor(0.0321)\n",
      "19742 Training Loss: tensor(0.0319)\n",
      "19743 Training Loss: tensor(0.0320)\n",
      "19744 Training Loss: tensor(0.0319)\n",
      "19745 Training Loss: tensor(0.0319)\n",
      "19746 Training Loss: tensor(0.0319)\n",
      "19747 Training Loss: tensor(0.0318)\n",
      "19748 Training Loss: tensor(0.0319)\n",
      "19749 Training Loss: tensor(0.0320)\n",
      "19750 Training Loss: tensor(0.0320)\n",
      "19751 Training Loss: tensor(0.0319)\n",
      "19752 Training Loss: tensor(0.0321)\n",
      "19753 Training Loss: tensor(0.0321)\n",
      "19754 Training Loss: tensor(0.0319)\n",
      "19755 Training Loss: tensor(0.0319)\n",
      "19756 Training Loss: tensor(0.0319)\n",
      "19757 Training Loss: tensor(0.0321)\n",
      "19758 Training Loss: tensor(0.0320)\n",
      "19759 Training Loss: tensor(0.0319)\n",
      "19760 Training Loss: tensor(0.0321)\n",
      "19761 Training Loss: tensor(0.0319)\n",
      "19762 Training Loss: tensor(0.0319)\n",
      "19763 Training Loss: tensor(0.0319)\n",
      "19764 Training Loss: tensor(0.0319)\n",
      "19765 Training Loss: tensor(0.0319)\n",
      "19766 Training Loss: tensor(0.0319)\n",
      "19767 Training Loss: tensor(0.0319)\n",
      "19768 Training Loss: tensor(0.0321)\n",
      "19769 Training Loss: tensor(0.0319)\n",
      "19770 Training Loss: tensor(0.0319)\n",
      "19771 Training Loss: tensor(0.0320)\n",
      "19772 Training Loss: tensor(0.0319)\n",
      "19773 Training Loss: tensor(0.0319)\n",
      "19774 Training Loss: tensor(0.0319)\n",
      "19775 Training Loss: tensor(0.0320)\n",
      "19776 Training Loss: tensor(0.0319)\n",
      "19777 Training Loss: tensor(0.0320)\n",
      "19778 Training Loss: tensor(0.0321)\n",
      "19779 Training Loss: tensor(0.0319)\n",
      "19780 Training Loss: tensor(0.0319)\n",
      "19781 Training Loss: tensor(0.0321)\n",
      "19782 Training Loss: tensor(0.0319)\n",
      "19783 Training Loss: tensor(0.0320)\n",
      "19784 Training Loss: tensor(0.0319)\n",
      "19785 Training Loss: tensor(0.0320)\n",
      "19786 Training Loss: tensor(0.0319)\n",
      "19787 Training Loss: tensor(0.0318)\n",
      "19788 Training Loss: tensor(0.0321)\n",
      "19789 Training Loss: tensor(0.0319)\n",
      "19790 Training Loss: tensor(0.0319)\n",
      "19791 Training Loss: tensor(0.0319)\n",
      "19792 Training Loss: tensor(0.0319)\n",
      "19793 Training Loss: tensor(0.0320)\n",
      "19794 Training Loss: tensor(0.0319)\n",
      "19795 Training Loss: tensor(0.0319)\n",
      "19796 Training Loss: tensor(0.0319)\n",
      "19797 Training Loss: tensor(0.0320)\n",
      "19798 Training Loss: tensor(0.0320)\n",
      "19799 Training Loss: tensor(0.0319)\n",
      "19800 Training Loss: tensor(0.0319)\n",
      "19801 Training Loss: tensor(0.0319)\n",
      "19802 Training Loss: tensor(0.0319)\n",
      "19803 Training Loss: tensor(0.0320)\n",
      "19804 Training Loss: tensor(0.0320)\n",
      "19805 Training Loss: tensor(0.0319)\n",
      "19806 Training Loss: tensor(0.0320)\n",
      "19807 Training Loss: tensor(0.0319)\n",
      "19808 Training Loss: tensor(0.0318)\n",
      "19809 Training Loss: tensor(0.0319)\n",
      "19810 Training Loss: tensor(0.0321)\n",
      "19811 Training Loss: tensor(0.0319)\n",
      "19812 Training Loss: tensor(0.0320)\n",
      "19813 Training Loss: tensor(0.0320)\n",
      "19814 Training Loss: tensor(0.0320)\n",
      "19815 Training Loss: tensor(0.0320)\n",
      "19816 Training Loss: tensor(0.0319)\n",
      "19817 Training Loss: tensor(0.0320)\n",
      "19818 Training Loss: tensor(0.0319)\n",
      "19819 Training Loss: tensor(0.0320)\n",
      "19820 Training Loss: tensor(0.0319)\n",
      "19821 Training Loss: tensor(0.0320)\n",
      "19822 Training Loss: tensor(0.0320)\n",
      "19823 Training Loss: tensor(0.0319)\n",
      "19824 Training Loss: tensor(0.0319)\n",
      "19825 Training Loss: tensor(0.0320)\n",
      "19826 Training Loss: tensor(0.0319)\n",
      "19827 Training Loss: tensor(0.0318)\n",
      "19828 Training Loss: tensor(0.0320)\n",
      "19829 Training Loss: tensor(0.0320)\n",
      "19830 Training Loss: tensor(0.0320)\n",
      "19831 Training Loss: tensor(0.0319)\n",
      "19832 Training Loss: tensor(0.0319)\n",
      "19833 Training Loss: tensor(0.0320)\n",
      "19834 Training Loss: tensor(0.0318)\n",
      "19835 Training Loss: tensor(0.0318)\n",
      "19836 Training Loss: tensor(0.0319)\n",
      "19837 Training Loss: tensor(0.0319)\n",
      "19838 Training Loss: tensor(0.0319)\n",
      "19839 Training Loss: tensor(0.0321)\n",
      "19840 Training Loss: tensor(0.0319)\n",
      "19841 Training Loss: tensor(0.0322)\n",
      "19842 Training Loss: tensor(0.0320)\n",
      "19843 Training Loss: tensor(0.0318)\n",
      "19844 Training Loss: tensor(0.0318)\n",
      "19845 Training Loss: tensor(0.0319)\n",
      "19846 Training Loss: tensor(0.0319)\n",
      "19847 Training Loss: tensor(0.0318)\n",
      "19848 Training Loss: tensor(0.0319)\n",
      "19849 Training Loss: tensor(0.0318)\n",
      "19850 Training Loss: tensor(0.0319)\n",
      "19851 Training Loss: tensor(0.0319)\n",
      "19852 Training Loss: tensor(0.0319)\n",
      "19853 Training Loss: tensor(0.0320)\n",
      "19854 Training Loss: tensor(0.0320)\n",
      "19855 Training Loss: tensor(0.0320)\n",
      "19856 Training Loss: tensor(0.0320)\n",
      "19857 Training Loss: tensor(0.0319)\n",
      "19858 Training Loss: tensor(0.0320)\n",
      "19859 Training Loss: tensor(0.0319)\n",
      "19860 Training Loss: tensor(0.0321)\n",
      "19861 Training Loss: tensor(0.0321)\n",
      "19862 Training Loss: tensor(0.0320)\n",
      "19863 Training Loss: tensor(0.0318)\n",
      "19864 Training Loss: tensor(0.0319)\n",
      "19865 Training Loss: tensor(0.0319)\n",
      "19866 Training Loss: tensor(0.0319)\n",
      "19867 Training Loss: tensor(0.0319)\n",
      "19868 Training Loss: tensor(0.0319)\n",
      "19869 Training Loss: tensor(0.0319)\n",
      "19870 Training Loss: tensor(0.0319)\n",
      "19871 Training Loss: tensor(0.0319)\n",
      "19872 Training Loss: tensor(0.0320)\n",
      "19873 Training Loss: tensor(0.0319)\n",
      "19874 Training Loss: tensor(0.0318)\n",
      "19875 Training Loss: tensor(0.0319)\n",
      "19876 Training Loss: tensor(0.0320)\n",
      "19877 Training Loss: tensor(0.0319)\n",
      "19878 Training Loss: tensor(0.0318)\n",
      "19879 Training Loss: tensor(0.0319)\n",
      "19880 Training Loss: tensor(0.0319)\n",
      "19881 Training Loss: tensor(0.0319)\n",
      "19882 Training Loss: tensor(0.0320)\n",
      "19883 Training Loss: tensor(0.0319)\n",
      "19884 Training Loss: tensor(0.0319)\n",
      "19885 Training Loss: tensor(0.0319)\n",
      "19886 Training Loss: tensor(0.0320)\n",
      "19887 Training Loss: tensor(0.0318)\n",
      "19888 Training Loss: tensor(0.0319)\n",
      "19889 Training Loss: tensor(0.0321)\n",
      "19890 Training Loss: tensor(0.0319)\n",
      "19891 Training Loss: tensor(0.0319)\n",
      "19892 Training Loss: tensor(0.0319)\n",
      "19893 Training Loss: tensor(0.0321)\n",
      "19894 Training Loss: tensor(0.0321)\n",
      "19895 Training Loss: tensor(0.0319)\n",
      "19896 Training Loss: tensor(0.0320)\n",
      "19897 Training Loss: tensor(0.0320)\n",
      "19898 Training Loss: tensor(0.0319)\n",
      "19899 Training Loss: tensor(0.0320)\n",
      "19900 Training Loss: tensor(0.0319)\n",
      "19901 Training Loss: tensor(0.0321)\n",
      "19902 Training Loss: tensor(0.0318)\n",
      "19903 Training Loss: tensor(0.0319)\n",
      "19904 Training Loss: tensor(0.0319)\n",
      "19905 Training Loss: tensor(0.0318)\n",
      "19906 Training Loss: tensor(0.0320)\n",
      "19907 Training Loss: tensor(0.0319)\n",
      "19908 Training Loss: tensor(0.0319)\n",
      "19909 Training Loss: tensor(0.0319)\n",
      "19910 Training Loss: tensor(0.0319)\n",
      "19911 Training Loss: tensor(0.0318)\n",
      "19912 Training Loss: tensor(0.0319)\n",
      "19913 Training Loss: tensor(0.0321)\n",
      "19914 Training Loss: tensor(0.0320)\n",
      "19915 Training Loss: tensor(0.0319)\n",
      "19916 Training Loss: tensor(0.0320)\n",
      "19917 Training Loss: tensor(0.0319)\n",
      "19918 Training Loss: tensor(0.0319)\n",
      "19919 Training Loss: tensor(0.0318)\n",
      "19920 Training Loss: tensor(0.0319)\n",
      "19921 Training Loss: tensor(0.0319)\n",
      "19922 Training Loss: tensor(0.0319)\n",
      "19923 Training Loss: tensor(0.0319)\n",
      "19924 Training Loss: tensor(0.0318)\n",
      "19925 Training Loss: tensor(0.0322)\n",
      "19926 Training Loss: tensor(0.0319)\n",
      "19927 Training Loss: tensor(0.0319)\n",
      "19928 Training Loss: tensor(0.0319)\n",
      "19929 Training Loss: tensor(0.0319)\n",
      "19930 Training Loss: tensor(0.0319)\n",
      "19931 Training Loss: tensor(0.0319)\n",
      "19932 Training Loss: tensor(0.0320)\n",
      "19933 Training Loss: tensor(0.0319)\n",
      "19934 Training Loss: tensor(0.0318)\n",
      "19935 Training Loss: tensor(0.0319)\n",
      "19936 Training Loss: tensor(0.0319)\n",
      "19937 Training Loss: tensor(0.0319)\n",
      "19938 Training Loss: tensor(0.0318)\n",
      "19939 Training Loss: tensor(0.0318)\n",
      "19940 Training Loss: tensor(0.0318)\n",
      "19941 Training Loss: tensor(0.0318)\n",
      "19942 Training Loss: tensor(0.0320)\n",
      "19943 Training Loss: tensor(0.0319)\n",
      "19944 Training Loss: tensor(0.0319)\n",
      "19945 Training Loss: tensor(0.0320)\n",
      "19946 Training Loss: tensor(0.0319)\n",
      "19947 Training Loss: tensor(0.0319)\n",
      "19948 Training Loss: tensor(0.0320)\n",
      "19949 Training Loss: tensor(0.0319)\n",
      "19950 Training Loss: tensor(0.0320)\n",
      "19951 Training Loss: tensor(0.0319)\n",
      "19952 Training Loss: tensor(0.0318)\n",
      "19953 Training Loss: tensor(0.0319)\n",
      "19954 Training Loss: tensor(0.0319)\n",
      "19955 Training Loss: tensor(0.0320)\n",
      "19956 Training Loss: tensor(0.0319)\n",
      "19957 Training Loss: tensor(0.0319)\n",
      "19958 Training Loss: tensor(0.0319)\n",
      "19959 Training Loss: tensor(0.0318)\n",
      "19960 Training Loss: tensor(0.0320)\n",
      "19961 Training Loss: tensor(0.0319)\n",
      "19962 Training Loss: tensor(0.0318)\n",
      "19963 Training Loss: tensor(0.0318)\n",
      "19964 Training Loss: tensor(0.0319)\n",
      "19965 Training Loss: tensor(0.0319)\n",
      "19966 Training Loss: tensor(0.0319)\n",
      "19967 Training Loss: tensor(0.0319)\n",
      "19968 Training Loss: tensor(0.0319)\n",
      "19969 Training Loss: tensor(0.0320)\n",
      "19970 Training Loss: tensor(0.0319)\n",
      "19971 Training Loss: tensor(0.0318)\n",
      "19972 Training Loss: tensor(0.0318)\n",
      "19973 Training Loss: tensor(0.0318)\n",
      "19974 Training Loss: tensor(0.0319)\n",
      "19975 Training Loss: tensor(0.0319)\n",
      "19976 Training Loss: tensor(0.0319)\n",
      "19977 Training Loss: tensor(0.0319)\n",
      "19978 Training Loss: tensor(0.0320)\n",
      "19979 Training Loss: tensor(0.0318)\n",
      "19980 Training Loss: tensor(0.0321)\n",
      "19981 Training Loss: tensor(0.0319)\n",
      "19982 Training Loss: tensor(0.0319)\n",
      "19983 Training Loss: tensor(0.0320)\n",
      "19984 Training Loss: tensor(0.0320)\n",
      "19985 Training Loss: tensor(0.0319)\n",
      "19986 Training Loss: tensor(0.0319)\n",
      "19987 Training Loss: tensor(0.0319)\n",
      "19988 Training Loss: tensor(0.0319)\n",
      "19989 Training Loss: tensor(0.0320)\n",
      "19990 Training Loss: tensor(0.0319)\n",
      "19991 Training Loss: tensor(0.0319)\n",
      "19992 Training Loss: tensor(0.0318)\n",
      "19993 Training Loss: tensor(0.0320)\n",
      "19994 Training Loss: tensor(0.0319)\n",
      "19995 Training Loss: tensor(0.0319)\n",
      "19996 Training Loss: tensor(0.0319)\n",
      "19997 Training Loss: tensor(0.0319)\n",
      "19998 Training Loss: tensor(0.0318)\n",
      "19999 Training Loss: tensor(0.0318)\n"
     ]
    }
   ],
   "source": [
    "# actural training of the nn model\n",
    "iterations = 20000\n",
    "previous_validation_loss = 99999999.0\n",
    "loss_data = []\n",
    "for epoch in range(iterations):\n",
    "    optimizer.zero_grad() # to make the gradients zero\n",
    "\n",
    "    # loss based on initial boundary conditions\n",
    "    pt_x_ibc = Variable(torch.from_numpy(x_ibc).float(), requires_grad=False).to(device)\n",
    "    pt_y_ibc = Variable(torch.from_numpy(y_ibc).float(), requires_grad=False).to(device)\n",
    "    pt_t_ibc = Variable(torch.from_numpy(t_ibc).float(), requires_grad=False).to(device)\n",
    "    pt_u_ibc = Variable(torch.from_numpy(u_ibc).float(), requires_grad=False).to(device)\n",
    "\n",
    "    net_bc_out = net(pt_x_ibc,pt_y_ibc,pt_t_ibc)\n",
    "    mse_u = mse_cost_function(net_bc_out,pt_u_ibc)\n",
    "\n",
    "    # loss based on PDE\n",
    "    x_collocation = np.random.uniform(low=-0.5, high=0.5, size=(2200,1))\n",
    "    y_collocation = np.random.uniform(low=-0.5, high=0.5, size=(2200,1))\n",
    "    t_collocation = np.random.uniform(low=0.0, high=1.0, size=(2200,1))\n",
    "    all_zeros = np.zeros((2200,1))\n",
    "\n",
    "    pt_x_collocation = Variable(torch.from_numpy(x_collocation).float(), requires_grad=True).to(device)\n",
    "    pt_y_collocation = Variable(torch.from_numpy(y_collocation).float(), requires_grad=True).to(device)\n",
    "    pt_t_collocation = Variable(torch.from_numpy(t_collocation).float(), requires_grad=True).to(device)\n",
    "    pt_all_zeros = Variable(torch.from_numpy(all_zeros).float(), requires_grad=False).to(device)\n",
    "    \n",
    "    f_out = f(pt_x_collocation,pt_y_collocation,pt_t_collocation,net)\n",
    "    mse_f = mse_cost_function(f_out,pt_all_zeros)\n",
    "\n",
    "    # combining the loss function\n",
    "    loss = mse_u + mse_f\n",
    "\n",
    "    loss.backward() # computing gradients using backward propagation\n",
    "    optimizer.step()\n",
    "\n",
    "    with torch.autograd.no_grad():\n",
    "        print(epoch,\"Training Loss:\",loss.data)\n",
    "        loss_data.append(loss.data.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8418cef2b0>]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAWp0lEQVR4nO3de3BcZ3nH8d+zu7rY8kW2pcSOLUeSMQmmuThRndCEW0NJYkpMgJYEWlIuk2ZKuAzDFEM6DAwDTShlmNAUk7YZaIfW4V4DzoQ0KaQtkFghzsVJTBTbxLJjS7Fjy3Zs3fbpH3sk7a5W0kpa6ey7/n5mNHv27Nlznjm7+u2773nPWXN3AQDCl4i7AABAaRDoAFAhCHQAqBAEOgBUCAIdACpEKq4NNzQ0eHNzc1ybB4AgPfLIIy+6e2Ohx2IL9ObmZrW3t8e1eQAIkpn9bqzH6HIBgApBoANAhSDQAaBCEOgAUCEIdACoEAQ6AFQIAh0AKkRwgb7zwDF95Wc79eLx3rhLAYCyElygd3Qd1+0PdOjwib64SwGAshJcoCcsc5vmhzkAIEdwgW6WSfR0OuZCAKDMBBfotNABoLAAAz2T6OQ5AOQKL9CjimmhA0Cu4AJ9uA+dQAeAHMEFemI40GMuBADKTICBnrmlhQ4AuYIL9OTwsEUCHQCyBRfoRpcLABQUXKAPdbk4XS4AkCO8QE/QQgeAQsILdA6KAkBBwQU649ABoLDgAp1T/wGgsAADPXNLCx0AcgUY6JlEH+SoKADkCDbQyXMAyBVeoEcVMw4dAHKFF+i00AGgoAADPXPLQVEAyBVcoDMOHQAKKyrQzewqM9tpZh1mtnGc5X7fzAbN7J2lKzEX49ABoLAJA93MkpLukHS1pDWSrjezNWMsd5uke0tdZDa6XACgsGJa6Oskdbj7Lnfvk7RZ0oYCy31Y0vcldZWwvlE4KAoAhRUT6Msl7c263xnNG2ZmyyVdK2nTeCsysxvNrN3M2ru7uydba7SOzC0/cAEAuYoJdCswLz9Nvyrpk+4+ON6K3P1Od29z97bGxsYiS8yV4KAoABSUKmKZTklNWfdXSNqft0ybpM3RCJQGSevNbMDdf1SKIrMluR46ABRUTKBvk7TazFok7ZN0naR3Zy/g7i1D02b2TUk/mYkwz6w/c0sLHQByTRjo7j5gZjcrM3olKekud99hZjdFj4/bb15qI8MWCXQAyFZMC13uvlXS1rx5BYPc3f9i+mWNjVEuAFBYcGeKMg4dAAoLLtCNFjoAFBRcoA+10OlDB4BcAQY6v1gEAIUEG+jkOQDkCi/Qo4o5KAoAucILdMahA0BBwQY6XS4AkCvAQM/c0uUCALmCC3TGoQNAYcEFupRppdOHDgC5Ag10o8sFAPIEHOhxVwEA5SXIQDfjJ+gAIF+QgZ5M0OUCAPmCDHS6XABgtCAD3Yxx6ACQL8hAT5iJPAeAXIEGOi10AMgXaKBzUBQA8gUZ6MZBUQAYJchA59R/ABgt0EA3foIOAPIEGeiZE4virgIAykuQgc44dAAYLchAZxw6AIwWaKDTQgeAfIEGOn3oAJAvyECnDx0ARgsy0DN96AQ6AGQLNtDT6birAIDyEmSgm0mDtNABIEeQgZ5M0OUCAPmCDHRGuQDAaIEGuriWCwDkCTPQ+ZFoABglyEBPJbjaIgDkCzLQE2YaINABIEdRgW5mV5nZTjPrMLONBR7fYGaPm9l2M2s3s8tLX+qIVNKUJtABIEdqogXMLCnpDkl/JKlT0jYz2+LuT2Utdr+kLe7uZna+pO9IOncmCpakZCKhgfTgTK0eAIJUTAt9naQOd9/l7n2SNkvakL2Aux/3kYHhdZJmtPmcZJQLAIxSTKAvl7Q3635nNC+HmV1rZs9I+qmk9xdakZndGHXJtHd3d0+lXkmZFjqBDgC5igl0KzBvVJq6+w/d/VxJb5P0+UIrcvc73b3N3dsaGxsnVWg2RrkAwGjFBHqnpKas+ysk7R9rYXd/UNIqM2uYZm1jSiZMA1ydCwByFBPo2yStNrMWM6uWdJ2kLdkLmNkrzMyi6YskVUs6VOpihyRpoQPAKBOOcnH3ATO7WdK9kpKS7nL3HWZ2U/T4JknvkPReM+uXdFLSu3wGr56VShhXWwSAPBMGuiS5+1ZJW/Pmbcqavk3SbaUtbWzJhGlwkEAHgGxBnima6UMn0AEgW7CBzsW5ACBXkIGeooUOAKMEGegJ+tABYJQgA51RLgAwWpCBnrk4F4EOANkCDXQuzgUA+QIN9MzFuWbw3CUACE6QgZ5KZK4XRiMdAEYEGejJKNC5QBcAjAg60OlHB4ARQQZ6ikAHgFGCDHRa6AAwWtCBzlh0ABgRdKDTQgeAEUEGOn3oADBakIGeTGTKJtABYESggZ65pQ8dAEYEGui00AEgX5CBTh86AIwWZKAPjXLpH+TUfwAYEmSgV0ed6AQ6AIwIM9BTmbL7Bgh0ABgSZKBXDbfQ6UMHgCFBBvpwC31wMOZKAKB8hBnoSbpcACBfmIEetdB7CXQAGBZmoNOHDgCjhBnojHIBgFECD3QOigLAkLADnROLAGBYkIFelRw69Z8+dAAYEmSgDx0UZZQLAIwIMtDNTNXJBAdFASBLkIEuZfrRCXQAGBFsoFcljastAkCWYAOdFjoA5Coq0M3sKjPbaWYdZraxwOPvMbPHo79fmtkFpS81V21VUqcYhw4AwyYMdDNLSrpD0tWS1ki63szW5C22W9Lr3f18SZ+XdGepC803tzqlE70DM70ZAAhGMS30dZI63H2Xu/dJ2ixpQ/YC7v5Ld38puvtrSStKW+Zo82qSOk6gA8CwYgJ9uaS9Wfc7o3lj+YCke6ZTVDHqalI60UuXCwAMSRWxjBWYV/AUTTN7ozKBfvkYj98o6UZJWrlyZZElFlZXk9Lzh16e1joAoJIU00LvlNSUdX+FpP35C5nZ+ZL+WdIGdz9UaEXufqe7t7l7W2Nj41TqHTavOkWXCwBkKSbQt0labWYtZlYt6TpJW7IXMLOVkn4g6c/d/belL3O0upqUXu6jywUAhkzY5eLuA2Z2s6R7JSUl3eXuO8zspujxTZI+I2mJpH80M0kacPe2mSs7c1D0RN+A3F3RNgHgtFZMH7rcfaukrXnzNmVNf1DSB0tb2vjm11bJXTrWO6AFtVWzuWkAKEvBnil6xoIaSVJXT2/MlQBAeQg30OfXSpK6ek7FXAkAlIdgA/3MqIV+8BiBDgBSwIF+Vv0cpRKmZw8ej7sUACgLwQZ6bVVSr1q2QA/tPhx3KQBQFooa5VKurrngLH1h69O67NYHVFuVUCqRUFXKVJVMRH8j0wvnVGnpglqdVT9HFzbV65yl85VMMNwRQOUIOtDfd1mzEgnT451HNDDoGkinNTDo6htMq38wrd7+tI6fGlDvQFpPnuxX17FeDaYzVy1YuqBWf/n6Vt3wmsw6ACB0QQd6KpnQBy5vKXr5wbRr/5GT2rbnsL7b3qnP/fgpte95Sbdfv5bWOoDgBR3ok5VMmJoWz1XT4rm6du1yfePBXbr1nmd07tL5+vAVq+MuDwCmJdiDotNlZrrp9av0lvOX6WsPdOgg49kBBO60DfQhf33lOepPp/WtX+6JuxQAmJbTPtDPXlKnK849Q997pFPpdMHLvANAEE77QJek9ectU9exXj2+72jcpQDAlBHokv7w3DOUTJjue+pA3KUAwJQR6JLq51brghUL9TBnnQIIGIEeubBpkZ7Yd1T9g+m4SwGAKSHQI2tX1utUf1o7DxyLuxQAmBICPXJhU70k6dG9R2KtAwCmikCPrFg0Rw3zavTo8y/FXQoATAmBHjEzXdhUr+200AEEikDPsnZlvXZ1n9DRl/vjLgUAJo1Az7I26kff3nkk1joAYCoI9CznrVgoM9GPDiBIBHqW+bVVal5Sx9BFAEEi0PO0NtRpV/eJuMsAgEkj0PO0NtZp96ETXHkRQHAI9DytjfPUN5DWviMn4y4FACaFQM/T0lAnSdr9It0uAMJCoOdpbcwE+q7u4zFXAgCTQ6DnaZxXo3k1Ke2ihQ4gMAR6HjPLHBgl0AEEhkAvgKGLAEJEoBfQ0jBP+46c1Kn+wbhLAYCiEegFDB0YpdsFQEgI9AJGRroQ6ADCQaAXMDIWnaGLAMJBoBcwtzqlZQtraaEDCAqBPoaWhjo9Rx86gIAUFehmdpWZ7TSzDjPbWODxc83sV2bWa2afKH2Zs6+1sU67u4/LnYt0AQjDhIFuZklJd0i6WtIaSdeb2Zq8xQ5L+oikL5e8wpi0NsxTz6kBHTrRF3cpAFCUYlro6yR1uPsud++TtFnShuwF3L3L3bdJqpgf42xh6CKAwBQT6Msl7c263xnNmzQzu9HM2s2svbu7eyqrmDWrGuZJ4iJdAMJRTKBbgXlT6lh29zvdvc3d2xobG6eyilmzfNEcSdLmbXsnWBIAykMxgd4pqSnr/gpJ+2emnPKRTGQ+xx59/ki8hQBAkYoJ9G2SVptZi5lVS7pO0paZLau8MNIFQAgmDHR3H5B0s6R7JT0t6TvuvsPMbjKzmyTJzJaaWaekj0v6GzPrNLMFM1n4bPjIFaslSQd7emOuBAAmVtQ4dHff6u6vdPdV7v6FaN4md98UTR9w9xXuvsDd66PpnpksfDa88ZxMP/9Duw/FXAkATIwzRcfx6rMWSpI+unl7vIUAQBEI9HFUp9g9AMJBYhXpqf3B9yABqHAE+gS+9M7zJUnrb/+fmCsBgPER6BP4k4tXDE8zfBFAOSPQJ2A2cqJsy6e2xlgJAIyPQC/Cw7dcMTz9EldfBFCmCPQinDG/VvNrUpKktZ+/T109p2KuCABGI9CL9MTnrhyeXvfF+/W3W5+OsRoAGI1An4Q9t75lePobD+5S88afag/XSwdQJgj0Sdpz61t0zQVnDd9/w5d/ruaNP9VBumEAxMziGorX1tbm7e3tsWy7FNy94KiXbbe8SY3za2KoCMDpwMwecfe2go8R6NOTTrtaPz062J/47Js1v7YqhooAVDICfRaMFezvv6xFn3lr/m9qA8DUEOizaGAwrVfcck/Bx1IJ048+dJnWLFugRKLQL/sBwPgI9BiMF+zZrl+3Ul+89vdyzkgFgLEQ6DF75kCPrvpq8Rf3et9lzbr5ja/Q4rpqgh5ADgK9zPSc6tc1X/tf7Tn08oxto7WhTm9bu1yL66olSResqNeSedVySY3zalSVND4sgAAR6GXO3bXlsf0V/8tIbzinURevXKRLVy3RmfNr1TswqOWL5kiS5lQlJYkPGWACBHoFGBhMq+tYr77+8+d0oOeU7nvqYNwlBefS1sXD31QuaVmiRXOrdcaCGqUSplSSc+wQBgIdk+Luyn5buKSjJ/tlkh7fd1QHj57Sy30DevDZF/XAM11xlVkW3tXWpLvb9+qilfW6pHWJLlixUOcsXaDFc6tVW51QTSoZd4moMAQ6Koa7q28wLXfphaOn1NVzSkdP9uvJfUf1kyde0K7uzLV1UgnTQLpyfpDkfZc161XLFqh5SZ3OW75QNakEQ19PUwQ6UCIn+wb1XPdxPbz7sIa6+z/346fiLWoKLj57kf5g1RJd+eqlOnNBrRbOqeJAeSAIdKDM9Q2k9XLfgLbvPaL/63hR9+44qOcPz9woqJny9ouWa15NSid6B7X+vKVatnCOWhrqVFuVUNolk/hmMU0EOnCa6RtI69CJXj25r0ebH35e95/mxzrGcknLYj20+7Ak6c8uXakldTXa/eIJ1aQSWr5ojs5cUCuTVJ1KaNHcap1VP0e9A4PqH0xr2cI56h9Ma1E0NNgk1UajtdLuSiUSSpjUN5gePpbi7tP+FkSgA5gyd9eJvkF1dB3XU/t7dO+OA/rFb7vjLiton15/rm583aopPXe8QE9NqyoAFc/MNK8mpQub6nVhU73efcnKWOoYanwOpjMfMD0n+3Wg55QSJh042quaVEL3P9OltU31+sGjnapJJfXa1Q36r6cP6te7Dqu1sW74oPkQM2m227RnzK/R0oVzZmTdtNABICDjtdA5mwIAKgSBDgAVgkAHgApBoANAhSDQAaBCEOgAUCEIdACoEAQ6AFSI2E4sMrNuSb+b4tMbJL1YwnJKpVzrksq3NuqaHOqanEqs62x3byz0QGyBPh1m1j7WmVJxKte6pPKtjbomh7om53Sriy4XAKgQBDoAVIhQA/3OuAsYQ7nWJZVvbdQ1OdQ1OadVXUH2oQMARgu1hQ4AyEOgA0CFCC7QzewqM9tpZh1mtnGGt9VkZv9tZk+b2Q4z+2g0/7Nmts/Mtkd/67Oe86motp1mdmXW/IvN7InosdutBD+vbmZ7onVuN7P2aN5iM7vPzJ6NbhfNZm1mdk7WftluZj1m9rE49pmZ3WVmXWb2ZNa8ku0fM6sxs7uj+Q+ZWfM06vo7M3vGzB43sx+aWX00v9nMTmbtt02zXFfJXrcS13V3Vk17zGx7DPtrrHyI7z3m7sH8SUpKek5Sq6RqSY9JWjOD21sm6aJoer6k30paI+mzkj5RYPk1UU01klqiWpPRYw9Leo0yvyV7j6SrS1DfHkkNefO+JGljNL1R0m1x1Jb1eh2QdHYc+0zS6yRdJOnJmdg/kv5K0qZo+jpJd0+jrjdLSkXTt2XV1Zy9XN56ZqOukr1upawr7/G/l/SZGPbXWPkQ23sstBb6Okkd7r7L3fskbZa0YaY25u4vuPtvouljkp6WtHycp2yQtNnde919t6QOSevMbJmkBe7+K8+8Mv8q6W0zVPYGSd+Kpr+VtZ04artC0nPuPt4ZwTNWl7s/KOlwge2Vav9kr+t7kq4o5ltEobrc/WfuPhDd/bWkFeOtY7bqGkes+2tI9Pw/lfQf461jhuoaKx9ie4+FFujLJe3Nut+p8QO2ZKKvOmslPRTNujn6enxX1leqsepbHk3nz58ul/QzM3vEzG6M5p3p7i9ImTecpDNiqk3KtCiy/9HKYZ+Vcv8MPycK46OSlpSgxvcr00ob0mJmj5rZL8zstVnbnq26SvW6zcT+eq2kg+7+bNa8Wd9fefkQ23sstEAv9Mk04+MuzWyepO9L+pi790j6uqRVki6U9IIyX/nGq2+m6r7M3S+SdLWkD5nZ68ZZdlZrM7NqSddI+m40q1z22VimUkfJazSzWyQNSPp2NOsFSSvdfa2kj0v6dzNbMIt1lfJ1m4nX9HrlNhpmfX8VyIcxFx1jOyWrLbRA75TUlHV/haT9M7lBM6tS5sX6trv/QJLc/aC7D7p7WtI/KdMVNF59ncr9Cl2Sut19f3TbJemHUR0Ho69wQ18zu+KoTZkPmd+4+8GoxrLYZyrt/hl+jpmlJC1U8V0Wo5jZDZL+WNJ7oq/eir6eH4qmH1Gm3/WVs1VXiV+3Uu+vlKS3S7o7q95Z3V+F8kExvsdCC/RtklabWUvUArxO0paZ2ljUV/Uvkp52969kzV+Wtdi1koaOvm+RdF10ZLpF0mpJD0dfu46Z2aXROt8r6T+nWVudmc0fmlbmoNqTUQ03RIvdkLWdWastktNyKod9lrW9Uu2f7HW9U9IDQ0E8WWZ2laRPSrrG3V/Omt9oZsloujWqa9cs1lXK161kdUXeJOkZdx/urpjN/TVWPijO99h4R0zL8U/SemWOJj8n6ZYZ3tblyny9eVzS9uhvvaR/k/RENH+LpGVZz7klqm2nskZlSGpT5p/hOUn/oOgs3WnU1qrMEfPHJO0Y2hfK9K/dL+nZ6HZxDLXNlXRI0sKsebO+z5T5QHlBUr8yLZ0PlHL/SKpVpkupQ5lRCq3TqKtDmb7SoffZ0MiGd0Sv72OSfiPprbNcV8let1LWFc3/pqSb8padzf01Vj7E9h7j1H8AqBChdbkAAMZAoANAhSDQAaBCEOgAUCEIdACoEAQ6AFQIAh0AKsT/A9YIZT4MPu0GAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "torch.save(net.state_dict(), \"heat_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-0.5,0.5,0.01)\n",
    "y = np.arange(-0.5,0.5,0.01)\n",
    "t = np.arange(0.0,1.0,0.1)\n",
    "ms_xx, ms_yy, ms_tt = np.meshgrid(x,y,t)\n",
    "x = np.ravel(ms_xx).reshape(-1,1)\n",
    "y = np.ravel(ms_yy).reshape(-1,1)\n",
    "t = np.ravel(ms_tt).reshape(-1,1)\n",
    "\n",
    "pt_x = Variable(torch.from_numpy(x).float(), requires_grad=True).to(device)\n",
    "pt_y = Variable(torch.from_numpy(y).float(), requires_grad=True).to(device)\n",
    "pt_t = Variable(torch.from_numpy(t).float(), requires_grad=True).to(device) \n",
    "pt_u = net(pt_x,pt_y,pt_t)\n",
    "u = pt_u.data.cpu().numpy()\n",
    "ms_uu = u.reshape(ms_xx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    fig = plt.figure()\n",
    "    plt.contourf(np.arange(-0.5,0.5,0.01),np.arange(-0.5,0.5,0.01),ms_uu[:,:,i])\n",
    "    plt.axis('scaled')\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "926ad5c238a4420011e83b59655d6cd21b021d9ee1aa0438d38aa969cfe55744"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch11')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
