{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Linear Laplace equation with nonhomogeneous Dirichlet boundary condition.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Number of inputs and outputs: #inputs = 2 (x,y); #outputs = 1 (u)\n",
    "\n",
    "This NN has 5 hidden layer with 10 neurons each\n",
    "\"\"\"\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.hidden_layer1 = nn.Linear(2,10)\n",
    "        self.hidden_layer2 = nn.Linear(10,10)\n",
    "        self.hidden_layer3 = nn.Linear(10,10)\n",
    "        self.hidden_layer4 = nn.Linear(10,10)\n",
    "        self.hidden_layer5 = nn.Linear(10,10)\n",
    "        self.output_layer = nn.Linear(10,1)\n",
    "\n",
    "    def forward(self,x,y):\n",
    "        inputs = torch.cat([x,y],axis=1)\n",
    "        layer1_out = torch.sigmoid(self.hidden_layer1(inputs))\n",
    "        layer2_out = torch.sigmoid(self.hidden_layer2(layer1_out))\n",
    "        layer3_out = torch.sigmoid(self.hidden_layer3(layer2_out))\n",
    "        layer4_out = torch.sigmoid(self.hidden_layer4(layer3_out))\n",
    "        layer5_out = torch.sigmoid(self.hidden_layer5(layer4_out))\n",
    "        output = self.output_layer(layer5_out) ## For regression, no activation is used in output layer\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a nn model\n",
    "net = Net()\n",
    "net = net.to(device)\n",
    "mse_cost_function = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficient\n",
    "def coef(x,y):\n",
    "    r2 = x**2 + y**2\n",
    "    internal_idx = (r2<=0.5*0.5)\n",
    "    out_idx = (r2>0.5*0.5)\n",
    "    r2[internal_idx] = 20\n",
    "    r2[out_idx] = 1\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The linear Laplace equation\n",
    "def f(x,y,net):\n",
    "    a = coef(x,y)\n",
    "    u = net(x,y)\n",
    "    a_x = torch.autograd.grad(a.sum(), x, create_graph=True)[0]\n",
    "    a_y = torch.autograd.grad(a.sum(), y, create_graph=True)[0]\n",
    "    u_x = torch.autograd.grad(u.sum(), x, create_graph=True)[0]\n",
    "    u_xx = torch.autograd.grad(u_x.sum(), x, create_graph=True)[0]\n",
    "    u_y = torch.autograd.grad(u.sum(), y, create_graph=True)[0]\n",
    "    u_yy = torch.autograd.grad(u_y.sum(), y, create_graph=True)[0]\n",
    "    pde = a_x * u_x + a_y * u_y + a*(u_xx + u_yy) + 1\n",
    "    return pde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundary_points(nb = 50):\n",
    "    '''\n",
    "    nb: number of points along each boundary\n",
    "    '''\n",
    "    x_bc,y_bc,u_bc = np.zeros((nb*4,1)),np.zeros((nb*4,1)),np.zeros((nb*4,1))\n",
    "    interval = 2/nb\n",
    "    \n",
    "    # all boundaries are prescribed as 0\n",
    "    # top\n",
    "    x_bc[0*nb:(0+1)*nb,0] = np.linspace(-1+interval/2,1-interval/2,nb)\n",
    "    y_bc[0*nb:(0+1)*nb,0] = np.full((nb,),1) # y coordinate\n",
    "    # bottom\n",
    "    x_bc[(0+1)*nb:(0+2)*nb,0] = np.linspace(-1+interval/2,1-interval/2,nb)\n",
    "    y_bc[(0+1)*nb:(0+2)*nb,0] = np.full((nb,),-1) # y coordinate\n",
    "    # left\n",
    "    x_bc[(0+2)*nb:(0+3)*nb,0] = np.full((nb,),-1) # x coordinate\n",
    "    y_bc[(0+2)*nb:(0+3)*nb,0] = np.linspace(-1+interval/2,1-interval/2,nb)\n",
    "    # right\n",
    "    x_bc[(0+3)*nb:(0+4)*nb,0] = np.full((nb,),1) # x coordinate\n",
    "    y_bc[(0+3)*nb:(0+4)*nb,0] = np.linspace(-1+interval/2,1-interval/2,nb)\n",
    "    return x_bc,y_bc,u_bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# obtain training data from boundary conditions\n",
    "# boundary condition: all boundaries = 0\n",
    "x_bc,y_bc,u_bc = boundary_points()\n",
    "\n",
    "# plot boundary points\n",
    "plt.Figure()\n",
    "plt.scatter(x_bc,y_bc,s=2,marker=\"x\")\n",
    "axes=plt.gca()\n",
    "axes.set_aspect(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot collocation point distribution\n",
    "x_collocation = np.random.uniform(low=-1, high=1, size=(1800,1))\n",
    "y_collocation = np.random.uniform(low=-1, high=1, size=(1800,1))\n",
    "plt.Figure()\n",
    "plt.scatter(x_collocation,y_collocation,s=2,marker=\"x\")\n",
    "axes=plt.gca()\n",
    "axes.set_aspect(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Training Loss: tensor(1.0666)\n",
      "1 Training Loss: tensor(1.0623)\n",
      "2 Training Loss: tensor(1.0580)\n",
      "3 Training Loss: tensor(1.0539)\n",
      "4 Training Loss: tensor(1.0500)\n",
      "5 Training Loss: tensor(1.0462)\n",
      "6 Training Loss: tensor(1.0426)\n",
      "7 Training Loss: tensor(1.0392)\n",
      "8 Training Loss: tensor(1.0359)\n",
      "9 Training Loss: tensor(1.0328)\n",
      "10 Training Loss: tensor(1.0298)\n",
      "11 Training Loss: tensor(1.0270)\n",
      "12 Training Loss: tensor(1.0244)\n",
      "13 Training Loss: tensor(1.0219)\n",
      "14 Training Loss: tensor(1.0196)\n",
      "15 Training Loss: tensor(1.0174)\n",
      "16 Training Loss: tensor(1.0154)\n",
      "17 Training Loss: tensor(1.0136)\n",
      "18 Training Loss: tensor(1.0119)\n",
      "19 Training Loss: tensor(1.0103)\n",
      "20 Training Loss: tensor(1.0089)\n",
      "21 Training Loss: tensor(1.0076)\n",
      "22 Training Loss: tensor(1.0065)\n",
      "23 Training Loss: tensor(1.0054)\n",
      "24 Training Loss: tensor(1.0045)\n",
      "25 Training Loss: tensor(1.0037)\n",
      "26 Training Loss: tensor(1.0030)\n",
      "27 Training Loss: tensor(1.0024)\n",
      "28 Training Loss: tensor(1.0019)\n",
      "29 Training Loss: tensor(1.0014)\n",
      "30 Training Loss: tensor(1.0011)\n",
      "31 Training Loss: tensor(1.0008)\n",
      "32 Training Loss: tensor(1.0005)\n",
      "33 Training Loss: tensor(1.0003)\n",
      "34 Training Loss: tensor(1.0002)\n",
      "35 Training Loss: tensor(1.0001)\n",
      "36 Training Loss: tensor(1.0000)\n",
      "37 Training Loss: tensor(1.0000)\n",
      "38 Training Loss: tensor(0.9999)\n",
      "39 Training Loss: tensor(0.9999)\n",
      "40 Training Loss: tensor(0.9999)\n",
      "41 Training Loss: tensor(1.0000)\n",
      "42 Training Loss: tensor(1.0000)\n",
      "43 Training Loss: tensor(1.0000)\n",
      "44 Training Loss: tensor(1.0001)\n",
      "45 Training Loss: tensor(1.0001)\n",
      "46 Training Loss: tensor(1.0001)\n",
      "47 Training Loss: tensor(1.0001)\n",
      "48 Training Loss: tensor(1.0002)\n",
      "49 Training Loss: tensor(1.0002)\n",
      "50 Training Loss: tensor(1.0002)\n",
      "51 Training Loss: tensor(1.0002)\n",
      "52 Training Loss: tensor(1.0002)\n",
      "53 Training Loss: tensor(1.0002)\n",
      "54 Training Loss: tensor(1.0002)\n",
      "55 Training Loss: tensor(1.0002)\n",
      "56 Training Loss: tensor(1.0001)\n",
      "57 Training Loss: tensor(1.0001)\n",
      "58 Training Loss: tensor(1.0001)\n",
      "59 Training Loss: tensor(1.0001)\n",
      "60 Training Loss: tensor(1.0001)\n",
      "61 Training Loss: tensor(1.0000)\n",
      "62 Training Loss: tensor(1.0000)\n",
      "63 Training Loss: tensor(1.0000)\n",
      "64 Training Loss: tensor(1.0000)\n",
      "65 Training Loss: tensor(1.0000)\n",
      "66 Training Loss: tensor(0.9999)\n",
      "67 Training Loss: tensor(0.9999)\n",
      "68 Training Loss: tensor(0.9999)\n",
      "69 Training Loss: tensor(0.9999)\n",
      "70 Training Loss: tensor(0.9999)\n",
      "71 Training Loss: tensor(0.9999)\n",
      "72 Training Loss: tensor(0.9999)\n",
      "73 Training Loss: tensor(0.9999)\n",
      "74 Training Loss: tensor(0.9998)\n",
      "75 Training Loss: tensor(0.9998)\n",
      "76 Training Loss: tensor(0.9998)\n",
      "77 Training Loss: tensor(0.9998)\n",
      "78 Training Loss: tensor(0.9998)\n",
      "79 Training Loss: tensor(0.9998)\n",
      "80 Training Loss: tensor(0.9998)\n",
      "81 Training Loss: tensor(0.9998)\n",
      "82 Training Loss: tensor(0.9998)\n",
      "83 Training Loss: tensor(0.9998)\n",
      "84 Training Loss: tensor(0.9998)\n",
      "85 Training Loss: tensor(0.9998)\n",
      "86 Training Loss: tensor(0.9998)\n",
      "87 Training Loss: tensor(0.9997)\n",
      "88 Training Loss: tensor(0.9998)\n",
      "89 Training Loss: tensor(0.9997)\n",
      "90 Training Loss: tensor(0.9998)\n",
      "91 Training Loss: tensor(0.9997)\n",
      "92 Training Loss: tensor(0.9997)\n",
      "93 Training Loss: tensor(0.9997)\n",
      "94 Training Loss: tensor(0.9997)\n",
      "95 Training Loss: tensor(0.9997)\n",
      "96 Training Loss: tensor(0.9997)\n",
      "97 Training Loss: tensor(0.9997)\n",
      "98 Training Loss: tensor(0.9997)\n",
      "99 Training Loss: tensor(0.9997)\n",
      "100 Training Loss: tensor(0.9996)\n",
      "101 Training Loss: tensor(0.9996)\n",
      "102 Training Loss: tensor(0.9996)\n",
      "103 Training Loss: tensor(0.9996)\n",
      "104 Training Loss: tensor(0.9995)\n",
      "105 Training Loss: tensor(0.9996)\n",
      "106 Training Loss: tensor(0.9995)\n",
      "107 Training Loss: tensor(0.9995)\n",
      "108 Training Loss: tensor(0.9995)\n",
      "109 Training Loss: tensor(0.9995)\n",
      "110 Training Loss: tensor(0.9995)\n",
      "111 Training Loss: tensor(0.9995)\n",
      "112 Training Loss: tensor(0.9995)\n",
      "113 Training Loss: tensor(0.9994)\n",
      "114 Training Loss: tensor(0.9994)\n",
      "115 Training Loss: tensor(0.9994)\n",
      "116 Training Loss: tensor(0.9993)\n",
      "117 Training Loss: tensor(0.9993)\n",
      "118 Training Loss: tensor(0.9993)\n",
      "119 Training Loss: tensor(0.9993)\n",
      "120 Training Loss: tensor(0.9992)\n",
      "121 Training Loss: tensor(0.9992)\n",
      "122 Training Loss: tensor(0.9992)\n",
      "123 Training Loss: tensor(0.9992)\n",
      "124 Training Loss: tensor(0.9992)\n",
      "125 Training Loss: tensor(0.9991)\n",
      "126 Training Loss: tensor(0.9991)\n",
      "127 Training Loss: tensor(0.9990)\n",
      "128 Training Loss: tensor(0.9990)\n",
      "129 Training Loss: tensor(0.9988)\n",
      "130 Training Loss: tensor(0.9989)\n",
      "131 Training Loss: tensor(0.9988)\n",
      "132 Training Loss: tensor(0.9988)\n",
      "133 Training Loss: tensor(0.9988)\n",
      "134 Training Loss: tensor(0.9987)\n",
      "135 Training Loss: tensor(0.9988)\n",
      "136 Training Loss: tensor(0.9986)\n",
      "137 Training Loss: tensor(0.9985)\n",
      "138 Training Loss: tensor(0.9986)\n",
      "139 Training Loss: tensor(0.9985)\n",
      "140 Training Loss: tensor(0.9984)\n",
      "141 Training Loss: tensor(0.9982)\n",
      "142 Training Loss: tensor(0.9982)\n",
      "143 Training Loss: tensor(0.9982)\n",
      "144 Training Loss: tensor(0.9981)\n",
      "145 Training Loss: tensor(0.9979)\n",
      "146 Training Loss: tensor(0.9980)\n",
      "147 Training Loss: tensor(0.9977)\n",
      "148 Training Loss: tensor(0.9977)\n",
      "149 Training Loss: tensor(0.9976)\n",
      "150 Training Loss: tensor(0.9975)\n",
      "151 Training Loss: tensor(0.9976)\n",
      "152 Training Loss: tensor(0.9971)\n",
      "153 Training Loss: tensor(0.9973)\n",
      "154 Training Loss: tensor(0.9968)\n",
      "155 Training Loss: tensor(0.9969)\n",
      "156 Training Loss: tensor(0.9966)\n",
      "157 Training Loss: tensor(0.9965)\n",
      "158 Training Loss: tensor(0.9966)\n",
      "159 Training Loss: tensor(0.9962)\n",
      "160 Training Loss: tensor(0.9960)\n",
      "161 Training Loss: tensor(0.9960)\n",
      "162 Training Loss: tensor(0.9956)\n",
      "163 Training Loss: tensor(0.9954)\n",
      "164 Training Loss: tensor(0.9952)\n",
      "165 Training Loss: tensor(0.9950)\n",
      "166 Training Loss: tensor(0.9945)\n",
      "167 Training Loss: tensor(0.9946)\n",
      "168 Training Loss: tensor(0.9942)\n",
      "169 Training Loss: tensor(0.9934)\n",
      "170 Training Loss: tensor(0.9933)\n",
      "171 Training Loss: tensor(0.9930)\n",
      "172 Training Loss: tensor(0.9936)\n",
      "173 Training Loss: tensor(0.9923)\n",
      "174 Training Loss: tensor(0.9919)\n",
      "175 Training Loss: tensor(0.9916)\n",
      "176 Training Loss: tensor(0.9911)\n",
      "177 Training Loss: tensor(0.9906)\n",
      "178 Training Loss: tensor(0.9902)\n",
      "179 Training Loss: tensor(0.9892)\n",
      "180 Training Loss: tensor(0.9890)\n",
      "181 Training Loss: tensor(0.9887)\n",
      "182 Training Loss: tensor(0.9880)\n",
      "183 Training Loss: tensor(0.9872)\n",
      "184 Training Loss: tensor(0.9873)\n",
      "185 Training Loss: tensor(0.9861)\n",
      "186 Training Loss: tensor(0.9854)\n",
      "187 Training Loss: tensor(0.9843)\n",
      "188 Training Loss: tensor(0.9841)\n",
      "189 Training Loss: tensor(0.9831)\n",
      "190 Training Loss: tensor(0.9831)\n",
      "191 Training Loss: tensor(0.9811)\n",
      "192 Training Loss: tensor(0.9800)\n",
      "193 Training Loss: tensor(0.9800)\n",
      "194 Training Loss: tensor(0.9784)\n",
      "195 Training Loss: tensor(0.9766)\n",
      "196 Training Loss: tensor(0.9741)\n",
      "197 Training Loss: tensor(0.9733)\n",
      "198 Training Loss: tensor(0.9736)\n",
      "199 Training Loss: tensor(0.9701)\n",
      "200 Training Loss: tensor(0.9705)\n",
      "201 Training Loss: tensor(0.9679)\n",
      "202 Training Loss: tensor(0.9673)\n",
      "203 Training Loss: tensor(0.9632)\n",
      "204 Training Loss: tensor(0.9608)\n",
      "205 Training Loss: tensor(0.9602)\n",
      "206 Training Loss: tensor(0.9584)\n",
      "207 Training Loss: tensor(0.9552)\n",
      "208 Training Loss: tensor(0.9535)\n",
      "209 Training Loss: tensor(0.9466)\n",
      "210 Training Loss: tensor(0.9493)\n",
      "211 Training Loss: tensor(0.9468)\n",
      "212 Training Loss: tensor(0.9434)\n",
      "213 Training Loss: tensor(0.9350)\n",
      "214 Training Loss: tensor(0.9360)\n",
      "215 Training Loss: tensor(0.9320)\n",
      "216 Training Loss: tensor(0.9253)\n",
      "217 Training Loss: tensor(0.9193)\n",
      "218 Training Loss: tensor(0.9220)\n",
      "219 Training Loss: tensor(0.9151)\n",
      "220 Training Loss: tensor(0.9100)\n",
      "221 Training Loss: tensor(0.9097)\n",
      "222 Training Loss: tensor(0.9014)\n",
      "223 Training Loss: tensor(0.8962)\n",
      "224 Training Loss: tensor(0.8872)\n",
      "225 Training Loss: tensor(0.8866)\n",
      "226 Training Loss: tensor(0.8818)\n",
      "227 Training Loss: tensor(0.8657)\n",
      "228 Training Loss: tensor(0.8755)\n",
      "229 Training Loss: tensor(0.8586)\n",
      "230 Training Loss: tensor(0.8488)\n",
      "231 Training Loss: tensor(0.8531)\n",
      "232 Training Loss: tensor(0.8471)\n",
      "233 Training Loss: tensor(0.8431)\n",
      "234 Training Loss: tensor(0.8310)\n",
      "235 Training Loss: tensor(0.8153)\n",
      "236 Training Loss: tensor(0.8168)\n",
      "237 Training Loss: tensor(0.8127)\n",
      "238 Training Loss: tensor(0.8150)\n",
      "239 Training Loss: tensor(0.7990)\n",
      "240 Training Loss: tensor(0.7830)\n",
      "241 Training Loss: tensor(0.7762)\n",
      "242 Training Loss: tensor(0.7696)\n",
      "243 Training Loss: tensor(0.7743)\n",
      "244 Training Loss: tensor(0.7744)\n",
      "245 Training Loss: tensor(0.7657)\n",
      "246 Training Loss: tensor(0.7674)\n",
      "247 Training Loss: tensor(0.7487)\n",
      "248 Training Loss: tensor(0.7586)\n",
      "249 Training Loss: tensor(0.7614)\n",
      "250 Training Loss: tensor(0.7530)\n",
      "251 Training Loss: tensor(0.7550)\n",
      "252 Training Loss: tensor(0.7393)\n",
      "253 Training Loss: tensor(0.7574)\n",
      "254 Training Loss: tensor(0.7623)\n",
      "255 Training Loss: tensor(0.7414)\n",
      "256 Training Loss: tensor(0.7584)\n",
      "257 Training Loss: tensor(0.7651)\n",
      "258 Training Loss: tensor(0.7367)\n",
      "259 Training Loss: tensor(0.7463)\n",
      "260 Training Loss: tensor(0.7558)\n",
      "261 Training Loss: tensor(0.7505)\n",
      "262 Training Loss: tensor(0.7562)\n",
      "263 Training Loss: tensor(0.7540)\n",
      "264 Training Loss: tensor(0.7567)\n",
      "265 Training Loss: tensor(0.7424)\n",
      "266 Training Loss: tensor(0.7521)\n",
      "267 Training Loss: tensor(0.7350)\n",
      "268 Training Loss: tensor(0.7392)\n",
      "269 Training Loss: tensor(0.7428)\n",
      "270 Training Loss: tensor(0.7348)\n",
      "271 Training Loss: tensor(0.7378)\n",
      "272 Training Loss: tensor(0.7311)\n",
      "273 Training Loss: tensor(0.7331)\n",
      "274 Training Loss: tensor(0.7440)\n",
      "275 Training Loss: tensor(0.7382)\n",
      "276 Training Loss: tensor(0.7357)\n",
      "277 Training Loss: tensor(0.7481)\n",
      "278 Training Loss: tensor(0.7525)\n",
      "279 Training Loss: tensor(0.7333)\n",
      "280 Training Loss: tensor(0.7574)\n",
      "281 Training Loss: tensor(0.7381)\n",
      "282 Training Loss: tensor(0.7366)\n",
      "283 Training Loss: tensor(0.7463)\n",
      "284 Training Loss: tensor(0.7354)\n",
      "285 Training Loss: tensor(0.7424)\n",
      "286 Training Loss: tensor(0.7455)\n",
      "287 Training Loss: tensor(0.7536)\n",
      "288 Training Loss: tensor(0.7367)\n",
      "289 Training Loss: tensor(0.7257)\n",
      "290 Training Loss: tensor(0.7483)\n",
      "291 Training Loss: tensor(0.7364)\n",
      "292 Training Loss: tensor(0.7477)\n",
      "293 Training Loss: tensor(0.7451)\n",
      "294 Training Loss: tensor(0.7447)\n",
      "295 Training Loss: tensor(0.7304)\n",
      "296 Training Loss: tensor(0.7240)\n",
      "297 Training Loss: tensor(0.7363)\n",
      "298 Training Loss: tensor(0.7347)\n",
      "299 Training Loss: tensor(0.7358)\n",
      "300 Training Loss: tensor(0.7417)\n",
      "301 Training Loss: tensor(0.7458)\n",
      "302 Training Loss: tensor(0.7390)\n",
      "303 Training Loss: tensor(0.7446)\n",
      "304 Training Loss: tensor(0.7532)\n",
      "305 Training Loss: tensor(0.7346)\n",
      "306 Training Loss: tensor(0.7363)\n",
      "307 Training Loss: tensor(0.7501)\n",
      "308 Training Loss: tensor(0.7471)\n",
      "309 Training Loss: tensor(0.7441)\n",
      "310 Training Loss: tensor(0.7323)\n",
      "311 Training Loss: tensor(0.7453)\n",
      "312 Training Loss: tensor(0.7309)\n",
      "313 Training Loss: tensor(0.7389)\n",
      "314 Training Loss: tensor(0.7366)\n",
      "315 Training Loss: tensor(0.7438)\n",
      "316 Training Loss: tensor(0.7428)\n",
      "317 Training Loss: tensor(0.7505)\n",
      "318 Training Loss: tensor(0.7440)\n",
      "319 Training Loss: tensor(0.7455)\n",
      "320 Training Loss: tensor(0.7319)\n",
      "321 Training Loss: tensor(0.7471)\n",
      "322 Training Loss: tensor(0.7554)\n",
      "323 Training Loss: tensor(0.7322)\n",
      "324 Training Loss: tensor(0.7460)\n",
      "325 Training Loss: tensor(0.7521)\n",
      "326 Training Loss: tensor(0.7503)\n",
      "327 Training Loss: tensor(0.7321)\n",
      "328 Training Loss: tensor(0.7279)\n",
      "329 Training Loss: tensor(0.7380)\n",
      "330 Training Loss: tensor(0.7340)\n",
      "331 Training Loss: tensor(0.7476)\n",
      "332 Training Loss: tensor(0.7482)\n",
      "333 Training Loss: tensor(0.7330)\n",
      "334 Training Loss: tensor(0.7265)\n",
      "335 Training Loss: tensor(0.7448)\n",
      "336 Training Loss: tensor(0.7495)\n",
      "337 Training Loss: tensor(0.7173)\n",
      "338 Training Loss: tensor(0.7331)\n",
      "339 Training Loss: tensor(0.7343)\n",
      "340 Training Loss: tensor(0.7424)\n",
      "341 Training Loss: tensor(0.7342)\n",
      "342 Training Loss: tensor(0.7457)\n",
      "343 Training Loss: tensor(0.7216)\n",
      "344 Training Loss: tensor(0.7373)\n",
      "345 Training Loss: tensor(0.7283)\n",
      "346 Training Loss: tensor(0.7327)\n",
      "347 Training Loss: tensor(0.7386)\n",
      "348 Training Loss: tensor(0.7501)\n",
      "349 Training Loss: tensor(0.7502)\n",
      "350 Training Loss: tensor(0.7430)\n",
      "351 Training Loss: tensor(0.7387)\n",
      "352 Training Loss: tensor(0.7448)\n",
      "353 Training Loss: tensor(0.7411)\n",
      "354 Training Loss: tensor(0.7445)\n",
      "355 Training Loss: tensor(0.7297)\n",
      "356 Training Loss: tensor(0.7343)\n",
      "357 Training Loss: tensor(0.7223)\n",
      "358 Training Loss: tensor(0.7300)\n",
      "359 Training Loss: tensor(0.7199)\n",
      "360 Training Loss: tensor(0.7384)\n",
      "361 Training Loss: tensor(0.7211)\n",
      "362 Training Loss: tensor(0.7375)\n",
      "363 Training Loss: tensor(0.7444)\n",
      "364 Training Loss: tensor(0.7256)\n",
      "365 Training Loss: tensor(0.7480)\n",
      "366 Training Loss: tensor(0.7302)\n",
      "367 Training Loss: tensor(0.7269)\n",
      "368 Training Loss: tensor(0.7279)\n",
      "369 Training Loss: tensor(0.7475)\n",
      "370 Training Loss: tensor(0.7289)\n",
      "371 Training Loss: tensor(0.7392)\n",
      "372 Training Loss: tensor(0.7315)\n",
      "373 Training Loss: tensor(0.7277)\n",
      "374 Training Loss: tensor(0.7372)\n",
      "375 Training Loss: tensor(0.7302)\n",
      "376 Training Loss: tensor(0.7412)\n",
      "377 Training Loss: tensor(0.7257)\n",
      "378 Training Loss: tensor(0.7526)\n",
      "379 Training Loss: tensor(0.7319)\n",
      "380 Training Loss: tensor(0.7235)\n",
      "381 Training Loss: tensor(0.7540)\n",
      "382 Training Loss: tensor(0.7405)\n",
      "383 Training Loss: tensor(0.7549)\n",
      "384 Training Loss: tensor(0.7257)\n",
      "385 Training Loss: tensor(0.7432)\n",
      "386 Training Loss: tensor(0.7303)\n",
      "387 Training Loss: tensor(0.7416)\n",
      "388 Training Loss: tensor(0.7315)\n",
      "389 Training Loss: tensor(0.7384)\n",
      "390 Training Loss: tensor(0.7280)\n",
      "391 Training Loss: tensor(0.7362)\n",
      "392 Training Loss: tensor(0.7412)\n",
      "393 Training Loss: tensor(0.7392)\n",
      "394 Training Loss: tensor(0.7363)\n",
      "395 Training Loss: tensor(0.7336)\n",
      "396 Training Loss: tensor(0.7196)\n",
      "397 Training Loss: tensor(0.7318)\n",
      "398 Training Loss: tensor(0.7201)\n",
      "399 Training Loss: tensor(0.7356)\n",
      "400 Training Loss: tensor(0.7451)\n",
      "401 Training Loss: tensor(0.7205)\n",
      "402 Training Loss: tensor(0.7345)\n",
      "403 Training Loss: tensor(0.7368)\n",
      "404 Training Loss: tensor(0.7394)\n",
      "405 Training Loss: tensor(0.7197)\n",
      "406 Training Loss: tensor(0.7304)\n",
      "407 Training Loss: tensor(0.7286)\n",
      "408 Training Loss: tensor(0.7426)\n",
      "409 Training Loss: tensor(0.7356)\n",
      "410 Training Loss: tensor(0.7338)\n",
      "411 Training Loss: tensor(0.7278)\n",
      "412 Training Loss: tensor(0.7393)\n",
      "413 Training Loss: tensor(0.7239)\n",
      "414 Training Loss: tensor(0.7339)\n",
      "415 Training Loss: tensor(0.7367)\n",
      "416 Training Loss: tensor(0.7245)\n",
      "417 Training Loss: tensor(0.7363)\n",
      "418 Training Loss: tensor(0.7238)\n",
      "419 Training Loss: tensor(0.7420)\n",
      "420 Training Loss: tensor(0.7325)\n",
      "421 Training Loss: tensor(0.7413)\n",
      "422 Training Loss: tensor(0.7321)\n",
      "423 Training Loss: tensor(0.7293)\n",
      "424 Training Loss: tensor(0.7387)\n",
      "425 Training Loss: tensor(0.7313)\n",
      "426 Training Loss: tensor(0.7411)\n",
      "427 Training Loss: tensor(0.7334)\n",
      "428 Training Loss: tensor(0.7397)\n",
      "429 Training Loss: tensor(0.7471)\n",
      "430 Training Loss: tensor(0.7327)\n",
      "431 Training Loss: tensor(0.7076)\n",
      "432 Training Loss: tensor(0.7317)\n",
      "433 Training Loss: tensor(0.7411)\n",
      "434 Training Loss: tensor(0.7419)\n",
      "435 Training Loss: tensor(0.7324)\n",
      "436 Training Loss: tensor(0.7285)\n",
      "437 Training Loss: tensor(0.7467)\n",
      "438 Training Loss: tensor(0.7187)\n",
      "439 Training Loss: tensor(0.7319)\n",
      "440 Training Loss: tensor(0.7312)\n",
      "441 Training Loss: tensor(0.7316)\n",
      "442 Training Loss: tensor(0.7380)\n",
      "443 Training Loss: tensor(0.7240)\n",
      "444 Training Loss: tensor(0.7401)\n",
      "445 Training Loss: tensor(0.7266)\n",
      "446 Training Loss: tensor(0.7249)\n",
      "447 Training Loss: tensor(0.7373)\n",
      "448 Training Loss: tensor(0.7319)\n",
      "449 Training Loss: tensor(0.7328)\n",
      "450 Training Loss: tensor(0.7233)\n",
      "451 Training Loss: tensor(0.7420)\n",
      "452 Training Loss: tensor(0.7534)\n",
      "453 Training Loss: tensor(0.7473)\n",
      "454 Training Loss: tensor(0.7413)\n",
      "455 Training Loss: tensor(0.7205)\n",
      "456 Training Loss: tensor(0.7276)\n",
      "457 Training Loss: tensor(0.7177)\n",
      "458 Training Loss: tensor(0.7305)\n",
      "459 Training Loss: tensor(0.7333)\n",
      "460 Training Loss: tensor(0.7321)\n",
      "461 Training Loss: tensor(0.7432)\n",
      "462 Training Loss: tensor(0.7205)\n",
      "463 Training Loss: tensor(0.7356)\n",
      "464 Training Loss: tensor(0.7254)\n",
      "465 Training Loss: tensor(0.7256)\n",
      "466 Training Loss: tensor(0.7234)\n",
      "467 Training Loss: tensor(0.7261)\n",
      "468 Training Loss: tensor(0.7285)\n",
      "469 Training Loss: tensor(0.7327)\n",
      "470 Training Loss: tensor(0.7238)\n",
      "471 Training Loss: tensor(0.7479)\n",
      "472 Training Loss: tensor(0.7380)\n",
      "473 Training Loss: tensor(0.7350)\n",
      "474 Training Loss: tensor(0.7384)\n",
      "475 Training Loss: tensor(0.7388)\n",
      "476 Training Loss: tensor(0.7338)\n",
      "477 Training Loss: tensor(0.7240)\n",
      "478 Training Loss: tensor(0.7394)\n",
      "479 Training Loss: tensor(0.7344)\n",
      "480 Training Loss: tensor(0.7490)\n",
      "481 Training Loss: tensor(0.7356)\n",
      "482 Training Loss: tensor(0.7258)\n",
      "483 Training Loss: tensor(0.7351)\n",
      "484 Training Loss: tensor(0.7202)\n",
      "485 Training Loss: tensor(0.7358)\n",
      "486 Training Loss: tensor(0.7221)\n",
      "487 Training Loss: tensor(0.7347)\n",
      "488 Training Loss: tensor(0.7467)\n",
      "489 Training Loss: tensor(0.7327)\n",
      "490 Training Loss: tensor(0.7243)\n",
      "491 Training Loss: tensor(0.7415)\n",
      "492 Training Loss: tensor(0.7322)\n",
      "493 Training Loss: tensor(0.7317)\n",
      "494 Training Loss: tensor(0.7319)\n",
      "495 Training Loss: tensor(0.7333)\n",
      "496 Training Loss: tensor(0.7444)\n",
      "497 Training Loss: tensor(0.7313)\n",
      "498 Training Loss: tensor(0.7177)\n",
      "499 Training Loss: tensor(0.7243)\n",
      "500 Training Loss: tensor(0.7193)\n",
      "501 Training Loss: tensor(0.7265)\n",
      "502 Training Loss: tensor(0.7250)\n",
      "503 Training Loss: tensor(0.7308)\n",
      "504 Training Loss: tensor(0.7141)\n",
      "505 Training Loss: tensor(0.7360)\n",
      "506 Training Loss: tensor(0.7402)\n",
      "507 Training Loss: tensor(0.7190)\n",
      "508 Training Loss: tensor(0.7290)\n",
      "509 Training Loss: tensor(0.7339)\n",
      "510 Training Loss: tensor(0.7310)\n",
      "511 Training Loss: tensor(0.7329)\n",
      "512 Training Loss: tensor(0.7210)\n",
      "513 Training Loss: tensor(0.7257)\n",
      "514 Training Loss: tensor(0.7311)\n",
      "515 Training Loss: tensor(0.7275)\n",
      "516 Training Loss: tensor(0.7221)\n",
      "517 Training Loss: tensor(0.7288)\n",
      "518 Training Loss: tensor(0.7193)\n",
      "519 Training Loss: tensor(0.7200)\n",
      "520 Training Loss: tensor(0.7246)\n",
      "521 Training Loss: tensor(0.7211)\n",
      "522 Training Loss: tensor(0.7372)\n",
      "523 Training Loss: tensor(0.7218)\n",
      "524 Training Loss: tensor(0.7419)\n",
      "525 Training Loss: tensor(0.7267)\n",
      "526 Training Loss: tensor(0.7319)\n",
      "527 Training Loss: tensor(0.7282)\n",
      "528 Training Loss: tensor(0.7233)\n",
      "529 Training Loss: tensor(0.7296)\n",
      "530 Training Loss: tensor(0.7384)\n",
      "531 Training Loss: tensor(0.7261)\n",
      "532 Training Loss: tensor(0.7239)\n",
      "533 Training Loss: tensor(0.7448)\n",
      "534 Training Loss: tensor(0.7306)\n",
      "535 Training Loss: tensor(0.7238)\n",
      "536 Training Loss: tensor(0.7391)\n",
      "537 Training Loss: tensor(0.7149)\n",
      "538 Training Loss: tensor(0.7256)\n",
      "539 Training Loss: tensor(0.7265)\n",
      "540 Training Loss: tensor(0.7187)\n",
      "541 Training Loss: tensor(0.7194)\n",
      "542 Training Loss: tensor(0.7215)\n",
      "543 Training Loss: tensor(0.7277)\n",
      "544 Training Loss: tensor(0.7267)\n",
      "545 Training Loss: tensor(0.7341)\n",
      "546 Training Loss: tensor(0.7351)\n",
      "547 Training Loss: tensor(0.7234)\n",
      "548 Training Loss: tensor(0.7284)\n",
      "549 Training Loss: tensor(0.7414)\n",
      "550 Training Loss: tensor(0.7248)\n",
      "551 Training Loss: tensor(0.7178)\n",
      "552 Training Loss: tensor(0.7371)\n",
      "553 Training Loss: tensor(0.7333)\n",
      "554 Training Loss: tensor(0.7289)\n",
      "555 Training Loss: tensor(0.7301)\n",
      "556 Training Loss: tensor(0.7231)\n",
      "557 Training Loss: tensor(0.7204)\n",
      "558 Training Loss: tensor(0.7279)\n",
      "559 Training Loss: tensor(0.7169)\n",
      "560 Training Loss: tensor(0.7431)\n",
      "561 Training Loss: tensor(0.7218)\n",
      "562 Training Loss: tensor(0.7199)\n",
      "563 Training Loss: tensor(0.7340)\n",
      "564 Training Loss: tensor(0.7247)\n",
      "565 Training Loss: tensor(0.7235)\n",
      "566 Training Loss: tensor(0.7124)\n",
      "567 Training Loss: tensor(0.7182)\n",
      "568 Training Loss: tensor(0.7408)\n",
      "569 Training Loss: tensor(0.7223)\n",
      "570 Training Loss: tensor(0.7260)\n",
      "571 Training Loss: tensor(0.7204)\n",
      "572 Training Loss: tensor(0.7398)\n",
      "573 Training Loss: tensor(0.7096)\n",
      "574 Training Loss: tensor(0.7201)\n",
      "575 Training Loss: tensor(0.7179)\n",
      "576 Training Loss: tensor(0.7268)\n",
      "577 Training Loss: tensor(0.7223)\n",
      "578 Training Loss: tensor(0.7136)\n",
      "579 Training Loss: tensor(0.7174)\n",
      "580 Training Loss: tensor(0.7251)\n",
      "581 Training Loss: tensor(0.7351)\n",
      "582 Training Loss: tensor(0.7307)\n",
      "583 Training Loss: tensor(0.7318)\n",
      "584 Training Loss: tensor(0.7356)\n",
      "585 Training Loss: tensor(0.7114)\n",
      "586 Training Loss: tensor(0.7158)\n",
      "587 Training Loss: tensor(0.7202)\n",
      "588 Training Loss: tensor(0.7221)\n",
      "589 Training Loss: tensor(0.7210)\n",
      "590 Training Loss: tensor(0.7274)\n",
      "591 Training Loss: tensor(0.7351)\n",
      "592 Training Loss: tensor(0.7225)\n",
      "593 Training Loss: tensor(0.7312)\n",
      "594 Training Loss: tensor(0.7248)\n",
      "595 Training Loss: tensor(0.7261)\n",
      "596 Training Loss: tensor(0.7206)\n",
      "597 Training Loss: tensor(0.7207)\n",
      "598 Training Loss: tensor(0.7080)\n",
      "599 Training Loss: tensor(0.7036)\n",
      "600 Training Loss: tensor(0.7228)\n",
      "601 Training Loss: tensor(0.7113)\n",
      "602 Training Loss: tensor(0.7303)\n",
      "603 Training Loss: tensor(0.7294)\n",
      "604 Training Loss: tensor(0.7038)\n",
      "605 Training Loss: tensor(0.7164)\n",
      "606 Training Loss: tensor(0.7117)\n",
      "607 Training Loss: tensor(0.7224)\n",
      "608 Training Loss: tensor(0.7322)\n",
      "609 Training Loss: tensor(0.7121)\n",
      "610 Training Loss: tensor(0.7241)\n",
      "611 Training Loss: tensor(0.7455)\n",
      "612 Training Loss: tensor(0.7179)\n",
      "613 Training Loss: tensor(0.7141)\n",
      "614 Training Loss: tensor(0.7139)\n",
      "615 Training Loss: tensor(0.7111)\n",
      "616 Training Loss: tensor(0.7256)\n",
      "617 Training Loss: tensor(0.7183)\n",
      "618 Training Loss: tensor(0.7262)\n",
      "619 Training Loss: tensor(0.7162)\n",
      "620 Training Loss: tensor(0.7203)\n",
      "621 Training Loss: tensor(0.7034)\n",
      "622 Training Loss: tensor(0.7118)\n",
      "623 Training Loss: tensor(0.7253)\n",
      "624 Training Loss: tensor(0.7166)\n",
      "625 Training Loss: tensor(0.7305)\n",
      "626 Training Loss: tensor(0.7374)\n",
      "627 Training Loss: tensor(0.7170)\n",
      "628 Training Loss: tensor(0.7242)\n",
      "629 Training Loss: tensor(0.7132)\n",
      "630 Training Loss: tensor(0.7326)\n",
      "631 Training Loss: tensor(0.7081)\n",
      "632 Training Loss: tensor(0.7185)\n",
      "633 Training Loss: tensor(0.7190)\n",
      "634 Training Loss: tensor(0.7259)\n",
      "635 Training Loss: tensor(0.7182)\n",
      "636 Training Loss: tensor(0.7345)\n",
      "637 Training Loss: tensor(0.7027)\n",
      "638 Training Loss: tensor(0.7010)\n",
      "639 Training Loss: tensor(0.7078)\n",
      "640 Training Loss: tensor(0.7175)\n",
      "641 Training Loss: tensor(0.7279)\n",
      "642 Training Loss: tensor(0.7127)\n",
      "643 Training Loss: tensor(0.7198)\n",
      "644 Training Loss: tensor(0.7169)\n",
      "645 Training Loss: tensor(0.7228)\n",
      "646 Training Loss: tensor(0.7126)\n",
      "647 Training Loss: tensor(0.7129)\n",
      "648 Training Loss: tensor(0.7187)\n",
      "649 Training Loss: tensor(0.7144)\n",
      "650 Training Loss: tensor(0.7037)\n",
      "651 Training Loss: tensor(0.7244)\n",
      "652 Training Loss: tensor(0.7167)\n",
      "653 Training Loss: tensor(0.7215)\n",
      "654 Training Loss: tensor(0.7299)\n",
      "655 Training Loss: tensor(0.7111)\n",
      "656 Training Loss: tensor(0.7374)\n",
      "657 Training Loss: tensor(0.7179)\n",
      "658 Training Loss: tensor(0.7067)\n",
      "659 Training Loss: tensor(0.7224)\n",
      "660 Training Loss: tensor(0.7138)\n",
      "661 Training Loss: tensor(0.7356)\n",
      "662 Training Loss: tensor(0.7155)\n",
      "663 Training Loss: tensor(0.7311)\n",
      "664 Training Loss: tensor(0.7076)\n",
      "665 Training Loss: tensor(0.7136)\n",
      "666 Training Loss: tensor(0.7176)\n",
      "667 Training Loss: tensor(0.7059)\n",
      "668 Training Loss: tensor(0.7106)\n",
      "669 Training Loss: tensor(0.7210)\n",
      "670 Training Loss: tensor(0.7200)\n",
      "671 Training Loss: tensor(0.7134)\n",
      "672 Training Loss: tensor(0.7202)\n",
      "673 Training Loss: tensor(0.6992)\n",
      "674 Training Loss: tensor(0.7154)\n",
      "675 Training Loss: tensor(0.7237)\n",
      "676 Training Loss: tensor(0.7272)\n",
      "677 Training Loss: tensor(0.7237)\n",
      "678 Training Loss: tensor(0.7051)\n",
      "679 Training Loss: tensor(0.7066)\n",
      "680 Training Loss: tensor(0.7036)\n",
      "681 Training Loss: tensor(0.7043)\n",
      "682 Training Loss: tensor(0.7176)\n",
      "683 Training Loss: tensor(0.7143)\n",
      "684 Training Loss: tensor(0.7202)\n",
      "685 Training Loss: tensor(0.7253)\n",
      "686 Training Loss: tensor(0.7152)\n",
      "687 Training Loss: tensor(0.7147)\n",
      "688 Training Loss: tensor(0.7071)\n",
      "689 Training Loss: tensor(0.7166)\n",
      "690 Training Loss: tensor(0.7358)\n",
      "691 Training Loss: tensor(0.7188)\n",
      "692 Training Loss: tensor(0.7286)\n",
      "693 Training Loss: tensor(0.7248)\n",
      "694 Training Loss: tensor(0.7238)\n",
      "695 Training Loss: tensor(0.7107)\n",
      "696 Training Loss: tensor(0.7162)\n",
      "697 Training Loss: tensor(0.7320)\n",
      "698 Training Loss: tensor(0.7357)\n",
      "699 Training Loss: tensor(0.7031)\n",
      "700 Training Loss: tensor(0.7151)\n",
      "701 Training Loss: tensor(0.7047)\n",
      "702 Training Loss: tensor(0.7224)\n",
      "703 Training Loss: tensor(0.7070)\n",
      "704 Training Loss: tensor(0.7189)\n",
      "705 Training Loss: tensor(0.7156)\n",
      "706 Training Loss: tensor(0.7180)\n",
      "707 Training Loss: tensor(0.7103)\n",
      "708 Training Loss: tensor(0.7196)\n",
      "709 Training Loss: tensor(0.6963)\n",
      "710 Training Loss: tensor(0.7138)\n",
      "711 Training Loss: tensor(0.7123)\n",
      "712 Training Loss: tensor(0.7213)\n",
      "713 Training Loss: tensor(0.7181)\n",
      "714 Training Loss: tensor(0.7175)\n",
      "715 Training Loss: tensor(0.7070)\n",
      "716 Training Loss: tensor(0.7061)\n",
      "717 Training Loss: tensor(0.7173)\n",
      "718 Training Loss: tensor(0.7185)\n",
      "719 Training Loss: tensor(0.7110)\n",
      "720 Training Loss: tensor(0.7127)\n",
      "721 Training Loss: tensor(0.6975)\n",
      "722 Training Loss: tensor(0.7136)\n",
      "723 Training Loss: tensor(0.7139)\n",
      "724 Training Loss: tensor(0.7166)\n",
      "725 Training Loss: tensor(0.7252)\n",
      "726 Training Loss: tensor(0.7192)\n",
      "727 Training Loss: tensor(0.7052)\n",
      "728 Training Loss: tensor(0.6995)\n",
      "729 Training Loss: tensor(0.7063)\n",
      "730 Training Loss: tensor(0.7104)\n",
      "731 Training Loss: tensor(0.7216)\n",
      "732 Training Loss: tensor(0.7045)\n",
      "733 Training Loss: tensor(0.7269)\n",
      "734 Training Loss: tensor(0.7156)\n",
      "735 Training Loss: tensor(0.7141)\n",
      "736 Training Loss: tensor(0.7071)\n",
      "737 Training Loss: tensor(0.7012)\n",
      "738 Training Loss: tensor(0.7151)\n",
      "739 Training Loss: tensor(0.7078)\n",
      "740 Training Loss: tensor(0.7125)\n",
      "741 Training Loss: tensor(0.7138)\n",
      "742 Training Loss: tensor(0.7226)\n",
      "743 Training Loss: tensor(0.7164)\n",
      "744 Training Loss: tensor(0.7189)\n",
      "745 Training Loss: tensor(0.7004)\n",
      "746 Training Loss: tensor(0.7171)\n",
      "747 Training Loss: tensor(0.7006)\n",
      "748 Training Loss: tensor(0.7106)\n",
      "749 Training Loss: tensor(0.7096)\n",
      "750 Training Loss: tensor(0.7225)\n",
      "751 Training Loss: tensor(0.7154)\n",
      "752 Training Loss: tensor(0.7104)\n",
      "753 Training Loss: tensor(0.7125)\n",
      "754 Training Loss: tensor(0.6915)\n",
      "755 Training Loss: tensor(0.7073)\n",
      "756 Training Loss: tensor(0.7035)\n",
      "757 Training Loss: tensor(0.7049)\n",
      "758 Training Loss: tensor(0.7239)\n",
      "759 Training Loss: tensor(0.7079)\n",
      "760 Training Loss: tensor(0.6943)\n",
      "761 Training Loss: tensor(0.7104)\n",
      "762 Training Loss: tensor(0.7022)\n",
      "763 Training Loss: tensor(0.7084)\n",
      "764 Training Loss: tensor(0.7125)\n",
      "765 Training Loss: tensor(0.7021)\n",
      "766 Training Loss: tensor(0.7113)\n",
      "767 Training Loss: tensor(0.7044)\n",
      "768 Training Loss: tensor(0.7182)\n",
      "769 Training Loss: tensor(0.7124)\n",
      "770 Training Loss: tensor(0.7078)\n",
      "771 Training Loss: tensor(0.7145)\n",
      "772 Training Loss: tensor(0.7132)\n",
      "773 Training Loss: tensor(0.7097)\n",
      "774 Training Loss: tensor(0.7212)\n",
      "775 Training Loss: tensor(0.7053)\n",
      "776 Training Loss: tensor(0.6965)\n",
      "777 Training Loss: tensor(0.7087)\n",
      "778 Training Loss: tensor(0.7117)\n",
      "779 Training Loss: tensor(0.7112)\n",
      "780 Training Loss: tensor(0.7153)\n",
      "781 Training Loss: tensor(0.7053)\n",
      "782 Training Loss: tensor(0.7102)\n",
      "783 Training Loss: tensor(0.7074)\n",
      "784 Training Loss: tensor(0.7187)\n",
      "785 Training Loss: tensor(0.7005)\n",
      "786 Training Loss: tensor(0.7137)\n",
      "787 Training Loss: tensor(0.7095)\n",
      "788 Training Loss: tensor(0.6995)\n",
      "789 Training Loss: tensor(0.7103)\n",
      "790 Training Loss: tensor(0.7089)\n",
      "791 Training Loss: tensor(0.7128)\n",
      "792 Training Loss: tensor(0.6993)\n",
      "793 Training Loss: tensor(0.6975)\n",
      "794 Training Loss: tensor(0.7043)\n",
      "795 Training Loss: tensor(0.6998)\n",
      "796 Training Loss: tensor(0.7147)\n",
      "797 Training Loss: tensor(0.7012)\n",
      "798 Training Loss: tensor(0.7170)\n",
      "799 Training Loss: tensor(0.7026)\n",
      "800 Training Loss: tensor(0.7034)\n",
      "801 Training Loss: tensor(0.7252)\n",
      "802 Training Loss: tensor(0.7013)\n",
      "803 Training Loss: tensor(0.6988)\n",
      "804 Training Loss: tensor(0.7072)\n",
      "805 Training Loss: tensor(0.7236)\n",
      "806 Training Loss: tensor(0.7030)\n",
      "807 Training Loss: tensor(0.7035)\n",
      "808 Training Loss: tensor(0.7191)\n",
      "809 Training Loss: tensor(0.7053)\n",
      "810 Training Loss: tensor(0.6985)\n",
      "811 Training Loss: tensor(0.7087)\n",
      "812 Training Loss: tensor(0.7041)\n",
      "813 Training Loss: tensor(0.7041)\n",
      "814 Training Loss: tensor(0.7170)\n",
      "815 Training Loss: tensor(0.7022)\n",
      "816 Training Loss: tensor(0.7003)\n",
      "817 Training Loss: tensor(0.7160)\n",
      "818 Training Loss: tensor(0.6885)\n",
      "819 Training Loss: tensor(0.7032)\n",
      "820 Training Loss: tensor(0.7055)\n",
      "821 Training Loss: tensor(0.7059)\n",
      "822 Training Loss: tensor(0.7058)\n",
      "823 Training Loss: tensor(0.7092)\n",
      "824 Training Loss: tensor(0.7074)\n",
      "825 Training Loss: tensor(0.7062)\n",
      "826 Training Loss: tensor(0.7011)\n",
      "827 Training Loss: tensor(0.7083)\n",
      "828 Training Loss: tensor(0.7016)\n",
      "829 Training Loss: tensor(0.6981)\n",
      "830 Training Loss: tensor(0.7043)\n",
      "831 Training Loss: tensor(0.6969)\n",
      "832 Training Loss: tensor(0.7046)\n",
      "833 Training Loss: tensor(0.7066)\n",
      "834 Training Loss: tensor(0.6956)\n",
      "835 Training Loss: tensor(0.6853)\n",
      "836 Training Loss: tensor(0.6968)\n",
      "837 Training Loss: tensor(0.6930)\n",
      "838 Training Loss: tensor(0.7094)\n",
      "839 Training Loss: tensor(0.7063)\n",
      "840 Training Loss: tensor(0.7010)\n",
      "841 Training Loss: tensor(0.7063)\n",
      "842 Training Loss: tensor(0.7009)\n",
      "843 Training Loss: tensor(0.6982)\n",
      "844 Training Loss: tensor(0.7101)\n",
      "845 Training Loss: tensor(0.7072)\n",
      "846 Training Loss: tensor(0.6969)\n",
      "847 Training Loss: tensor(0.6913)\n",
      "848 Training Loss: tensor(0.6916)\n",
      "849 Training Loss: tensor(0.7019)\n",
      "850 Training Loss: tensor(0.7151)\n",
      "851 Training Loss: tensor(0.6981)\n",
      "852 Training Loss: tensor(0.6952)\n",
      "853 Training Loss: tensor(0.6979)\n",
      "854 Training Loss: tensor(0.7009)\n",
      "855 Training Loss: tensor(0.6968)\n",
      "856 Training Loss: tensor(0.6963)\n",
      "857 Training Loss: tensor(0.7057)\n",
      "858 Training Loss: tensor(0.6946)\n",
      "859 Training Loss: tensor(0.7044)\n",
      "860 Training Loss: tensor(0.6917)\n",
      "861 Training Loss: tensor(0.7007)\n",
      "862 Training Loss: tensor(0.6986)\n",
      "863 Training Loss: tensor(0.7075)\n",
      "864 Training Loss: tensor(0.6866)\n",
      "865 Training Loss: tensor(0.6919)\n",
      "866 Training Loss: tensor(0.7061)\n",
      "867 Training Loss: tensor(0.6951)\n",
      "868 Training Loss: tensor(0.6892)\n",
      "869 Training Loss: tensor(0.6980)\n",
      "870 Training Loss: tensor(0.6951)\n",
      "871 Training Loss: tensor(0.6909)\n",
      "872 Training Loss: tensor(0.7018)\n",
      "873 Training Loss: tensor(0.7038)\n",
      "874 Training Loss: tensor(0.6947)\n",
      "875 Training Loss: tensor(0.6854)\n",
      "876 Training Loss: tensor(0.7000)\n",
      "877 Training Loss: tensor(0.6980)\n",
      "878 Training Loss: tensor(0.6895)\n",
      "879 Training Loss: tensor(0.7009)\n",
      "880 Training Loss: tensor(0.6833)\n",
      "881 Training Loss: tensor(0.6988)\n",
      "882 Training Loss: tensor(0.6834)\n",
      "883 Training Loss: tensor(0.6945)\n",
      "884 Training Loss: tensor(0.6915)\n",
      "885 Training Loss: tensor(0.6897)\n",
      "886 Training Loss: tensor(0.6992)\n",
      "887 Training Loss: tensor(0.6925)\n",
      "888 Training Loss: tensor(0.6948)\n",
      "889 Training Loss: tensor(0.7020)\n",
      "890 Training Loss: tensor(0.6826)\n",
      "891 Training Loss: tensor(0.6886)\n",
      "892 Training Loss: tensor(0.6899)\n",
      "893 Training Loss: tensor(0.6845)\n",
      "894 Training Loss: tensor(0.6772)\n",
      "895 Training Loss: tensor(0.6876)\n",
      "896 Training Loss: tensor(0.6899)\n",
      "897 Training Loss: tensor(0.6836)\n",
      "898 Training Loss: tensor(0.6969)\n",
      "899 Training Loss: tensor(0.6889)\n",
      "900 Training Loss: tensor(0.6811)\n",
      "901 Training Loss: tensor(0.6964)\n",
      "902 Training Loss: tensor(0.6967)\n",
      "903 Training Loss: tensor(0.6863)\n",
      "904 Training Loss: tensor(0.7008)\n",
      "905 Training Loss: tensor(0.6940)\n",
      "906 Training Loss: tensor(0.6796)\n",
      "907 Training Loss: tensor(0.6943)\n",
      "908 Training Loss: tensor(0.6930)\n",
      "909 Training Loss: tensor(0.6814)\n",
      "910 Training Loss: tensor(0.6881)\n",
      "911 Training Loss: tensor(0.6800)\n",
      "912 Training Loss: tensor(0.6833)\n",
      "913 Training Loss: tensor(0.6866)\n",
      "914 Training Loss: tensor(0.6740)\n",
      "915 Training Loss: tensor(0.6781)\n",
      "916 Training Loss: tensor(0.6775)\n",
      "917 Training Loss: tensor(0.6754)\n",
      "918 Training Loss: tensor(0.6908)\n",
      "919 Training Loss: tensor(0.6878)\n",
      "920 Training Loss: tensor(0.6863)\n",
      "921 Training Loss: tensor(0.6750)\n",
      "922 Training Loss: tensor(0.6791)\n",
      "923 Training Loss: tensor(0.6816)\n",
      "924 Training Loss: tensor(0.6766)\n",
      "925 Training Loss: tensor(0.6788)\n",
      "926 Training Loss: tensor(0.6966)\n",
      "927 Training Loss: tensor(0.6854)\n",
      "928 Training Loss: tensor(0.6767)\n",
      "929 Training Loss: tensor(0.6849)\n",
      "930 Training Loss: tensor(0.6902)\n",
      "931 Training Loss: tensor(0.6712)\n",
      "932 Training Loss: tensor(0.6713)\n",
      "933 Training Loss: tensor(0.6845)\n",
      "934 Training Loss: tensor(0.6878)\n",
      "935 Training Loss: tensor(0.6793)\n",
      "936 Training Loss: tensor(0.6753)\n",
      "937 Training Loss: tensor(0.6714)\n",
      "938 Training Loss: tensor(0.6803)\n",
      "939 Training Loss: tensor(0.6858)\n",
      "940 Training Loss: tensor(0.6862)\n",
      "941 Training Loss: tensor(0.6825)\n",
      "942 Training Loss: tensor(0.6658)\n",
      "943 Training Loss: tensor(0.7010)\n",
      "944 Training Loss: tensor(0.6742)\n",
      "945 Training Loss: tensor(0.6793)\n",
      "946 Training Loss: tensor(0.6788)\n",
      "947 Training Loss: tensor(0.6762)\n",
      "948 Training Loss: tensor(0.6849)\n",
      "949 Training Loss: tensor(0.6719)\n",
      "950 Training Loss: tensor(0.6744)\n",
      "951 Training Loss: tensor(0.6727)\n",
      "952 Training Loss: tensor(0.6727)\n",
      "953 Training Loss: tensor(0.6681)\n",
      "954 Training Loss: tensor(0.6827)\n",
      "955 Training Loss: tensor(0.6801)\n",
      "956 Training Loss: tensor(0.6658)\n",
      "957 Training Loss: tensor(0.6639)\n",
      "958 Training Loss: tensor(0.6750)\n",
      "959 Training Loss: tensor(0.6701)\n",
      "960 Training Loss: tensor(0.6660)\n",
      "961 Training Loss: tensor(0.6685)\n",
      "962 Training Loss: tensor(0.6568)\n",
      "963 Training Loss: tensor(0.6626)\n",
      "964 Training Loss: tensor(0.6698)\n",
      "965 Training Loss: tensor(0.6711)\n",
      "966 Training Loss: tensor(0.6679)\n",
      "967 Training Loss: tensor(0.6732)\n",
      "968 Training Loss: tensor(0.6461)\n",
      "969 Training Loss: tensor(0.6640)\n",
      "970 Training Loss: tensor(0.6656)\n",
      "971 Training Loss: tensor(0.6586)\n",
      "972 Training Loss: tensor(0.6479)\n",
      "973 Training Loss: tensor(0.6740)\n",
      "974 Training Loss: tensor(0.6673)\n",
      "975 Training Loss: tensor(0.6470)\n",
      "976 Training Loss: tensor(0.6645)\n",
      "977 Training Loss: tensor(0.6495)\n",
      "978 Training Loss: tensor(0.6761)\n",
      "979 Training Loss: tensor(0.6573)\n",
      "980 Training Loss: tensor(0.6534)\n",
      "981 Training Loss: tensor(0.6705)\n",
      "982 Training Loss: tensor(0.6583)\n",
      "983 Training Loss: tensor(0.6572)\n",
      "984 Training Loss: tensor(0.6532)\n",
      "985 Training Loss: tensor(0.6617)\n",
      "986 Training Loss: tensor(0.6457)\n",
      "987 Training Loss: tensor(0.6483)\n",
      "988 Training Loss: tensor(0.6514)\n",
      "989 Training Loss: tensor(0.6540)\n",
      "990 Training Loss: tensor(0.6526)\n",
      "991 Training Loss: tensor(0.6610)\n",
      "992 Training Loss: tensor(0.6520)\n",
      "993 Training Loss: tensor(0.6555)\n",
      "994 Training Loss: tensor(0.6544)\n",
      "995 Training Loss: tensor(0.6512)\n",
      "996 Training Loss: tensor(0.6593)\n",
      "997 Training Loss: tensor(0.6430)\n",
      "998 Training Loss: tensor(0.6407)\n",
      "999 Training Loss: tensor(0.6497)\n",
      "1000 Training Loss: tensor(0.6526)\n",
      "1001 Training Loss: tensor(0.6460)\n",
      "1002 Training Loss: tensor(0.6408)\n",
      "1003 Training Loss: tensor(0.6307)\n",
      "1004 Training Loss: tensor(0.6463)\n",
      "1005 Training Loss: tensor(0.6392)\n",
      "1006 Training Loss: tensor(0.6459)\n",
      "1007 Training Loss: tensor(0.6404)\n",
      "1008 Training Loss: tensor(0.6427)\n",
      "1009 Training Loss: tensor(0.6459)\n",
      "1010 Training Loss: tensor(0.6225)\n",
      "1011 Training Loss: tensor(0.6361)\n",
      "1012 Training Loss: tensor(0.6343)\n",
      "1013 Training Loss: tensor(0.6208)\n",
      "1014 Training Loss: tensor(0.6398)\n",
      "1015 Training Loss: tensor(0.6308)\n",
      "1016 Training Loss: tensor(0.6276)\n",
      "1017 Training Loss: tensor(0.6240)\n",
      "1018 Training Loss: tensor(0.6321)\n",
      "1019 Training Loss: tensor(0.6260)\n",
      "1020 Training Loss: tensor(0.6258)\n",
      "1021 Training Loss: tensor(0.6264)\n",
      "1022 Training Loss: tensor(0.6265)\n",
      "1023 Training Loss: tensor(0.6199)\n",
      "1024 Training Loss: tensor(0.6253)\n",
      "1025 Training Loss: tensor(0.6325)\n",
      "1026 Training Loss: tensor(0.6293)\n",
      "1027 Training Loss: tensor(0.6202)\n",
      "1028 Training Loss: tensor(0.6279)\n",
      "1029 Training Loss: tensor(0.6142)\n",
      "1030 Training Loss: tensor(0.6082)\n",
      "1031 Training Loss: tensor(0.6219)\n",
      "1032 Training Loss: tensor(0.6148)\n",
      "1033 Training Loss: tensor(0.6070)\n",
      "1034 Training Loss: tensor(0.6070)\n",
      "1035 Training Loss: tensor(0.6006)\n",
      "1036 Training Loss: tensor(0.5966)\n",
      "1037 Training Loss: tensor(0.6036)\n",
      "1038 Training Loss: tensor(0.6150)\n",
      "1039 Training Loss: tensor(0.6102)\n",
      "1040 Training Loss: tensor(0.5992)\n",
      "1041 Training Loss: tensor(0.5934)\n",
      "1042 Training Loss: tensor(0.5958)\n",
      "1043 Training Loss: tensor(0.5961)\n",
      "1044 Training Loss: tensor(0.5871)\n",
      "1045 Training Loss: tensor(0.5992)\n",
      "1046 Training Loss: tensor(0.5954)\n",
      "1047 Training Loss: tensor(0.6062)\n",
      "1048 Training Loss: tensor(0.5873)\n",
      "1049 Training Loss: tensor(0.5920)\n",
      "1050 Training Loss: tensor(0.5693)\n",
      "1051 Training Loss: tensor(0.5911)\n",
      "1052 Training Loss: tensor(0.5867)\n",
      "1053 Training Loss: tensor(0.5840)\n",
      "1054 Training Loss: tensor(0.5711)\n",
      "1055 Training Loss: tensor(0.5610)\n",
      "1056 Training Loss: tensor(0.5782)\n",
      "1057 Training Loss: tensor(0.5793)\n",
      "1058 Training Loss: tensor(0.5777)\n",
      "1059 Training Loss: tensor(0.5622)\n",
      "1060 Training Loss: tensor(0.5852)\n",
      "1061 Training Loss: tensor(0.5627)\n",
      "1062 Training Loss: tensor(0.5573)\n",
      "1063 Training Loss: tensor(0.5696)\n",
      "1064 Training Loss: tensor(0.5567)\n",
      "1065 Training Loss: tensor(0.5721)\n",
      "1066 Training Loss: tensor(0.5601)\n",
      "1067 Training Loss: tensor(0.5713)\n",
      "1068 Training Loss: tensor(0.5555)\n",
      "1069 Training Loss: tensor(0.5524)\n",
      "1070 Training Loss: tensor(0.5520)\n",
      "1071 Training Loss: tensor(0.5578)\n",
      "1072 Training Loss: tensor(0.5521)\n",
      "1073 Training Loss: tensor(0.5438)\n",
      "1074 Training Loss: tensor(0.5594)\n",
      "1075 Training Loss: tensor(0.5370)\n",
      "1076 Training Loss: tensor(0.5512)\n",
      "1077 Training Loss: tensor(0.5496)\n",
      "1078 Training Loss: tensor(0.5516)\n",
      "1079 Training Loss: tensor(0.5372)\n",
      "1080 Training Loss: tensor(0.5308)\n",
      "1081 Training Loss: tensor(0.5311)\n",
      "1082 Training Loss: tensor(0.5443)\n",
      "1083 Training Loss: tensor(0.5368)\n",
      "1084 Training Loss: tensor(0.5479)\n",
      "1085 Training Loss: tensor(0.5269)\n",
      "1086 Training Loss: tensor(0.5369)\n",
      "1087 Training Loss: tensor(0.5289)\n",
      "1088 Training Loss: tensor(0.5401)\n",
      "1089 Training Loss: tensor(0.5214)\n",
      "1090 Training Loss: tensor(0.5273)\n",
      "1091 Training Loss: tensor(0.5250)\n",
      "1092 Training Loss: tensor(0.5130)\n",
      "1093 Training Loss: tensor(0.5156)\n",
      "1094 Training Loss: tensor(0.5344)\n",
      "1095 Training Loss: tensor(0.5298)\n",
      "1096 Training Loss: tensor(0.5285)\n",
      "1097 Training Loss: tensor(0.5133)\n",
      "1098 Training Loss: tensor(0.5236)\n",
      "1099 Training Loss: tensor(0.5189)\n",
      "1100 Training Loss: tensor(0.5047)\n",
      "1101 Training Loss: tensor(0.5067)\n",
      "1102 Training Loss: tensor(0.5107)\n",
      "1103 Training Loss: tensor(0.5195)\n",
      "1104 Training Loss: tensor(0.5115)\n",
      "1105 Training Loss: tensor(0.5151)\n",
      "1106 Training Loss: tensor(0.4992)\n",
      "1107 Training Loss: tensor(0.5203)\n",
      "1108 Training Loss: tensor(0.5137)\n",
      "1109 Training Loss: tensor(0.4914)\n",
      "1110 Training Loss: tensor(0.5011)\n",
      "1111 Training Loss: tensor(0.5128)\n",
      "1112 Training Loss: tensor(0.5073)\n",
      "1113 Training Loss: tensor(0.4844)\n",
      "1114 Training Loss: tensor(0.5124)\n",
      "1115 Training Loss: tensor(0.5021)\n",
      "1116 Training Loss: tensor(0.4956)\n",
      "1117 Training Loss: tensor(0.5004)\n",
      "1118 Training Loss: tensor(0.4886)\n",
      "1119 Training Loss: tensor(0.5171)\n",
      "1120 Training Loss: tensor(0.5096)\n",
      "1121 Training Loss: tensor(0.5026)\n",
      "1122 Training Loss: tensor(0.5072)\n",
      "1123 Training Loss: tensor(0.5022)\n",
      "1124 Training Loss: tensor(0.5071)\n",
      "1125 Training Loss: tensor(0.5099)\n",
      "1126 Training Loss: tensor(0.5113)\n",
      "1127 Training Loss: tensor(0.5064)\n",
      "1128 Training Loss: tensor(0.5136)\n",
      "1129 Training Loss: tensor(0.4940)\n",
      "1130 Training Loss: tensor(0.4907)\n",
      "1131 Training Loss: tensor(0.5042)\n",
      "1132 Training Loss: tensor(0.5095)\n",
      "1133 Training Loss: tensor(0.5032)\n",
      "1134 Training Loss: tensor(0.5035)\n",
      "1135 Training Loss: tensor(0.4958)\n",
      "1136 Training Loss: tensor(0.4970)\n",
      "1137 Training Loss: tensor(0.4989)\n",
      "1138 Training Loss: tensor(0.4929)\n",
      "1139 Training Loss: tensor(0.4992)\n",
      "1140 Training Loss: tensor(0.4922)\n",
      "1141 Training Loss: tensor(0.4991)\n",
      "1142 Training Loss: tensor(0.4934)\n",
      "1143 Training Loss: tensor(0.5035)\n",
      "1144 Training Loss: tensor(0.4972)\n",
      "1145 Training Loss: tensor(0.4863)\n",
      "1146 Training Loss: tensor(0.4835)\n",
      "1147 Training Loss: tensor(0.4969)\n",
      "1148 Training Loss: tensor(0.5102)\n",
      "1149 Training Loss: tensor(0.4986)\n",
      "1150 Training Loss: tensor(0.4987)\n",
      "1151 Training Loss: tensor(0.5068)\n",
      "1152 Training Loss: tensor(0.4975)\n",
      "1153 Training Loss: tensor(0.4759)\n",
      "1154 Training Loss: tensor(0.4882)\n",
      "1155 Training Loss: tensor(0.5006)\n",
      "1156 Training Loss: tensor(0.4996)\n",
      "1157 Training Loss: tensor(0.4976)\n",
      "1158 Training Loss: tensor(0.5037)\n",
      "1159 Training Loss: tensor(0.4959)\n",
      "1160 Training Loss: tensor(0.4961)\n",
      "1161 Training Loss: tensor(0.4966)\n",
      "1162 Training Loss: tensor(0.5027)\n",
      "1163 Training Loss: tensor(0.4960)\n",
      "1164 Training Loss: tensor(0.5051)\n",
      "1165 Training Loss: tensor(0.4869)\n",
      "1166 Training Loss: tensor(0.4934)\n",
      "1167 Training Loss: tensor(0.4834)\n",
      "1168 Training Loss: tensor(0.4798)\n",
      "1169 Training Loss: tensor(0.4830)\n",
      "1170 Training Loss: tensor(0.4887)\n",
      "1171 Training Loss: tensor(0.4893)\n",
      "1172 Training Loss: tensor(0.4861)\n",
      "1173 Training Loss: tensor(0.4921)\n",
      "1174 Training Loss: tensor(0.4794)\n",
      "1175 Training Loss: tensor(0.4898)\n",
      "1176 Training Loss: tensor(0.4880)\n",
      "1177 Training Loss: tensor(0.4927)\n",
      "1178 Training Loss: tensor(0.4997)\n",
      "1179 Training Loss: tensor(0.4923)\n",
      "1180 Training Loss: tensor(0.4830)\n",
      "1181 Training Loss: tensor(0.4790)\n",
      "1182 Training Loss: tensor(0.4936)\n",
      "1183 Training Loss: tensor(0.4910)\n",
      "1184 Training Loss: tensor(0.4941)\n",
      "1185 Training Loss: tensor(0.4785)\n",
      "1186 Training Loss: tensor(0.4890)\n",
      "1187 Training Loss: tensor(0.4645)\n",
      "1188 Training Loss: tensor(0.4772)\n",
      "1189 Training Loss: tensor(0.4873)\n",
      "1190 Training Loss: tensor(0.4994)\n",
      "1191 Training Loss: tensor(0.4771)\n",
      "1192 Training Loss: tensor(0.4975)\n",
      "1193 Training Loss: tensor(0.4910)\n",
      "1194 Training Loss: tensor(0.4769)\n",
      "1195 Training Loss: tensor(0.4816)\n",
      "1196 Training Loss: tensor(0.4830)\n",
      "1197 Training Loss: tensor(0.4809)\n",
      "1198 Training Loss: tensor(0.4660)\n",
      "1199 Training Loss: tensor(0.4954)\n",
      "1200 Training Loss: tensor(0.4750)\n",
      "1201 Training Loss: tensor(0.4755)\n",
      "1202 Training Loss: tensor(0.4940)\n",
      "1203 Training Loss: tensor(0.4609)\n",
      "1204 Training Loss: tensor(0.4916)\n",
      "1205 Training Loss: tensor(0.4875)\n",
      "1206 Training Loss: tensor(0.4766)\n",
      "1207 Training Loss: tensor(0.4710)\n",
      "1208 Training Loss: tensor(0.4861)\n",
      "1209 Training Loss: tensor(0.4669)\n",
      "1210 Training Loss: tensor(0.4791)\n",
      "1211 Training Loss: tensor(0.4710)\n",
      "1212 Training Loss: tensor(0.4691)\n",
      "1213 Training Loss: tensor(0.4769)\n",
      "1214 Training Loss: tensor(0.4676)\n",
      "1215 Training Loss: tensor(0.4611)\n",
      "1216 Training Loss: tensor(0.4760)\n",
      "1217 Training Loss: tensor(0.4767)\n",
      "1218 Training Loss: tensor(0.4603)\n",
      "1219 Training Loss: tensor(0.4787)\n",
      "1220 Training Loss: tensor(0.4702)\n",
      "1221 Training Loss: tensor(0.4592)\n",
      "1222 Training Loss: tensor(0.4855)\n",
      "1223 Training Loss: tensor(0.4863)\n",
      "1224 Training Loss: tensor(0.4754)\n",
      "1225 Training Loss: tensor(0.4744)\n",
      "1226 Training Loss: tensor(0.4834)\n",
      "1227 Training Loss: tensor(0.4666)\n",
      "1228 Training Loss: tensor(0.4665)\n",
      "1229 Training Loss: tensor(0.4884)\n",
      "1230 Training Loss: tensor(0.4642)\n",
      "1231 Training Loss: tensor(0.4804)\n",
      "1232 Training Loss: tensor(0.4674)\n",
      "1233 Training Loss: tensor(0.4705)\n",
      "1234 Training Loss: tensor(0.4744)\n",
      "1235 Training Loss: tensor(0.4698)\n",
      "1236 Training Loss: tensor(0.4549)\n",
      "1237 Training Loss: tensor(0.4801)\n",
      "1238 Training Loss: tensor(0.4705)\n",
      "1239 Training Loss: tensor(0.4636)\n",
      "1240 Training Loss: tensor(0.4646)\n",
      "1241 Training Loss: tensor(0.4656)\n",
      "1242 Training Loss: tensor(0.4643)\n",
      "1243 Training Loss: tensor(0.4639)\n",
      "1244 Training Loss: tensor(0.4595)\n",
      "1245 Training Loss: tensor(0.4766)\n",
      "1246 Training Loss: tensor(0.4737)\n",
      "1247 Training Loss: tensor(0.4484)\n",
      "1248 Training Loss: tensor(0.4859)\n",
      "1249 Training Loss: tensor(0.4668)\n",
      "1250 Training Loss: tensor(0.4769)\n",
      "1251 Training Loss: tensor(0.4588)\n",
      "1252 Training Loss: tensor(0.4493)\n",
      "1253 Training Loss: tensor(0.4674)\n",
      "1254 Training Loss: tensor(0.4679)\n",
      "1255 Training Loss: tensor(0.4670)\n",
      "1256 Training Loss: tensor(0.4626)\n",
      "1257 Training Loss: tensor(0.4687)\n",
      "1258 Training Loss: tensor(0.4629)\n",
      "1259 Training Loss: tensor(0.4696)\n",
      "1260 Training Loss: tensor(0.4617)\n",
      "1261 Training Loss: tensor(0.4579)\n",
      "1262 Training Loss: tensor(0.4436)\n",
      "1263 Training Loss: tensor(0.4546)\n",
      "1264 Training Loss: tensor(0.4750)\n",
      "1265 Training Loss: tensor(0.4606)\n",
      "1266 Training Loss: tensor(0.4489)\n",
      "1267 Training Loss: tensor(0.4693)\n",
      "1268 Training Loss: tensor(0.4677)\n",
      "1269 Training Loss: tensor(0.4656)\n",
      "1270 Training Loss: tensor(0.4587)\n",
      "1271 Training Loss: tensor(0.4675)\n",
      "1272 Training Loss: tensor(0.4739)\n",
      "1273 Training Loss: tensor(0.4597)\n",
      "1274 Training Loss: tensor(0.4567)\n",
      "1275 Training Loss: tensor(0.4651)\n",
      "1276 Training Loss: tensor(0.4702)\n",
      "1277 Training Loss: tensor(0.4637)\n",
      "1278 Training Loss: tensor(0.4732)\n",
      "1279 Training Loss: tensor(0.4801)\n",
      "1280 Training Loss: tensor(0.4513)\n",
      "1281 Training Loss: tensor(0.4673)\n",
      "1282 Training Loss: tensor(0.4510)\n",
      "1283 Training Loss: tensor(0.4684)\n",
      "1284 Training Loss: tensor(0.4654)\n",
      "1285 Training Loss: tensor(0.4375)\n",
      "1286 Training Loss: tensor(0.4464)\n",
      "1287 Training Loss: tensor(0.4437)\n",
      "1288 Training Loss: tensor(0.4605)\n",
      "1289 Training Loss: tensor(0.4633)\n",
      "1290 Training Loss: tensor(0.4737)\n",
      "1291 Training Loss: tensor(0.4499)\n",
      "1292 Training Loss: tensor(0.4459)\n",
      "1293 Training Loss: tensor(0.4426)\n",
      "1294 Training Loss: tensor(0.4550)\n",
      "1295 Training Loss: tensor(0.4424)\n",
      "1296 Training Loss: tensor(0.4714)\n",
      "1297 Training Loss: tensor(0.4568)\n",
      "1298 Training Loss: tensor(0.4582)\n",
      "1299 Training Loss: tensor(0.4348)\n",
      "1300 Training Loss: tensor(0.4534)\n",
      "1301 Training Loss: tensor(0.4540)\n",
      "1302 Training Loss: tensor(0.4629)\n",
      "1303 Training Loss: tensor(0.4510)\n",
      "1304 Training Loss: tensor(0.4434)\n",
      "1305 Training Loss: tensor(0.4459)\n",
      "1306 Training Loss: tensor(0.4608)\n",
      "1307 Training Loss: tensor(0.4412)\n",
      "1308 Training Loss: tensor(0.4437)\n",
      "1309 Training Loss: tensor(0.4479)\n",
      "1310 Training Loss: tensor(0.4252)\n",
      "1311 Training Loss: tensor(0.4441)\n",
      "1312 Training Loss: tensor(0.4523)\n",
      "1313 Training Loss: tensor(0.4500)\n",
      "1314 Training Loss: tensor(0.4393)\n",
      "1315 Training Loss: tensor(0.4433)\n",
      "1316 Training Loss: tensor(0.4557)\n",
      "1317 Training Loss: tensor(0.4423)\n",
      "1318 Training Loss: tensor(0.4535)\n",
      "1319 Training Loss: tensor(0.4584)\n",
      "1320 Training Loss: tensor(0.4373)\n",
      "1321 Training Loss: tensor(0.4454)\n",
      "1322 Training Loss: tensor(0.4379)\n",
      "1323 Training Loss: tensor(0.4392)\n",
      "1324 Training Loss: tensor(0.4346)\n",
      "1325 Training Loss: tensor(0.4346)\n",
      "1326 Training Loss: tensor(0.4520)\n",
      "1327 Training Loss: tensor(0.4491)\n",
      "1328 Training Loss: tensor(0.4418)\n",
      "1329 Training Loss: tensor(0.4359)\n",
      "1330 Training Loss: tensor(0.4402)\n",
      "1331 Training Loss: tensor(0.4583)\n",
      "1332 Training Loss: tensor(0.4334)\n",
      "1333 Training Loss: tensor(0.4490)\n",
      "1334 Training Loss: tensor(0.4308)\n",
      "1335 Training Loss: tensor(0.4273)\n",
      "1336 Training Loss: tensor(0.4232)\n",
      "1337 Training Loss: tensor(0.4310)\n",
      "1338 Training Loss: tensor(0.4301)\n",
      "1339 Training Loss: tensor(0.4412)\n",
      "1340 Training Loss: tensor(0.4369)\n",
      "1341 Training Loss: tensor(0.4396)\n",
      "1342 Training Loss: tensor(0.4164)\n",
      "1343 Training Loss: tensor(0.4290)\n",
      "1344 Training Loss: tensor(0.4406)\n",
      "1345 Training Loss: tensor(0.4368)\n",
      "1346 Training Loss: tensor(0.4362)\n",
      "1347 Training Loss: tensor(0.4243)\n",
      "1348 Training Loss: tensor(0.4235)\n",
      "1349 Training Loss: tensor(0.4248)\n",
      "1350 Training Loss: tensor(0.4372)\n",
      "1351 Training Loss: tensor(0.4226)\n",
      "1352 Training Loss: tensor(0.4535)\n",
      "1353 Training Loss: tensor(0.4299)\n",
      "1354 Training Loss: tensor(0.4458)\n",
      "1355 Training Loss: tensor(0.4314)\n",
      "1356 Training Loss: tensor(0.4410)\n",
      "1357 Training Loss: tensor(0.4278)\n",
      "1358 Training Loss: tensor(0.4317)\n",
      "1359 Training Loss: tensor(0.4124)\n",
      "1360 Training Loss: tensor(0.4224)\n",
      "1361 Training Loss: tensor(0.4066)\n",
      "1362 Training Loss: tensor(0.4400)\n",
      "1363 Training Loss: tensor(0.4360)\n",
      "1364 Training Loss: tensor(0.4309)\n",
      "1365 Training Loss: tensor(0.4233)\n",
      "1366 Training Loss: tensor(0.4255)\n",
      "1367 Training Loss: tensor(0.4270)\n",
      "1368 Training Loss: tensor(0.4231)\n",
      "1369 Training Loss: tensor(0.4162)\n",
      "1370 Training Loss: tensor(0.4176)\n",
      "1371 Training Loss: tensor(0.3979)\n",
      "1372 Training Loss: tensor(0.4271)\n",
      "1373 Training Loss: tensor(0.4207)\n",
      "1374 Training Loss: tensor(0.4279)\n",
      "1375 Training Loss: tensor(0.4244)\n",
      "1376 Training Loss: tensor(0.4086)\n",
      "1377 Training Loss: tensor(0.4185)\n",
      "1378 Training Loss: tensor(0.4234)\n",
      "1379 Training Loss: tensor(0.4130)\n",
      "1380 Training Loss: tensor(0.4095)\n",
      "1381 Training Loss: tensor(0.4137)\n",
      "1382 Training Loss: tensor(0.4262)\n",
      "1383 Training Loss: tensor(0.4104)\n",
      "1384 Training Loss: tensor(0.4138)\n",
      "1385 Training Loss: tensor(0.3958)\n",
      "1386 Training Loss: tensor(0.3948)\n",
      "1387 Training Loss: tensor(0.4097)\n",
      "1388 Training Loss: tensor(0.4064)\n",
      "1389 Training Loss: tensor(0.3940)\n",
      "1390 Training Loss: tensor(0.3846)\n",
      "1391 Training Loss: tensor(0.3957)\n",
      "1392 Training Loss: tensor(0.4110)\n",
      "1393 Training Loss: tensor(0.4006)\n",
      "1394 Training Loss: tensor(0.4166)\n",
      "1395 Training Loss: tensor(0.4159)\n",
      "1396 Training Loss: tensor(0.3922)\n",
      "1397 Training Loss: tensor(0.3912)\n",
      "1398 Training Loss: tensor(0.4109)\n",
      "1399 Training Loss: tensor(0.4043)\n",
      "1400 Training Loss: tensor(0.4155)\n",
      "1401 Training Loss: tensor(0.3924)\n",
      "1402 Training Loss: tensor(0.3916)\n",
      "1403 Training Loss: tensor(0.4046)\n",
      "1404 Training Loss: tensor(0.4057)\n",
      "1405 Training Loss: tensor(0.4028)\n",
      "1406 Training Loss: tensor(0.4055)\n",
      "1407 Training Loss: tensor(0.3832)\n",
      "1408 Training Loss: tensor(0.3838)\n",
      "1409 Training Loss: tensor(0.3827)\n",
      "1410 Training Loss: tensor(0.3795)\n",
      "1411 Training Loss: tensor(0.3977)\n",
      "1412 Training Loss: tensor(0.3964)\n",
      "1413 Training Loss: tensor(0.3725)\n",
      "1414 Training Loss: tensor(0.3764)\n",
      "1415 Training Loss: tensor(0.3960)\n",
      "1416 Training Loss: tensor(0.3923)\n",
      "1417 Training Loss: tensor(0.3923)\n",
      "1418 Training Loss: tensor(0.3901)\n",
      "1419 Training Loss: tensor(0.3865)\n",
      "1420 Training Loss: tensor(0.3817)\n",
      "1421 Training Loss: tensor(0.3818)\n",
      "1422 Training Loss: tensor(0.3685)\n",
      "1423 Training Loss: tensor(0.3706)\n",
      "1424 Training Loss: tensor(0.3864)\n",
      "1425 Training Loss: tensor(0.3596)\n",
      "1426 Training Loss: tensor(0.3976)\n",
      "1427 Training Loss: tensor(0.3642)\n",
      "1428 Training Loss: tensor(0.3796)\n",
      "1429 Training Loss: tensor(0.3733)\n",
      "1430 Training Loss: tensor(0.3771)\n",
      "1431 Training Loss: tensor(0.3625)\n",
      "1432 Training Loss: tensor(0.3558)\n",
      "1433 Training Loss: tensor(0.3520)\n",
      "1434 Training Loss: tensor(0.3702)\n",
      "1435 Training Loss: tensor(0.3711)\n",
      "1436 Training Loss: tensor(0.3643)\n",
      "1437 Training Loss: tensor(0.3788)\n",
      "1438 Training Loss: tensor(0.3697)\n",
      "1439 Training Loss: tensor(0.3544)\n",
      "1440 Training Loss: tensor(0.3398)\n",
      "1441 Training Loss: tensor(0.3621)\n",
      "1442 Training Loss: tensor(0.3630)\n",
      "1443 Training Loss: tensor(0.3391)\n",
      "1444 Training Loss: tensor(0.3627)\n",
      "1445 Training Loss: tensor(0.3552)\n",
      "1446 Training Loss: tensor(0.3442)\n",
      "1447 Training Loss: tensor(0.3379)\n",
      "1448 Training Loss: tensor(0.3512)\n",
      "1449 Training Loss: tensor(0.3399)\n",
      "1450 Training Loss: tensor(0.3363)\n",
      "1451 Training Loss: tensor(0.3608)\n",
      "1452 Training Loss: tensor(0.3351)\n",
      "1453 Training Loss: tensor(0.3287)\n",
      "1454 Training Loss: tensor(0.3465)\n",
      "1455 Training Loss: tensor(0.3393)\n",
      "1456 Training Loss: tensor(0.3312)\n",
      "1457 Training Loss: tensor(0.3370)\n",
      "1458 Training Loss: tensor(0.3292)\n",
      "1459 Training Loss: tensor(0.3404)\n",
      "1460 Training Loss: tensor(0.3411)\n",
      "1461 Training Loss: tensor(0.3385)\n",
      "1462 Training Loss: tensor(0.3389)\n",
      "1463 Training Loss: tensor(0.3330)\n",
      "1464 Training Loss: tensor(0.3204)\n",
      "1465 Training Loss: tensor(0.3427)\n",
      "1466 Training Loss: tensor(0.3337)\n",
      "1467 Training Loss: tensor(0.3290)\n",
      "1468 Training Loss: tensor(0.3055)\n",
      "1469 Training Loss: tensor(0.3104)\n",
      "1470 Training Loss: tensor(0.3185)\n",
      "1471 Training Loss: tensor(0.3309)\n",
      "1472 Training Loss: tensor(0.3048)\n",
      "1473 Training Loss: tensor(0.3148)\n",
      "1474 Training Loss: tensor(0.3028)\n",
      "1475 Training Loss: tensor(0.3156)\n",
      "1476 Training Loss: tensor(0.3089)\n",
      "1477 Training Loss: tensor(0.2940)\n",
      "1478 Training Loss: tensor(0.3073)\n",
      "1479 Training Loss: tensor(0.3081)\n",
      "1480 Training Loss: tensor(0.3128)\n",
      "1481 Training Loss: tensor(0.3147)\n",
      "1482 Training Loss: tensor(0.3093)\n",
      "1483 Training Loss: tensor(0.3186)\n",
      "1484 Training Loss: tensor(0.3086)\n",
      "1485 Training Loss: tensor(0.2994)\n",
      "1486 Training Loss: tensor(0.3178)\n",
      "1487 Training Loss: tensor(0.2966)\n",
      "1488 Training Loss: tensor(0.3023)\n",
      "1489 Training Loss: tensor(0.2985)\n",
      "1490 Training Loss: tensor(0.3005)\n",
      "1491 Training Loss: tensor(0.3076)\n",
      "1492 Training Loss: tensor(0.3128)\n",
      "1493 Training Loss: tensor(0.2997)\n",
      "1494 Training Loss: tensor(0.3021)\n",
      "1495 Training Loss: tensor(0.2861)\n",
      "1496 Training Loss: tensor(0.2929)\n",
      "1497 Training Loss: tensor(0.3122)\n",
      "1498 Training Loss: tensor(0.2915)\n",
      "1499 Training Loss: tensor(0.2859)\n",
      "1500 Training Loss: tensor(0.2971)\n",
      "1501 Training Loss: tensor(0.2784)\n",
      "1502 Training Loss: tensor(0.2864)\n",
      "1503 Training Loss: tensor(0.3009)\n",
      "1504 Training Loss: tensor(0.2940)\n",
      "1505 Training Loss: tensor(0.2701)\n",
      "1506 Training Loss: tensor(0.2857)\n",
      "1507 Training Loss: tensor(0.2861)\n",
      "1508 Training Loss: tensor(0.2786)\n",
      "1509 Training Loss: tensor(0.2873)\n",
      "1510 Training Loss: tensor(0.2930)\n",
      "1511 Training Loss: tensor(0.2801)\n",
      "1512 Training Loss: tensor(0.2734)\n",
      "1513 Training Loss: tensor(0.2798)\n",
      "1514 Training Loss: tensor(0.2747)\n",
      "1515 Training Loss: tensor(0.2806)\n",
      "1516 Training Loss: tensor(0.2804)\n",
      "1517 Training Loss: tensor(0.2634)\n",
      "1518 Training Loss: tensor(0.2807)\n",
      "1519 Training Loss: tensor(0.2834)\n",
      "1520 Training Loss: tensor(0.2689)\n",
      "1521 Training Loss: tensor(0.2822)\n",
      "1522 Training Loss: tensor(0.2830)\n",
      "1523 Training Loss: tensor(0.2736)\n",
      "1524 Training Loss: tensor(0.2628)\n",
      "1525 Training Loss: tensor(0.2869)\n",
      "1526 Training Loss: tensor(0.2750)\n",
      "1527 Training Loss: tensor(0.2772)\n",
      "1528 Training Loss: tensor(0.2689)\n",
      "1529 Training Loss: tensor(0.2636)\n",
      "1530 Training Loss: tensor(0.2707)\n",
      "1531 Training Loss: tensor(0.2757)\n",
      "1532 Training Loss: tensor(0.2828)\n",
      "1533 Training Loss: tensor(0.2615)\n",
      "1534 Training Loss: tensor(0.2645)\n",
      "1535 Training Loss: tensor(0.2638)\n",
      "1536 Training Loss: tensor(0.2741)\n",
      "1537 Training Loss: tensor(0.2695)\n",
      "1538 Training Loss: tensor(0.2522)\n",
      "1539 Training Loss: tensor(0.2736)\n",
      "1540 Training Loss: tensor(0.2661)\n",
      "1541 Training Loss: tensor(0.2601)\n",
      "1542 Training Loss: tensor(0.2532)\n",
      "1543 Training Loss: tensor(0.2700)\n",
      "1544 Training Loss: tensor(0.2562)\n",
      "1545 Training Loss: tensor(0.2577)\n",
      "1546 Training Loss: tensor(0.2451)\n",
      "1547 Training Loss: tensor(0.2638)\n",
      "1548 Training Loss: tensor(0.2648)\n",
      "1549 Training Loss: tensor(0.2646)\n",
      "1550 Training Loss: tensor(0.2520)\n",
      "1551 Training Loss: tensor(0.2697)\n",
      "1552 Training Loss: tensor(0.2543)\n",
      "1553 Training Loss: tensor(0.2534)\n",
      "1554 Training Loss: tensor(0.2473)\n",
      "1555 Training Loss: tensor(0.2484)\n",
      "1556 Training Loss: tensor(0.2600)\n",
      "1557 Training Loss: tensor(0.2598)\n",
      "1558 Training Loss: tensor(0.2598)\n",
      "1559 Training Loss: tensor(0.2613)\n",
      "1560 Training Loss: tensor(0.2543)\n",
      "1561 Training Loss: tensor(0.2451)\n",
      "1562 Training Loss: tensor(0.2571)\n",
      "1563 Training Loss: tensor(0.2599)\n",
      "1564 Training Loss: tensor(0.2628)\n",
      "1565 Training Loss: tensor(0.2617)\n",
      "1566 Training Loss: tensor(0.2683)\n",
      "1567 Training Loss: tensor(0.2472)\n",
      "1568 Training Loss: tensor(0.2389)\n",
      "1569 Training Loss: tensor(0.2492)\n",
      "1570 Training Loss: tensor(0.2518)\n",
      "1571 Training Loss: tensor(0.2468)\n",
      "1572 Training Loss: tensor(0.2441)\n",
      "1573 Training Loss: tensor(0.2574)\n",
      "1574 Training Loss: tensor(0.2596)\n",
      "1575 Training Loss: tensor(0.2507)\n",
      "1576 Training Loss: tensor(0.2548)\n",
      "1577 Training Loss: tensor(0.2485)\n",
      "1578 Training Loss: tensor(0.2569)\n",
      "1579 Training Loss: tensor(0.2457)\n",
      "1580 Training Loss: tensor(0.2564)\n",
      "1581 Training Loss: tensor(0.2506)\n",
      "1582 Training Loss: tensor(0.2436)\n",
      "1583 Training Loss: tensor(0.2578)\n",
      "1584 Training Loss: tensor(0.2403)\n",
      "1585 Training Loss: tensor(0.2587)\n",
      "1586 Training Loss: tensor(0.2471)\n",
      "1587 Training Loss: tensor(0.2475)\n",
      "1588 Training Loss: tensor(0.2453)\n",
      "1589 Training Loss: tensor(0.2591)\n",
      "1590 Training Loss: tensor(0.2480)\n",
      "1591 Training Loss: tensor(0.2377)\n",
      "1592 Training Loss: tensor(0.2662)\n",
      "1593 Training Loss: tensor(0.2622)\n",
      "1594 Training Loss: tensor(0.2608)\n",
      "1595 Training Loss: tensor(0.2521)\n",
      "1596 Training Loss: tensor(0.2515)\n",
      "1597 Training Loss: tensor(0.2435)\n",
      "1598 Training Loss: tensor(0.2484)\n",
      "1599 Training Loss: tensor(0.2505)\n",
      "1600 Training Loss: tensor(0.2481)\n",
      "1601 Training Loss: tensor(0.2424)\n",
      "1602 Training Loss: tensor(0.2347)\n",
      "1603 Training Loss: tensor(0.2474)\n",
      "1604 Training Loss: tensor(0.2334)\n",
      "1605 Training Loss: tensor(0.2446)\n",
      "1606 Training Loss: tensor(0.2486)\n",
      "1607 Training Loss: tensor(0.2536)\n",
      "1608 Training Loss: tensor(0.2449)\n",
      "1609 Training Loss: tensor(0.2378)\n",
      "1610 Training Loss: tensor(0.2514)\n",
      "1611 Training Loss: tensor(0.2433)\n",
      "1612 Training Loss: tensor(0.2474)\n",
      "1613 Training Loss: tensor(0.2388)\n",
      "1614 Training Loss: tensor(0.2479)\n",
      "1615 Training Loss: tensor(0.2402)\n",
      "1616 Training Loss: tensor(0.2465)\n",
      "1617 Training Loss: tensor(0.2481)\n",
      "1618 Training Loss: tensor(0.2608)\n",
      "1619 Training Loss: tensor(0.2449)\n",
      "1620 Training Loss: tensor(0.2493)\n",
      "1621 Training Loss: tensor(0.2416)\n",
      "1622 Training Loss: tensor(0.2527)\n",
      "1623 Training Loss: tensor(0.2435)\n",
      "1624 Training Loss: tensor(0.2429)\n",
      "1625 Training Loss: tensor(0.2383)\n",
      "1626 Training Loss: tensor(0.2517)\n",
      "1627 Training Loss: tensor(0.2517)\n",
      "1628 Training Loss: tensor(0.2413)\n",
      "1629 Training Loss: tensor(0.2432)\n",
      "1630 Training Loss: tensor(0.2328)\n",
      "1631 Training Loss: tensor(0.2416)\n",
      "1632 Training Loss: tensor(0.2441)\n",
      "1633 Training Loss: tensor(0.2381)\n",
      "1634 Training Loss: tensor(0.2397)\n",
      "1635 Training Loss: tensor(0.2382)\n",
      "1636 Training Loss: tensor(0.2380)\n",
      "1637 Training Loss: tensor(0.2488)\n",
      "1638 Training Loss: tensor(0.2332)\n",
      "1639 Training Loss: tensor(0.2180)\n",
      "1640 Training Loss: tensor(0.2345)\n",
      "1641 Training Loss: tensor(0.2280)\n",
      "1642 Training Loss: tensor(0.2394)\n",
      "1643 Training Loss: tensor(0.2327)\n",
      "1644 Training Loss: tensor(0.2258)\n",
      "1645 Training Loss: tensor(0.2311)\n",
      "1646 Training Loss: tensor(0.2349)\n",
      "1647 Training Loss: tensor(0.2356)\n",
      "1648 Training Loss: tensor(0.2397)\n",
      "1649 Training Loss: tensor(0.2395)\n",
      "1650 Training Loss: tensor(0.2439)\n",
      "1651 Training Loss: tensor(0.2275)\n",
      "1652 Training Loss: tensor(0.2427)\n",
      "1653 Training Loss: tensor(0.2406)\n",
      "1654 Training Loss: tensor(0.2551)\n",
      "1655 Training Loss: tensor(0.2419)\n",
      "1656 Training Loss: tensor(0.2548)\n",
      "1657 Training Loss: tensor(0.2331)\n",
      "1658 Training Loss: tensor(0.2449)\n",
      "1659 Training Loss: tensor(0.2369)\n",
      "1660 Training Loss: tensor(0.2292)\n",
      "1661 Training Loss: tensor(0.2363)\n",
      "1662 Training Loss: tensor(0.2280)\n",
      "1663 Training Loss: tensor(0.2385)\n",
      "1664 Training Loss: tensor(0.2457)\n",
      "1665 Training Loss: tensor(0.2321)\n",
      "1666 Training Loss: tensor(0.2392)\n",
      "1667 Training Loss: tensor(0.2313)\n",
      "1668 Training Loss: tensor(0.2352)\n",
      "1669 Training Loss: tensor(0.2322)\n",
      "1670 Training Loss: tensor(0.2354)\n",
      "1671 Training Loss: tensor(0.2266)\n",
      "1672 Training Loss: tensor(0.2445)\n",
      "1673 Training Loss: tensor(0.2285)\n",
      "1674 Training Loss: tensor(0.2365)\n",
      "1675 Training Loss: tensor(0.2387)\n",
      "1676 Training Loss: tensor(0.2422)\n",
      "1677 Training Loss: tensor(0.2226)\n",
      "1678 Training Loss: tensor(0.2423)\n",
      "1679 Training Loss: tensor(0.2348)\n",
      "1680 Training Loss: tensor(0.2259)\n",
      "1681 Training Loss: tensor(0.2347)\n",
      "1682 Training Loss: tensor(0.2280)\n",
      "1683 Training Loss: tensor(0.2274)\n",
      "1684 Training Loss: tensor(0.2328)\n",
      "1685 Training Loss: tensor(0.2394)\n",
      "1686 Training Loss: tensor(0.2217)\n",
      "1687 Training Loss: tensor(0.2314)\n",
      "1688 Training Loss: tensor(0.2363)\n",
      "1689 Training Loss: tensor(0.2368)\n",
      "1690 Training Loss: tensor(0.2295)\n",
      "1691 Training Loss: tensor(0.2291)\n",
      "1692 Training Loss: tensor(0.2288)\n",
      "1693 Training Loss: tensor(0.2344)\n",
      "1694 Training Loss: tensor(0.2272)\n",
      "1695 Training Loss: tensor(0.2387)\n",
      "1696 Training Loss: tensor(0.2173)\n",
      "1697 Training Loss: tensor(0.2304)\n",
      "1698 Training Loss: tensor(0.2281)\n",
      "1699 Training Loss: tensor(0.2393)\n",
      "1700 Training Loss: tensor(0.2323)\n",
      "1701 Training Loss: tensor(0.2239)\n",
      "1702 Training Loss: tensor(0.2309)\n",
      "1703 Training Loss: tensor(0.2180)\n",
      "1704 Training Loss: tensor(0.2224)\n",
      "1705 Training Loss: tensor(0.2248)\n",
      "1706 Training Loss: tensor(0.2372)\n",
      "1707 Training Loss: tensor(0.2320)\n",
      "1708 Training Loss: tensor(0.2232)\n",
      "1709 Training Loss: tensor(0.2352)\n",
      "1710 Training Loss: tensor(0.2535)\n",
      "1711 Training Loss: tensor(0.2225)\n",
      "1712 Training Loss: tensor(0.2281)\n",
      "1713 Training Loss: tensor(0.2345)\n",
      "1714 Training Loss: tensor(0.2251)\n",
      "1715 Training Loss: tensor(0.2365)\n",
      "1716 Training Loss: tensor(0.2266)\n",
      "1717 Training Loss: tensor(0.2229)\n",
      "1718 Training Loss: tensor(0.2279)\n",
      "1719 Training Loss: tensor(0.2157)\n",
      "1720 Training Loss: tensor(0.2277)\n",
      "1721 Training Loss: tensor(0.2198)\n",
      "1722 Training Loss: tensor(0.2270)\n",
      "1723 Training Loss: tensor(0.2218)\n",
      "1724 Training Loss: tensor(0.2267)\n",
      "1725 Training Loss: tensor(0.2208)\n",
      "1726 Training Loss: tensor(0.2207)\n",
      "1727 Training Loss: tensor(0.2297)\n",
      "1728 Training Loss: tensor(0.2247)\n",
      "1729 Training Loss: tensor(0.2325)\n",
      "1730 Training Loss: tensor(0.2295)\n",
      "1731 Training Loss: tensor(0.2203)\n",
      "1732 Training Loss: tensor(0.2189)\n",
      "1733 Training Loss: tensor(0.2168)\n",
      "1734 Training Loss: tensor(0.2200)\n",
      "1735 Training Loss: tensor(0.2218)\n",
      "1736 Training Loss: tensor(0.2335)\n",
      "1737 Training Loss: tensor(0.2155)\n",
      "1738 Training Loss: tensor(0.2234)\n",
      "1739 Training Loss: tensor(0.2292)\n",
      "1740 Training Loss: tensor(0.2191)\n",
      "1741 Training Loss: tensor(0.2172)\n",
      "1742 Training Loss: tensor(0.2200)\n",
      "1743 Training Loss: tensor(0.2299)\n",
      "1744 Training Loss: tensor(0.2300)\n",
      "1745 Training Loss: tensor(0.2249)\n",
      "1746 Training Loss: tensor(0.2290)\n",
      "1747 Training Loss: tensor(0.2259)\n",
      "1748 Training Loss: tensor(0.2312)\n",
      "1749 Training Loss: tensor(0.2179)\n",
      "1750 Training Loss: tensor(0.2243)\n",
      "1751 Training Loss: tensor(0.2104)\n",
      "1752 Training Loss: tensor(0.2211)\n",
      "1753 Training Loss: tensor(0.2264)\n",
      "1754 Training Loss: tensor(0.2210)\n",
      "1755 Training Loss: tensor(0.2261)\n",
      "1756 Training Loss: tensor(0.2107)\n",
      "1757 Training Loss: tensor(0.2182)\n",
      "1758 Training Loss: tensor(0.2351)\n",
      "1759 Training Loss: tensor(0.2099)\n",
      "1760 Training Loss: tensor(0.2173)\n",
      "1761 Training Loss: tensor(0.2193)\n",
      "1762 Training Loss: tensor(0.2195)\n",
      "1763 Training Loss: tensor(0.2165)\n",
      "1764 Training Loss: tensor(0.2199)\n",
      "1765 Training Loss: tensor(0.2232)\n",
      "1766 Training Loss: tensor(0.2189)\n",
      "1767 Training Loss: tensor(0.2151)\n",
      "1768 Training Loss: tensor(0.2241)\n",
      "1769 Training Loss: tensor(0.2236)\n",
      "1770 Training Loss: tensor(0.2078)\n",
      "1771 Training Loss: tensor(0.2235)\n",
      "1772 Training Loss: tensor(0.2098)\n",
      "1773 Training Loss: tensor(0.2090)\n",
      "1774 Training Loss: tensor(0.2140)\n",
      "1775 Training Loss: tensor(0.2324)\n",
      "1776 Training Loss: tensor(0.2268)\n",
      "1777 Training Loss: tensor(0.2247)\n",
      "1778 Training Loss: tensor(0.2166)\n",
      "1779 Training Loss: tensor(0.2230)\n",
      "1780 Training Loss: tensor(0.2301)\n",
      "1781 Training Loss: tensor(0.2028)\n",
      "1782 Training Loss: tensor(0.2201)\n",
      "1783 Training Loss: tensor(0.2125)\n",
      "1784 Training Loss: tensor(0.2242)\n",
      "1785 Training Loss: tensor(0.2263)\n",
      "1786 Training Loss: tensor(0.2142)\n",
      "1787 Training Loss: tensor(0.2202)\n",
      "1788 Training Loss: tensor(0.2183)\n",
      "1789 Training Loss: tensor(0.2146)\n",
      "1790 Training Loss: tensor(0.2332)\n",
      "1791 Training Loss: tensor(0.2169)\n",
      "1792 Training Loss: tensor(0.2078)\n",
      "1793 Training Loss: tensor(0.2236)\n",
      "1794 Training Loss: tensor(0.2110)\n",
      "1795 Training Loss: tensor(0.2143)\n",
      "1796 Training Loss: tensor(0.2077)\n",
      "1797 Training Loss: tensor(0.2164)\n",
      "1798 Training Loss: tensor(0.2195)\n",
      "1799 Training Loss: tensor(0.2112)\n",
      "1800 Training Loss: tensor(0.2355)\n",
      "1801 Training Loss: tensor(0.2174)\n",
      "1802 Training Loss: tensor(0.2113)\n",
      "1803 Training Loss: tensor(0.2173)\n",
      "1804 Training Loss: tensor(0.2232)\n",
      "1805 Training Loss: tensor(0.2100)\n",
      "1806 Training Loss: tensor(0.2232)\n",
      "1807 Training Loss: tensor(0.2191)\n",
      "1808 Training Loss: tensor(0.2140)\n",
      "1809 Training Loss: tensor(0.2066)\n",
      "1810 Training Loss: tensor(0.2149)\n",
      "1811 Training Loss: tensor(0.2057)\n",
      "1812 Training Loss: tensor(0.2243)\n",
      "1813 Training Loss: tensor(0.2191)\n",
      "1814 Training Loss: tensor(0.2096)\n",
      "1815 Training Loss: tensor(0.2144)\n",
      "1816 Training Loss: tensor(0.2141)\n",
      "1817 Training Loss: tensor(0.2193)\n",
      "1818 Training Loss: tensor(0.2131)\n",
      "1819 Training Loss: tensor(0.2212)\n",
      "1820 Training Loss: tensor(0.2247)\n",
      "1821 Training Loss: tensor(0.2126)\n",
      "1822 Training Loss: tensor(0.2118)\n",
      "1823 Training Loss: tensor(0.2164)\n",
      "1824 Training Loss: tensor(0.2125)\n",
      "1825 Training Loss: tensor(0.2145)\n",
      "1826 Training Loss: tensor(0.2028)\n",
      "1827 Training Loss: tensor(0.2123)\n",
      "1828 Training Loss: tensor(0.2204)\n",
      "1829 Training Loss: tensor(0.1966)\n",
      "1830 Training Loss: tensor(0.2115)\n",
      "1831 Training Loss: tensor(0.2115)\n",
      "1832 Training Loss: tensor(0.2096)\n",
      "1833 Training Loss: tensor(0.2097)\n",
      "1834 Training Loss: tensor(0.2125)\n",
      "1835 Training Loss: tensor(0.2212)\n",
      "1836 Training Loss: tensor(0.2190)\n",
      "1837 Training Loss: tensor(0.2222)\n",
      "1838 Training Loss: tensor(0.2121)\n",
      "1839 Training Loss: tensor(0.2121)\n",
      "1840 Training Loss: tensor(0.2012)\n",
      "1841 Training Loss: tensor(0.2091)\n",
      "1842 Training Loss: tensor(0.2101)\n",
      "1843 Training Loss: tensor(0.1960)\n",
      "1844 Training Loss: tensor(0.2102)\n",
      "1845 Training Loss: tensor(0.2076)\n",
      "1846 Training Loss: tensor(0.2063)\n",
      "1847 Training Loss: tensor(0.2191)\n",
      "1848 Training Loss: tensor(0.2142)\n",
      "1849 Training Loss: tensor(0.2179)\n",
      "1850 Training Loss: tensor(0.2172)\n",
      "1851 Training Loss: tensor(0.2206)\n",
      "1852 Training Loss: tensor(0.2132)\n",
      "1853 Training Loss: tensor(0.2047)\n",
      "1854 Training Loss: tensor(0.2168)\n",
      "1855 Training Loss: tensor(0.2072)\n",
      "1856 Training Loss: tensor(0.2094)\n",
      "1857 Training Loss: tensor(0.2003)\n",
      "1858 Training Loss: tensor(0.2194)\n",
      "1859 Training Loss: tensor(0.2096)\n",
      "1860 Training Loss: tensor(0.2075)\n",
      "1861 Training Loss: tensor(0.2058)\n",
      "1862 Training Loss: tensor(0.2125)\n",
      "1863 Training Loss: tensor(0.2233)\n",
      "1864 Training Loss: tensor(0.2133)\n",
      "1865 Training Loss: tensor(0.2129)\n",
      "1866 Training Loss: tensor(0.2092)\n",
      "1867 Training Loss: tensor(0.2086)\n",
      "1868 Training Loss: tensor(0.2102)\n",
      "1869 Training Loss: tensor(0.2080)\n",
      "1870 Training Loss: tensor(0.1967)\n",
      "1871 Training Loss: tensor(0.2197)\n",
      "1872 Training Loss: tensor(0.2113)\n",
      "1873 Training Loss: tensor(0.2039)\n",
      "1874 Training Loss: tensor(0.2116)\n",
      "1875 Training Loss: tensor(0.2207)\n",
      "1876 Training Loss: tensor(0.1960)\n",
      "1877 Training Loss: tensor(0.2112)\n",
      "1878 Training Loss: tensor(0.2131)\n",
      "1879 Training Loss: tensor(0.2118)\n",
      "1880 Training Loss: tensor(0.2064)\n",
      "1881 Training Loss: tensor(0.2105)\n",
      "1882 Training Loss: tensor(0.2119)\n",
      "1883 Training Loss: tensor(0.2134)\n",
      "1884 Training Loss: tensor(0.2123)\n",
      "1885 Training Loss: tensor(0.1990)\n",
      "1886 Training Loss: tensor(0.2130)\n",
      "1887 Training Loss: tensor(0.2072)\n",
      "1888 Training Loss: tensor(0.2041)\n",
      "1889 Training Loss: tensor(0.2112)\n",
      "1890 Training Loss: tensor(0.2093)\n",
      "1891 Training Loss: tensor(0.2074)\n",
      "1892 Training Loss: tensor(0.2034)\n",
      "1893 Training Loss: tensor(0.2143)\n",
      "1894 Training Loss: tensor(0.2109)\n",
      "1895 Training Loss: tensor(0.2066)\n",
      "1896 Training Loss: tensor(0.2063)\n",
      "1897 Training Loss: tensor(0.2088)\n",
      "1898 Training Loss: tensor(0.2100)\n",
      "1899 Training Loss: tensor(0.2048)\n",
      "1900 Training Loss: tensor(0.2135)\n",
      "1901 Training Loss: tensor(0.2178)\n",
      "1902 Training Loss: tensor(0.2062)\n",
      "1903 Training Loss: tensor(0.2061)\n",
      "1904 Training Loss: tensor(0.1942)\n",
      "1905 Training Loss: tensor(0.2046)\n",
      "1906 Training Loss: tensor(0.2157)\n",
      "1907 Training Loss: tensor(0.2033)\n",
      "1908 Training Loss: tensor(0.2110)\n",
      "1909 Training Loss: tensor(0.1936)\n",
      "1910 Training Loss: tensor(0.2122)\n",
      "1911 Training Loss: tensor(0.2035)\n",
      "1912 Training Loss: tensor(0.1956)\n",
      "1913 Training Loss: tensor(0.1938)\n",
      "1914 Training Loss: tensor(0.2005)\n",
      "1915 Training Loss: tensor(0.1937)\n",
      "1916 Training Loss: tensor(0.2120)\n",
      "1917 Training Loss: tensor(0.2053)\n",
      "1918 Training Loss: tensor(0.2087)\n",
      "1919 Training Loss: tensor(0.2000)\n",
      "1920 Training Loss: tensor(0.2112)\n",
      "1921 Training Loss: tensor(0.2136)\n",
      "1922 Training Loss: tensor(0.2050)\n",
      "1923 Training Loss: tensor(0.1994)\n",
      "1924 Training Loss: tensor(0.2083)\n",
      "1925 Training Loss: tensor(0.2028)\n",
      "1926 Training Loss: tensor(0.1937)\n",
      "1927 Training Loss: tensor(0.1974)\n",
      "1928 Training Loss: tensor(0.2115)\n",
      "1929 Training Loss: tensor(0.2078)\n",
      "1930 Training Loss: tensor(0.2037)\n",
      "1931 Training Loss: tensor(0.2064)\n",
      "1932 Training Loss: tensor(0.2103)\n",
      "1933 Training Loss: tensor(0.1928)\n",
      "1934 Training Loss: tensor(0.2088)\n",
      "1935 Training Loss: tensor(0.1985)\n",
      "1936 Training Loss: tensor(0.2153)\n",
      "1937 Training Loss: tensor(0.1979)\n",
      "1938 Training Loss: tensor(0.2039)\n",
      "1939 Training Loss: tensor(0.2020)\n",
      "1940 Training Loss: tensor(0.1966)\n",
      "1941 Training Loss: tensor(0.1980)\n",
      "1942 Training Loss: tensor(0.2039)\n",
      "1943 Training Loss: tensor(0.2083)\n",
      "1944 Training Loss: tensor(0.2156)\n",
      "1945 Training Loss: tensor(0.2039)\n",
      "1946 Training Loss: tensor(0.2132)\n",
      "1947 Training Loss: tensor(0.2074)\n",
      "1948 Training Loss: tensor(0.1940)\n",
      "1949 Training Loss: tensor(0.1950)\n",
      "1950 Training Loss: tensor(0.2098)\n",
      "1951 Training Loss: tensor(0.1946)\n",
      "1952 Training Loss: tensor(0.2028)\n",
      "1953 Training Loss: tensor(0.1936)\n",
      "1954 Training Loss: tensor(0.1953)\n",
      "1955 Training Loss: tensor(0.2042)\n",
      "1956 Training Loss: tensor(0.1992)\n",
      "1957 Training Loss: tensor(0.1940)\n",
      "1958 Training Loss: tensor(0.1941)\n",
      "1959 Training Loss: tensor(0.2068)\n",
      "1960 Training Loss: tensor(0.1948)\n",
      "1961 Training Loss: tensor(0.1848)\n",
      "1962 Training Loss: tensor(0.1975)\n",
      "1963 Training Loss: tensor(0.1965)\n",
      "1964 Training Loss: tensor(0.1952)\n",
      "1965 Training Loss: tensor(0.2010)\n",
      "1966 Training Loss: tensor(0.1911)\n",
      "1967 Training Loss: tensor(0.2034)\n",
      "1968 Training Loss: tensor(0.1936)\n",
      "1969 Training Loss: tensor(0.2147)\n",
      "1970 Training Loss: tensor(0.2038)\n",
      "1971 Training Loss: tensor(0.2082)\n",
      "1972 Training Loss: tensor(0.1876)\n",
      "1973 Training Loss: tensor(0.2058)\n",
      "1974 Training Loss: tensor(0.1964)\n",
      "1975 Training Loss: tensor(0.1961)\n",
      "1976 Training Loss: tensor(0.1958)\n",
      "1977 Training Loss: tensor(0.2144)\n",
      "1978 Training Loss: tensor(0.2085)\n",
      "1979 Training Loss: tensor(0.1992)\n",
      "1980 Training Loss: tensor(0.1980)\n",
      "1981 Training Loss: tensor(0.1955)\n",
      "1982 Training Loss: tensor(0.2042)\n",
      "1983 Training Loss: tensor(0.1882)\n",
      "1984 Training Loss: tensor(0.2005)\n",
      "1985 Training Loss: tensor(0.1970)\n",
      "1986 Training Loss: tensor(0.1977)\n",
      "1987 Training Loss: tensor(0.2062)\n",
      "1988 Training Loss: tensor(0.1956)\n",
      "1989 Training Loss: tensor(0.2031)\n",
      "1990 Training Loss: tensor(0.1999)\n",
      "1991 Training Loss: tensor(0.1970)\n",
      "1992 Training Loss: tensor(0.1968)\n",
      "1993 Training Loss: tensor(0.2047)\n",
      "1994 Training Loss: tensor(0.1997)\n",
      "1995 Training Loss: tensor(0.2021)\n",
      "1996 Training Loss: tensor(0.1786)\n",
      "1997 Training Loss: tensor(0.1907)\n",
      "1998 Training Loss: tensor(0.1938)\n",
      "1999 Training Loss: tensor(0.1949)\n",
      "2000 Training Loss: tensor(0.1980)\n",
      "2001 Training Loss: tensor(0.1954)\n",
      "2002 Training Loss: tensor(0.1980)\n",
      "2003 Training Loss: tensor(0.1882)\n",
      "2004 Training Loss: tensor(0.1965)\n",
      "2005 Training Loss: tensor(0.1913)\n",
      "2006 Training Loss: tensor(0.1857)\n",
      "2007 Training Loss: tensor(0.1889)\n",
      "2008 Training Loss: tensor(0.1901)\n",
      "2009 Training Loss: tensor(0.1944)\n",
      "2010 Training Loss: tensor(0.1864)\n",
      "2011 Training Loss: tensor(0.1933)\n",
      "2012 Training Loss: tensor(0.1920)\n",
      "2013 Training Loss: tensor(0.1918)\n",
      "2014 Training Loss: tensor(0.1935)\n",
      "2015 Training Loss: tensor(0.1794)\n",
      "2016 Training Loss: tensor(0.1786)\n",
      "2017 Training Loss: tensor(0.1922)\n",
      "2018 Training Loss: tensor(0.1867)\n",
      "2019 Training Loss: tensor(0.1984)\n",
      "2020 Training Loss: tensor(0.1957)\n",
      "2021 Training Loss: tensor(0.1947)\n",
      "2022 Training Loss: tensor(0.1973)\n",
      "2023 Training Loss: tensor(0.1863)\n",
      "2024 Training Loss: tensor(0.1861)\n",
      "2025 Training Loss: tensor(0.1954)\n",
      "2026 Training Loss: tensor(0.1998)\n",
      "2027 Training Loss: tensor(0.1829)\n",
      "2028 Training Loss: tensor(0.1966)\n",
      "2029 Training Loss: tensor(0.1928)\n",
      "2030 Training Loss: tensor(0.1896)\n",
      "2031 Training Loss: tensor(0.1900)\n",
      "2032 Training Loss: tensor(0.1888)\n",
      "2033 Training Loss: tensor(0.1908)\n",
      "2034 Training Loss: tensor(0.2000)\n",
      "2035 Training Loss: tensor(0.1976)\n",
      "2036 Training Loss: tensor(0.1855)\n",
      "2037 Training Loss: tensor(0.1790)\n",
      "2038 Training Loss: tensor(0.1972)\n",
      "2039 Training Loss: tensor(0.1838)\n",
      "2040 Training Loss: tensor(0.1986)\n",
      "2041 Training Loss: tensor(0.1832)\n",
      "2042 Training Loss: tensor(0.2023)\n",
      "2043 Training Loss: tensor(0.1853)\n",
      "2044 Training Loss: tensor(0.1849)\n",
      "2045 Training Loss: tensor(0.1853)\n",
      "2046 Training Loss: tensor(0.1982)\n",
      "2047 Training Loss: tensor(0.1891)\n",
      "2048 Training Loss: tensor(0.1841)\n",
      "2049 Training Loss: tensor(0.1860)\n",
      "2050 Training Loss: tensor(0.1866)\n",
      "2051 Training Loss: tensor(0.1769)\n",
      "2052 Training Loss: tensor(0.1872)\n",
      "2053 Training Loss: tensor(0.1770)\n",
      "2054 Training Loss: tensor(0.1859)\n",
      "2055 Training Loss: tensor(0.1759)\n",
      "2056 Training Loss: tensor(0.1900)\n",
      "2057 Training Loss: tensor(0.1963)\n",
      "2058 Training Loss: tensor(0.1754)\n",
      "2059 Training Loss: tensor(0.1861)\n",
      "2060 Training Loss: tensor(0.1753)\n",
      "2061 Training Loss: tensor(0.1853)\n",
      "2062 Training Loss: tensor(0.1864)\n",
      "2063 Training Loss: tensor(0.1993)\n",
      "2064 Training Loss: tensor(0.1861)\n",
      "2065 Training Loss: tensor(0.1798)\n",
      "2066 Training Loss: tensor(0.1733)\n",
      "2067 Training Loss: tensor(0.1749)\n",
      "2068 Training Loss: tensor(0.1860)\n",
      "2069 Training Loss: tensor(0.1737)\n",
      "2070 Training Loss: tensor(0.1832)\n",
      "2071 Training Loss: tensor(0.1869)\n",
      "2072 Training Loss: tensor(0.1862)\n",
      "2073 Training Loss: tensor(0.1857)\n",
      "2074 Training Loss: tensor(0.1750)\n",
      "2075 Training Loss: tensor(0.1853)\n",
      "2076 Training Loss: tensor(0.1820)\n",
      "2077 Training Loss: tensor(0.1794)\n",
      "2078 Training Loss: tensor(0.1679)\n",
      "2079 Training Loss: tensor(0.1776)\n",
      "2080 Training Loss: tensor(0.1668)\n",
      "2081 Training Loss: tensor(0.1797)\n",
      "2082 Training Loss: tensor(0.1665)\n",
      "2083 Training Loss: tensor(0.1776)\n",
      "2084 Training Loss: tensor(0.1739)\n",
      "2085 Training Loss: tensor(0.1830)\n",
      "2086 Training Loss: tensor(0.1803)\n",
      "2087 Training Loss: tensor(0.1846)\n",
      "2088 Training Loss: tensor(0.1774)\n",
      "2089 Training Loss: tensor(0.1850)\n",
      "2090 Training Loss: tensor(0.1806)\n",
      "2091 Training Loss: tensor(0.1735)\n",
      "2092 Training Loss: tensor(0.1779)\n",
      "2093 Training Loss: tensor(0.1801)\n",
      "2094 Training Loss: tensor(0.1735)\n",
      "2095 Training Loss: tensor(0.1639)\n",
      "2096 Training Loss: tensor(0.1661)\n",
      "2097 Training Loss: tensor(0.1759)\n",
      "2098 Training Loss: tensor(0.1713)\n",
      "2099 Training Loss: tensor(0.1742)\n",
      "2100 Training Loss: tensor(0.1759)\n",
      "2101 Training Loss: tensor(0.1749)\n",
      "2102 Training Loss: tensor(0.1638)\n",
      "2103 Training Loss: tensor(0.1676)\n",
      "2104 Training Loss: tensor(0.1656)\n",
      "2105 Training Loss: tensor(0.1652)\n",
      "2106 Training Loss: tensor(0.1789)\n",
      "2107 Training Loss: tensor(0.1847)\n",
      "2108 Training Loss: tensor(0.1677)\n",
      "2109 Training Loss: tensor(0.1571)\n",
      "2110 Training Loss: tensor(0.1639)\n",
      "2111 Training Loss: tensor(0.1634)\n",
      "2112 Training Loss: tensor(0.1652)\n",
      "2113 Training Loss: tensor(0.1690)\n",
      "2114 Training Loss: tensor(0.1693)\n",
      "2115 Training Loss: tensor(0.1655)\n",
      "2116 Training Loss: tensor(0.1712)\n",
      "2117 Training Loss: tensor(0.1644)\n",
      "2118 Training Loss: tensor(0.1669)\n",
      "2119 Training Loss: tensor(0.1622)\n",
      "2120 Training Loss: tensor(0.1679)\n",
      "2121 Training Loss: tensor(0.1527)\n",
      "2122 Training Loss: tensor(0.1743)\n",
      "2123 Training Loss: tensor(0.1551)\n",
      "2124 Training Loss: tensor(0.1611)\n",
      "2125 Training Loss: tensor(0.1706)\n",
      "2126 Training Loss: tensor(0.1643)\n",
      "2127 Training Loss: tensor(0.1627)\n",
      "2128 Training Loss: tensor(0.1668)\n",
      "2129 Training Loss: tensor(0.1576)\n",
      "2130 Training Loss: tensor(0.1665)\n",
      "2131 Training Loss: tensor(0.1503)\n",
      "2132 Training Loss: tensor(0.1652)\n",
      "2133 Training Loss: tensor(0.1400)\n",
      "2134 Training Loss: tensor(0.1508)\n",
      "2135 Training Loss: tensor(0.1526)\n",
      "2136 Training Loss: tensor(0.1461)\n",
      "2137 Training Loss: tensor(0.1499)\n",
      "2138 Training Loss: tensor(0.1506)\n",
      "2139 Training Loss: tensor(0.1531)\n",
      "2140 Training Loss: tensor(0.1422)\n",
      "2141 Training Loss: tensor(0.1413)\n",
      "2142 Training Loss: tensor(0.1518)\n",
      "2143 Training Loss: tensor(0.1503)\n",
      "2144 Training Loss: tensor(0.1517)\n",
      "2145 Training Loss: tensor(0.1524)\n",
      "2146 Training Loss: tensor(0.1533)\n",
      "2147 Training Loss: tensor(0.1458)\n",
      "2148 Training Loss: tensor(0.1496)\n",
      "2149 Training Loss: tensor(0.1519)\n",
      "2150 Training Loss: tensor(0.1383)\n",
      "2151 Training Loss: tensor(0.1461)\n",
      "2152 Training Loss: tensor(0.1497)\n",
      "2153 Training Loss: tensor(0.1512)\n",
      "2154 Training Loss: tensor(0.1369)\n",
      "2155 Training Loss: tensor(0.1383)\n",
      "2156 Training Loss: tensor(0.1509)\n",
      "2157 Training Loss: tensor(0.1461)\n",
      "2158 Training Loss: tensor(0.1362)\n",
      "2159 Training Loss: tensor(0.1376)\n",
      "2160 Training Loss: tensor(0.1415)\n",
      "2161 Training Loss: tensor(0.1380)\n",
      "2162 Training Loss: tensor(0.1331)\n",
      "2163 Training Loss: tensor(0.1352)\n",
      "2164 Training Loss: tensor(0.1405)\n",
      "2165 Training Loss: tensor(0.1415)\n",
      "2166 Training Loss: tensor(0.1344)\n",
      "2167 Training Loss: tensor(0.1406)\n",
      "2168 Training Loss: tensor(0.1356)\n",
      "2169 Training Loss: tensor(0.1438)\n",
      "2170 Training Loss: tensor(0.1347)\n",
      "2171 Training Loss: tensor(0.1365)\n",
      "2172 Training Loss: tensor(0.1345)\n",
      "2173 Training Loss: tensor(0.1262)\n",
      "2174 Training Loss: tensor(0.1316)\n",
      "2175 Training Loss: tensor(0.1294)\n",
      "2176 Training Loss: tensor(0.1280)\n",
      "2177 Training Loss: tensor(0.1341)\n",
      "2178 Training Loss: tensor(0.1320)\n",
      "2179 Training Loss: tensor(0.1275)\n",
      "2180 Training Loss: tensor(0.1405)\n",
      "2181 Training Loss: tensor(0.1322)\n",
      "2182 Training Loss: tensor(0.1300)\n",
      "2183 Training Loss: tensor(0.1364)\n",
      "2184 Training Loss: tensor(0.1262)\n",
      "2185 Training Loss: tensor(0.1323)\n",
      "2186 Training Loss: tensor(0.1269)\n",
      "2187 Training Loss: tensor(0.1359)\n",
      "2188 Training Loss: tensor(0.1292)\n",
      "2189 Training Loss: tensor(0.1293)\n",
      "2190 Training Loss: tensor(0.1327)\n",
      "2191 Training Loss: tensor(0.1394)\n",
      "2192 Training Loss: tensor(0.1268)\n",
      "2193 Training Loss: tensor(0.1214)\n",
      "2194 Training Loss: tensor(0.1240)\n",
      "2195 Training Loss: tensor(0.1307)\n",
      "2196 Training Loss: tensor(0.1211)\n",
      "2197 Training Loss: tensor(0.1356)\n",
      "2198 Training Loss: tensor(0.1167)\n",
      "2199 Training Loss: tensor(0.1211)\n",
      "2200 Training Loss: tensor(0.1201)\n",
      "2201 Training Loss: tensor(0.1278)\n",
      "2202 Training Loss: tensor(0.1232)\n",
      "2203 Training Loss: tensor(0.1274)\n",
      "2204 Training Loss: tensor(0.1298)\n",
      "2205 Training Loss: tensor(0.1211)\n",
      "2206 Training Loss: tensor(0.1312)\n",
      "2207 Training Loss: tensor(0.1275)\n",
      "2208 Training Loss: tensor(0.1281)\n",
      "2209 Training Loss: tensor(0.1218)\n",
      "2210 Training Loss: tensor(0.1255)\n",
      "2211 Training Loss: tensor(0.1205)\n",
      "2212 Training Loss: tensor(0.1167)\n",
      "2213 Training Loss: tensor(0.1230)\n",
      "2214 Training Loss: tensor(0.1228)\n",
      "2215 Training Loss: tensor(0.1146)\n",
      "2216 Training Loss: tensor(0.1142)\n",
      "2217 Training Loss: tensor(0.1259)\n",
      "2218 Training Loss: tensor(0.1310)\n",
      "2219 Training Loss: tensor(0.1253)\n",
      "2220 Training Loss: tensor(0.1182)\n",
      "2221 Training Loss: tensor(0.1226)\n",
      "2222 Training Loss: tensor(0.1208)\n",
      "2223 Training Loss: tensor(0.1181)\n",
      "2224 Training Loss: tensor(0.1211)\n",
      "2225 Training Loss: tensor(0.1140)\n",
      "2226 Training Loss: tensor(0.1185)\n",
      "2227 Training Loss: tensor(0.1313)\n",
      "2228 Training Loss: tensor(0.1201)\n",
      "2229 Training Loss: tensor(0.1290)\n",
      "2230 Training Loss: tensor(0.1187)\n",
      "2231 Training Loss: tensor(0.1208)\n",
      "2232 Training Loss: tensor(0.1287)\n",
      "2233 Training Loss: tensor(0.1221)\n",
      "2234 Training Loss: tensor(0.1113)\n",
      "2235 Training Loss: tensor(0.1303)\n",
      "2236 Training Loss: tensor(0.1240)\n",
      "2237 Training Loss: tensor(0.1188)\n",
      "2238 Training Loss: tensor(0.1289)\n",
      "2239 Training Loss: tensor(0.1278)\n",
      "2240 Training Loss: tensor(0.1320)\n",
      "2241 Training Loss: tensor(0.1271)\n",
      "2242 Training Loss: tensor(0.1296)\n",
      "2243 Training Loss: tensor(0.1113)\n",
      "2244 Training Loss: tensor(0.1161)\n",
      "2245 Training Loss: tensor(0.1204)\n",
      "2246 Training Loss: tensor(0.1193)\n",
      "2247 Training Loss: tensor(0.1191)\n",
      "2248 Training Loss: tensor(0.1152)\n",
      "2249 Training Loss: tensor(0.1092)\n",
      "2250 Training Loss: tensor(0.1213)\n",
      "2251 Training Loss: tensor(0.1161)\n",
      "2252 Training Loss: tensor(0.1108)\n",
      "2253 Training Loss: tensor(0.1167)\n",
      "2254 Training Loss: tensor(0.1220)\n",
      "2255 Training Loss: tensor(0.1106)\n",
      "2256 Training Loss: tensor(0.1207)\n",
      "2257 Training Loss: tensor(0.1167)\n",
      "2258 Training Loss: tensor(0.1159)\n",
      "2259 Training Loss: tensor(0.1181)\n",
      "2260 Training Loss: tensor(0.1129)\n",
      "2261 Training Loss: tensor(0.1264)\n",
      "2262 Training Loss: tensor(0.1193)\n",
      "2263 Training Loss: tensor(0.1245)\n",
      "2264 Training Loss: tensor(0.1101)\n",
      "2265 Training Loss: tensor(0.1116)\n",
      "2266 Training Loss: tensor(0.1252)\n",
      "2267 Training Loss: tensor(0.1161)\n",
      "2268 Training Loss: tensor(0.1241)\n",
      "2269 Training Loss: tensor(0.1211)\n",
      "2270 Training Loss: tensor(0.1138)\n",
      "2271 Training Loss: tensor(0.1219)\n",
      "2272 Training Loss: tensor(0.1188)\n",
      "2273 Training Loss: tensor(0.1146)\n",
      "2274 Training Loss: tensor(0.1197)\n",
      "2275 Training Loss: tensor(0.1044)\n",
      "2276 Training Loss: tensor(0.1169)\n",
      "2277 Training Loss: tensor(0.1102)\n",
      "2278 Training Loss: tensor(0.1183)\n",
      "2279 Training Loss: tensor(0.1172)\n",
      "2280 Training Loss: tensor(0.1167)\n",
      "2281 Training Loss: tensor(0.1056)\n",
      "2282 Training Loss: tensor(0.1217)\n",
      "2283 Training Loss: tensor(0.1172)\n",
      "2284 Training Loss: tensor(0.1190)\n",
      "2285 Training Loss: tensor(0.1137)\n",
      "2286 Training Loss: tensor(0.1150)\n",
      "2287 Training Loss: tensor(0.1184)\n",
      "2288 Training Loss: tensor(0.1101)\n",
      "2289 Training Loss: tensor(0.1138)\n",
      "2290 Training Loss: tensor(0.1205)\n",
      "2291 Training Loss: tensor(0.1158)\n",
      "2292 Training Loss: tensor(0.1102)\n",
      "2293 Training Loss: tensor(0.1095)\n",
      "2294 Training Loss: tensor(0.1117)\n",
      "2295 Training Loss: tensor(0.1166)\n",
      "2296 Training Loss: tensor(0.1136)\n",
      "2297 Training Loss: tensor(0.1159)\n",
      "2298 Training Loss: tensor(0.1144)\n",
      "2299 Training Loss: tensor(0.1155)\n",
      "2300 Training Loss: tensor(0.1142)\n",
      "2301 Training Loss: tensor(0.1206)\n",
      "2302 Training Loss: tensor(0.1235)\n",
      "2303 Training Loss: tensor(0.1159)\n",
      "2304 Training Loss: tensor(0.1205)\n",
      "2305 Training Loss: tensor(0.1112)\n",
      "2306 Training Loss: tensor(0.1149)\n",
      "2307 Training Loss: tensor(0.1083)\n",
      "2308 Training Loss: tensor(0.1105)\n",
      "2309 Training Loss: tensor(0.1109)\n",
      "2310 Training Loss: tensor(0.1197)\n",
      "2311 Training Loss: tensor(0.1167)\n",
      "2312 Training Loss: tensor(0.1105)\n",
      "2313 Training Loss: tensor(0.1198)\n",
      "2314 Training Loss: tensor(0.1233)\n",
      "2315 Training Loss: tensor(0.1223)\n",
      "2316 Training Loss: tensor(0.1174)\n",
      "2317 Training Loss: tensor(0.1151)\n",
      "2318 Training Loss: tensor(0.1076)\n",
      "2319 Training Loss: tensor(0.1189)\n",
      "2320 Training Loss: tensor(0.1213)\n",
      "2321 Training Loss: tensor(0.1058)\n",
      "2322 Training Loss: tensor(0.1107)\n",
      "2323 Training Loss: tensor(0.1179)\n",
      "2324 Training Loss: tensor(0.1138)\n",
      "2325 Training Loss: tensor(0.1117)\n",
      "2326 Training Loss: tensor(0.1137)\n",
      "2327 Training Loss: tensor(0.1185)\n",
      "2328 Training Loss: tensor(0.1249)\n",
      "2329 Training Loss: tensor(0.1177)\n",
      "2330 Training Loss: tensor(0.1201)\n",
      "2331 Training Loss: tensor(0.1083)\n",
      "2332 Training Loss: tensor(0.1153)\n",
      "2333 Training Loss: tensor(0.1148)\n",
      "2334 Training Loss: tensor(0.1095)\n",
      "2335 Training Loss: tensor(0.1146)\n",
      "2336 Training Loss: tensor(0.1179)\n",
      "2337 Training Loss: tensor(0.1104)\n",
      "2338 Training Loss: tensor(0.1044)\n",
      "2339 Training Loss: tensor(0.1169)\n",
      "2340 Training Loss: tensor(0.1187)\n",
      "2341 Training Loss: tensor(0.1202)\n",
      "2342 Training Loss: tensor(0.1150)\n",
      "2343 Training Loss: tensor(0.1080)\n",
      "2344 Training Loss: tensor(0.1239)\n",
      "2345 Training Loss: tensor(0.1089)\n",
      "2346 Training Loss: tensor(0.1089)\n",
      "2347 Training Loss: tensor(0.1218)\n",
      "2348 Training Loss: tensor(0.1094)\n",
      "2349 Training Loss: tensor(0.1206)\n",
      "2350 Training Loss: tensor(0.1146)\n",
      "2351 Training Loss: tensor(0.1176)\n",
      "2352 Training Loss: tensor(0.1085)\n",
      "2353 Training Loss: tensor(0.1249)\n",
      "2354 Training Loss: tensor(0.1160)\n",
      "2355 Training Loss: tensor(0.1194)\n",
      "2356 Training Loss: tensor(0.1117)\n",
      "2357 Training Loss: tensor(0.1155)\n",
      "2358 Training Loss: tensor(0.1112)\n",
      "2359 Training Loss: tensor(0.1159)\n",
      "2360 Training Loss: tensor(0.1065)\n",
      "2361 Training Loss: tensor(0.1133)\n",
      "2362 Training Loss: tensor(0.1195)\n",
      "2363 Training Loss: tensor(0.1207)\n",
      "2364 Training Loss: tensor(0.1139)\n",
      "2365 Training Loss: tensor(0.1144)\n",
      "2366 Training Loss: tensor(0.1238)\n",
      "2367 Training Loss: tensor(0.1233)\n",
      "2368 Training Loss: tensor(0.1084)\n",
      "2369 Training Loss: tensor(0.1102)\n",
      "2370 Training Loss: tensor(0.1164)\n",
      "2371 Training Loss: tensor(0.1121)\n",
      "2372 Training Loss: tensor(0.1198)\n",
      "2373 Training Loss: tensor(0.1219)\n",
      "2374 Training Loss: tensor(0.1152)\n",
      "2375 Training Loss: tensor(0.1138)\n",
      "2376 Training Loss: tensor(0.1221)\n",
      "2377 Training Loss: tensor(0.1101)\n",
      "2378 Training Loss: tensor(0.1093)\n",
      "2379 Training Loss: tensor(0.1202)\n",
      "2380 Training Loss: tensor(0.1182)\n",
      "2381 Training Loss: tensor(0.1129)\n",
      "2382 Training Loss: tensor(0.1139)\n",
      "2383 Training Loss: tensor(0.1186)\n",
      "2384 Training Loss: tensor(0.1122)\n",
      "2385 Training Loss: tensor(0.1200)\n",
      "2386 Training Loss: tensor(0.1136)\n",
      "2387 Training Loss: tensor(0.1142)\n",
      "2388 Training Loss: tensor(0.1092)\n",
      "2389 Training Loss: tensor(0.1220)\n",
      "2390 Training Loss: tensor(0.1172)\n",
      "2391 Training Loss: tensor(0.1167)\n",
      "2392 Training Loss: tensor(0.1164)\n",
      "2393 Training Loss: tensor(0.1148)\n",
      "2394 Training Loss: tensor(0.1125)\n",
      "2395 Training Loss: tensor(0.1112)\n",
      "2396 Training Loss: tensor(0.1119)\n",
      "2397 Training Loss: tensor(0.1066)\n",
      "2398 Training Loss: tensor(0.1106)\n",
      "2399 Training Loss: tensor(0.1108)\n",
      "2400 Training Loss: tensor(0.1157)\n",
      "2401 Training Loss: tensor(0.1070)\n",
      "2402 Training Loss: tensor(0.1124)\n",
      "2403 Training Loss: tensor(0.1202)\n",
      "2404 Training Loss: tensor(0.1207)\n",
      "2405 Training Loss: tensor(0.1114)\n",
      "2406 Training Loss: tensor(0.1110)\n",
      "2407 Training Loss: tensor(0.1191)\n",
      "2408 Training Loss: tensor(0.1066)\n",
      "2409 Training Loss: tensor(0.1045)\n",
      "2410 Training Loss: tensor(0.1098)\n",
      "2411 Training Loss: tensor(0.1066)\n",
      "2412 Training Loss: tensor(0.1130)\n",
      "2413 Training Loss: tensor(0.1210)\n",
      "2414 Training Loss: tensor(0.1181)\n",
      "2415 Training Loss: tensor(0.1113)\n",
      "2416 Training Loss: tensor(0.1138)\n",
      "2417 Training Loss: tensor(0.1201)\n",
      "2418 Training Loss: tensor(0.1038)\n",
      "2419 Training Loss: tensor(0.1096)\n",
      "2420 Training Loss: tensor(0.1160)\n",
      "2421 Training Loss: tensor(0.1102)\n",
      "2422 Training Loss: tensor(0.1214)\n",
      "2423 Training Loss: tensor(0.1127)\n",
      "2424 Training Loss: tensor(0.1088)\n",
      "2425 Training Loss: tensor(0.1098)\n",
      "2426 Training Loss: tensor(0.1182)\n",
      "2427 Training Loss: tensor(0.1083)\n",
      "2428 Training Loss: tensor(0.1137)\n",
      "2429 Training Loss: tensor(0.1130)\n",
      "2430 Training Loss: tensor(0.1225)\n",
      "2431 Training Loss: tensor(0.1141)\n",
      "2432 Training Loss: tensor(0.1127)\n",
      "2433 Training Loss: tensor(0.1193)\n",
      "2434 Training Loss: tensor(0.1127)\n",
      "2435 Training Loss: tensor(0.1137)\n",
      "2436 Training Loss: tensor(0.1087)\n",
      "2437 Training Loss: tensor(0.1084)\n",
      "2438 Training Loss: tensor(0.1093)\n",
      "2439 Training Loss: tensor(0.1227)\n",
      "2440 Training Loss: tensor(0.1106)\n",
      "2441 Training Loss: tensor(0.1087)\n",
      "2442 Training Loss: tensor(0.1140)\n",
      "2443 Training Loss: tensor(0.1125)\n",
      "2444 Training Loss: tensor(0.1113)\n",
      "2445 Training Loss: tensor(0.1078)\n",
      "2446 Training Loss: tensor(0.1112)\n",
      "2447 Training Loss: tensor(0.1211)\n",
      "2448 Training Loss: tensor(0.1090)\n",
      "2449 Training Loss: tensor(0.1148)\n",
      "2450 Training Loss: tensor(0.1136)\n",
      "2451 Training Loss: tensor(0.1164)\n",
      "2452 Training Loss: tensor(0.1065)\n",
      "2453 Training Loss: tensor(0.1188)\n",
      "2454 Training Loss: tensor(0.1108)\n",
      "2455 Training Loss: tensor(0.1109)\n",
      "2456 Training Loss: tensor(0.1141)\n",
      "2457 Training Loss: tensor(0.1179)\n",
      "2458 Training Loss: tensor(0.1151)\n",
      "2459 Training Loss: tensor(0.1130)\n",
      "2460 Training Loss: tensor(0.1097)\n",
      "2461 Training Loss: tensor(0.1185)\n",
      "2462 Training Loss: tensor(0.1200)\n",
      "2463 Training Loss: tensor(0.1093)\n",
      "2464 Training Loss: tensor(0.1106)\n",
      "2465 Training Loss: tensor(0.1150)\n",
      "2466 Training Loss: tensor(0.1099)\n",
      "2467 Training Loss: tensor(0.1092)\n",
      "2468 Training Loss: tensor(0.1138)\n",
      "2469 Training Loss: tensor(0.1040)\n",
      "2470 Training Loss: tensor(0.1096)\n",
      "2471 Training Loss: tensor(0.1177)\n",
      "2472 Training Loss: tensor(0.1084)\n",
      "2473 Training Loss: tensor(0.1059)\n",
      "2474 Training Loss: tensor(0.1118)\n",
      "2475 Training Loss: tensor(0.1129)\n",
      "2476 Training Loss: tensor(0.1155)\n",
      "2477 Training Loss: tensor(0.1072)\n",
      "2478 Training Loss: tensor(0.1088)\n",
      "2479 Training Loss: tensor(0.1135)\n",
      "2480 Training Loss: tensor(0.1083)\n",
      "2481 Training Loss: tensor(0.1173)\n",
      "2482 Training Loss: tensor(0.1141)\n",
      "2483 Training Loss: tensor(0.1073)\n",
      "2484 Training Loss: tensor(0.1129)\n",
      "2485 Training Loss: tensor(0.1117)\n",
      "2486 Training Loss: tensor(0.1161)\n",
      "2487 Training Loss: tensor(0.1052)\n",
      "2488 Training Loss: tensor(0.1111)\n",
      "2489 Training Loss: tensor(0.1043)\n",
      "2490 Training Loss: tensor(0.1071)\n",
      "2491 Training Loss: tensor(0.1036)\n",
      "2492 Training Loss: tensor(0.1157)\n",
      "2493 Training Loss: tensor(0.1100)\n",
      "2494 Training Loss: tensor(0.1135)\n",
      "2495 Training Loss: tensor(0.1152)\n",
      "2496 Training Loss: tensor(0.1199)\n",
      "2497 Training Loss: tensor(0.1084)\n",
      "2498 Training Loss: tensor(0.1134)\n",
      "2499 Training Loss: tensor(0.1151)\n",
      "2500 Training Loss: tensor(0.0985)\n",
      "2501 Training Loss: tensor(0.1171)\n",
      "2502 Training Loss: tensor(0.1182)\n",
      "2503 Training Loss: tensor(0.1097)\n",
      "2504 Training Loss: tensor(0.1110)\n",
      "2505 Training Loss: tensor(0.1181)\n",
      "2506 Training Loss: tensor(0.1128)\n",
      "2507 Training Loss: tensor(0.1163)\n",
      "2508 Training Loss: tensor(0.1070)\n",
      "2509 Training Loss: tensor(0.1062)\n",
      "2510 Training Loss: tensor(0.1157)\n",
      "2511 Training Loss: tensor(0.1114)\n",
      "2512 Training Loss: tensor(0.1133)\n",
      "2513 Training Loss: tensor(0.1070)\n",
      "2514 Training Loss: tensor(0.1152)\n",
      "2515 Training Loss: tensor(0.1160)\n",
      "2516 Training Loss: tensor(0.1083)\n",
      "2517 Training Loss: tensor(0.1148)\n",
      "2518 Training Loss: tensor(0.1116)\n",
      "2519 Training Loss: tensor(0.1173)\n",
      "2520 Training Loss: tensor(0.1034)\n",
      "2521 Training Loss: tensor(0.1073)\n",
      "2522 Training Loss: tensor(0.1048)\n",
      "2523 Training Loss: tensor(0.1114)\n",
      "2524 Training Loss: tensor(0.1064)\n",
      "2525 Training Loss: tensor(0.1178)\n",
      "2526 Training Loss: tensor(0.1046)\n",
      "2527 Training Loss: tensor(0.0948)\n",
      "2528 Training Loss: tensor(0.1078)\n",
      "2529 Training Loss: tensor(0.1108)\n",
      "2530 Training Loss: tensor(0.1071)\n",
      "2531 Training Loss: tensor(0.1129)\n",
      "2532 Training Loss: tensor(0.1124)\n",
      "2533 Training Loss: tensor(0.1073)\n",
      "2534 Training Loss: tensor(0.1114)\n",
      "2535 Training Loss: tensor(0.1139)\n",
      "2536 Training Loss: tensor(0.1036)\n",
      "2537 Training Loss: tensor(0.1085)\n",
      "2538 Training Loss: tensor(0.1140)\n",
      "2539 Training Loss: tensor(0.1098)\n",
      "2540 Training Loss: tensor(0.1118)\n",
      "2541 Training Loss: tensor(0.1086)\n",
      "2542 Training Loss: tensor(0.1157)\n",
      "2543 Training Loss: tensor(0.1144)\n",
      "2544 Training Loss: tensor(0.1124)\n",
      "2545 Training Loss: tensor(0.1189)\n",
      "2546 Training Loss: tensor(0.1100)\n",
      "2547 Training Loss: tensor(0.1060)\n",
      "2548 Training Loss: tensor(0.1162)\n",
      "2549 Training Loss: tensor(0.1036)\n",
      "2550 Training Loss: tensor(0.1150)\n",
      "2551 Training Loss: tensor(0.1119)\n",
      "2552 Training Loss: tensor(0.1092)\n",
      "2553 Training Loss: tensor(0.1097)\n",
      "2554 Training Loss: tensor(0.1138)\n",
      "2555 Training Loss: tensor(0.1260)\n",
      "2556 Training Loss: tensor(0.1075)\n",
      "2557 Training Loss: tensor(0.1107)\n",
      "2558 Training Loss: tensor(0.1127)\n",
      "2559 Training Loss: tensor(0.1049)\n",
      "2560 Training Loss: tensor(0.1189)\n",
      "2561 Training Loss: tensor(0.1103)\n",
      "2562 Training Loss: tensor(0.1134)\n",
      "2563 Training Loss: tensor(0.1138)\n",
      "2564 Training Loss: tensor(0.1107)\n",
      "2565 Training Loss: tensor(0.1183)\n",
      "2566 Training Loss: tensor(0.1065)\n",
      "2567 Training Loss: tensor(0.1028)\n",
      "2568 Training Loss: tensor(0.1106)\n",
      "2569 Training Loss: tensor(0.1118)\n",
      "2570 Training Loss: tensor(0.1115)\n",
      "2571 Training Loss: tensor(0.1148)\n",
      "2572 Training Loss: tensor(0.1084)\n",
      "2573 Training Loss: tensor(0.1086)\n",
      "2574 Training Loss: tensor(0.1045)\n",
      "2575 Training Loss: tensor(0.1073)\n",
      "2576 Training Loss: tensor(0.1107)\n",
      "2577 Training Loss: tensor(0.1102)\n",
      "2578 Training Loss: tensor(0.1092)\n",
      "2579 Training Loss: tensor(0.1137)\n",
      "2580 Training Loss: tensor(0.1101)\n",
      "2581 Training Loss: tensor(0.1132)\n",
      "2582 Training Loss: tensor(0.1048)\n",
      "2583 Training Loss: tensor(0.1090)\n",
      "2584 Training Loss: tensor(0.1121)\n",
      "2585 Training Loss: tensor(0.1156)\n",
      "2586 Training Loss: tensor(0.1057)\n",
      "2587 Training Loss: tensor(0.1150)\n",
      "2588 Training Loss: tensor(0.1149)\n",
      "2589 Training Loss: tensor(0.1026)\n",
      "2590 Training Loss: tensor(0.1058)\n",
      "2591 Training Loss: tensor(0.1106)\n",
      "2592 Training Loss: tensor(0.1141)\n",
      "2593 Training Loss: tensor(0.1073)\n",
      "2594 Training Loss: tensor(0.1146)\n",
      "2595 Training Loss: tensor(0.1126)\n",
      "2596 Training Loss: tensor(0.1095)\n",
      "2597 Training Loss: tensor(0.1013)\n",
      "2598 Training Loss: tensor(0.1071)\n",
      "2599 Training Loss: tensor(0.1084)\n",
      "2600 Training Loss: tensor(0.1060)\n",
      "2601 Training Loss: tensor(0.1087)\n",
      "2602 Training Loss: tensor(0.0976)\n",
      "2603 Training Loss: tensor(0.1141)\n",
      "2604 Training Loss: tensor(0.1081)\n",
      "2605 Training Loss: tensor(0.1169)\n",
      "2606 Training Loss: tensor(0.1028)\n",
      "2607 Training Loss: tensor(0.1137)\n",
      "2608 Training Loss: tensor(0.1068)\n",
      "2609 Training Loss: tensor(0.1128)\n",
      "2610 Training Loss: tensor(0.1100)\n",
      "2611 Training Loss: tensor(0.1076)\n",
      "2612 Training Loss: tensor(0.1169)\n",
      "2613 Training Loss: tensor(0.1092)\n",
      "2614 Training Loss: tensor(0.1118)\n",
      "2615 Training Loss: tensor(0.1080)\n",
      "2616 Training Loss: tensor(0.1092)\n",
      "2617 Training Loss: tensor(0.1139)\n",
      "2618 Training Loss: tensor(0.1058)\n",
      "2619 Training Loss: tensor(0.1054)\n",
      "2620 Training Loss: tensor(0.1073)\n",
      "2621 Training Loss: tensor(0.1037)\n",
      "2622 Training Loss: tensor(0.0994)\n",
      "2623 Training Loss: tensor(0.1070)\n",
      "2624 Training Loss: tensor(0.1179)\n",
      "2625 Training Loss: tensor(0.1087)\n",
      "2626 Training Loss: tensor(0.1039)\n",
      "2627 Training Loss: tensor(0.1091)\n",
      "2628 Training Loss: tensor(0.1120)\n",
      "2629 Training Loss: tensor(0.1094)\n",
      "2630 Training Loss: tensor(0.1072)\n",
      "2631 Training Loss: tensor(0.1107)\n",
      "2632 Training Loss: tensor(0.1102)\n",
      "2633 Training Loss: tensor(0.1076)\n",
      "2634 Training Loss: tensor(0.1105)\n",
      "2635 Training Loss: tensor(0.1074)\n",
      "2636 Training Loss: tensor(0.1048)\n",
      "2637 Training Loss: tensor(0.1102)\n",
      "2638 Training Loss: tensor(0.1005)\n",
      "2639 Training Loss: tensor(0.1159)\n",
      "2640 Training Loss: tensor(0.1039)\n",
      "2641 Training Loss: tensor(0.1043)\n",
      "2642 Training Loss: tensor(0.1151)\n",
      "2643 Training Loss: tensor(0.1138)\n",
      "2644 Training Loss: tensor(0.0998)\n",
      "2645 Training Loss: tensor(0.1033)\n",
      "2646 Training Loss: tensor(0.1086)\n",
      "2647 Training Loss: tensor(0.1047)\n",
      "2648 Training Loss: tensor(0.1067)\n",
      "2649 Training Loss: tensor(0.1074)\n",
      "2650 Training Loss: tensor(0.1146)\n",
      "2651 Training Loss: tensor(0.1084)\n",
      "2652 Training Loss: tensor(0.1079)\n",
      "2653 Training Loss: tensor(0.1028)\n",
      "2654 Training Loss: tensor(0.1139)\n",
      "2655 Training Loss: tensor(0.1114)\n",
      "2656 Training Loss: tensor(0.1086)\n",
      "2657 Training Loss: tensor(0.1076)\n",
      "2658 Training Loss: tensor(0.1048)\n",
      "2659 Training Loss: tensor(0.1144)\n",
      "2660 Training Loss: tensor(0.1102)\n",
      "2661 Training Loss: tensor(0.1078)\n",
      "2662 Training Loss: tensor(0.1069)\n",
      "2663 Training Loss: tensor(0.1109)\n",
      "2664 Training Loss: tensor(0.1021)\n",
      "2665 Training Loss: tensor(0.1142)\n",
      "2666 Training Loss: tensor(0.1115)\n",
      "2667 Training Loss: tensor(0.1061)\n",
      "2668 Training Loss: tensor(0.1100)\n",
      "2669 Training Loss: tensor(0.1109)\n",
      "2670 Training Loss: tensor(0.1081)\n",
      "2671 Training Loss: tensor(0.1034)\n",
      "2672 Training Loss: tensor(0.1022)\n",
      "2673 Training Loss: tensor(0.1079)\n",
      "2674 Training Loss: tensor(0.1076)\n",
      "2675 Training Loss: tensor(0.1037)\n",
      "2676 Training Loss: tensor(0.1084)\n",
      "2677 Training Loss: tensor(0.1082)\n",
      "2678 Training Loss: tensor(0.1094)\n",
      "2679 Training Loss: tensor(0.1147)\n",
      "2680 Training Loss: tensor(0.1075)\n",
      "2681 Training Loss: tensor(0.1023)\n",
      "2682 Training Loss: tensor(0.1066)\n",
      "2683 Training Loss: tensor(0.1131)\n",
      "2684 Training Loss: tensor(0.1158)\n",
      "2685 Training Loss: tensor(0.1030)\n",
      "2686 Training Loss: tensor(0.1097)\n",
      "2687 Training Loss: tensor(0.1132)\n",
      "2688 Training Loss: tensor(0.1091)\n",
      "2689 Training Loss: tensor(0.1077)\n",
      "2690 Training Loss: tensor(0.1056)\n",
      "2691 Training Loss: tensor(0.1059)\n",
      "2692 Training Loss: tensor(0.1020)\n",
      "2693 Training Loss: tensor(0.1009)\n",
      "2694 Training Loss: tensor(0.1089)\n",
      "2695 Training Loss: tensor(0.1126)\n",
      "2696 Training Loss: tensor(0.1099)\n",
      "2697 Training Loss: tensor(0.1077)\n",
      "2698 Training Loss: tensor(0.1117)\n",
      "2699 Training Loss: tensor(0.1004)\n",
      "2700 Training Loss: tensor(0.1037)\n",
      "2701 Training Loss: tensor(0.1095)\n",
      "2702 Training Loss: tensor(0.1049)\n",
      "2703 Training Loss: tensor(0.1080)\n",
      "2704 Training Loss: tensor(0.1057)\n",
      "2705 Training Loss: tensor(0.1009)\n",
      "2706 Training Loss: tensor(0.1056)\n",
      "2707 Training Loss: tensor(0.1040)\n",
      "2708 Training Loss: tensor(0.1081)\n",
      "2709 Training Loss: tensor(0.1116)\n",
      "2710 Training Loss: tensor(0.1079)\n",
      "2711 Training Loss: tensor(0.1113)\n",
      "2712 Training Loss: tensor(0.1042)\n",
      "2713 Training Loss: tensor(0.1056)\n",
      "2714 Training Loss: tensor(0.1105)\n",
      "2715 Training Loss: tensor(0.1031)\n",
      "2716 Training Loss: tensor(0.1066)\n",
      "2717 Training Loss: tensor(0.1096)\n",
      "2718 Training Loss: tensor(0.1074)\n",
      "2719 Training Loss: tensor(0.1045)\n",
      "2720 Training Loss: tensor(0.1062)\n",
      "2721 Training Loss: tensor(0.1096)\n",
      "2722 Training Loss: tensor(0.1134)\n",
      "2723 Training Loss: tensor(0.1068)\n",
      "2724 Training Loss: tensor(0.1065)\n",
      "2725 Training Loss: tensor(0.1084)\n",
      "2726 Training Loss: tensor(0.1029)\n",
      "2727 Training Loss: tensor(0.1080)\n",
      "2728 Training Loss: tensor(0.1049)\n",
      "2729 Training Loss: tensor(0.1007)\n",
      "2730 Training Loss: tensor(0.1093)\n",
      "2731 Training Loss: tensor(0.1073)\n",
      "2732 Training Loss: tensor(0.1008)\n",
      "2733 Training Loss: tensor(0.1048)\n",
      "2734 Training Loss: tensor(0.1053)\n",
      "2735 Training Loss: tensor(0.1104)\n",
      "2736 Training Loss: tensor(0.1044)\n",
      "2737 Training Loss: tensor(0.1117)\n",
      "2738 Training Loss: tensor(0.0996)\n",
      "2739 Training Loss: tensor(0.1071)\n",
      "2740 Training Loss: tensor(0.1045)\n",
      "2741 Training Loss: tensor(0.1002)\n",
      "2742 Training Loss: tensor(0.0995)\n",
      "2743 Training Loss: tensor(0.1101)\n",
      "2744 Training Loss: tensor(0.1097)\n",
      "2745 Training Loss: tensor(0.1115)\n",
      "2746 Training Loss: tensor(0.1062)\n",
      "2747 Training Loss: tensor(0.1021)\n",
      "2748 Training Loss: tensor(0.1111)\n",
      "2749 Training Loss: tensor(0.1001)\n",
      "2750 Training Loss: tensor(0.0974)\n",
      "2751 Training Loss: tensor(0.1107)\n",
      "2752 Training Loss: tensor(0.1010)\n",
      "2753 Training Loss: tensor(0.1064)\n",
      "2754 Training Loss: tensor(0.1068)\n",
      "2755 Training Loss: tensor(0.1009)\n",
      "2756 Training Loss: tensor(0.1025)\n",
      "2757 Training Loss: tensor(0.0985)\n",
      "2758 Training Loss: tensor(0.1160)\n",
      "2759 Training Loss: tensor(0.1081)\n",
      "2760 Training Loss: tensor(0.0999)\n",
      "2761 Training Loss: tensor(0.1052)\n",
      "2762 Training Loss: tensor(0.1105)\n",
      "2763 Training Loss: tensor(0.1115)\n",
      "2764 Training Loss: tensor(0.1049)\n",
      "2765 Training Loss: tensor(0.1002)\n",
      "2766 Training Loss: tensor(0.1060)\n",
      "2767 Training Loss: tensor(0.1060)\n",
      "2768 Training Loss: tensor(0.1066)\n",
      "2769 Training Loss: tensor(0.1060)\n",
      "2770 Training Loss: tensor(0.1087)\n",
      "2771 Training Loss: tensor(0.0966)\n",
      "2772 Training Loss: tensor(0.1031)\n",
      "2773 Training Loss: tensor(0.0980)\n",
      "2774 Training Loss: tensor(0.1082)\n",
      "2775 Training Loss: tensor(0.1043)\n",
      "2776 Training Loss: tensor(0.1116)\n",
      "2777 Training Loss: tensor(0.1135)\n",
      "2778 Training Loss: tensor(0.1094)\n",
      "2779 Training Loss: tensor(0.1044)\n",
      "2780 Training Loss: tensor(0.1079)\n",
      "2781 Training Loss: tensor(0.1045)\n",
      "2782 Training Loss: tensor(0.1081)\n",
      "2783 Training Loss: tensor(0.1043)\n",
      "2784 Training Loss: tensor(0.0958)\n",
      "2785 Training Loss: tensor(0.1140)\n",
      "2786 Training Loss: tensor(0.1051)\n",
      "2787 Training Loss: tensor(0.1022)\n",
      "2788 Training Loss: tensor(0.1088)\n",
      "2789 Training Loss: tensor(0.1128)\n",
      "2790 Training Loss: tensor(0.1023)\n",
      "2791 Training Loss: tensor(0.1100)\n",
      "2792 Training Loss: tensor(0.1111)\n",
      "2793 Training Loss: tensor(0.1050)\n",
      "2794 Training Loss: tensor(0.1049)\n",
      "2795 Training Loss: tensor(0.1119)\n",
      "2796 Training Loss: tensor(0.1062)\n",
      "2797 Training Loss: tensor(0.1004)\n",
      "2798 Training Loss: tensor(0.1141)\n",
      "2799 Training Loss: tensor(0.1060)\n",
      "2800 Training Loss: tensor(0.1080)\n",
      "2801 Training Loss: tensor(0.1110)\n",
      "2802 Training Loss: tensor(0.1034)\n",
      "2803 Training Loss: tensor(0.1001)\n",
      "2804 Training Loss: tensor(0.1028)\n",
      "2805 Training Loss: tensor(0.1103)\n",
      "2806 Training Loss: tensor(0.1171)\n",
      "2807 Training Loss: tensor(0.1170)\n",
      "2808 Training Loss: tensor(0.1138)\n",
      "2809 Training Loss: tensor(0.1047)\n",
      "2810 Training Loss: tensor(0.1077)\n",
      "2811 Training Loss: tensor(0.1005)\n",
      "2812 Training Loss: tensor(0.1031)\n",
      "2813 Training Loss: tensor(0.1091)\n",
      "2814 Training Loss: tensor(0.1047)\n",
      "2815 Training Loss: tensor(0.1107)\n",
      "2816 Training Loss: tensor(0.1060)\n",
      "2817 Training Loss: tensor(0.1078)\n",
      "2818 Training Loss: tensor(0.1035)\n",
      "2819 Training Loss: tensor(0.1053)\n",
      "2820 Training Loss: tensor(0.1005)\n",
      "2821 Training Loss: tensor(0.1050)\n",
      "2822 Training Loss: tensor(0.1052)\n",
      "2823 Training Loss: tensor(0.0994)\n",
      "2824 Training Loss: tensor(0.0961)\n",
      "2825 Training Loss: tensor(0.1082)\n",
      "2826 Training Loss: tensor(0.1088)\n",
      "2827 Training Loss: tensor(0.1038)\n",
      "2828 Training Loss: tensor(0.1091)\n",
      "2829 Training Loss: tensor(0.1016)\n",
      "2830 Training Loss: tensor(0.1046)\n",
      "2831 Training Loss: tensor(0.0996)\n",
      "2832 Training Loss: tensor(0.1022)\n",
      "2833 Training Loss: tensor(0.1044)\n",
      "2834 Training Loss: tensor(0.1117)\n",
      "2835 Training Loss: tensor(0.1192)\n",
      "2836 Training Loss: tensor(0.1066)\n",
      "2837 Training Loss: tensor(0.1022)\n",
      "2838 Training Loss: tensor(0.1040)\n",
      "2839 Training Loss: tensor(0.1121)\n",
      "2840 Training Loss: tensor(0.1088)\n",
      "2841 Training Loss: tensor(0.1029)\n",
      "2842 Training Loss: tensor(0.1064)\n",
      "2843 Training Loss: tensor(0.0989)\n",
      "2844 Training Loss: tensor(0.1077)\n",
      "2845 Training Loss: tensor(0.1040)\n",
      "2846 Training Loss: tensor(0.1028)\n",
      "2847 Training Loss: tensor(0.1091)\n",
      "2848 Training Loss: tensor(0.1012)\n",
      "2849 Training Loss: tensor(0.1091)\n",
      "2850 Training Loss: tensor(0.1067)\n",
      "2851 Training Loss: tensor(0.1054)\n",
      "2852 Training Loss: tensor(0.1051)\n",
      "2853 Training Loss: tensor(0.1050)\n",
      "2854 Training Loss: tensor(0.1014)\n",
      "2855 Training Loss: tensor(0.1005)\n",
      "2856 Training Loss: tensor(0.1050)\n",
      "2857 Training Loss: tensor(0.1026)\n",
      "2858 Training Loss: tensor(0.1012)\n",
      "2859 Training Loss: tensor(0.1089)\n",
      "2860 Training Loss: tensor(0.1114)\n",
      "2861 Training Loss: tensor(0.1030)\n",
      "2862 Training Loss: tensor(0.1028)\n",
      "2863 Training Loss: tensor(0.1071)\n",
      "2864 Training Loss: tensor(0.1014)\n",
      "2865 Training Loss: tensor(0.1041)\n",
      "2866 Training Loss: tensor(0.0973)\n",
      "2867 Training Loss: tensor(0.1154)\n",
      "2868 Training Loss: tensor(0.1009)\n",
      "2869 Training Loss: tensor(0.1003)\n",
      "2870 Training Loss: tensor(0.0959)\n",
      "2871 Training Loss: tensor(0.1113)\n",
      "2872 Training Loss: tensor(0.1069)\n",
      "2873 Training Loss: tensor(0.1103)\n",
      "2874 Training Loss: tensor(0.0991)\n",
      "2875 Training Loss: tensor(0.0968)\n",
      "2876 Training Loss: tensor(0.1059)\n",
      "2877 Training Loss: tensor(0.1030)\n",
      "2878 Training Loss: tensor(0.1005)\n",
      "2879 Training Loss: tensor(0.0964)\n",
      "2880 Training Loss: tensor(0.1055)\n",
      "2881 Training Loss: tensor(0.1072)\n",
      "2882 Training Loss: tensor(0.1123)\n",
      "2883 Training Loss: tensor(0.1056)\n",
      "2884 Training Loss: tensor(0.0998)\n",
      "2885 Training Loss: tensor(0.1045)\n",
      "2886 Training Loss: tensor(0.1026)\n",
      "2887 Training Loss: tensor(0.0999)\n",
      "2888 Training Loss: tensor(0.1015)\n",
      "2889 Training Loss: tensor(0.1064)\n",
      "2890 Training Loss: tensor(0.1032)\n",
      "2891 Training Loss: tensor(0.1053)\n",
      "2892 Training Loss: tensor(0.1062)\n",
      "2893 Training Loss: tensor(0.1019)\n",
      "2894 Training Loss: tensor(0.1036)\n",
      "2895 Training Loss: tensor(0.1084)\n",
      "2896 Training Loss: tensor(0.1115)\n",
      "2897 Training Loss: tensor(0.1050)\n",
      "2898 Training Loss: tensor(0.1110)\n",
      "2899 Training Loss: tensor(0.1060)\n",
      "2900 Training Loss: tensor(0.1052)\n",
      "2901 Training Loss: tensor(0.1115)\n",
      "2902 Training Loss: tensor(0.1118)\n",
      "2903 Training Loss: tensor(0.1072)\n",
      "2904 Training Loss: tensor(0.1068)\n",
      "2905 Training Loss: tensor(0.1013)\n",
      "2906 Training Loss: tensor(0.0988)\n",
      "2907 Training Loss: tensor(0.1049)\n",
      "2908 Training Loss: tensor(0.1085)\n",
      "2909 Training Loss: tensor(0.1063)\n",
      "2910 Training Loss: tensor(0.0889)\n",
      "2911 Training Loss: tensor(0.1012)\n",
      "2912 Training Loss: tensor(0.1058)\n",
      "2913 Training Loss: tensor(0.1078)\n",
      "2914 Training Loss: tensor(0.1010)\n",
      "2915 Training Loss: tensor(0.1056)\n",
      "2916 Training Loss: tensor(0.1017)\n",
      "2917 Training Loss: tensor(0.1131)\n",
      "2918 Training Loss: tensor(0.1039)\n",
      "2919 Training Loss: tensor(0.1110)\n",
      "2920 Training Loss: tensor(0.1059)\n",
      "2921 Training Loss: tensor(0.1046)\n",
      "2922 Training Loss: tensor(0.1012)\n",
      "2923 Training Loss: tensor(0.1103)\n",
      "2924 Training Loss: tensor(0.1009)\n",
      "2925 Training Loss: tensor(0.1073)\n",
      "2926 Training Loss: tensor(0.1053)\n",
      "2927 Training Loss: tensor(0.1055)\n",
      "2928 Training Loss: tensor(0.1084)\n",
      "2929 Training Loss: tensor(0.1037)\n",
      "2930 Training Loss: tensor(0.1049)\n",
      "2931 Training Loss: tensor(0.0981)\n",
      "2932 Training Loss: tensor(0.1101)\n",
      "2933 Training Loss: tensor(0.1084)\n",
      "2934 Training Loss: tensor(0.1008)\n",
      "2935 Training Loss: tensor(0.0986)\n",
      "2936 Training Loss: tensor(0.1054)\n",
      "2937 Training Loss: tensor(0.0996)\n",
      "2938 Training Loss: tensor(0.0970)\n",
      "2939 Training Loss: tensor(0.1097)\n",
      "2940 Training Loss: tensor(0.1069)\n",
      "2941 Training Loss: tensor(0.1025)\n",
      "2942 Training Loss: tensor(0.1028)\n",
      "2943 Training Loss: tensor(0.0991)\n",
      "2944 Training Loss: tensor(0.0956)\n",
      "2945 Training Loss: tensor(0.1040)\n",
      "2946 Training Loss: tensor(0.1005)\n",
      "2947 Training Loss: tensor(0.0974)\n",
      "2948 Training Loss: tensor(0.1026)\n",
      "2949 Training Loss: tensor(0.1021)\n",
      "2950 Training Loss: tensor(0.0996)\n",
      "2951 Training Loss: tensor(0.1083)\n",
      "2952 Training Loss: tensor(0.0971)\n",
      "2953 Training Loss: tensor(0.1084)\n",
      "2954 Training Loss: tensor(0.1094)\n",
      "2955 Training Loss: tensor(0.0973)\n",
      "2956 Training Loss: tensor(0.1010)\n",
      "2957 Training Loss: tensor(0.1085)\n",
      "2958 Training Loss: tensor(0.0994)\n",
      "2959 Training Loss: tensor(0.0994)\n",
      "2960 Training Loss: tensor(0.1005)\n",
      "2961 Training Loss: tensor(0.1012)\n",
      "2962 Training Loss: tensor(0.1001)\n",
      "2963 Training Loss: tensor(0.0963)\n",
      "2964 Training Loss: tensor(0.0994)\n",
      "2965 Training Loss: tensor(0.1024)\n",
      "2966 Training Loss: tensor(0.1046)\n",
      "2967 Training Loss: tensor(0.0952)\n",
      "2968 Training Loss: tensor(0.1079)\n",
      "2969 Training Loss: tensor(0.0978)\n",
      "2970 Training Loss: tensor(0.0970)\n",
      "2971 Training Loss: tensor(0.1003)\n",
      "2972 Training Loss: tensor(0.1111)\n",
      "2973 Training Loss: tensor(0.1092)\n",
      "2974 Training Loss: tensor(0.1105)\n",
      "2975 Training Loss: tensor(0.1066)\n",
      "2976 Training Loss: tensor(0.1029)\n",
      "2977 Training Loss: tensor(0.1036)\n",
      "2978 Training Loss: tensor(0.1050)\n",
      "2979 Training Loss: tensor(0.1073)\n",
      "2980 Training Loss: tensor(0.0993)\n",
      "2981 Training Loss: tensor(0.0996)\n",
      "2982 Training Loss: tensor(0.1086)\n",
      "2983 Training Loss: tensor(0.1083)\n",
      "2984 Training Loss: tensor(0.0979)\n",
      "2985 Training Loss: tensor(0.0974)\n",
      "2986 Training Loss: tensor(0.1004)\n",
      "2987 Training Loss: tensor(0.1018)\n",
      "2988 Training Loss: tensor(0.0977)\n",
      "2989 Training Loss: tensor(0.1087)\n",
      "2990 Training Loss: tensor(0.1034)\n",
      "2991 Training Loss: tensor(0.0959)\n",
      "2992 Training Loss: tensor(0.0912)\n",
      "2993 Training Loss: tensor(0.1012)\n",
      "2994 Training Loss: tensor(0.1068)\n",
      "2995 Training Loss: tensor(0.1051)\n",
      "2996 Training Loss: tensor(0.0990)\n",
      "2997 Training Loss: tensor(0.0931)\n",
      "2998 Training Loss: tensor(0.1089)\n",
      "2999 Training Loss: tensor(0.1040)\n",
      "3000 Training Loss: tensor(0.1082)\n",
      "3001 Training Loss: tensor(0.1030)\n",
      "3002 Training Loss: tensor(0.0978)\n",
      "3003 Training Loss: tensor(0.1092)\n",
      "3004 Training Loss: tensor(0.0953)\n",
      "3005 Training Loss: tensor(0.1054)\n",
      "3006 Training Loss: tensor(0.0987)\n",
      "3007 Training Loss: tensor(0.1080)\n",
      "3008 Training Loss: tensor(0.1088)\n",
      "3009 Training Loss: tensor(0.1108)\n",
      "3010 Training Loss: tensor(0.1045)\n",
      "3011 Training Loss: tensor(0.0983)\n",
      "3012 Training Loss: tensor(0.0981)\n",
      "3013 Training Loss: tensor(0.1007)\n",
      "3014 Training Loss: tensor(0.0974)\n",
      "3015 Training Loss: tensor(0.1081)\n",
      "3016 Training Loss: tensor(0.1111)\n",
      "3017 Training Loss: tensor(0.1024)\n",
      "3018 Training Loss: tensor(0.1040)\n",
      "3019 Training Loss: tensor(0.1065)\n",
      "3020 Training Loss: tensor(0.1060)\n",
      "3021 Training Loss: tensor(0.1078)\n",
      "3022 Training Loss: tensor(0.0975)\n",
      "3023 Training Loss: tensor(0.1053)\n",
      "3024 Training Loss: tensor(0.1007)\n",
      "3025 Training Loss: tensor(0.0994)\n",
      "3026 Training Loss: tensor(0.1064)\n",
      "3027 Training Loss: tensor(0.1084)\n",
      "3028 Training Loss: tensor(0.1022)\n",
      "3029 Training Loss: tensor(0.1030)\n",
      "3030 Training Loss: tensor(0.1003)\n",
      "3031 Training Loss: tensor(0.1027)\n",
      "3032 Training Loss: tensor(0.1017)\n",
      "3033 Training Loss: tensor(0.1059)\n",
      "3034 Training Loss: tensor(0.1077)\n",
      "3035 Training Loss: tensor(0.1055)\n",
      "3036 Training Loss: tensor(0.0935)\n",
      "3037 Training Loss: tensor(0.1010)\n",
      "3038 Training Loss: tensor(0.1020)\n",
      "3039 Training Loss: tensor(0.1037)\n",
      "3040 Training Loss: tensor(0.0980)\n",
      "3041 Training Loss: tensor(0.1039)\n",
      "3042 Training Loss: tensor(0.1055)\n",
      "3043 Training Loss: tensor(0.1117)\n",
      "3044 Training Loss: tensor(0.0968)\n",
      "3045 Training Loss: tensor(0.1025)\n",
      "3046 Training Loss: tensor(0.0937)\n",
      "3047 Training Loss: tensor(0.0938)\n",
      "3048 Training Loss: tensor(0.1029)\n",
      "3049 Training Loss: tensor(0.1000)\n",
      "3050 Training Loss: tensor(0.1039)\n",
      "3051 Training Loss: tensor(0.1002)\n",
      "3052 Training Loss: tensor(0.0984)\n",
      "3053 Training Loss: tensor(0.1014)\n",
      "3054 Training Loss: tensor(0.1020)\n",
      "3055 Training Loss: tensor(0.1055)\n",
      "3056 Training Loss: tensor(0.1054)\n",
      "3057 Training Loss: tensor(0.1032)\n",
      "3058 Training Loss: tensor(0.0975)\n",
      "3059 Training Loss: tensor(0.1117)\n",
      "3060 Training Loss: tensor(0.0988)\n",
      "3061 Training Loss: tensor(0.1024)\n",
      "3062 Training Loss: tensor(0.0966)\n",
      "3063 Training Loss: tensor(0.1053)\n",
      "3064 Training Loss: tensor(0.1044)\n",
      "3065 Training Loss: tensor(0.0943)\n",
      "3066 Training Loss: tensor(0.0980)\n",
      "3067 Training Loss: tensor(0.0995)\n",
      "3068 Training Loss: tensor(0.0912)\n",
      "3069 Training Loss: tensor(0.1048)\n",
      "3070 Training Loss: tensor(0.1052)\n",
      "3071 Training Loss: tensor(0.1045)\n",
      "3072 Training Loss: tensor(0.1028)\n",
      "3073 Training Loss: tensor(0.1021)\n",
      "3074 Training Loss: tensor(0.0942)\n",
      "3075 Training Loss: tensor(0.1011)\n",
      "3076 Training Loss: tensor(0.0979)\n",
      "3077 Training Loss: tensor(0.1011)\n",
      "3078 Training Loss: tensor(0.1024)\n",
      "3079 Training Loss: tensor(0.1035)\n",
      "3080 Training Loss: tensor(0.0897)\n",
      "3081 Training Loss: tensor(0.1032)\n",
      "3082 Training Loss: tensor(0.1047)\n",
      "3083 Training Loss: tensor(0.1034)\n",
      "3084 Training Loss: tensor(0.1029)\n",
      "3085 Training Loss: tensor(0.0970)\n",
      "3086 Training Loss: tensor(0.1013)\n",
      "3087 Training Loss: tensor(0.0990)\n",
      "3088 Training Loss: tensor(0.1155)\n",
      "3089 Training Loss: tensor(0.1037)\n",
      "3090 Training Loss: tensor(0.1024)\n",
      "3091 Training Loss: tensor(0.0949)\n",
      "3092 Training Loss: tensor(0.1020)\n",
      "3093 Training Loss: tensor(0.0941)\n",
      "3094 Training Loss: tensor(0.1065)\n",
      "3095 Training Loss: tensor(0.0984)\n",
      "3096 Training Loss: tensor(0.0993)\n",
      "3097 Training Loss: tensor(0.1018)\n",
      "3098 Training Loss: tensor(0.0913)\n",
      "3099 Training Loss: tensor(0.0999)\n",
      "3100 Training Loss: tensor(0.1077)\n",
      "3101 Training Loss: tensor(0.1077)\n",
      "3102 Training Loss: tensor(0.0985)\n",
      "3103 Training Loss: tensor(0.1035)\n",
      "3104 Training Loss: tensor(0.0982)\n",
      "3105 Training Loss: tensor(0.0999)\n",
      "3106 Training Loss: tensor(0.1042)\n",
      "3107 Training Loss: tensor(0.0968)\n",
      "3108 Training Loss: tensor(0.0935)\n",
      "3109 Training Loss: tensor(0.0943)\n",
      "3110 Training Loss: tensor(0.0955)\n",
      "3111 Training Loss: tensor(0.1077)\n",
      "3112 Training Loss: tensor(0.0960)\n",
      "3113 Training Loss: tensor(0.0983)\n",
      "3114 Training Loss: tensor(0.0984)\n",
      "3115 Training Loss: tensor(0.0974)\n",
      "3116 Training Loss: tensor(0.1013)\n",
      "3117 Training Loss: tensor(0.1021)\n",
      "3118 Training Loss: tensor(0.0986)\n",
      "3119 Training Loss: tensor(0.0995)\n",
      "3120 Training Loss: tensor(0.1044)\n",
      "3121 Training Loss: tensor(0.1087)\n",
      "3122 Training Loss: tensor(0.0982)\n",
      "3123 Training Loss: tensor(0.1061)\n",
      "3124 Training Loss: tensor(0.0966)\n",
      "3125 Training Loss: tensor(0.1005)\n",
      "3126 Training Loss: tensor(0.0978)\n",
      "3127 Training Loss: tensor(0.1045)\n",
      "3128 Training Loss: tensor(0.1011)\n",
      "3129 Training Loss: tensor(0.1032)\n",
      "3130 Training Loss: tensor(0.1062)\n",
      "3131 Training Loss: tensor(0.1011)\n",
      "3132 Training Loss: tensor(0.1086)\n",
      "3133 Training Loss: tensor(0.1039)\n",
      "3134 Training Loss: tensor(0.1022)\n",
      "3135 Training Loss: tensor(0.0992)\n",
      "3136 Training Loss: tensor(0.1066)\n",
      "3137 Training Loss: tensor(0.0914)\n",
      "3138 Training Loss: tensor(0.1041)\n",
      "3139 Training Loss: tensor(0.1022)\n",
      "3140 Training Loss: tensor(0.1067)\n",
      "3141 Training Loss: tensor(0.0968)\n",
      "3142 Training Loss: tensor(0.1023)\n",
      "3143 Training Loss: tensor(0.0972)\n",
      "3144 Training Loss: tensor(0.0966)\n",
      "3145 Training Loss: tensor(0.0970)\n",
      "3146 Training Loss: tensor(0.1034)\n",
      "3147 Training Loss: tensor(0.0997)\n",
      "3148 Training Loss: tensor(0.1038)\n",
      "3149 Training Loss: tensor(0.1011)\n",
      "3150 Training Loss: tensor(0.1109)\n",
      "3151 Training Loss: tensor(0.1031)\n",
      "3152 Training Loss: tensor(0.0913)\n",
      "3153 Training Loss: tensor(0.1051)\n",
      "3154 Training Loss: tensor(0.1017)\n",
      "3155 Training Loss: tensor(0.0960)\n",
      "3156 Training Loss: tensor(0.0987)\n",
      "3157 Training Loss: tensor(0.0988)\n",
      "3158 Training Loss: tensor(0.1031)\n",
      "3159 Training Loss: tensor(0.1042)\n",
      "3160 Training Loss: tensor(0.1019)\n",
      "3161 Training Loss: tensor(0.0990)\n",
      "3162 Training Loss: tensor(0.0963)\n",
      "3163 Training Loss: tensor(0.1041)\n",
      "3164 Training Loss: tensor(0.0940)\n",
      "3165 Training Loss: tensor(0.0984)\n",
      "3166 Training Loss: tensor(0.0905)\n",
      "3167 Training Loss: tensor(0.0983)\n",
      "3168 Training Loss: tensor(0.1067)\n",
      "3169 Training Loss: tensor(0.1010)\n",
      "3170 Training Loss: tensor(0.1000)\n",
      "3171 Training Loss: tensor(0.0963)\n",
      "3172 Training Loss: tensor(0.1062)\n",
      "3173 Training Loss: tensor(0.1017)\n",
      "3174 Training Loss: tensor(0.0923)\n",
      "3175 Training Loss: tensor(0.0918)\n",
      "3176 Training Loss: tensor(0.1054)\n",
      "3177 Training Loss: tensor(0.0912)\n",
      "3178 Training Loss: tensor(0.0978)\n",
      "3179 Training Loss: tensor(0.0960)\n",
      "3180 Training Loss: tensor(0.1002)\n",
      "3181 Training Loss: tensor(0.1012)\n",
      "3182 Training Loss: tensor(0.0995)\n",
      "3183 Training Loss: tensor(0.0965)\n",
      "3184 Training Loss: tensor(0.1002)\n",
      "3185 Training Loss: tensor(0.0987)\n",
      "3186 Training Loss: tensor(0.1015)\n",
      "3187 Training Loss: tensor(0.1008)\n",
      "3188 Training Loss: tensor(0.1038)\n",
      "3189 Training Loss: tensor(0.0995)\n",
      "3190 Training Loss: tensor(0.0982)\n",
      "3191 Training Loss: tensor(0.0988)\n",
      "3192 Training Loss: tensor(0.0977)\n",
      "3193 Training Loss: tensor(0.0927)\n",
      "3194 Training Loss: tensor(0.1108)\n",
      "3195 Training Loss: tensor(0.0965)\n",
      "3196 Training Loss: tensor(0.1085)\n",
      "3197 Training Loss: tensor(0.1022)\n",
      "3198 Training Loss: tensor(0.1040)\n",
      "3199 Training Loss: tensor(0.1010)\n",
      "3200 Training Loss: tensor(0.1056)\n",
      "3201 Training Loss: tensor(0.1047)\n",
      "3202 Training Loss: tensor(0.1008)\n",
      "3203 Training Loss: tensor(0.0995)\n",
      "3204 Training Loss: tensor(0.0969)\n",
      "3205 Training Loss: tensor(0.0988)\n",
      "3206 Training Loss: tensor(0.0960)\n",
      "3207 Training Loss: tensor(0.1009)\n",
      "3208 Training Loss: tensor(0.0987)\n",
      "3209 Training Loss: tensor(0.1017)\n",
      "3210 Training Loss: tensor(0.0989)\n",
      "3211 Training Loss: tensor(0.1022)\n",
      "3212 Training Loss: tensor(0.0966)\n",
      "3213 Training Loss: tensor(0.1023)\n",
      "3214 Training Loss: tensor(0.0998)\n",
      "3215 Training Loss: tensor(0.0970)\n",
      "3216 Training Loss: tensor(0.0991)\n",
      "3217 Training Loss: tensor(0.0998)\n",
      "3218 Training Loss: tensor(0.1056)\n",
      "3219 Training Loss: tensor(0.1004)\n",
      "3220 Training Loss: tensor(0.1016)\n",
      "3221 Training Loss: tensor(0.1007)\n",
      "3222 Training Loss: tensor(0.0943)\n",
      "3223 Training Loss: tensor(0.0974)\n",
      "3224 Training Loss: tensor(0.0980)\n",
      "3225 Training Loss: tensor(0.0958)\n",
      "3226 Training Loss: tensor(0.0929)\n",
      "3227 Training Loss: tensor(0.0965)\n",
      "3228 Training Loss: tensor(0.0918)\n",
      "3229 Training Loss: tensor(0.1049)\n",
      "3230 Training Loss: tensor(0.1027)\n",
      "3231 Training Loss: tensor(0.0915)\n",
      "3232 Training Loss: tensor(0.0988)\n",
      "3233 Training Loss: tensor(0.1008)\n",
      "3234 Training Loss: tensor(0.0929)\n",
      "3235 Training Loss: tensor(0.0931)\n",
      "3236 Training Loss: tensor(0.1017)\n",
      "3237 Training Loss: tensor(0.0968)\n",
      "3238 Training Loss: tensor(0.1002)\n",
      "3239 Training Loss: tensor(0.0998)\n",
      "3240 Training Loss: tensor(0.0965)\n",
      "3241 Training Loss: tensor(0.0981)\n",
      "3242 Training Loss: tensor(0.0945)\n",
      "3243 Training Loss: tensor(0.0944)\n",
      "3244 Training Loss: tensor(0.1016)\n",
      "3245 Training Loss: tensor(0.1021)\n",
      "3246 Training Loss: tensor(0.0939)\n",
      "3247 Training Loss: tensor(0.1022)\n",
      "3248 Training Loss: tensor(0.1062)\n",
      "3249 Training Loss: tensor(0.0955)\n",
      "3250 Training Loss: tensor(0.1010)\n",
      "3251 Training Loss: tensor(0.0991)\n",
      "3252 Training Loss: tensor(0.0993)\n",
      "3253 Training Loss: tensor(0.0976)\n",
      "3254 Training Loss: tensor(0.1097)\n",
      "3255 Training Loss: tensor(0.0993)\n",
      "3256 Training Loss: tensor(0.1055)\n",
      "3257 Training Loss: tensor(0.0987)\n",
      "3258 Training Loss: tensor(0.1035)\n",
      "3259 Training Loss: tensor(0.1038)\n",
      "3260 Training Loss: tensor(0.0997)\n",
      "3261 Training Loss: tensor(0.0932)\n",
      "3262 Training Loss: tensor(0.1049)\n",
      "3263 Training Loss: tensor(0.0944)\n",
      "3264 Training Loss: tensor(0.0928)\n",
      "3265 Training Loss: tensor(0.1044)\n",
      "3266 Training Loss: tensor(0.0994)\n",
      "3267 Training Loss: tensor(0.0863)\n",
      "3268 Training Loss: tensor(0.1024)\n",
      "3269 Training Loss: tensor(0.1053)\n",
      "3270 Training Loss: tensor(0.0970)\n",
      "3271 Training Loss: tensor(0.0883)\n",
      "3272 Training Loss: tensor(0.1021)\n",
      "3273 Training Loss: tensor(0.0972)\n",
      "3274 Training Loss: tensor(0.0975)\n",
      "3275 Training Loss: tensor(0.0963)\n",
      "3276 Training Loss: tensor(0.0999)\n",
      "3277 Training Loss: tensor(0.0946)\n",
      "3278 Training Loss: tensor(0.0921)\n",
      "3279 Training Loss: tensor(0.0990)\n",
      "3280 Training Loss: tensor(0.0986)\n",
      "3281 Training Loss: tensor(0.1015)\n",
      "3282 Training Loss: tensor(0.0981)\n",
      "3283 Training Loss: tensor(0.0933)\n",
      "3284 Training Loss: tensor(0.0954)\n",
      "3285 Training Loss: tensor(0.1010)\n",
      "3286 Training Loss: tensor(0.0942)\n",
      "3287 Training Loss: tensor(0.0906)\n",
      "3288 Training Loss: tensor(0.0978)\n",
      "3289 Training Loss: tensor(0.1007)\n",
      "3290 Training Loss: tensor(0.1005)\n",
      "3291 Training Loss: tensor(0.0910)\n",
      "3292 Training Loss: tensor(0.1036)\n",
      "3293 Training Loss: tensor(0.1000)\n",
      "3294 Training Loss: tensor(0.0905)\n",
      "3295 Training Loss: tensor(0.1014)\n",
      "3296 Training Loss: tensor(0.0924)\n",
      "3297 Training Loss: tensor(0.0987)\n",
      "3298 Training Loss: tensor(0.1008)\n",
      "3299 Training Loss: tensor(0.0896)\n",
      "3300 Training Loss: tensor(0.1025)\n",
      "3301 Training Loss: tensor(0.0961)\n",
      "3302 Training Loss: tensor(0.1033)\n",
      "3303 Training Loss: tensor(0.0977)\n",
      "3304 Training Loss: tensor(0.0994)\n",
      "3305 Training Loss: tensor(0.1008)\n",
      "3306 Training Loss: tensor(0.0996)\n",
      "3307 Training Loss: tensor(0.0955)\n",
      "3308 Training Loss: tensor(0.0950)\n",
      "3309 Training Loss: tensor(0.0969)\n",
      "3310 Training Loss: tensor(0.1007)\n",
      "3311 Training Loss: tensor(0.0906)\n",
      "3312 Training Loss: tensor(0.0936)\n",
      "3313 Training Loss: tensor(0.0994)\n",
      "3314 Training Loss: tensor(0.0995)\n",
      "3315 Training Loss: tensor(0.0939)\n",
      "3316 Training Loss: tensor(0.0967)\n",
      "3317 Training Loss: tensor(0.1058)\n",
      "3318 Training Loss: tensor(0.0947)\n",
      "3319 Training Loss: tensor(0.1013)\n",
      "3320 Training Loss: tensor(0.1068)\n",
      "3321 Training Loss: tensor(0.1037)\n",
      "3322 Training Loss: tensor(0.0916)\n",
      "3323 Training Loss: tensor(0.0942)\n",
      "3324 Training Loss: tensor(0.1033)\n",
      "3325 Training Loss: tensor(0.0942)\n",
      "3326 Training Loss: tensor(0.0956)\n",
      "3327 Training Loss: tensor(0.0918)\n",
      "3328 Training Loss: tensor(0.0946)\n",
      "3329 Training Loss: tensor(0.1004)\n",
      "3330 Training Loss: tensor(0.1025)\n",
      "3331 Training Loss: tensor(0.0969)\n",
      "3332 Training Loss: tensor(0.0939)\n",
      "3333 Training Loss: tensor(0.1039)\n",
      "3334 Training Loss: tensor(0.0904)\n",
      "3335 Training Loss: tensor(0.0972)\n",
      "3336 Training Loss: tensor(0.0835)\n",
      "3337 Training Loss: tensor(0.0944)\n",
      "3338 Training Loss: tensor(0.1000)\n",
      "3339 Training Loss: tensor(0.1003)\n",
      "3340 Training Loss: tensor(0.1032)\n",
      "3341 Training Loss: tensor(0.1017)\n",
      "3342 Training Loss: tensor(0.0899)\n",
      "3343 Training Loss: tensor(0.0893)\n",
      "3344 Training Loss: tensor(0.0945)\n",
      "3345 Training Loss: tensor(0.0940)\n",
      "3346 Training Loss: tensor(0.0951)\n",
      "3347 Training Loss: tensor(0.1015)\n",
      "3348 Training Loss: tensor(0.0878)\n",
      "3349 Training Loss: tensor(0.0903)\n",
      "3350 Training Loss: tensor(0.0932)\n",
      "3351 Training Loss: tensor(0.1030)\n",
      "3352 Training Loss: tensor(0.0963)\n",
      "3353 Training Loss: tensor(0.0949)\n",
      "3354 Training Loss: tensor(0.0976)\n",
      "3355 Training Loss: tensor(0.0932)\n",
      "3356 Training Loss: tensor(0.1008)\n",
      "3357 Training Loss: tensor(0.0911)\n",
      "3358 Training Loss: tensor(0.1059)\n",
      "3359 Training Loss: tensor(0.0970)\n",
      "3360 Training Loss: tensor(0.1034)\n",
      "3361 Training Loss: tensor(0.0945)\n",
      "3362 Training Loss: tensor(0.0935)\n",
      "3363 Training Loss: tensor(0.0916)\n",
      "3364 Training Loss: tensor(0.0956)\n",
      "3365 Training Loss: tensor(0.0971)\n",
      "3366 Training Loss: tensor(0.0911)\n",
      "3367 Training Loss: tensor(0.0968)\n",
      "3368 Training Loss: tensor(0.0956)\n",
      "3369 Training Loss: tensor(0.0966)\n",
      "3370 Training Loss: tensor(0.0948)\n",
      "3371 Training Loss: tensor(0.0986)\n",
      "3372 Training Loss: tensor(0.0991)\n",
      "3373 Training Loss: tensor(0.0938)\n",
      "3374 Training Loss: tensor(0.0945)\n",
      "3375 Training Loss: tensor(0.0977)\n",
      "3376 Training Loss: tensor(0.0957)\n",
      "3377 Training Loss: tensor(0.0998)\n",
      "3378 Training Loss: tensor(0.1048)\n",
      "3379 Training Loss: tensor(0.1015)\n",
      "3380 Training Loss: tensor(0.1039)\n",
      "3381 Training Loss: tensor(0.0956)\n",
      "3382 Training Loss: tensor(0.0894)\n",
      "3383 Training Loss: tensor(0.0976)\n",
      "3384 Training Loss: tensor(0.0984)\n",
      "3385 Training Loss: tensor(0.0949)\n",
      "3386 Training Loss: tensor(0.0950)\n",
      "3387 Training Loss: tensor(0.0945)\n",
      "3388 Training Loss: tensor(0.0994)\n",
      "3389 Training Loss: tensor(0.1023)\n",
      "3390 Training Loss: tensor(0.0939)\n",
      "3391 Training Loss: tensor(0.0962)\n",
      "3392 Training Loss: tensor(0.0959)\n",
      "3393 Training Loss: tensor(0.1028)\n",
      "3394 Training Loss: tensor(0.0972)\n",
      "3395 Training Loss: tensor(0.1019)\n",
      "3396 Training Loss: tensor(0.0963)\n",
      "3397 Training Loss: tensor(0.0974)\n",
      "3398 Training Loss: tensor(0.0930)\n",
      "3399 Training Loss: tensor(0.0940)\n",
      "3400 Training Loss: tensor(0.0897)\n",
      "3401 Training Loss: tensor(0.0956)\n",
      "3402 Training Loss: tensor(0.0893)\n",
      "3403 Training Loss: tensor(0.0954)\n",
      "3404 Training Loss: tensor(0.0977)\n",
      "3405 Training Loss: tensor(0.0997)\n",
      "3406 Training Loss: tensor(0.0961)\n",
      "3407 Training Loss: tensor(0.1042)\n",
      "3408 Training Loss: tensor(0.0934)\n",
      "3409 Training Loss: tensor(0.0989)\n",
      "3410 Training Loss: tensor(0.0913)\n",
      "3411 Training Loss: tensor(0.0946)\n",
      "3412 Training Loss: tensor(0.0949)\n",
      "3413 Training Loss: tensor(0.0940)\n",
      "3414 Training Loss: tensor(0.0952)\n",
      "3415 Training Loss: tensor(0.0928)\n",
      "3416 Training Loss: tensor(0.0927)\n",
      "3417 Training Loss: tensor(0.0992)\n",
      "3418 Training Loss: tensor(0.0943)\n",
      "3419 Training Loss: tensor(0.0945)\n",
      "3420 Training Loss: tensor(0.0970)\n",
      "3421 Training Loss: tensor(0.0989)\n",
      "3422 Training Loss: tensor(0.0990)\n",
      "3423 Training Loss: tensor(0.1038)\n",
      "3424 Training Loss: tensor(0.0922)\n",
      "3425 Training Loss: tensor(0.0947)\n",
      "3426 Training Loss: tensor(0.0849)\n",
      "3427 Training Loss: tensor(0.0925)\n",
      "3428 Training Loss: tensor(0.0845)\n",
      "3429 Training Loss: tensor(0.0915)\n",
      "3430 Training Loss: tensor(0.0938)\n",
      "3431 Training Loss: tensor(0.0965)\n",
      "3432 Training Loss: tensor(0.0990)\n",
      "3433 Training Loss: tensor(0.0993)\n",
      "3434 Training Loss: tensor(0.1013)\n",
      "3435 Training Loss: tensor(0.0964)\n",
      "3436 Training Loss: tensor(0.0976)\n",
      "3437 Training Loss: tensor(0.0950)\n",
      "3438 Training Loss: tensor(0.0938)\n",
      "3439 Training Loss: tensor(0.0980)\n",
      "3440 Training Loss: tensor(0.1023)\n",
      "3441 Training Loss: tensor(0.0903)\n",
      "3442 Training Loss: tensor(0.0983)\n",
      "3443 Training Loss: tensor(0.0952)\n",
      "3444 Training Loss: tensor(0.0919)\n",
      "3445 Training Loss: tensor(0.0934)\n",
      "3446 Training Loss: tensor(0.0924)\n",
      "3447 Training Loss: tensor(0.0854)\n",
      "3448 Training Loss: tensor(0.0954)\n",
      "3449 Training Loss: tensor(0.0888)\n",
      "3450 Training Loss: tensor(0.0943)\n",
      "3451 Training Loss: tensor(0.0940)\n",
      "3452 Training Loss: tensor(0.0968)\n",
      "3453 Training Loss: tensor(0.0911)\n",
      "3454 Training Loss: tensor(0.0990)\n",
      "3455 Training Loss: tensor(0.0900)\n",
      "3456 Training Loss: tensor(0.0915)\n",
      "3457 Training Loss: tensor(0.0941)\n",
      "3458 Training Loss: tensor(0.0942)\n",
      "3459 Training Loss: tensor(0.0932)\n",
      "3460 Training Loss: tensor(0.0962)\n",
      "3461 Training Loss: tensor(0.1006)\n",
      "3462 Training Loss: tensor(0.0928)\n",
      "3463 Training Loss: tensor(0.1014)\n",
      "3464 Training Loss: tensor(0.0856)\n",
      "3465 Training Loss: tensor(0.0960)\n",
      "3466 Training Loss: tensor(0.0873)\n",
      "3467 Training Loss: tensor(0.0919)\n",
      "3468 Training Loss: tensor(0.0958)\n",
      "3469 Training Loss: tensor(0.0955)\n",
      "3470 Training Loss: tensor(0.0987)\n",
      "3471 Training Loss: tensor(0.0909)\n",
      "3472 Training Loss: tensor(0.0879)\n",
      "3473 Training Loss: tensor(0.0833)\n",
      "3474 Training Loss: tensor(0.0948)\n",
      "3475 Training Loss: tensor(0.0898)\n",
      "3476 Training Loss: tensor(0.0917)\n",
      "3477 Training Loss: tensor(0.0962)\n",
      "3478 Training Loss: tensor(0.0919)\n",
      "3479 Training Loss: tensor(0.0877)\n",
      "3480 Training Loss: tensor(0.0957)\n",
      "3481 Training Loss: tensor(0.0893)\n",
      "3482 Training Loss: tensor(0.0923)\n",
      "3483 Training Loss: tensor(0.0933)\n",
      "3484 Training Loss: tensor(0.0880)\n",
      "3485 Training Loss: tensor(0.0962)\n",
      "3486 Training Loss: tensor(0.1006)\n",
      "3487 Training Loss: tensor(0.0997)\n",
      "3488 Training Loss: tensor(0.0990)\n",
      "3489 Training Loss: tensor(0.0974)\n",
      "3490 Training Loss: tensor(0.0993)\n",
      "3491 Training Loss: tensor(0.0998)\n",
      "3492 Training Loss: tensor(0.0907)\n",
      "3493 Training Loss: tensor(0.1064)\n",
      "3494 Training Loss: tensor(0.0900)\n",
      "3495 Training Loss: tensor(0.1013)\n",
      "3496 Training Loss: tensor(0.0981)\n",
      "3497 Training Loss: tensor(0.0994)\n",
      "3498 Training Loss: tensor(0.0943)\n",
      "3499 Training Loss: tensor(0.0954)\n",
      "3500 Training Loss: tensor(0.0947)\n",
      "3501 Training Loss: tensor(0.0936)\n",
      "3502 Training Loss: tensor(0.0902)\n",
      "3503 Training Loss: tensor(0.0812)\n",
      "3504 Training Loss: tensor(0.0916)\n",
      "3505 Training Loss: tensor(0.0974)\n",
      "3506 Training Loss: tensor(0.0906)\n",
      "3507 Training Loss: tensor(0.0897)\n",
      "3508 Training Loss: tensor(0.0935)\n",
      "3509 Training Loss: tensor(0.0949)\n",
      "3510 Training Loss: tensor(0.0875)\n",
      "3511 Training Loss: tensor(0.1029)\n",
      "3512 Training Loss: tensor(0.0875)\n",
      "3513 Training Loss: tensor(0.0891)\n",
      "3514 Training Loss: tensor(0.0956)\n",
      "3515 Training Loss: tensor(0.0915)\n",
      "3516 Training Loss: tensor(0.0959)\n",
      "3517 Training Loss: tensor(0.0921)\n",
      "3518 Training Loss: tensor(0.0961)\n",
      "3519 Training Loss: tensor(0.0864)\n",
      "3520 Training Loss: tensor(0.0995)\n",
      "3521 Training Loss: tensor(0.0932)\n",
      "3522 Training Loss: tensor(0.0954)\n",
      "3523 Training Loss: tensor(0.1012)\n",
      "3524 Training Loss: tensor(0.1003)\n",
      "3525 Training Loss: tensor(0.0849)\n",
      "3526 Training Loss: tensor(0.0926)\n",
      "3527 Training Loss: tensor(0.0902)\n",
      "3528 Training Loss: tensor(0.0942)\n",
      "3529 Training Loss: tensor(0.0961)\n",
      "3530 Training Loss: tensor(0.0935)\n",
      "3531 Training Loss: tensor(0.0860)\n",
      "3532 Training Loss: tensor(0.0923)\n",
      "3533 Training Loss: tensor(0.0891)\n",
      "3534 Training Loss: tensor(0.0915)\n",
      "3535 Training Loss: tensor(0.0894)\n",
      "3536 Training Loss: tensor(0.0930)\n",
      "3537 Training Loss: tensor(0.0856)\n",
      "3538 Training Loss: tensor(0.0992)\n",
      "3539 Training Loss: tensor(0.0832)\n",
      "3540 Training Loss: tensor(0.0901)\n",
      "3541 Training Loss: tensor(0.0944)\n",
      "3542 Training Loss: tensor(0.0901)\n",
      "3543 Training Loss: tensor(0.0957)\n",
      "3544 Training Loss: tensor(0.0886)\n",
      "3545 Training Loss: tensor(0.0886)\n",
      "3546 Training Loss: tensor(0.0941)\n",
      "3547 Training Loss: tensor(0.0843)\n",
      "3548 Training Loss: tensor(0.0947)\n",
      "3549 Training Loss: tensor(0.0993)\n",
      "3550 Training Loss: tensor(0.0941)\n",
      "3551 Training Loss: tensor(0.0910)\n",
      "3552 Training Loss: tensor(0.0974)\n",
      "3553 Training Loss: tensor(0.0939)\n",
      "3554 Training Loss: tensor(0.0909)\n",
      "3555 Training Loss: tensor(0.0904)\n",
      "3556 Training Loss: tensor(0.0996)\n",
      "3557 Training Loss: tensor(0.0951)\n",
      "3558 Training Loss: tensor(0.0875)\n",
      "3559 Training Loss: tensor(0.0939)\n",
      "3560 Training Loss: tensor(0.0865)\n",
      "3561 Training Loss: tensor(0.0890)\n",
      "3562 Training Loss: tensor(0.0930)\n",
      "3563 Training Loss: tensor(0.0901)\n",
      "3564 Training Loss: tensor(0.1007)\n",
      "3565 Training Loss: tensor(0.0966)\n",
      "3566 Training Loss: tensor(0.0932)\n",
      "3567 Training Loss: tensor(0.0937)\n",
      "3568 Training Loss: tensor(0.1001)\n",
      "3569 Training Loss: tensor(0.0852)\n",
      "3570 Training Loss: tensor(0.0950)\n",
      "3571 Training Loss: tensor(0.0905)\n",
      "3572 Training Loss: tensor(0.0844)\n",
      "3573 Training Loss: tensor(0.0878)\n",
      "3574 Training Loss: tensor(0.0963)\n",
      "3575 Training Loss: tensor(0.0915)\n",
      "3576 Training Loss: tensor(0.0933)\n",
      "3577 Training Loss: tensor(0.0953)\n",
      "3578 Training Loss: tensor(0.0925)\n",
      "3579 Training Loss: tensor(0.0881)\n",
      "3580 Training Loss: tensor(0.0921)\n",
      "3581 Training Loss: tensor(0.0860)\n",
      "3582 Training Loss: tensor(0.0906)\n",
      "3583 Training Loss: tensor(0.0910)\n",
      "3584 Training Loss: tensor(0.0997)\n",
      "3585 Training Loss: tensor(0.0917)\n",
      "3586 Training Loss: tensor(0.1016)\n",
      "3587 Training Loss: tensor(0.0938)\n",
      "3588 Training Loss: tensor(0.0864)\n",
      "3589 Training Loss: tensor(0.0952)\n",
      "3590 Training Loss: tensor(0.0931)\n",
      "3591 Training Loss: tensor(0.0903)\n",
      "3592 Training Loss: tensor(0.0901)\n",
      "3593 Training Loss: tensor(0.0873)\n",
      "3594 Training Loss: tensor(0.0871)\n",
      "3595 Training Loss: tensor(0.0924)\n",
      "3596 Training Loss: tensor(0.0900)\n",
      "3597 Training Loss: tensor(0.0843)\n",
      "3598 Training Loss: tensor(0.0940)\n",
      "3599 Training Loss: tensor(0.0943)\n",
      "3600 Training Loss: tensor(0.0943)\n",
      "3601 Training Loss: tensor(0.0971)\n",
      "3602 Training Loss: tensor(0.0840)\n",
      "3603 Training Loss: tensor(0.0871)\n",
      "3604 Training Loss: tensor(0.0937)\n",
      "3605 Training Loss: tensor(0.0913)\n",
      "3606 Training Loss: tensor(0.0902)\n",
      "3607 Training Loss: tensor(0.0860)\n",
      "3608 Training Loss: tensor(0.0943)\n",
      "3609 Training Loss: tensor(0.0837)\n",
      "3610 Training Loss: tensor(0.0889)\n",
      "3611 Training Loss: tensor(0.0989)\n",
      "3612 Training Loss: tensor(0.0935)\n",
      "3613 Training Loss: tensor(0.0907)\n",
      "3614 Training Loss: tensor(0.0908)\n",
      "3615 Training Loss: tensor(0.0900)\n",
      "3616 Training Loss: tensor(0.0939)\n",
      "3617 Training Loss: tensor(0.0910)\n",
      "3618 Training Loss: tensor(0.0888)\n",
      "3619 Training Loss: tensor(0.0875)\n",
      "3620 Training Loss: tensor(0.0877)\n",
      "3621 Training Loss: tensor(0.0877)\n",
      "3622 Training Loss: tensor(0.0847)\n",
      "3623 Training Loss: tensor(0.0911)\n",
      "3624 Training Loss: tensor(0.0956)\n",
      "3625 Training Loss: tensor(0.0886)\n",
      "3626 Training Loss: tensor(0.0887)\n",
      "3627 Training Loss: tensor(0.1018)\n",
      "3628 Training Loss: tensor(0.0860)\n",
      "3629 Training Loss: tensor(0.0878)\n",
      "3630 Training Loss: tensor(0.0886)\n",
      "3631 Training Loss: tensor(0.0924)\n",
      "3632 Training Loss: tensor(0.0924)\n",
      "3633 Training Loss: tensor(0.0870)\n",
      "3634 Training Loss: tensor(0.0925)\n",
      "3635 Training Loss: tensor(0.0889)\n",
      "3636 Training Loss: tensor(0.0931)\n",
      "3637 Training Loss: tensor(0.0883)\n",
      "3638 Training Loss: tensor(0.0860)\n",
      "3639 Training Loss: tensor(0.0934)\n",
      "3640 Training Loss: tensor(0.0887)\n",
      "3641 Training Loss: tensor(0.0985)\n",
      "3642 Training Loss: tensor(0.0941)\n",
      "3643 Training Loss: tensor(0.0930)\n",
      "3644 Training Loss: tensor(0.0910)\n",
      "3645 Training Loss: tensor(0.0945)\n",
      "3646 Training Loss: tensor(0.0951)\n",
      "3647 Training Loss: tensor(0.0941)\n",
      "3648 Training Loss: tensor(0.0936)\n",
      "3649 Training Loss: tensor(0.0905)\n",
      "3650 Training Loss: tensor(0.0880)\n",
      "3651 Training Loss: tensor(0.0875)\n",
      "3652 Training Loss: tensor(0.0890)\n",
      "3653 Training Loss: tensor(0.0946)\n",
      "3654 Training Loss: tensor(0.0853)\n",
      "3655 Training Loss: tensor(0.0855)\n",
      "3656 Training Loss: tensor(0.0940)\n",
      "3657 Training Loss: tensor(0.0876)\n",
      "3658 Training Loss: tensor(0.0918)\n",
      "3659 Training Loss: tensor(0.0916)\n",
      "3660 Training Loss: tensor(0.0875)\n",
      "3661 Training Loss: tensor(0.0893)\n",
      "3662 Training Loss: tensor(0.0879)\n",
      "3663 Training Loss: tensor(0.0864)\n",
      "3664 Training Loss: tensor(0.0900)\n",
      "3665 Training Loss: tensor(0.0967)\n",
      "3666 Training Loss: tensor(0.0944)\n",
      "3667 Training Loss: tensor(0.0854)\n",
      "3668 Training Loss: tensor(0.0874)\n",
      "3669 Training Loss: tensor(0.0888)\n",
      "3670 Training Loss: tensor(0.0843)\n",
      "3671 Training Loss: tensor(0.0919)\n",
      "3672 Training Loss: tensor(0.0912)\n",
      "3673 Training Loss: tensor(0.0857)\n",
      "3674 Training Loss: tensor(0.0873)\n",
      "3675 Training Loss: tensor(0.0868)\n",
      "3676 Training Loss: tensor(0.0850)\n",
      "3677 Training Loss: tensor(0.0879)\n",
      "3678 Training Loss: tensor(0.0877)\n",
      "3679 Training Loss: tensor(0.0887)\n",
      "3680 Training Loss: tensor(0.0880)\n",
      "3681 Training Loss: tensor(0.0877)\n",
      "3682 Training Loss: tensor(0.0882)\n",
      "3683 Training Loss: tensor(0.0883)\n",
      "3684 Training Loss: tensor(0.0859)\n",
      "3685 Training Loss: tensor(0.0935)\n",
      "3686 Training Loss: tensor(0.0952)\n",
      "3687 Training Loss: tensor(0.0992)\n",
      "3688 Training Loss: tensor(0.0938)\n",
      "3689 Training Loss: tensor(0.0876)\n",
      "3690 Training Loss: tensor(0.0972)\n",
      "3691 Training Loss: tensor(0.0935)\n",
      "3692 Training Loss: tensor(0.0938)\n",
      "3693 Training Loss: tensor(0.0925)\n",
      "3694 Training Loss: tensor(0.0921)\n",
      "3695 Training Loss: tensor(0.0973)\n",
      "3696 Training Loss: tensor(0.0892)\n",
      "3697 Training Loss: tensor(0.0899)\n",
      "3698 Training Loss: tensor(0.0909)\n",
      "3699 Training Loss: tensor(0.0922)\n",
      "3700 Training Loss: tensor(0.0939)\n",
      "3701 Training Loss: tensor(0.1001)\n",
      "3702 Training Loss: tensor(0.0919)\n",
      "3703 Training Loss: tensor(0.0915)\n",
      "3704 Training Loss: tensor(0.0910)\n",
      "3705 Training Loss: tensor(0.0873)\n",
      "3706 Training Loss: tensor(0.0846)\n",
      "3707 Training Loss: tensor(0.0887)\n",
      "3708 Training Loss: tensor(0.0896)\n",
      "3709 Training Loss: tensor(0.0893)\n",
      "3710 Training Loss: tensor(0.0910)\n",
      "3711 Training Loss: tensor(0.0847)\n",
      "3712 Training Loss: tensor(0.0911)\n",
      "3713 Training Loss: tensor(0.0900)\n",
      "3714 Training Loss: tensor(0.0883)\n",
      "3715 Training Loss: tensor(0.0893)\n",
      "3716 Training Loss: tensor(0.0917)\n",
      "3717 Training Loss: tensor(0.0875)\n",
      "3718 Training Loss: tensor(0.0832)\n",
      "3719 Training Loss: tensor(0.0912)\n",
      "3720 Training Loss: tensor(0.0766)\n",
      "3721 Training Loss: tensor(0.0832)\n",
      "3722 Training Loss: tensor(0.0917)\n",
      "3723 Training Loss: tensor(0.0914)\n",
      "3724 Training Loss: tensor(0.0968)\n",
      "3725 Training Loss: tensor(0.0952)\n",
      "3726 Training Loss: tensor(0.0804)\n",
      "3727 Training Loss: tensor(0.0867)\n",
      "3728 Training Loss: tensor(0.0887)\n",
      "3729 Training Loss: tensor(0.0888)\n",
      "3730 Training Loss: tensor(0.0860)\n",
      "3731 Training Loss: tensor(0.0901)\n",
      "3732 Training Loss: tensor(0.0881)\n",
      "3733 Training Loss: tensor(0.0984)\n",
      "3734 Training Loss: tensor(0.0809)\n",
      "3735 Training Loss: tensor(0.0927)\n",
      "3736 Training Loss: tensor(0.0878)\n",
      "3737 Training Loss: tensor(0.0911)\n",
      "3738 Training Loss: tensor(0.0834)\n",
      "3739 Training Loss: tensor(0.0937)\n",
      "3740 Training Loss: tensor(0.0812)\n",
      "3741 Training Loss: tensor(0.0854)\n",
      "3742 Training Loss: tensor(0.0856)\n",
      "3743 Training Loss: tensor(0.0836)\n",
      "3744 Training Loss: tensor(0.0858)\n",
      "3745 Training Loss: tensor(0.0855)\n",
      "3746 Training Loss: tensor(0.0972)\n",
      "3747 Training Loss: tensor(0.0914)\n",
      "3748 Training Loss: tensor(0.0937)\n",
      "3749 Training Loss: tensor(0.0876)\n",
      "3750 Training Loss: tensor(0.0849)\n",
      "3751 Training Loss: tensor(0.0883)\n",
      "3752 Training Loss: tensor(0.0912)\n",
      "3753 Training Loss: tensor(0.0884)\n",
      "3754 Training Loss: tensor(0.0909)\n",
      "3755 Training Loss: tensor(0.0867)\n",
      "3756 Training Loss: tensor(0.0848)\n",
      "3757 Training Loss: tensor(0.0798)\n",
      "3758 Training Loss: tensor(0.0964)\n",
      "3759 Training Loss: tensor(0.0826)\n",
      "3760 Training Loss: tensor(0.0910)\n",
      "3761 Training Loss: tensor(0.0915)\n",
      "3762 Training Loss: tensor(0.0873)\n",
      "3763 Training Loss: tensor(0.0890)\n",
      "3764 Training Loss: tensor(0.0807)\n",
      "3765 Training Loss: tensor(0.0857)\n",
      "3766 Training Loss: tensor(0.0912)\n",
      "3767 Training Loss: tensor(0.0843)\n",
      "3768 Training Loss: tensor(0.0934)\n",
      "3769 Training Loss: tensor(0.0924)\n",
      "3770 Training Loss: tensor(0.0815)\n",
      "3771 Training Loss: tensor(0.0960)\n",
      "3772 Training Loss: tensor(0.0862)\n",
      "3773 Training Loss: tensor(0.0885)\n",
      "3774 Training Loss: tensor(0.0932)\n",
      "3775 Training Loss: tensor(0.0884)\n",
      "3776 Training Loss: tensor(0.0874)\n",
      "3777 Training Loss: tensor(0.0926)\n",
      "3778 Training Loss: tensor(0.0903)\n",
      "3779 Training Loss: tensor(0.0904)\n",
      "3780 Training Loss: tensor(0.1034)\n",
      "3781 Training Loss: tensor(0.0898)\n",
      "3782 Training Loss: tensor(0.0888)\n",
      "3783 Training Loss: tensor(0.0877)\n",
      "3784 Training Loss: tensor(0.0924)\n",
      "3785 Training Loss: tensor(0.0842)\n",
      "3786 Training Loss: tensor(0.0919)\n",
      "3787 Training Loss: tensor(0.0860)\n",
      "3788 Training Loss: tensor(0.0892)\n",
      "3789 Training Loss: tensor(0.0845)\n",
      "3790 Training Loss: tensor(0.0922)\n",
      "3791 Training Loss: tensor(0.0843)\n",
      "3792 Training Loss: tensor(0.0874)\n",
      "3793 Training Loss: tensor(0.0890)\n",
      "3794 Training Loss: tensor(0.0865)\n",
      "3795 Training Loss: tensor(0.0818)\n",
      "3796 Training Loss: tensor(0.0867)\n",
      "3797 Training Loss: tensor(0.0802)\n",
      "3798 Training Loss: tensor(0.0864)\n",
      "3799 Training Loss: tensor(0.0859)\n",
      "3800 Training Loss: tensor(0.0893)\n",
      "3801 Training Loss: tensor(0.0822)\n",
      "3802 Training Loss: tensor(0.0967)\n",
      "3803 Training Loss: tensor(0.0839)\n",
      "3804 Training Loss: tensor(0.0943)\n",
      "3805 Training Loss: tensor(0.0918)\n",
      "3806 Training Loss: tensor(0.0847)\n",
      "3807 Training Loss: tensor(0.0876)\n",
      "3808 Training Loss: tensor(0.0918)\n",
      "3809 Training Loss: tensor(0.0874)\n",
      "3810 Training Loss: tensor(0.0843)\n",
      "3811 Training Loss: tensor(0.0945)\n",
      "3812 Training Loss: tensor(0.0904)\n",
      "3813 Training Loss: tensor(0.0856)\n",
      "3814 Training Loss: tensor(0.0816)\n",
      "3815 Training Loss: tensor(0.0881)\n",
      "3816 Training Loss: tensor(0.0964)\n",
      "3817 Training Loss: tensor(0.0808)\n",
      "3818 Training Loss: tensor(0.0939)\n",
      "3819 Training Loss: tensor(0.0847)\n",
      "3820 Training Loss: tensor(0.0923)\n",
      "3821 Training Loss: tensor(0.0978)\n",
      "3822 Training Loss: tensor(0.0895)\n",
      "3823 Training Loss: tensor(0.0870)\n",
      "3824 Training Loss: tensor(0.0822)\n",
      "3825 Training Loss: tensor(0.0976)\n",
      "3826 Training Loss: tensor(0.0871)\n",
      "3827 Training Loss: tensor(0.0938)\n",
      "3828 Training Loss: tensor(0.0832)\n",
      "3829 Training Loss: tensor(0.0858)\n",
      "3830 Training Loss: tensor(0.0891)\n",
      "3831 Training Loss: tensor(0.0870)\n",
      "3832 Training Loss: tensor(0.0901)\n",
      "3833 Training Loss: tensor(0.0898)\n",
      "3834 Training Loss: tensor(0.0858)\n",
      "3835 Training Loss: tensor(0.0915)\n",
      "3836 Training Loss: tensor(0.0859)\n",
      "3837 Training Loss: tensor(0.0945)\n",
      "3838 Training Loss: tensor(0.0904)\n",
      "3839 Training Loss: tensor(0.0785)\n",
      "3840 Training Loss: tensor(0.0833)\n",
      "3841 Training Loss: tensor(0.0916)\n",
      "3842 Training Loss: tensor(0.0897)\n",
      "3843 Training Loss: tensor(0.0878)\n",
      "3844 Training Loss: tensor(0.0879)\n",
      "3845 Training Loss: tensor(0.0856)\n",
      "3846 Training Loss: tensor(0.0906)\n",
      "3847 Training Loss: tensor(0.0845)\n",
      "3848 Training Loss: tensor(0.0934)\n",
      "3849 Training Loss: tensor(0.0833)\n",
      "3850 Training Loss: tensor(0.0752)\n",
      "3851 Training Loss: tensor(0.0870)\n",
      "3852 Training Loss: tensor(0.0819)\n",
      "3853 Training Loss: tensor(0.0790)\n",
      "3854 Training Loss: tensor(0.0924)\n",
      "3855 Training Loss: tensor(0.0863)\n",
      "3856 Training Loss: tensor(0.0803)\n",
      "3857 Training Loss: tensor(0.0888)\n",
      "3858 Training Loss: tensor(0.0798)\n",
      "3859 Training Loss: tensor(0.0886)\n",
      "3860 Training Loss: tensor(0.0885)\n",
      "3861 Training Loss: tensor(0.0905)\n",
      "3862 Training Loss: tensor(0.0744)\n",
      "3863 Training Loss: tensor(0.0971)\n",
      "3864 Training Loss: tensor(0.0848)\n",
      "3865 Training Loss: tensor(0.0809)\n",
      "3866 Training Loss: tensor(0.0791)\n",
      "3867 Training Loss: tensor(0.0859)\n",
      "3868 Training Loss: tensor(0.0936)\n",
      "3869 Training Loss: tensor(0.0871)\n",
      "3870 Training Loss: tensor(0.0859)\n",
      "3871 Training Loss: tensor(0.0874)\n",
      "3872 Training Loss: tensor(0.0879)\n",
      "3873 Training Loss: tensor(0.0791)\n",
      "3874 Training Loss: tensor(0.0905)\n",
      "3875 Training Loss: tensor(0.0868)\n",
      "3876 Training Loss: tensor(0.0831)\n",
      "3877 Training Loss: tensor(0.0829)\n",
      "3878 Training Loss: tensor(0.0924)\n",
      "3879 Training Loss: tensor(0.0866)\n",
      "3880 Training Loss: tensor(0.0874)\n",
      "3881 Training Loss: tensor(0.0891)\n",
      "3882 Training Loss: tensor(0.0865)\n",
      "3883 Training Loss: tensor(0.0860)\n",
      "3884 Training Loss: tensor(0.0950)\n",
      "3885 Training Loss: tensor(0.0784)\n",
      "3886 Training Loss: tensor(0.0826)\n",
      "3887 Training Loss: tensor(0.0936)\n",
      "3888 Training Loss: tensor(0.0839)\n",
      "3889 Training Loss: tensor(0.0884)\n",
      "3890 Training Loss: tensor(0.0887)\n",
      "3891 Training Loss: tensor(0.0886)\n",
      "3892 Training Loss: tensor(0.0914)\n",
      "3893 Training Loss: tensor(0.0878)\n",
      "3894 Training Loss: tensor(0.0844)\n",
      "3895 Training Loss: tensor(0.0848)\n",
      "3896 Training Loss: tensor(0.0768)\n",
      "3897 Training Loss: tensor(0.0916)\n",
      "3898 Training Loss: tensor(0.0861)\n",
      "3899 Training Loss: tensor(0.0847)\n",
      "3900 Training Loss: tensor(0.0951)\n",
      "3901 Training Loss: tensor(0.0871)\n",
      "3902 Training Loss: tensor(0.0808)\n",
      "3903 Training Loss: tensor(0.0844)\n",
      "3904 Training Loss: tensor(0.0822)\n",
      "3905 Training Loss: tensor(0.0856)\n",
      "3906 Training Loss: tensor(0.0849)\n",
      "3907 Training Loss: tensor(0.0915)\n",
      "3908 Training Loss: tensor(0.0893)\n",
      "3909 Training Loss: tensor(0.0818)\n",
      "3910 Training Loss: tensor(0.0891)\n",
      "3911 Training Loss: tensor(0.0909)\n",
      "3912 Training Loss: tensor(0.0876)\n",
      "3913 Training Loss: tensor(0.0827)\n",
      "3914 Training Loss: tensor(0.0856)\n",
      "3915 Training Loss: tensor(0.0841)\n",
      "3916 Training Loss: tensor(0.0819)\n",
      "3917 Training Loss: tensor(0.0879)\n",
      "3918 Training Loss: tensor(0.0814)\n",
      "3919 Training Loss: tensor(0.0976)\n",
      "3920 Training Loss: tensor(0.0879)\n",
      "3921 Training Loss: tensor(0.0907)\n",
      "3922 Training Loss: tensor(0.0854)\n",
      "3923 Training Loss: tensor(0.0895)\n",
      "3924 Training Loss: tensor(0.0836)\n",
      "3925 Training Loss: tensor(0.0829)\n",
      "3926 Training Loss: tensor(0.0880)\n",
      "3927 Training Loss: tensor(0.0725)\n",
      "3928 Training Loss: tensor(0.0849)\n",
      "3929 Training Loss: tensor(0.0836)\n",
      "3930 Training Loss: tensor(0.0805)\n",
      "3931 Training Loss: tensor(0.0945)\n",
      "3932 Training Loss: tensor(0.0845)\n",
      "3933 Training Loss: tensor(0.0879)\n",
      "3934 Training Loss: tensor(0.0829)\n",
      "3935 Training Loss: tensor(0.0920)\n",
      "3936 Training Loss: tensor(0.0875)\n",
      "3937 Training Loss: tensor(0.0911)\n",
      "3938 Training Loss: tensor(0.0769)\n",
      "3939 Training Loss: tensor(0.0793)\n",
      "3940 Training Loss: tensor(0.0806)\n",
      "3941 Training Loss: tensor(0.0816)\n",
      "3942 Training Loss: tensor(0.0854)\n",
      "3943 Training Loss: tensor(0.0918)\n",
      "3944 Training Loss: tensor(0.0832)\n",
      "3945 Training Loss: tensor(0.0849)\n",
      "3946 Training Loss: tensor(0.0818)\n",
      "3947 Training Loss: tensor(0.0865)\n",
      "3948 Training Loss: tensor(0.0976)\n",
      "3949 Training Loss: tensor(0.0799)\n",
      "3950 Training Loss: tensor(0.0813)\n",
      "3951 Training Loss: tensor(0.0790)\n",
      "3952 Training Loss: tensor(0.0849)\n",
      "3953 Training Loss: tensor(0.0837)\n",
      "3954 Training Loss: tensor(0.0897)\n",
      "3955 Training Loss: tensor(0.0844)\n",
      "3956 Training Loss: tensor(0.0811)\n",
      "3957 Training Loss: tensor(0.0876)\n",
      "3958 Training Loss: tensor(0.0854)\n",
      "3959 Training Loss: tensor(0.0837)\n",
      "3960 Training Loss: tensor(0.0864)\n",
      "3961 Training Loss: tensor(0.0843)\n",
      "3962 Training Loss: tensor(0.0930)\n",
      "3963 Training Loss: tensor(0.0863)\n",
      "3964 Training Loss: tensor(0.0885)\n",
      "3965 Training Loss: tensor(0.0892)\n",
      "3966 Training Loss: tensor(0.0841)\n",
      "3967 Training Loss: tensor(0.0857)\n",
      "3968 Training Loss: tensor(0.0841)\n",
      "3969 Training Loss: tensor(0.0899)\n",
      "3970 Training Loss: tensor(0.0801)\n",
      "3971 Training Loss: tensor(0.0817)\n",
      "3972 Training Loss: tensor(0.0819)\n",
      "3973 Training Loss: tensor(0.0879)\n",
      "3974 Training Loss: tensor(0.0810)\n",
      "3975 Training Loss: tensor(0.0813)\n",
      "3976 Training Loss: tensor(0.0853)\n",
      "3977 Training Loss: tensor(0.0834)\n",
      "3978 Training Loss: tensor(0.0836)\n",
      "3979 Training Loss: tensor(0.0846)\n",
      "3980 Training Loss: tensor(0.0854)\n",
      "3981 Training Loss: tensor(0.0977)\n",
      "3982 Training Loss: tensor(0.0778)\n",
      "3983 Training Loss: tensor(0.0891)\n",
      "3984 Training Loss: tensor(0.0871)\n",
      "3985 Training Loss: tensor(0.0791)\n",
      "3986 Training Loss: tensor(0.0825)\n",
      "3987 Training Loss: tensor(0.0840)\n",
      "3988 Training Loss: tensor(0.0817)\n",
      "3989 Training Loss: tensor(0.0916)\n",
      "3990 Training Loss: tensor(0.0865)\n",
      "3991 Training Loss: tensor(0.0850)\n",
      "3992 Training Loss: tensor(0.0815)\n",
      "3993 Training Loss: tensor(0.0860)\n",
      "3994 Training Loss: tensor(0.0894)\n",
      "3995 Training Loss: tensor(0.0810)\n",
      "3996 Training Loss: tensor(0.0910)\n",
      "3997 Training Loss: tensor(0.0823)\n",
      "3998 Training Loss: tensor(0.0821)\n",
      "3999 Training Loss: tensor(0.0887)\n",
      "4000 Training Loss: tensor(0.0848)\n",
      "4001 Training Loss: tensor(0.0912)\n",
      "4002 Training Loss: tensor(0.0802)\n",
      "4003 Training Loss: tensor(0.0886)\n",
      "4004 Training Loss: tensor(0.0838)\n",
      "4005 Training Loss: tensor(0.0823)\n",
      "4006 Training Loss: tensor(0.0905)\n",
      "4007 Training Loss: tensor(0.0833)\n",
      "4008 Training Loss: tensor(0.0871)\n",
      "4009 Training Loss: tensor(0.0883)\n",
      "4010 Training Loss: tensor(0.0955)\n",
      "4011 Training Loss: tensor(0.0860)\n",
      "4012 Training Loss: tensor(0.0844)\n",
      "4013 Training Loss: tensor(0.0859)\n",
      "4014 Training Loss: tensor(0.0838)\n",
      "4015 Training Loss: tensor(0.0859)\n",
      "4016 Training Loss: tensor(0.0816)\n",
      "4017 Training Loss: tensor(0.0855)\n",
      "4018 Training Loss: tensor(0.0817)\n",
      "4019 Training Loss: tensor(0.0870)\n",
      "4020 Training Loss: tensor(0.0860)\n",
      "4021 Training Loss: tensor(0.0868)\n",
      "4022 Training Loss: tensor(0.0842)\n",
      "4023 Training Loss: tensor(0.0837)\n",
      "4024 Training Loss: tensor(0.0805)\n",
      "4025 Training Loss: tensor(0.0899)\n",
      "4026 Training Loss: tensor(0.0836)\n",
      "4027 Training Loss: tensor(0.0841)\n",
      "4028 Training Loss: tensor(0.0902)\n",
      "4029 Training Loss: tensor(0.0880)\n",
      "4030 Training Loss: tensor(0.0816)\n",
      "4031 Training Loss: tensor(0.0881)\n",
      "4032 Training Loss: tensor(0.0871)\n",
      "4033 Training Loss: tensor(0.0905)\n",
      "4034 Training Loss: tensor(0.0910)\n",
      "4035 Training Loss: tensor(0.0802)\n",
      "4036 Training Loss: tensor(0.0862)\n",
      "4037 Training Loss: tensor(0.0869)\n",
      "4038 Training Loss: tensor(0.0791)\n",
      "4039 Training Loss: tensor(0.0875)\n",
      "4040 Training Loss: tensor(0.0890)\n",
      "4041 Training Loss: tensor(0.0860)\n",
      "4042 Training Loss: tensor(0.0810)\n",
      "4043 Training Loss: tensor(0.0800)\n",
      "4044 Training Loss: tensor(0.0802)\n",
      "4045 Training Loss: tensor(0.0868)\n",
      "4046 Training Loss: tensor(0.0900)\n",
      "4047 Training Loss: tensor(0.0894)\n",
      "4048 Training Loss: tensor(0.0819)\n",
      "4049 Training Loss: tensor(0.0839)\n",
      "4050 Training Loss: tensor(0.0822)\n",
      "4051 Training Loss: tensor(0.0851)\n",
      "4052 Training Loss: tensor(0.0824)\n",
      "4053 Training Loss: tensor(0.0828)\n",
      "4054 Training Loss: tensor(0.0788)\n",
      "4055 Training Loss: tensor(0.0842)\n",
      "4056 Training Loss: tensor(0.0836)\n",
      "4057 Training Loss: tensor(0.0792)\n",
      "4058 Training Loss: tensor(0.0789)\n",
      "4059 Training Loss: tensor(0.0867)\n",
      "4060 Training Loss: tensor(0.0833)\n",
      "4061 Training Loss: tensor(0.0791)\n",
      "4062 Training Loss: tensor(0.0730)\n",
      "4063 Training Loss: tensor(0.0766)\n",
      "4064 Training Loss: tensor(0.0897)\n",
      "4065 Training Loss: tensor(0.0870)\n",
      "4066 Training Loss: tensor(0.0888)\n",
      "4067 Training Loss: tensor(0.0856)\n",
      "4068 Training Loss: tensor(0.0856)\n",
      "4069 Training Loss: tensor(0.0882)\n",
      "4070 Training Loss: tensor(0.0818)\n",
      "4071 Training Loss: tensor(0.0787)\n",
      "4072 Training Loss: tensor(0.0814)\n",
      "4073 Training Loss: tensor(0.0886)\n",
      "4074 Training Loss: tensor(0.0832)\n",
      "4075 Training Loss: tensor(0.0836)\n",
      "4076 Training Loss: tensor(0.0797)\n",
      "4077 Training Loss: tensor(0.0833)\n",
      "4078 Training Loss: tensor(0.0816)\n",
      "4079 Training Loss: tensor(0.0852)\n",
      "4080 Training Loss: tensor(0.0826)\n",
      "4081 Training Loss: tensor(0.0897)\n",
      "4082 Training Loss: tensor(0.0810)\n",
      "4083 Training Loss: tensor(0.0824)\n",
      "4084 Training Loss: tensor(0.0852)\n",
      "4085 Training Loss: tensor(0.0827)\n",
      "4086 Training Loss: tensor(0.0840)\n",
      "4087 Training Loss: tensor(0.0882)\n",
      "4088 Training Loss: tensor(0.0817)\n",
      "4089 Training Loss: tensor(0.0759)\n",
      "4090 Training Loss: tensor(0.0807)\n",
      "4091 Training Loss: tensor(0.0839)\n",
      "4092 Training Loss: tensor(0.0853)\n",
      "4093 Training Loss: tensor(0.0779)\n",
      "4094 Training Loss: tensor(0.0838)\n",
      "4095 Training Loss: tensor(0.0912)\n",
      "4096 Training Loss: tensor(0.0768)\n",
      "4097 Training Loss: tensor(0.0850)\n",
      "4098 Training Loss: tensor(0.0863)\n",
      "4099 Training Loss: tensor(0.0793)\n",
      "4100 Training Loss: tensor(0.0848)\n",
      "4101 Training Loss: tensor(0.0800)\n",
      "4102 Training Loss: tensor(0.0845)\n",
      "4103 Training Loss: tensor(0.0839)\n",
      "4104 Training Loss: tensor(0.0758)\n",
      "4105 Training Loss: tensor(0.0793)\n",
      "4106 Training Loss: tensor(0.0850)\n",
      "4107 Training Loss: tensor(0.0842)\n",
      "4108 Training Loss: tensor(0.0820)\n",
      "4109 Training Loss: tensor(0.0833)\n",
      "4110 Training Loss: tensor(0.0834)\n",
      "4111 Training Loss: tensor(0.0843)\n",
      "4112 Training Loss: tensor(0.0825)\n",
      "4113 Training Loss: tensor(0.0841)\n",
      "4114 Training Loss: tensor(0.0790)\n",
      "4115 Training Loss: tensor(0.0782)\n",
      "4116 Training Loss: tensor(0.0853)\n",
      "4117 Training Loss: tensor(0.0743)\n",
      "4118 Training Loss: tensor(0.0781)\n",
      "4119 Training Loss: tensor(0.0768)\n",
      "4120 Training Loss: tensor(0.0801)\n",
      "4121 Training Loss: tensor(0.0824)\n",
      "4122 Training Loss: tensor(0.0845)\n",
      "4123 Training Loss: tensor(0.0782)\n",
      "4124 Training Loss: tensor(0.0835)\n",
      "4125 Training Loss: tensor(0.0753)\n",
      "4126 Training Loss: tensor(0.0924)\n",
      "4127 Training Loss: tensor(0.0773)\n",
      "4128 Training Loss: tensor(0.0878)\n",
      "4129 Training Loss: tensor(0.0837)\n",
      "4130 Training Loss: tensor(0.0750)\n",
      "4131 Training Loss: tensor(0.0845)\n",
      "4132 Training Loss: tensor(0.0855)\n",
      "4133 Training Loss: tensor(0.0766)\n",
      "4134 Training Loss: tensor(0.0789)\n",
      "4135 Training Loss: tensor(0.0822)\n",
      "4136 Training Loss: tensor(0.0844)\n",
      "4137 Training Loss: tensor(0.0824)\n",
      "4138 Training Loss: tensor(0.0800)\n",
      "4139 Training Loss: tensor(0.0813)\n",
      "4140 Training Loss: tensor(0.0811)\n",
      "4141 Training Loss: tensor(0.0879)\n",
      "4142 Training Loss: tensor(0.0850)\n",
      "4143 Training Loss: tensor(0.0790)\n",
      "4144 Training Loss: tensor(0.0811)\n",
      "4145 Training Loss: tensor(0.0792)\n",
      "4146 Training Loss: tensor(0.0871)\n",
      "4147 Training Loss: tensor(0.0847)\n",
      "4148 Training Loss: tensor(0.0847)\n",
      "4149 Training Loss: tensor(0.0828)\n",
      "4150 Training Loss: tensor(0.0839)\n",
      "4151 Training Loss: tensor(0.0805)\n",
      "4152 Training Loss: tensor(0.0857)\n",
      "4153 Training Loss: tensor(0.0849)\n",
      "4154 Training Loss: tensor(0.0814)\n",
      "4155 Training Loss: tensor(0.0812)\n",
      "4156 Training Loss: tensor(0.0767)\n",
      "4157 Training Loss: tensor(0.0807)\n",
      "4158 Training Loss: tensor(0.0896)\n",
      "4159 Training Loss: tensor(0.0853)\n",
      "4160 Training Loss: tensor(0.0816)\n",
      "4161 Training Loss: tensor(0.0850)\n",
      "4162 Training Loss: tensor(0.0798)\n",
      "4163 Training Loss: tensor(0.0897)\n",
      "4164 Training Loss: tensor(0.0795)\n",
      "4165 Training Loss: tensor(0.0842)\n",
      "4166 Training Loss: tensor(0.0734)\n",
      "4167 Training Loss: tensor(0.0785)\n",
      "4168 Training Loss: tensor(0.0857)\n",
      "4169 Training Loss: tensor(0.0814)\n",
      "4170 Training Loss: tensor(0.0811)\n",
      "4171 Training Loss: tensor(0.0766)\n",
      "4172 Training Loss: tensor(0.0787)\n",
      "4173 Training Loss: tensor(0.0798)\n",
      "4174 Training Loss: tensor(0.0897)\n",
      "4175 Training Loss: tensor(0.0778)\n",
      "4176 Training Loss: tensor(0.0764)\n",
      "4177 Training Loss: tensor(0.0835)\n",
      "4178 Training Loss: tensor(0.0777)\n",
      "4179 Training Loss: tensor(0.0881)\n",
      "4180 Training Loss: tensor(0.0798)\n",
      "4181 Training Loss: tensor(0.0786)\n",
      "4182 Training Loss: tensor(0.0834)\n",
      "4183 Training Loss: tensor(0.0813)\n",
      "4184 Training Loss: tensor(0.0784)\n",
      "4185 Training Loss: tensor(0.0808)\n",
      "4186 Training Loss: tensor(0.0824)\n",
      "4187 Training Loss: tensor(0.0738)\n",
      "4188 Training Loss: tensor(0.0780)\n",
      "4189 Training Loss: tensor(0.0792)\n",
      "4190 Training Loss: tensor(0.0865)\n",
      "4191 Training Loss: tensor(0.0876)\n",
      "4192 Training Loss: tensor(0.0913)\n",
      "4193 Training Loss: tensor(0.0769)\n",
      "4194 Training Loss: tensor(0.0799)\n",
      "4195 Training Loss: tensor(0.0846)\n",
      "4196 Training Loss: tensor(0.0793)\n",
      "4197 Training Loss: tensor(0.0881)\n",
      "4198 Training Loss: tensor(0.0724)\n",
      "4199 Training Loss: tensor(0.0827)\n",
      "4200 Training Loss: tensor(0.0837)\n",
      "4201 Training Loss: tensor(0.0826)\n",
      "4202 Training Loss: tensor(0.0857)\n",
      "4203 Training Loss: tensor(0.0779)\n",
      "4204 Training Loss: tensor(0.0830)\n",
      "4205 Training Loss: tensor(0.0914)\n",
      "4206 Training Loss: tensor(0.0808)\n",
      "4207 Training Loss: tensor(0.0713)\n",
      "4208 Training Loss: tensor(0.0760)\n",
      "4209 Training Loss: tensor(0.0789)\n",
      "4210 Training Loss: tensor(0.0773)\n",
      "4211 Training Loss: tensor(0.0804)\n",
      "4212 Training Loss: tensor(0.0818)\n",
      "4213 Training Loss: tensor(0.0784)\n",
      "4214 Training Loss: tensor(0.0893)\n",
      "4215 Training Loss: tensor(0.0851)\n",
      "4216 Training Loss: tensor(0.0785)\n",
      "4217 Training Loss: tensor(0.0794)\n",
      "4218 Training Loss: tensor(0.0807)\n",
      "4219 Training Loss: tensor(0.0825)\n",
      "4220 Training Loss: tensor(0.0843)\n",
      "4221 Training Loss: tensor(0.0826)\n",
      "4222 Training Loss: tensor(0.0890)\n",
      "4223 Training Loss: tensor(0.0823)\n",
      "4224 Training Loss: tensor(0.0862)\n",
      "4225 Training Loss: tensor(0.0809)\n",
      "4226 Training Loss: tensor(0.0771)\n",
      "4227 Training Loss: tensor(0.0775)\n",
      "4228 Training Loss: tensor(0.0727)\n",
      "4229 Training Loss: tensor(0.0818)\n",
      "4230 Training Loss: tensor(0.0861)\n",
      "4231 Training Loss: tensor(0.0846)\n",
      "4232 Training Loss: tensor(0.0787)\n",
      "4233 Training Loss: tensor(0.0818)\n",
      "4234 Training Loss: tensor(0.0802)\n",
      "4235 Training Loss: tensor(0.0793)\n",
      "4236 Training Loss: tensor(0.0846)\n",
      "4237 Training Loss: tensor(0.0865)\n",
      "4238 Training Loss: tensor(0.0778)\n",
      "4239 Training Loss: tensor(0.0790)\n",
      "4240 Training Loss: tensor(0.0804)\n",
      "4241 Training Loss: tensor(0.0752)\n",
      "4242 Training Loss: tensor(0.0790)\n",
      "4243 Training Loss: tensor(0.0802)\n",
      "4244 Training Loss: tensor(0.0850)\n",
      "4245 Training Loss: tensor(0.0813)\n",
      "4246 Training Loss: tensor(0.0780)\n",
      "4247 Training Loss: tensor(0.0846)\n",
      "4248 Training Loss: tensor(0.0780)\n",
      "4249 Training Loss: tensor(0.0743)\n",
      "4250 Training Loss: tensor(0.0810)\n",
      "4251 Training Loss: tensor(0.0801)\n",
      "4252 Training Loss: tensor(0.0817)\n",
      "4253 Training Loss: tensor(0.0815)\n",
      "4254 Training Loss: tensor(0.0844)\n",
      "4255 Training Loss: tensor(0.0722)\n",
      "4256 Training Loss: tensor(0.0793)\n",
      "4257 Training Loss: tensor(0.0812)\n",
      "4258 Training Loss: tensor(0.0799)\n",
      "4259 Training Loss: tensor(0.0740)\n",
      "4260 Training Loss: tensor(0.0747)\n",
      "4261 Training Loss: tensor(0.0815)\n",
      "4262 Training Loss: tensor(0.0815)\n",
      "4263 Training Loss: tensor(0.0765)\n",
      "4264 Training Loss: tensor(0.0776)\n",
      "4265 Training Loss: tensor(0.0791)\n",
      "4266 Training Loss: tensor(0.0824)\n",
      "4267 Training Loss: tensor(0.0800)\n",
      "4268 Training Loss: tensor(0.0839)\n",
      "4269 Training Loss: tensor(0.0783)\n",
      "4270 Training Loss: tensor(0.0799)\n",
      "4271 Training Loss: tensor(0.0756)\n",
      "4272 Training Loss: tensor(0.0799)\n",
      "4273 Training Loss: tensor(0.0793)\n",
      "4274 Training Loss: tensor(0.0753)\n",
      "4275 Training Loss: tensor(0.0853)\n",
      "4276 Training Loss: tensor(0.0809)\n",
      "4277 Training Loss: tensor(0.0814)\n",
      "4278 Training Loss: tensor(0.0818)\n",
      "4279 Training Loss: tensor(0.0840)\n",
      "4280 Training Loss: tensor(0.0811)\n",
      "4281 Training Loss: tensor(0.0842)\n",
      "4282 Training Loss: tensor(0.0819)\n",
      "4283 Training Loss: tensor(0.0749)\n",
      "4284 Training Loss: tensor(0.0823)\n",
      "4285 Training Loss: tensor(0.0804)\n",
      "4286 Training Loss: tensor(0.0789)\n",
      "4287 Training Loss: tensor(0.0788)\n",
      "4288 Training Loss: tensor(0.0760)\n",
      "4289 Training Loss: tensor(0.0774)\n",
      "4290 Training Loss: tensor(0.0738)\n",
      "4291 Training Loss: tensor(0.0749)\n",
      "4292 Training Loss: tensor(0.0778)\n",
      "4293 Training Loss: tensor(0.0798)\n",
      "4294 Training Loss: tensor(0.0799)\n",
      "4295 Training Loss: tensor(0.0747)\n",
      "4296 Training Loss: tensor(0.0796)\n",
      "4297 Training Loss: tensor(0.0817)\n",
      "4298 Training Loss: tensor(0.0872)\n",
      "4299 Training Loss: tensor(0.0841)\n",
      "4300 Training Loss: tensor(0.0822)\n",
      "4301 Training Loss: tensor(0.0868)\n",
      "4302 Training Loss: tensor(0.0808)\n",
      "4303 Training Loss: tensor(0.0672)\n",
      "4304 Training Loss: tensor(0.0840)\n",
      "4305 Training Loss: tensor(0.0851)\n",
      "4306 Training Loss: tensor(0.0727)\n",
      "4307 Training Loss: tensor(0.0874)\n",
      "4308 Training Loss: tensor(0.0791)\n",
      "4309 Training Loss: tensor(0.0809)\n",
      "4310 Training Loss: tensor(0.0808)\n",
      "4311 Training Loss: tensor(0.0725)\n",
      "4312 Training Loss: tensor(0.0777)\n",
      "4313 Training Loss: tensor(0.0791)\n",
      "4314 Training Loss: tensor(0.0771)\n",
      "4315 Training Loss: tensor(0.0799)\n",
      "4316 Training Loss: tensor(0.0772)\n",
      "4317 Training Loss: tensor(0.0835)\n",
      "4318 Training Loss: tensor(0.0741)\n",
      "4319 Training Loss: tensor(0.0778)\n",
      "4320 Training Loss: tensor(0.0763)\n",
      "4321 Training Loss: tensor(0.0781)\n",
      "4322 Training Loss: tensor(0.0825)\n",
      "4323 Training Loss: tensor(0.0764)\n",
      "4324 Training Loss: tensor(0.0793)\n",
      "4325 Training Loss: tensor(0.0856)\n",
      "4326 Training Loss: tensor(0.0773)\n",
      "4327 Training Loss: tensor(0.0782)\n",
      "4328 Training Loss: tensor(0.0792)\n",
      "4329 Training Loss: tensor(0.0780)\n",
      "4330 Training Loss: tensor(0.0826)\n",
      "4331 Training Loss: tensor(0.0768)\n",
      "4332 Training Loss: tensor(0.0816)\n",
      "4333 Training Loss: tensor(0.0720)\n",
      "4334 Training Loss: tensor(0.0799)\n",
      "4335 Training Loss: tensor(0.0773)\n",
      "4336 Training Loss: tensor(0.0793)\n",
      "4337 Training Loss: tensor(0.0765)\n",
      "4338 Training Loss: tensor(0.0786)\n",
      "4339 Training Loss: tensor(0.0822)\n",
      "4340 Training Loss: tensor(0.0840)\n",
      "4341 Training Loss: tensor(0.0819)\n",
      "4342 Training Loss: tensor(0.0851)\n",
      "4343 Training Loss: tensor(0.0726)\n",
      "4344 Training Loss: tensor(0.0854)\n",
      "4345 Training Loss: tensor(0.0831)\n",
      "4346 Training Loss: tensor(0.0855)\n",
      "4347 Training Loss: tensor(0.0734)\n",
      "4348 Training Loss: tensor(0.0792)\n",
      "4349 Training Loss: tensor(0.0755)\n",
      "4350 Training Loss: tensor(0.0691)\n",
      "4351 Training Loss: tensor(0.0735)\n",
      "4352 Training Loss: tensor(0.0760)\n",
      "4353 Training Loss: tensor(0.0716)\n",
      "4354 Training Loss: tensor(0.0813)\n",
      "4355 Training Loss: tensor(0.0821)\n",
      "4356 Training Loss: tensor(0.0737)\n",
      "4357 Training Loss: tensor(0.0828)\n",
      "4358 Training Loss: tensor(0.0877)\n",
      "4359 Training Loss: tensor(0.0804)\n",
      "4360 Training Loss: tensor(0.0768)\n",
      "4361 Training Loss: tensor(0.0769)\n",
      "4362 Training Loss: tensor(0.0811)\n",
      "4363 Training Loss: tensor(0.0865)\n",
      "4364 Training Loss: tensor(0.0787)\n",
      "4365 Training Loss: tensor(0.0805)\n",
      "4366 Training Loss: tensor(0.0707)\n",
      "4367 Training Loss: tensor(0.0757)\n",
      "4368 Training Loss: tensor(0.0731)\n",
      "4369 Training Loss: tensor(0.0819)\n",
      "4370 Training Loss: tensor(0.0789)\n",
      "4371 Training Loss: tensor(0.0825)\n",
      "4372 Training Loss: tensor(0.0764)\n",
      "4373 Training Loss: tensor(0.0780)\n",
      "4374 Training Loss: tensor(0.0780)\n",
      "4375 Training Loss: tensor(0.0741)\n",
      "4376 Training Loss: tensor(0.0773)\n",
      "4377 Training Loss: tensor(0.0834)\n",
      "4378 Training Loss: tensor(0.0842)\n",
      "4379 Training Loss: tensor(0.0761)\n",
      "4380 Training Loss: tensor(0.0772)\n",
      "4381 Training Loss: tensor(0.0813)\n",
      "4382 Training Loss: tensor(0.0794)\n",
      "4383 Training Loss: tensor(0.0796)\n",
      "4384 Training Loss: tensor(0.0749)\n",
      "4385 Training Loss: tensor(0.0762)\n",
      "4386 Training Loss: tensor(0.0698)\n",
      "4387 Training Loss: tensor(0.0752)\n",
      "4388 Training Loss: tensor(0.0800)\n",
      "4389 Training Loss: tensor(0.0766)\n",
      "4390 Training Loss: tensor(0.0796)\n",
      "4391 Training Loss: tensor(0.0759)\n",
      "4392 Training Loss: tensor(0.0802)\n",
      "4393 Training Loss: tensor(0.0724)\n",
      "4394 Training Loss: tensor(0.0774)\n",
      "4395 Training Loss: tensor(0.0791)\n",
      "4396 Training Loss: tensor(0.0789)\n",
      "4397 Training Loss: tensor(0.0753)\n",
      "4398 Training Loss: tensor(0.0724)\n",
      "4399 Training Loss: tensor(0.0830)\n",
      "4400 Training Loss: tensor(0.0767)\n",
      "4401 Training Loss: tensor(0.0806)\n",
      "4402 Training Loss: tensor(0.0776)\n",
      "4403 Training Loss: tensor(0.0764)\n",
      "4404 Training Loss: tensor(0.0801)\n",
      "4405 Training Loss: tensor(0.0735)\n",
      "4406 Training Loss: tensor(0.0775)\n",
      "4407 Training Loss: tensor(0.0757)\n",
      "4408 Training Loss: tensor(0.0797)\n",
      "4409 Training Loss: tensor(0.0734)\n",
      "4410 Training Loss: tensor(0.0779)\n",
      "4411 Training Loss: tensor(0.0777)\n",
      "4412 Training Loss: tensor(0.0732)\n",
      "4413 Training Loss: tensor(0.0723)\n",
      "4414 Training Loss: tensor(0.0790)\n",
      "4415 Training Loss: tensor(0.0810)\n",
      "4416 Training Loss: tensor(0.0776)\n",
      "4417 Training Loss: tensor(0.0758)\n",
      "4418 Training Loss: tensor(0.0711)\n",
      "4419 Training Loss: tensor(0.0725)\n",
      "4420 Training Loss: tensor(0.0764)\n",
      "4421 Training Loss: tensor(0.0730)\n",
      "4422 Training Loss: tensor(0.0784)\n",
      "4423 Training Loss: tensor(0.0792)\n",
      "4424 Training Loss: tensor(0.0740)\n",
      "4425 Training Loss: tensor(0.0767)\n",
      "4426 Training Loss: tensor(0.0713)\n",
      "4427 Training Loss: tensor(0.0808)\n",
      "4428 Training Loss: tensor(0.0823)\n",
      "4429 Training Loss: tensor(0.0786)\n",
      "4430 Training Loss: tensor(0.0758)\n",
      "4431 Training Loss: tensor(0.0784)\n",
      "4432 Training Loss: tensor(0.0713)\n",
      "4433 Training Loss: tensor(0.0753)\n",
      "4434 Training Loss: tensor(0.0687)\n",
      "4435 Training Loss: tensor(0.0747)\n",
      "4436 Training Loss: tensor(0.0806)\n",
      "4437 Training Loss: tensor(0.0756)\n",
      "4438 Training Loss: tensor(0.0755)\n",
      "4439 Training Loss: tensor(0.0806)\n",
      "4440 Training Loss: tensor(0.0704)\n",
      "4441 Training Loss: tensor(0.0769)\n",
      "4442 Training Loss: tensor(0.0837)\n",
      "4443 Training Loss: tensor(0.0882)\n",
      "4444 Training Loss: tensor(0.0749)\n",
      "4445 Training Loss: tensor(0.0831)\n",
      "4446 Training Loss: tensor(0.0775)\n",
      "4447 Training Loss: tensor(0.0762)\n",
      "4448 Training Loss: tensor(0.0781)\n",
      "4449 Training Loss: tensor(0.0704)\n",
      "4450 Training Loss: tensor(0.0804)\n",
      "4451 Training Loss: tensor(0.0741)\n",
      "4452 Training Loss: tensor(0.0775)\n",
      "4453 Training Loss: tensor(0.0753)\n",
      "4454 Training Loss: tensor(0.0772)\n",
      "4455 Training Loss: tensor(0.0798)\n",
      "4456 Training Loss: tensor(0.0822)\n",
      "4457 Training Loss: tensor(0.0817)\n",
      "4458 Training Loss: tensor(0.0878)\n",
      "4459 Training Loss: tensor(0.0825)\n",
      "4460 Training Loss: tensor(0.0806)\n",
      "4461 Training Loss: tensor(0.0798)\n",
      "4462 Training Loss: tensor(0.0758)\n",
      "4463 Training Loss: tensor(0.0731)\n",
      "4464 Training Loss: tensor(0.0747)\n",
      "4465 Training Loss: tensor(0.0745)\n",
      "4466 Training Loss: tensor(0.0809)\n",
      "4467 Training Loss: tensor(0.0767)\n",
      "4468 Training Loss: tensor(0.0784)\n",
      "4469 Training Loss: tensor(0.0798)\n",
      "4470 Training Loss: tensor(0.0804)\n",
      "4471 Training Loss: tensor(0.0777)\n",
      "4472 Training Loss: tensor(0.0740)\n",
      "4473 Training Loss: tensor(0.0773)\n",
      "4474 Training Loss: tensor(0.0749)\n",
      "4475 Training Loss: tensor(0.0730)\n",
      "4476 Training Loss: tensor(0.0766)\n",
      "4477 Training Loss: tensor(0.0739)\n",
      "4478 Training Loss: tensor(0.0738)\n",
      "4479 Training Loss: tensor(0.0624)\n",
      "4480 Training Loss: tensor(0.0781)\n",
      "4481 Training Loss: tensor(0.0770)\n",
      "4482 Training Loss: tensor(0.0730)\n",
      "4483 Training Loss: tensor(0.0709)\n",
      "4484 Training Loss: tensor(0.0762)\n",
      "4485 Training Loss: tensor(0.0825)\n",
      "4486 Training Loss: tensor(0.0774)\n",
      "4487 Training Loss: tensor(0.0790)\n",
      "4488 Training Loss: tensor(0.0746)\n",
      "4489 Training Loss: tensor(0.0755)\n",
      "4490 Training Loss: tensor(0.0770)\n",
      "4491 Training Loss: tensor(0.0764)\n",
      "4492 Training Loss: tensor(0.0782)\n",
      "4493 Training Loss: tensor(0.0698)\n",
      "4494 Training Loss: tensor(0.0751)\n",
      "4495 Training Loss: tensor(0.0769)\n",
      "4496 Training Loss: tensor(0.0775)\n",
      "4497 Training Loss: tensor(0.0848)\n",
      "4498 Training Loss: tensor(0.0815)\n",
      "4499 Training Loss: tensor(0.0719)\n",
      "4500 Training Loss: tensor(0.0735)\n",
      "4501 Training Loss: tensor(0.0771)\n",
      "4502 Training Loss: tensor(0.0795)\n",
      "4503 Training Loss: tensor(0.0708)\n",
      "4504 Training Loss: tensor(0.0869)\n",
      "4505 Training Loss: tensor(0.0772)\n",
      "4506 Training Loss: tensor(0.0753)\n",
      "4507 Training Loss: tensor(0.0757)\n",
      "4508 Training Loss: tensor(0.0809)\n",
      "4509 Training Loss: tensor(0.0798)\n",
      "4510 Training Loss: tensor(0.0774)\n",
      "4511 Training Loss: tensor(0.0700)\n",
      "4512 Training Loss: tensor(0.0722)\n",
      "4513 Training Loss: tensor(0.0744)\n",
      "4514 Training Loss: tensor(0.0783)\n",
      "4515 Training Loss: tensor(0.0735)\n",
      "4516 Training Loss: tensor(0.0787)\n",
      "4517 Training Loss: tensor(0.0790)\n",
      "4518 Training Loss: tensor(0.0724)\n",
      "4519 Training Loss: tensor(0.0780)\n",
      "4520 Training Loss: tensor(0.0761)\n",
      "4521 Training Loss: tensor(0.0698)\n",
      "4522 Training Loss: tensor(0.0697)\n",
      "4523 Training Loss: tensor(0.0708)\n",
      "4524 Training Loss: tensor(0.0737)\n",
      "4525 Training Loss: tensor(0.0743)\n",
      "4526 Training Loss: tensor(0.0721)\n",
      "4527 Training Loss: tensor(0.0732)\n",
      "4528 Training Loss: tensor(0.0776)\n",
      "4529 Training Loss: tensor(0.0770)\n",
      "4530 Training Loss: tensor(0.0760)\n",
      "4531 Training Loss: tensor(0.0721)\n",
      "4532 Training Loss: tensor(0.0694)\n",
      "4533 Training Loss: tensor(0.0704)\n",
      "4534 Training Loss: tensor(0.0664)\n",
      "4535 Training Loss: tensor(0.0740)\n",
      "4536 Training Loss: tensor(0.0792)\n",
      "4537 Training Loss: tensor(0.0796)\n",
      "4538 Training Loss: tensor(0.0739)\n",
      "4539 Training Loss: tensor(0.0755)\n",
      "4540 Training Loss: tensor(0.0811)\n",
      "4541 Training Loss: tensor(0.0759)\n",
      "4542 Training Loss: tensor(0.0763)\n",
      "4543 Training Loss: tensor(0.0711)\n",
      "4544 Training Loss: tensor(0.0794)\n",
      "4545 Training Loss: tensor(0.0758)\n",
      "4546 Training Loss: tensor(0.0845)\n",
      "4547 Training Loss: tensor(0.0739)\n",
      "4548 Training Loss: tensor(0.0710)\n",
      "4549 Training Loss: tensor(0.0676)\n",
      "4550 Training Loss: tensor(0.0789)\n",
      "4551 Training Loss: tensor(0.0745)\n",
      "4552 Training Loss: tensor(0.0801)\n",
      "4553 Training Loss: tensor(0.0740)\n",
      "4554 Training Loss: tensor(0.0763)\n",
      "4555 Training Loss: tensor(0.0728)\n",
      "4556 Training Loss: tensor(0.0675)\n",
      "4557 Training Loss: tensor(0.0737)\n",
      "4558 Training Loss: tensor(0.0725)\n",
      "4559 Training Loss: tensor(0.0820)\n",
      "4560 Training Loss: tensor(0.0695)\n",
      "4561 Training Loss: tensor(0.0844)\n",
      "4562 Training Loss: tensor(0.0760)\n",
      "4563 Training Loss: tensor(0.0778)\n",
      "4564 Training Loss: tensor(0.0730)\n",
      "4565 Training Loss: tensor(0.0821)\n",
      "4566 Training Loss: tensor(0.0782)\n",
      "4567 Training Loss: tensor(0.0753)\n",
      "4568 Training Loss: tensor(0.0780)\n",
      "4569 Training Loss: tensor(0.0787)\n",
      "4570 Training Loss: tensor(0.0729)\n",
      "4571 Training Loss: tensor(0.0741)\n",
      "4572 Training Loss: tensor(0.0700)\n",
      "4573 Training Loss: tensor(0.0749)\n",
      "4574 Training Loss: tensor(0.0739)\n",
      "4575 Training Loss: tensor(0.0751)\n",
      "4576 Training Loss: tensor(0.0744)\n",
      "4577 Training Loss: tensor(0.0735)\n",
      "4578 Training Loss: tensor(0.0806)\n",
      "4579 Training Loss: tensor(0.0715)\n",
      "4580 Training Loss: tensor(0.0803)\n",
      "4581 Training Loss: tensor(0.0721)\n",
      "4582 Training Loss: tensor(0.0725)\n",
      "4583 Training Loss: tensor(0.0757)\n",
      "4584 Training Loss: tensor(0.0727)\n",
      "4585 Training Loss: tensor(0.0690)\n",
      "4586 Training Loss: tensor(0.0667)\n",
      "4587 Training Loss: tensor(0.0766)\n",
      "4588 Training Loss: tensor(0.0747)\n",
      "4589 Training Loss: tensor(0.0743)\n",
      "4590 Training Loss: tensor(0.0756)\n",
      "4591 Training Loss: tensor(0.0735)\n",
      "4592 Training Loss: tensor(0.0673)\n",
      "4593 Training Loss: tensor(0.0731)\n",
      "4594 Training Loss: tensor(0.0735)\n",
      "4595 Training Loss: tensor(0.0745)\n",
      "4596 Training Loss: tensor(0.0771)\n",
      "4597 Training Loss: tensor(0.0718)\n",
      "4598 Training Loss: tensor(0.0673)\n",
      "4599 Training Loss: tensor(0.0750)\n",
      "4600 Training Loss: tensor(0.0740)\n",
      "4601 Training Loss: tensor(0.0787)\n",
      "4602 Training Loss: tensor(0.0865)\n",
      "4603 Training Loss: tensor(0.0720)\n",
      "4604 Training Loss: tensor(0.0741)\n",
      "4605 Training Loss: tensor(0.0755)\n",
      "4606 Training Loss: tensor(0.0760)\n",
      "4607 Training Loss: tensor(0.0633)\n",
      "4608 Training Loss: tensor(0.0799)\n",
      "4609 Training Loss: tensor(0.0756)\n",
      "4610 Training Loss: tensor(0.0815)\n",
      "4611 Training Loss: tensor(0.0815)\n",
      "4612 Training Loss: tensor(0.0780)\n",
      "4613 Training Loss: tensor(0.0694)\n",
      "4614 Training Loss: tensor(0.0747)\n",
      "4615 Training Loss: tensor(0.0733)\n",
      "4616 Training Loss: tensor(0.0754)\n",
      "4617 Training Loss: tensor(0.0709)\n",
      "4618 Training Loss: tensor(0.0694)\n",
      "4619 Training Loss: tensor(0.0709)\n",
      "4620 Training Loss: tensor(0.0802)\n",
      "4621 Training Loss: tensor(0.0766)\n",
      "4622 Training Loss: tensor(0.0660)\n",
      "4623 Training Loss: tensor(0.0728)\n",
      "4624 Training Loss: tensor(0.0744)\n",
      "4625 Training Loss: tensor(0.0707)\n",
      "4626 Training Loss: tensor(0.0752)\n",
      "4627 Training Loss: tensor(0.0809)\n",
      "4628 Training Loss: tensor(0.0773)\n",
      "4629 Training Loss: tensor(0.0680)\n",
      "4630 Training Loss: tensor(0.0724)\n",
      "4631 Training Loss: tensor(0.0802)\n",
      "4632 Training Loss: tensor(0.0635)\n",
      "4633 Training Loss: tensor(0.0737)\n",
      "4634 Training Loss: tensor(0.0769)\n",
      "4635 Training Loss: tensor(0.0756)\n",
      "4636 Training Loss: tensor(0.0653)\n",
      "4637 Training Loss: tensor(0.0669)\n",
      "4638 Training Loss: tensor(0.0767)\n",
      "4639 Training Loss: tensor(0.0741)\n",
      "4640 Training Loss: tensor(0.0688)\n",
      "4641 Training Loss: tensor(0.0726)\n",
      "4642 Training Loss: tensor(0.0727)\n",
      "4643 Training Loss: tensor(0.0660)\n",
      "4644 Training Loss: tensor(0.0753)\n",
      "4645 Training Loss: tensor(0.0738)\n",
      "4646 Training Loss: tensor(0.0737)\n",
      "4647 Training Loss: tensor(0.0719)\n",
      "4648 Training Loss: tensor(0.0721)\n",
      "4649 Training Loss: tensor(0.0793)\n",
      "4650 Training Loss: tensor(0.0726)\n",
      "4651 Training Loss: tensor(0.0724)\n",
      "4652 Training Loss: tensor(0.0692)\n",
      "4653 Training Loss: tensor(0.0694)\n",
      "4654 Training Loss: tensor(0.0661)\n",
      "4655 Training Loss: tensor(0.0673)\n",
      "4656 Training Loss: tensor(0.0789)\n",
      "4657 Training Loss: tensor(0.0823)\n",
      "4658 Training Loss: tensor(0.0674)\n",
      "4659 Training Loss: tensor(0.0737)\n",
      "4660 Training Loss: tensor(0.0767)\n",
      "4661 Training Loss: tensor(0.0792)\n",
      "4662 Training Loss: tensor(0.0653)\n",
      "4663 Training Loss: tensor(0.0693)\n",
      "4664 Training Loss: tensor(0.0801)\n",
      "4665 Training Loss: tensor(0.0760)\n",
      "4666 Training Loss: tensor(0.0694)\n",
      "4667 Training Loss: tensor(0.0661)\n",
      "4668 Training Loss: tensor(0.0795)\n",
      "4669 Training Loss: tensor(0.0769)\n",
      "4670 Training Loss: tensor(0.0716)\n",
      "4671 Training Loss: tensor(0.0728)\n",
      "4672 Training Loss: tensor(0.0746)\n",
      "4673 Training Loss: tensor(0.0677)\n",
      "4674 Training Loss: tensor(0.0704)\n",
      "4675 Training Loss: tensor(0.0756)\n",
      "4676 Training Loss: tensor(0.0759)\n",
      "4677 Training Loss: tensor(0.0723)\n",
      "4678 Training Loss: tensor(0.0704)\n",
      "4679 Training Loss: tensor(0.0737)\n",
      "4680 Training Loss: tensor(0.0698)\n",
      "4681 Training Loss: tensor(0.0696)\n",
      "4682 Training Loss: tensor(0.0709)\n",
      "4683 Training Loss: tensor(0.0685)\n",
      "4684 Training Loss: tensor(0.0725)\n",
      "4685 Training Loss: tensor(0.0695)\n",
      "4686 Training Loss: tensor(0.0811)\n",
      "4687 Training Loss: tensor(0.0764)\n",
      "4688 Training Loss: tensor(0.0765)\n",
      "4689 Training Loss: tensor(0.0764)\n",
      "4690 Training Loss: tensor(0.0738)\n",
      "4691 Training Loss: tensor(0.0731)\n",
      "4692 Training Loss: tensor(0.0698)\n",
      "4693 Training Loss: tensor(0.0776)\n",
      "4694 Training Loss: tensor(0.0750)\n",
      "4695 Training Loss: tensor(0.0735)\n",
      "4696 Training Loss: tensor(0.0786)\n",
      "4697 Training Loss: tensor(0.0770)\n",
      "4698 Training Loss: tensor(0.0724)\n",
      "4699 Training Loss: tensor(0.0756)\n",
      "4700 Training Loss: tensor(0.0722)\n",
      "4701 Training Loss: tensor(0.0722)\n",
      "4702 Training Loss: tensor(0.0719)\n",
      "4703 Training Loss: tensor(0.0731)\n",
      "4704 Training Loss: tensor(0.0704)\n",
      "4705 Training Loss: tensor(0.0693)\n",
      "4706 Training Loss: tensor(0.0754)\n",
      "4707 Training Loss: tensor(0.0780)\n",
      "4708 Training Loss: tensor(0.0749)\n",
      "4709 Training Loss: tensor(0.0694)\n",
      "4710 Training Loss: tensor(0.0752)\n",
      "4711 Training Loss: tensor(0.0737)\n",
      "4712 Training Loss: tensor(0.0681)\n",
      "4713 Training Loss: tensor(0.0742)\n",
      "4714 Training Loss: tensor(0.0671)\n",
      "4715 Training Loss: tensor(0.0732)\n",
      "4716 Training Loss: tensor(0.0746)\n",
      "4717 Training Loss: tensor(0.0762)\n",
      "4718 Training Loss: tensor(0.0736)\n",
      "4719 Training Loss: tensor(0.0720)\n",
      "4720 Training Loss: tensor(0.0770)\n",
      "4721 Training Loss: tensor(0.0732)\n",
      "4722 Training Loss: tensor(0.0767)\n",
      "4723 Training Loss: tensor(0.0781)\n",
      "4724 Training Loss: tensor(0.0775)\n",
      "4725 Training Loss: tensor(0.0671)\n",
      "4726 Training Loss: tensor(0.0725)\n",
      "4727 Training Loss: tensor(0.0740)\n",
      "4728 Training Loss: tensor(0.0803)\n",
      "4729 Training Loss: tensor(0.0677)\n",
      "4730 Training Loss: tensor(0.0833)\n",
      "4731 Training Loss: tensor(0.0738)\n",
      "4732 Training Loss: tensor(0.0703)\n",
      "4733 Training Loss: tensor(0.0755)\n",
      "4734 Training Loss: tensor(0.0786)\n",
      "4735 Training Loss: tensor(0.0735)\n",
      "4736 Training Loss: tensor(0.0689)\n",
      "4737 Training Loss: tensor(0.0702)\n",
      "4738 Training Loss: tensor(0.0677)\n",
      "4739 Training Loss: tensor(0.0691)\n",
      "4740 Training Loss: tensor(0.0692)\n",
      "4741 Training Loss: tensor(0.0686)\n",
      "4742 Training Loss: tensor(0.0668)\n",
      "4743 Training Loss: tensor(0.0723)\n",
      "4744 Training Loss: tensor(0.0767)\n",
      "4745 Training Loss: tensor(0.0672)\n",
      "4746 Training Loss: tensor(0.0707)\n",
      "4747 Training Loss: tensor(0.0702)\n",
      "4748 Training Loss: tensor(0.0699)\n",
      "4749 Training Loss: tensor(0.0727)\n",
      "4750 Training Loss: tensor(0.0690)\n",
      "4751 Training Loss: tensor(0.0751)\n",
      "4752 Training Loss: tensor(0.0778)\n",
      "4753 Training Loss: tensor(0.0704)\n",
      "4754 Training Loss: tensor(0.0704)\n",
      "4755 Training Loss: tensor(0.0655)\n",
      "4756 Training Loss: tensor(0.0798)\n",
      "4757 Training Loss: tensor(0.0773)\n",
      "4758 Training Loss: tensor(0.0707)\n",
      "4759 Training Loss: tensor(0.0665)\n",
      "4760 Training Loss: tensor(0.0712)\n",
      "4761 Training Loss: tensor(0.0703)\n",
      "4762 Training Loss: tensor(0.0646)\n",
      "4763 Training Loss: tensor(0.0741)\n",
      "4764 Training Loss: tensor(0.0689)\n",
      "4765 Training Loss: tensor(0.0705)\n",
      "4766 Training Loss: tensor(0.0697)\n",
      "4767 Training Loss: tensor(0.0668)\n",
      "4768 Training Loss: tensor(0.0812)\n",
      "4769 Training Loss: tensor(0.0684)\n",
      "4770 Training Loss: tensor(0.0724)\n",
      "4771 Training Loss: tensor(0.0719)\n",
      "4772 Training Loss: tensor(0.0709)\n",
      "4773 Training Loss: tensor(0.0660)\n",
      "4774 Training Loss: tensor(0.0703)\n",
      "4775 Training Loss: tensor(0.0705)\n",
      "4776 Training Loss: tensor(0.0690)\n",
      "4777 Training Loss: tensor(0.0703)\n",
      "4778 Training Loss: tensor(0.0706)\n",
      "4779 Training Loss: tensor(0.0691)\n",
      "4780 Training Loss: tensor(0.0719)\n",
      "4781 Training Loss: tensor(0.0669)\n",
      "4782 Training Loss: tensor(0.0611)\n",
      "4783 Training Loss: tensor(0.0762)\n",
      "4784 Training Loss: tensor(0.0696)\n",
      "4785 Training Loss: tensor(0.0719)\n",
      "4786 Training Loss: tensor(0.0765)\n",
      "4787 Training Loss: tensor(0.0684)\n",
      "4788 Training Loss: tensor(0.0760)\n",
      "4789 Training Loss: tensor(0.0767)\n",
      "4790 Training Loss: tensor(0.0797)\n",
      "4791 Training Loss: tensor(0.0611)\n",
      "4792 Training Loss: tensor(0.0711)\n",
      "4793 Training Loss: tensor(0.0759)\n",
      "4794 Training Loss: tensor(0.0712)\n",
      "4795 Training Loss: tensor(0.0753)\n",
      "4796 Training Loss: tensor(0.0703)\n",
      "4797 Training Loss: tensor(0.0694)\n",
      "4798 Training Loss: tensor(0.0719)\n",
      "4799 Training Loss: tensor(0.0743)\n",
      "4800 Training Loss: tensor(0.0725)\n",
      "4801 Training Loss: tensor(0.0743)\n",
      "4802 Training Loss: tensor(0.0718)\n",
      "4803 Training Loss: tensor(0.0622)\n",
      "4804 Training Loss: tensor(0.0678)\n",
      "4805 Training Loss: tensor(0.0791)\n",
      "4806 Training Loss: tensor(0.0717)\n",
      "4807 Training Loss: tensor(0.0675)\n",
      "4808 Training Loss: tensor(0.0652)\n",
      "4809 Training Loss: tensor(0.0671)\n",
      "4810 Training Loss: tensor(0.0633)\n",
      "4811 Training Loss: tensor(0.0691)\n",
      "4812 Training Loss: tensor(0.0711)\n",
      "4813 Training Loss: tensor(0.0644)\n",
      "4814 Training Loss: tensor(0.0769)\n",
      "4815 Training Loss: tensor(0.0743)\n",
      "4816 Training Loss: tensor(0.0732)\n",
      "4817 Training Loss: tensor(0.0750)\n",
      "4818 Training Loss: tensor(0.0758)\n",
      "4819 Training Loss: tensor(0.0758)\n",
      "4820 Training Loss: tensor(0.0747)\n",
      "4821 Training Loss: tensor(0.0715)\n",
      "4822 Training Loss: tensor(0.0722)\n",
      "4823 Training Loss: tensor(0.0668)\n",
      "4824 Training Loss: tensor(0.0682)\n",
      "4825 Training Loss: tensor(0.0672)\n",
      "4826 Training Loss: tensor(0.0673)\n",
      "4827 Training Loss: tensor(0.0714)\n",
      "4828 Training Loss: tensor(0.0763)\n",
      "4829 Training Loss: tensor(0.0679)\n",
      "4830 Training Loss: tensor(0.0646)\n",
      "4831 Training Loss: tensor(0.0707)\n",
      "4832 Training Loss: tensor(0.0715)\n",
      "4833 Training Loss: tensor(0.0776)\n",
      "4834 Training Loss: tensor(0.0657)\n",
      "4835 Training Loss: tensor(0.0744)\n",
      "4836 Training Loss: tensor(0.0840)\n",
      "4837 Training Loss: tensor(0.0693)\n",
      "4838 Training Loss: tensor(0.0716)\n",
      "4839 Training Loss: tensor(0.0723)\n",
      "4840 Training Loss: tensor(0.0746)\n",
      "4841 Training Loss: tensor(0.0725)\n",
      "4842 Training Loss: tensor(0.0675)\n",
      "4843 Training Loss: tensor(0.0763)\n",
      "4844 Training Loss: tensor(0.0716)\n",
      "4845 Training Loss: tensor(0.0703)\n",
      "4846 Training Loss: tensor(0.0624)\n",
      "4847 Training Loss: tensor(0.0793)\n",
      "4848 Training Loss: tensor(0.0733)\n",
      "4849 Training Loss: tensor(0.0714)\n",
      "4850 Training Loss: tensor(0.0681)\n",
      "4851 Training Loss: tensor(0.0661)\n",
      "4852 Training Loss: tensor(0.0720)\n",
      "4853 Training Loss: tensor(0.0729)\n",
      "4854 Training Loss: tensor(0.0615)\n",
      "4855 Training Loss: tensor(0.0749)\n",
      "4856 Training Loss: tensor(0.0679)\n",
      "4857 Training Loss: tensor(0.0669)\n",
      "4858 Training Loss: tensor(0.0652)\n",
      "4859 Training Loss: tensor(0.0673)\n",
      "4860 Training Loss: tensor(0.0710)\n",
      "4861 Training Loss: tensor(0.0678)\n",
      "4862 Training Loss: tensor(0.0641)\n",
      "4863 Training Loss: tensor(0.0691)\n",
      "4864 Training Loss: tensor(0.0703)\n",
      "4865 Training Loss: tensor(0.0631)\n",
      "4866 Training Loss: tensor(0.0695)\n",
      "4867 Training Loss: tensor(0.0740)\n",
      "4868 Training Loss: tensor(0.0713)\n",
      "4869 Training Loss: tensor(0.0710)\n",
      "4870 Training Loss: tensor(0.0674)\n",
      "4871 Training Loss: tensor(0.0664)\n",
      "4872 Training Loss: tensor(0.0710)\n",
      "4873 Training Loss: tensor(0.0692)\n",
      "4874 Training Loss: tensor(0.0626)\n",
      "4875 Training Loss: tensor(0.0632)\n",
      "4876 Training Loss: tensor(0.0671)\n",
      "4877 Training Loss: tensor(0.0674)\n",
      "4878 Training Loss: tensor(0.0718)\n",
      "4879 Training Loss: tensor(0.0730)\n",
      "4880 Training Loss: tensor(0.0702)\n",
      "4881 Training Loss: tensor(0.0690)\n",
      "4882 Training Loss: tensor(0.0695)\n",
      "4883 Training Loss: tensor(0.0685)\n",
      "4884 Training Loss: tensor(0.0734)\n",
      "4885 Training Loss: tensor(0.0729)\n",
      "4886 Training Loss: tensor(0.0702)\n",
      "4887 Training Loss: tensor(0.0770)\n",
      "4888 Training Loss: tensor(0.0741)\n",
      "4889 Training Loss: tensor(0.0816)\n",
      "4890 Training Loss: tensor(0.0627)\n",
      "4891 Training Loss: tensor(0.0694)\n",
      "4892 Training Loss: tensor(0.0679)\n",
      "4893 Training Loss: tensor(0.0679)\n",
      "4894 Training Loss: tensor(0.0702)\n",
      "4895 Training Loss: tensor(0.0731)\n",
      "4896 Training Loss: tensor(0.0744)\n",
      "4897 Training Loss: tensor(0.0729)\n",
      "4898 Training Loss: tensor(0.0698)\n",
      "4899 Training Loss: tensor(0.0678)\n",
      "4900 Training Loss: tensor(0.0759)\n",
      "4901 Training Loss: tensor(0.0691)\n",
      "4902 Training Loss: tensor(0.0704)\n",
      "4903 Training Loss: tensor(0.0703)\n",
      "4904 Training Loss: tensor(0.0623)\n",
      "4905 Training Loss: tensor(0.0703)\n",
      "4906 Training Loss: tensor(0.0749)\n",
      "4907 Training Loss: tensor(0.0699)\n",
      "4908 Training Loss: tensor(0.0684)\n",
      "4909 Training Loss: tensor(0.0679)\n",
      "4910 Training Loss: tensor(0.0656)\n",
      "4911 Training Loss: tensor(0.0706)\n",
      "4912 Training Loss: tensor(0.0681)\n",
      "4913 Training Loss: tensor(0.0672)\n",
      "4914 Training Loss: tensor(0.0709)\n",
      "4915 Training Loss: tensor(0.0666)\n",
      "4916 Training Loss: tensor(0.0681)\n",
      "4917 Training Loss: tensor(0.0641)\n",
      "4918 Training Loss: tensor(0.0734)\n",
      "4919 Training Loss: tensor(0.0677)\n",
      "4920 Training Loss: tensor(0.0712)\n",
      "4921 Training Loss: tensor(0.0735)\n",
      "4922 Training Loss: tensor(0.0588)\n",
      "4923 Training Loss: tensor(0.0636)\n",
      "4924 Training Loss: tensor(0.0720)\n",
      "4925 Training Loss: tensor(0.0681)\n",
      "4926 Training Loss: tensor(0.0744)\n",
      "4927 Training Loss: tensor(0.0652)\n",
      "4928 Training Loss: tensor(0.0583)\n",
      "4929 Training Loss: tensor(0.0631)\n",
      "4930 Training Loss: tensor(0.0670)\n",
      "4931 Training Loss: tensor(0.0586)\n",
      "4932 Training Loss: tensor(0.0686)\n",
      "4933 Training Loss: tensor(0.0690)\n",
      "4934 Training Loss: tensor(0.0701)\n",
      "4935 Training Loss: tensor(0.0622)\n",
      "4936 Training Loss: tensor(0.0669)\n",
      "4937 Training Loss: tensor(0.0628)\n",
      "4938 Training Loss: tensor(0.0637)\n",
      "4939 Training Loss: tensor(0.0625)\n",
      "4940 Training Loss: tensor(0.0648)\n",
      "4941 Training Loss: tensor(0.0747)\n",
      "4942 Training Loss: tensor(0.0654)\n",
      "4943 Training Loss: tensor(0.0678)\n",
      "4944 Training Loss: tensor(0.0603)\n",
      "4945 Training Loss: tensor(0.0648)\n",
      "4946 Training Loss: tensor(0.0696)\n",
      "4947 Training Loss: tensor(0.0666)\n",
      "4948 Training Loss: tensor(0.0640)\n",
      "4949 Training Loss: tensor(0.0653)\n",
      "4950 Training Loss: tensor(0.0737)\n",
      "4951 Training Loss: tensor(0.0662)\n",
      "4952 Training Loss: tensor(0.0671)\n",
      "4953 Training Loss: tensor(0.0680)\n",
      "4954 Training Loss: tensor(0.0677)\n",
      "4955 Training Loss: tensor(0.0673)\n",
      "4956 Training Loss: tensor(0.0687)\n",
      "4957 Training Loss: tensor(0.0751)\n",
      "4958 Training Loss: tensor(0.0630)\n",
      "4959 Training Loss: tensor(0.0738)\n",
      "4960 Training Loss: tensor(0.0720)\n",
      "4961 Training Loss: tensor(0.0720)\n",
      "4962 Training Loss: tensor(0.0734)\n",
      "4963 Training Loss: tensor(0.0628)\n",
      "4964 Training Loss: tensor(0.0717)\n",
      "4965 Training Loss: tensor(0.0736)\n",
      "4966 Training Loss: tensor(0.0727)\n",
      "4967 Training Loss: tensor(0.0681)\n",
      "4968 Training Loss: tensor(0.0650)\n",
      "4969 Training Loss: tensor(0.0646)\n",
      "4970 Training Loss: tensor(0.0658)\n",
      "4971 Training Loss: tensor(0.0669)\n",
      "4972 Training Loss: tensor(0.0679)\n",
      "4973 Training Loss: tensor(0.0675)\n",
      "4974 Training Loss: tensor(0.0679)\n",
      "4975 Training Loss: tensor(0.0606)\n",
      "4976 Training Loss: tensor(0.0694)\n",
      "4977 Training Loss: tensor(0.0594)\n",
      "4978 Training Loss: tensor(0.0659)\n",
      "4979 Training Loss: tensor(0.0712)\n",
      "4980 Training Loss: tensor(0.0685)\n",
      "4981 Training Loss: tensor(0.0736)\n",
      "4982 Training Loss: tensor(0.0714)\n",
      "4983 Training Loss: tensor(0.0687)\n",
      "4984 Training Loss: tensor(0.0639)\n",
      "4985 Training Loss: tensor(0.0722)\n",
      "4986 Training Loss: tensor(0.0606)\n",
      "4987 Training Loss: tensor(0.0681)\n",
      "4988 Training Loss: tensor(0.0656)\n",
      "4989 Training Loss: tensor(0.0697)\n",
      "4990 Training Loss: tensor(0.0701)\n",
      "4991 Training Loss: tensor(0.0691)\n",
      "4992 Training Loss: tensor(0.0677)\n",
      "4993 Training Loss: tensor(0.0667)\n",
      "4994 Training Loss: tensor(0.0713)\n",
      "4995 Training Loss: tensor(0.0640)\n",
      "4996 Training Loss: tensor(0.0690)\n",
      "4997 Training Loss: tensor(0.0697)\n",
      "4998 Training Loss: tensor(0.0737)\n",
      "4999 Training Loss: tensor(0.0692)\n",
      "5000 Training Loss: tensor(0.0688)\n",
      "5001 Training Loss: tensor(0.0686)\n",
      "5002 Training Loss: tensor(0.0693)\n",
      "5003 Training Loss: tensor(0.0665)\n",
      "5004 Training Loss: tensor(0.0655)\n",
      "5005 Training Loss: tensor(0.0698)\n",
      "5006 Training Loss: tensor(0.0718)\n",
      "5007 Training Loss: tensor(0.0686)\n",
      "5008 Training Loss: tensor(0.0647)\n",
      "5009 Training Loss: tensor(0.0692)\n",
      "5010 Training Loss: tensor(0.0723)\n",
      "5011 Training Loss: tensor(0.0694)\n",
      "5012 Training Loss: tensor(0.0709)\n",
      "5013 Training Loss: tensor(0.0641)\n",
      "5014 Training Loss: tensor(0.0720)\n",
      "5015 Training Loss: tensor(0.0669)\n",
      "5016 Training Loss: tensor(0.0713)\n",
      "5017 Training Loss: tensor(0.0713)\n",
      "5018 Training Loss: tensor(0.0628)\n",
      "5019 Training Loss: tensor(0.0678)\n",
      "5020 Training Loss: tensor(0.0743)\n",
      "5021 Training Loss: tensor(0.0656)\n",
      "5022 Training Loss: tensor(0.0611)\n",
      "5023 Training Loss: tensor(0.0608)\n",
      "5024 Training Loss: tensor(0.0714)\n",
      "5025 Training Loss: tensor(0.0674)\n",
      "5026 Training Loss: tensor(0.0601)\n",
      "5027 Training Loss: tensor(0.0657)\n",
      "5028 Training Loss: tensor(0.0673)\n",
      "5029 Training Loss: tensor(0.0626)\n",
      "5030 Training Loss: tensor(0.0625)\n",
      "5031 Training Loss: tensor(0.0731)\n",
      "5032 Training Loss: tensor(0.0702)\n",
      "5033 Training Loss: tensor(0.0680)\n",
      "5034 Training Loss: tensor(0.0674)\n",
      "5035 Training Loss: tensor(0.0682)\n",
      "5036 Training Loss: tensor(0.0634)\n",
      "5037 Training Loss: tensor(0.0701)\n",
      "5038 Training Loss: tensor(0.0656)\n",
      "5039 Training Loss: tensor(0.0559)\n",
      "5040 Training Loss: tensor(0.0630)\n",
      "5041 Training Loss: tensor(0.0651)\n",
      "5042 Training Loss: tensor(0.0733)\n",
      "5043 Training Loss: tensor(0.0674)\n",
      "5044 Training Loss: tensor(0.0721)\n",
      "5045 Training Loss: tensor(0.0696)\n",
      "5046 Training Loss: tensor(0.0676)\n",
      "5047 Training Loss: tensor(0.0665)\n",
      "5048 Training Loss: tensor(0.0651)\n",
      "5049 Training Loss: tensor(0.0687)\n",
      "5050 Training Loss: tensor(0.0704)\n",
      "5051 Training Loss: tensor(0.0638)\n",
      "5052 Training Loss: tensor(0.0699)\n",
      "5053 Training Loss: tensor(0.0620)\n",
      "5054 Training Loss: tensor(0.0682)\n",
      "5055 Training Loss: tensor(0.0648)\n",
      "5056 Training Loss: tensor(0.0718)\n",
      "5057 Training Loss: tensor(0.0641)\n",
      "5058 Training Loss: tensor(0.0602)\n",
      "5059 Training Loss: tensor(0.0622)\n",
      "5060 Training Loss: tensor(0.0594)\n",
      "5061 Training Loss: tensor(0.0665)\n",
      "5062 Training Loss: tensor(0.0701)\n",
      "5063 Training Loss: tensor(0.0597)\n",
      "5064 Training Loss: tensor(0.0656)\n",
      "5065 Training Loss: tensor(0.0677)\n",
      "5066 Training Loss: tensor(0.0686)\n",
      "5067 Training Loss: tensor(0.0641)\n",
      "5068 Training Loss: tensor(0.0771)\n",
      "5069 Training Loss: tensor(0.0675)\n",
      "5070 Training Loss: tensor(0.0656)\n",
      "5071 Training Loss: tensor(0.0644)\n",
      "5072 Training Loss: tensor(0.0674)\n",
      "5073 Training Loss: tensor(0.0666)\n",
      "5074 Training Loss: tensor(0.0678)\n",
      "5075 Training Loss: tensor(0.0620)\n",
      "5076 Training Loss: tensor(0.0595)\n",
      "5077 Training Loss: tensor(0.0656)\n",
      "5078 Training Loss: tensor(0.0640)\n",
      "5079 Training Loss: tensor(0.0616)\n",
      "5080 Training Loss: tensor(0.0704)\n",
      "5081 Training Loss: tensor(0.0615)\n",
      "5082 Training Loss: tensor(0.0695)\n",
      "5083 Training Loss: tensor(0.0654)\n",
      "5084 Training Loss: tensor(0.0682)\n",
      "5085 Training Loss: tensor(0.0670)\n",
      "5086 Training Loss: tensor(0.0661)\n",
      "5087 Training Loss: tensor(0.0784)\n",
      "5088 Training Loss: tensor(0.0681)\n",
      "5089 Training Loss: tensor(0.0631)\n",
      "5090 Training Loss: tensor(0.0685)\n",
      "5091 Training Loss: tensor(0.0620)\n",
      "5092 Training Loss: tensor(0.0689)\n",
      "5093 Training Loss: tensor(0.0685)\n",
      "5094 Training Loss: tensor(0.0662)\n",
      "5095 Training Loss: tensor(0.0631)\n",
      "5096 Training Loss: tensor(0.0651)\n",
      "5097 Training Loss: tensor(0.0662)\n",
      "5098 Training Loss: tensor(0.0633)\n",
      "5099 Training Loss: tensor(0.0692)\n",
      "5100 Training Loss: tensor(0.0708)\n",
      "5101 Training Loss: tensor(0.0671)\n",
      "5102 Training Loss: tensor(0.0601)\n",
      "5103 Training Loss: tensor(0.0713)\n",
      "5104 Training Loss: tensor(0.0675)\n",
      "5105 Training Loss: tensor(0.0668)\n",
      "5106 Training Loss: tensor(0.0618)\n",
      "5107 Training Loss: tensor(0.0660)\n",
      "5108 Training Loss: tensor(0.0596)\n",
      "5109 Training Loss: tensor(0.0716)\n",
      "5110 Training Loss: tensor(0.0673)\n",
      "5111 Training Loss: tensor(0.0692)\n",
      "5112 Training Loss: tensor(0.0677)\n",
      "5113 Training Loss: tensor(0.0639)\n",
      "5114 Training Loss: tensor(0.0740)\n",
      "5115 Training Loss: tensor(0.0642)\n",
      "5116 Training Loss: tensor(0.0695)\n",
      "5117 Training Loss: tensor(0.0732)\n",
      "5118 Training Loss: tensor(0.0658)\n",
      "5119 Training Loss: tensor(0.0684)\n",
      "5120 Training Loss: tensor(0.0674)\n",
      "5121 Training Loss: tensor(0.0672)\n",
      "5122 Training Loss: tensor(0.0656)\n",
      "5123 Training Loss: tensor(0.0664)\n",
      "5124 Training Loss: tensor(0.0714)\n",
      "5125 Training Loss: tensor(0.0677)\n",
      "5126 Training Loss: tensor(0.0716)\n",
      "5127 Training Loss: tensor(0.0626)\n",
      "5128 Training Loss: tensor(0.0707)\n",
      "5129 Training Loss: tensor(0.0679)\n",
      "5130 Training Loss: tensor(0.0676)\n",
      "5131 Training Loss: tensor(0.0642)\n",
      "5132 Training Loss: tensor(0.0666)\n",
      "5133 Training Loss: tensor(0.0703)\n",
      "5134 Training Loss: tensor(0.0683)\n",
      "5135 Training Loss: tensor(0.0670)\n",
      "5136 Training Loss: tensor(0.0661)\n",
      "5137 Training Loss: tensor(0.0610)\n",
      "5138 Training Loss: tensor(0.0607)\n",
      "5139 Training Loss: tensor(0.0648)\n",
      "5140 Training Loss: tensor(0.0693)\n",
      "5141 Training Loss: tensor(0.0625)\n",
      "5142 Training Loss: tensor(0.0675)\n",
      "5143 Training Loss: tensor(0.0658)\n",
      "5144 Training Loss: tensor(0.0625)\n",
      "5145 Training Loss: tensor(0.0614)\n",
      "5146 Training Loss: tensor(0.0688)\n",
      "5147 Training Loss: tensor(0.0696)\n",
      "5148 Training Loss: tensor(0.0710)\n",
      "5149 Training Loss: tensor(0.0587)\n",
      "5150 Training Loss: tensor(0.0617)\n",
      "5151 Training Loss: tensor(0.0673)\n",
      "5152 Training Loss: tensor(0.0607)\n",
      "5153 Training Loss: tensor(0.0669)\n",
      "5154 Training Loss: tensor(0.0587)\n",
      "5155 Training Loss: tensor(0.0707)\n",
      "5156 Training Loss: tensor(0.0645)\n",
      "5157 Training Loss: tensor(0.0619)\n",
      "5158 Training Loss: tensor(0.0631)\n",
      "5159 Training Loss: tensor(0.0651)\n",
      "5160 Training Loss: tensor(0.0689)\n",
      "5161 Training Loss: tensor(0.0646)\n",
      "5162 Training Loss: tensor(0.0612)\n",
      "5163 Training Loss: tensor(0.0620)\n",
      "5164 Training Loss: tensor(0.0664)\n",
      "5165 Training Loss: tensor(0.0651)\n",
      "5166 Training Loss: tensor(0.0587)\n",
      "5167 Training Loss: tensor(0.0617)\n",
      "5168 Training Loss: tensor(0.0623)\n",
      "5169 Training Loss: tensor(0.0682)\n",
      "5170 Training Loss: tensor(0.0598)\n",
      "5171 Training Loss: tensor(0.0635)\n",
      "5172 Training Loss: tensor(0.0684)\n",
      "5173 Training Loss: tensor(0.0656)\n",
      "5174 Training Loss: tensor(0.0578)\n",
      "5175 Training Loss: tensor(0.0618)\n",
      "5176 Training Loss: tensor(0.0627)\n",
      "5177 Training Loss: tensor(0.0615)\n",
      "5178 Training Loss: tensor(0.0624)\n",
      "5179 Training Loss: tensor(0.0680)\n",
      "5180 Training Loss: tensor(0.0645)\n",
      "5181 Training Loss: tensor(0.0699)\n",
      "5182 Training Loss: tensor(0.0691)\n",
      "5183 Training Loss: tensor(0.0631)\n",
      "5184 Training Loss: tensor(0.0700)\n",
      "5185 Training Loss: tensor(0.0595)\n",
      "5186 Training Loss: tensor(0.0609)\n",
      "5187 Training Loss: tensor(0.0646)\n",
      "5188 Training Loss: tensor(0.0651)\n",
      "5189 Training Loss: tensor(0.0678)\n",
      "5190 Training Loss: tensor(0.0630)\n",
      "5191 Training Loss: tensor(0.0649)\n",
      "5192 Training Loss: tensor(0.0636)\n",
      "5193 Training Loss: tensor(0.0681)\n",
      "5194 Training Loss: tensor(0.0651)\n",
      "5195 Training Loss: tensor(0.0622)\n",
      "5196 Training Loss: tensor(0.0661)\n",
      "5197 Training Loss: tensor(0.0622)\n",
      "5198 Training Loss: tensor(0.0675)\n",
      "5199 Training Loss: tensor(0.0604)\n",
      "5200 Training Loss: tensor(0.0675)\n",
      "5201 Training Loss: tensor(0.0643)\n",
      "5202 Training Loss: tensor(0.0588)\n",
      "5203 Training Loss: tensor(0.0636)\n",
      "5204 Training Loss: tensor(0.0673)\n",
      "5205 Training Loss: tensor(0.0680)\n",
      "5206 Training Loss: tensor(0.0673)\n",
      "5207 Training Loss: tensor(0.0690)\n",
      "5208 Training Loss: tensor(0.0677)\n",
      "5209 Training Loss: tensor(0.0644)\n",
      "5210 Training Loss: tensor(0.0627)\n",
      "5211 Training Loss: tensor(0.0655)\n",
      "5212 Training Loss: tensor(0.0645)\n",
      "5213 Training Loss: tensor(0.0651)\n",
      "5214 Training Loss: tensor(0.0617)\n",
      "5215 Training Loss: tensor(0.0665)\n",
      "5216 Training Loss: tensor(0.0612)\n",
      "5217 Training Loss: tensor(0.0632)\n",
      "5218 Training Loss: tensor(0.0591)\n",
      "5219 Training Loss: tensor(0.0569)\n",
      "5220 Training Loss: tensor(0.0568)\n",
      "5221 Training Loss: tensor(0.0609)\n",
      "5222 Training Loss: tensor(0.0615)\n",
      "5223 Training Loss: tensor(0.0628)\n",
      "5224 Training Loss: tensor(0.0648)\n",
      "5225 Training Loss: tensor(0.0594)\n",
      "5226 Training Loss: tensor(0.0595)\n",
      "5227 Training Loss: tensor(0.0666)\n",
      "5228 Training Loss: tensor(0.0636)\n",
      "5229 Training Loss: tensor(0.0640)\n",
      "5230 Training Loss: tensor(0.0665)\n",
      "5231 Training Loss: tensor(0.0726)\n",
      "5232 Training Loss: tensor(0.0621)\n",
      "5233 Training Loss: tensor(0.0567)\n",
      "5234 Training Loss: tensor(0.0647)\n",
      "5235 Training Loss: tensor(0.0613)\n",
      "5236 Training Loss: tensor(0.0637)\n",
      "5237 Training Loss: tensor(0.0651)\n",
      "5238 Training Loss: tensor(0.0680)\n",
      "5239 Training Loss: tensor(0.0638)\n",
      "5240 Training Loss: tensor(0.0646)\n",
      "5241 Training Loss: tensor(0.0658)\n",
      "5242 Training Loss: tensor(0.0606)\n",
      "5243 Training Loss: tensor(0.0626)\n",
      "5244 Training Loss: tensor(0.0630)\n",
      "5245 Training Loss: tensor(0.0595)\n",
      "5246 Training Loss: tensor(0.0586)\n",
      "5247 Training Loss: tensor(0.0628)\n",
      "5248 Training Loss: tensor(0.0590)\n",
      "5249 Training Loss: tensor(0.0648)\n",
      "5250 Training Loss: tensor(0.0595)\n",
      "5251 Training Loss: tensor(0.0666)\n",
      "5252 Training Loss: tensor(0.0573)\n",
      "5253 Training Loss: tensor(0.0636)\n",
      "5254 Training Loss: tensor(0.0689)\n",
      "5255 Training Loss: tensor(0.0577)\n",
      "5256 Training Loss: tensor(0.0643)\n",
      "5257 Training Loss: tensor(0.0621)\n",
      "5258 Training Loss: tensor(0.0634)\n",
      "5259 Training Loss: tensor(0.0602)\n",
      "5260 Training Loss: tensor(0.0620)\n",
      "5261 Training Loss: tensor(0.0624)\n",
      "5262 Training Loss: tensor(0.0690)\n",
      "5263 Training Loss: tensor(0.0639)\n",
      "5264 Training Loss: tensor(0.0533)\n",
      "5265 Training Loss: tensor(0.0606)\n",
      "5266 Training Loss: tensor(0.0584)\n",
      "5267 Training Loss: tensor(0.0643)\n",
      "5268 Training Loss: tensor(0.0618)\n",
      "5269 Training Loss: tensor(0.0632)\n",
      "5270 Training Loss: tensor(0.0683)\n",
      "5271 Training Loss: tensor(0.0630)\n",
      "5272 Training Loss: tensor(0.0619)\n",
      "5273 Training Loss: tensor(0.0656)\n",
      "5274 Training Loss: tensor(0.0728)\n",
      "5275 Training Loss: tensor(0.0641)\n",
      "5276 Training Loss: tensor(0.0578)\n",
      "5277 Training Loss: tensor(0.0546)\n",
      "5278 Training Loss: tensor(0.0574)\n",
      "5279 Training Loss: tensor(0.0577)\n",
      "5280 Training Loss: tensor(0.0631)\n",
      "5281 Training Loss: tensor(0.0587)\n",
      "5282 Training Loss: tensor(0.0601)\n",
      "5283 Training Loss: tensor(0.0589)\n",
      "5284 Training Loss: tensor(0.0624)\n",
      "5285 Training Loss: tensor(0.0637)\n",
      "5286 Training Loss: tensor(0.0620)\n",
      "5287 Training Loss: tensor(0.0619)\n",
      "5288 Training Loss: tensor(0.0633)\n",
      "5289 Training Loss: tensor(0.0595)\n",
      "5290 Training Loss: tensor(0.0649)\n",
      "5291 Training Loss: tensor(0.0646)\n",
      "5292 Training Loss: tensor(0.0648)\n",
      "5293 Training Loss: tensor(0.0719)\n",
      "5294 Training Loss: tensor(0.0641)\n",
      "5295 Training Loss: tensor(0.0594)\n",
      "5296 Training Loss: tensor(0.0648)\n",
      "5297 Training Loss: tensor(0.0603)\n",
      "5298 Training Loss: tensor(0.0637)\n",
      "5299 Training Loss: tensor(0.0646)\n",
      "5300 Training Loss: tensor(0.0626)\n",
      "5301 Training Loss: tensor(0.0627)\n",
      "5302 Training Loss: tensor(0.0651)\n",
      "5303 Training Loss: tensor(0.0655)\n",
      "5304 Training Loss: tensor(0.0597)\n",
      "5305 Training Loss: tensor(0.0639)\n",
      "5306 Training Loss: tensor(0.0683)\n",
      "5307 Training Loss: tensor(0.0608)\n",
      "5308 Training Loss: tensor(0.0608)\n",
      "5309 Training Loss: tensor(0.0671)\n",
      "5310 Training Loss: tensor(0.0543)\n",
      "5311 Training Loss: tensor(0.0657)\n",
      "5312 Training Loss: tensor(0.0652)\n",
      "5313 Training Loss: tensor(0.0627)\n",
      "5314 Training Loss: tensor(0.0655)\n",
      "5315 Training Loss: tensor(0.0715)\n",
      "5316 Training Loss: tensor(0.0619)\n",
      "5317 Training Loss: tensor(0.0649)\n",
      "5318 Training Loss: tensor(0.0647)\n",
      "5319 Training Loss: tensor(0.0625)\n",
      "5320 Training Loss: tensor(0.0624)\n",
      "5321 Training Loss: tensor(0.0637)\n",
      "5322 Training Loss: tensor(0.0682)\n",
      "5323 Training Loss: tensor(0.0701)\n",
      "5324 Training Loss: tensor(0.0582)\n",
      "5325 Training Loss: tensor(0.0545)\n",
      "5326 Training Loss: tensor(0.0720)\n",
      "5327 Training Loss: tensor(0.0627)\n",
      "5328 Training Loss: tensor(0.0609)\n",
      "5329 Training Loss: tensor(0.0548)\n",
      "5330 Training Loss: tensor(0.0590)\n",
      "5331 Training Loss: tensor(0.0620)\n",
      "5332 Training Loss: tensor(0.0573)\n",
      "5333 Training Loss: tensor(0.0591)\n",
      "5334 Training Loss: tensor(0.0603)\n",
      "5335 Training Loss: tensor(0.0542)\n",
      "5336 Training Loss: tensor(0.0623)\n",
      "5337 Training Loss: tensor(0.0591)\n",
      "5338 Training Loss: tensor(0.0657)\n",
      "5339 Training Loss: tensor(0.0611)\n",
      "5340 Training Loss: tensor(0.0600)\n",
      "5341 Training Loss: tensor(0.0567)\n",
      "5342 Training Loss: tensor(0.0623)\n",
      "5343 Training Loss: tensor(0.0696)\n",
      "5344 Training Loss: tensor(0.0633)\n",
      "5345 Training Loss: tensor(0.0676)\n",
      "5346 Training Loss: tensor(0.0639)\n",
      "5347 Training Loss: tensor(0.0611)\n",
      "5348 Training Loss: tensor(0.0643)\n",
      "5349 Training Loss: tensor(0.0608)\n",
      "5350 Training Loss: tensor(0.0608)\n",
      "5351 Training Loss: tensor(0.0626)\n",
      "5352 Training Loss: tensor(0.0644)\n",
      "5353 Training Loss: tensor(0.0557)\n",
      "5354 Training Loss: tensor(0.0631)\n",
      "5355 Training Loss: tensor(0.0589)\n",
      "5356 Training Loss: tensor(0.0602)\n",
      "5357 Training Loss: tensor(0.0572)\n",
      "5358 Training Loss: tensor(0.0607)\n",
      "5359 Training Loss: tensor(0.0611)\n",
      "5360 Training Loss: tensor(0.0612)\n",
      "5361 Training Loss: tensor(0.0581)\n",
      "5362 Training Loss: tensor(0.0592)\n",
      "5363 Training Loss: tensor(0.0570)\n",
      "5364 Training Loss: tensor(0.0650)\n",
      "5365 Training Loss: tensor(0.0629)\n",
      "5366 Training Loss: tensor(0.0590)\n",
      "5367 Training Loss: tensor(0.0642)\n",
      "5368 Training Loss: tensor(0.0640)\n",
      "5369 Training Loss: tensor(0.0604)\n",
      "5370 Training Loss: tensor(0.0600)\n",
      "5371 Training Loss: tensor(0.0650)\n",
      "5372 Training Loss: tensor(0.0643)\n",
      "5373 Training Loss: tensor(0.0581)\n",
      "5374 Training Loss: tensor(0.0615)\n",
      "5375 Training Loss: tensor(0.0602)\n",
      "5376 Training Loss: tensor(0.0626)\n",
      "5377 Training Loss: tensor(0.0582)\n",
      "5378 Training Loss: tensor(0.0603)\n",
      "5379 Training Loss: tensor(0.0702)\n",
      "5380 Training Loss: tensor(0.0591)\n",
      "5381 Training Loss: tensor(0.0587)\n",
      "5382 Training Loss: tensor(0.0630)\n",
      "5383 Training Loss: tensor(0.0562)\n",
      "5384 Training Loss: tensor(0.0581)\n",
      "5385 Training Loss: tensor(0.0584)\n",
      "5386 Training Loss: tensor(0.0557)\n",
      "5387 Training Loss: tensor(0.0554)\n",
      "5388 Training Loss: tensor(0.0638)\n",
      "5389 Training Loss: tensor(0.0605)\n",
      "5390 Training Loss: tensor(0.0665)\n",
      "5391 Training Loss: tensor(0.0671)\n",
      "5392 Training Loss: tensor(0.0567)\n",
      "5393 Training Loss: tensor(0.0615)\n",
      "5394 Training Loss: tensor(0.0627)\n",
      "5395 Training Loss: tensor(0.0585)\n",
      "5396 Training Loss: tensor(0.0647)\n",
      "5397 Training Loss: tensor(0.0558)\n",
      "5398 Training Loss: tensor(0.0605)\n",
      "5399 Training Loss: tensor(0.0623)\n",
      "5400 Training Loss: tensor(0.0574)\n",
      "5401 Training Loss: tensor(0.0599)\n",
      "5402 Training Loss: tensor(0.0638)\n",
      "5403 Training Loss: tensor(0.0619)\n",
      "5404 Training Loss: tensor(0.0595)\n",
      "5405 Training Loss: tensor(0.0572)\n",
      "5406 Training Loss: tensor(0.0601)\n",
      "5407 Training Loss: tensor(0.0589)\n",
      "5408 Training Loss: tensor(0.0711)\n",
      "5409 Training Loss: tensor(0.0591)\n",
      "5410 Training Loss: tensor(0.0595)\n",
      "5411 Training Loss: tensor(0.0618)\n",
      "5412 Training Loss: tensor(0.0597)\n",
      "5413 Training Loss: tensor(0.0590)\n",
      "5414 Training Loss: tensor(0.0600)\n",
      "5415 Training Loss: tensor(0.0604)\n",
      "5416 Training Loss: tensor(0.0598)\n",
      "5417 Training Loss: tensor(0.0630)\n",
      "5418 Training Loss: tensor(0.0620)\n",
      "5419 Training Loss: tensor(0.0601)\n",
      "5420 Training Loss: tensor(0.0574)\n",
      "5421 Training Loss: tensor(0.0615)\n",
      "5422 Training Loss: tensor(0.0614)\n",
      "5423 Training Loss: tensor(0.0600)\n",
      "5424 Training Loss: tensor(0.0621)\n",
      "5425 Training Loss: tensor(0.0579)\n",
      "5426 Training Loss: tensor(0.0709)\n",
      "5427 Training Loss: tensor(0.0620)\n",
      "5428 Training Loss: tensor(0.0625)\n",
      "5429 Training Loss: tensor(0.0637)\n",
      "5430 Training Loss: tensor(0.0600)\n",
      "5431 Training Loss: tensor(0.0610)\n",
      "5432 Training Loss: tensor(0.0572)\n",
      "5433 Training Loss: tensor(0.0593)\n",
      "5434 Training Loss: tensor(0.0555)\n",
      "5435 Training Loss: tensor(0.0636)\n",
      "5436 Training Loss: tensor(0.0652)\n",
      "5437 Training Loss: tensor(0.0548)\n",
      "5438 Training Loss: tensor(0.0582)\n",
      "5439 Training Loss: tensor(0.0586)\n",
      "5440 Training Loss: tensor(0.0614)\n",
      "5441 Training Loss: tensor(0.0607)\n",
      "5442 Training Loss: tensor(0.0644)\n",
      "5443 Training Loss: tensor(0.0631)\n",
      "5444 Training Loss: tensor(0.0574)\n",
      "5445 Training Loss: tensor(0.0621)\n",
      "5446 Training Loss: tensor(0.0626)\n",
      "5447 Training Loss: tensor(0.0621)\n",
      "5448 Training Loss: tensor(0.0656)\n",
      "5449 Training Loss: tensor(0.0550)\n",
      "5450 Training Loss: tensor(0.0560)\n",
      "5451 Training Loss: tensor(0.0643)\n",
      "5452 Training Loss: tensor(0.0676)\n",
      "5453 Training Loss: tensor(0.0614)\n",
      "5454 Training Loss: tensor(0.0620)\n",
      "5455 Training Loss: tensor(0.0637)\n",
      "5456 Training Loss: tensor(0.0599)\n",
      "5457 Training Loss: tensor(0.0603)\n",
      "5458 Training Loss: tensor(0.0656)\n",
      "5459 Training Loss: tensor(0.0563)\n",
      "5460 Training Loss: tensor(0.0594)\n",
      "5461 Training Loss: tensor(0.0614)\n",
      "5462 Training Loss: tensor(0.0645)\n",
      "5463 Training Loss: tensor(0.0584)\n",
      "5464 Training Loss: tensor(0.0630)\n",
      "5465 Training Loss: tensor(0.0611)\n",
      "5466 Training Loss: tensor(0.0544)\n",
      "5467 Training Loss: tensor(0.0560)\n",
      "5468 Training Loss: tensor(0.0592)\n",
      "5469 Training Loss: tensor(0.0704)\n",
      "5470 Training Loss: tensor(0.0581)\n",
      "5471 Training Loss: tensor(0.0649)\n",
      "5472 Training Loss: tensor(0.0550)\n",
      "5473 Training Loss: tensor(0.0587)\n",
      "5474 Training Loss: tensor(0.0580)\n",
      "5475 Training Loss: tensor(0.0649)\n",
      "5476 Training Loss: tensor(0.0621)\n",
      "5477 Training Loss: tensor(0.0613)\n",
      "5478 Training Loss: tensor(0.0587)\n",
      "5479 Training Loss: tensor(0.0580)\n",
      "5480 Training Loss: tensor(0.0656)\n",
      "5481 Training Loss: tensor(0.0620)\n",
      "5482 Training Loss: tensor(0.0639)\n",
      "5483 Training Loss: tensor(0.0617)\n",
      "5484 Training Loss: tensor(0.0701)\n",
      "5485 Training Loss: tensor(0.0615)\n",
      "5486 Training Loss: tensor(0.0574)\n",
      "5487 Training Loss: tensor(0.0628)\n",
      "5488 Training Loss: tensor(0.0644)\n",
      "5489 Training Loss: tensor(0.0550)\n",
      "5490 Training Loss: tensor(0.0608)\n",
      "5491 Training Loss: tensor(0.0557)\n",
      "5492 Training Loss: tensor(0.0542)\n",
      "5493 Training Loss: tensor(0.0626)\n",
      "5494 Training Loss: tensor(0.0600)\n",
      "5495 Training Loss: tensor(0.0567)\n",
      "5496 Training Loss: tensor(0.0561)\n",
      "5497 Training Loss: tensor(0.0631)\n",
      "5498 Training Loss: tensor(0.0601)\n",
      "5499 Training Loss: tensor(0.0546)\n",
      "5500 Training Loss: tensor(0.0604)\n",
      "5501 Training Loss: tensor(0.0635)\n",
      "5502 Training Loss: tensor(0.0623)\n",
      "5503 Training Loss: tensor(0.0572)\n",
      "5504 Training Loss: tensor(0.0527)\n",
      "5505 Training Loss: tensor(0.0629)\n",
      "5506 Training Loss: tensor(0.0653)\n",
      "5507 Training Loss: tensor(0.0665)\n",
      "5508 Training Loss: tensor(0.0525)\n",
      "5509 Training Loss: tensor(0.0640)\n",
      "5510 Training Loss: tensor(0.0559)\n",
      "5511 Training Loss: tensor(0.0524)\n",
      "5512 Training Loss: tensor(0.0633)\n",
      "5513 Training Loss: tensor(0.0577)\n",
      "5514 Training Loss: tensor(0.0664)\n",
      "5515 Training Loss: tensor(0.0550)\n",
      "5516 Training Loss: tensor(0.0603)\n",
      "5517 Training Loss: tensor(0.0551)\n",
      "5518 Training Loss: tensor(0.0565)\n",
      "5519 Training Loss: tensor(0.0606)\n",
      "5520 Training Loss: tensor(0.0590)\n",
      "5521 Training Loss: tensor(0.0540)\n",
      "5522 Training Loss: tensor(0.0608)\n",
      "5523 Training Loss: tensor(0.0607)\n",
      "5524 Training Loss: tensor(0.0568)\n",
      "5525 Training Loss: tensor(0.0551)\n",
      "5526 Training Loss: tensor(0.0608)\n",
      "5527 Training Loss: tensor(0.0568)\n",
      "5528 Training Loss: tensor(0.0595)\n",
      "5529 Training Loss: tensor(0.0682)\n",
      "5530 Training Loss: tensor(0.0633)\n",
      "5531 Training Loss: tensor(0.0626)\n",
      "5532 Training Loss: tensor(0.0613)\n",
      "5533 Training Loss: tensor(0.0585)\n",
      "5534 Training Loss: tensor(0.0614)\n",
      "5535 Training Loss: tensor(0.0609)\n",
      "5536 Training Loss: tensor(0.0571)\n",
      "5537 Training Loss: tensor(0.0612)\n",
      "5538 Training Loss: tensor(0.0588)\n",
      "5539 Training Loss: tensor(0.0615)\n",
      "5540 Training Loss: tensor(0.0570)\n",
      "5541 Training Loss: tensor(0.0595)\n",
      "5542 Training Loss: tensor(0.0627)\n",
      "5543 Training Loss: tensor(0.0580)\n",
      "5544 Training Loss: tensor(0.0541)\n",
      "5545 Training Loss: tensor(0.0590)\n",
      "5546 Training Loss: tensor(0.0572)\n",
      "5547 Training Loss: tensor(0.0599)\n",
      "5548 Training Loss: tensor(0.0538)\n",
      "5549 Training Loss: tensor(0.0608)\n",
      "5550 Training Loss: tensor(0.0590)\n",
      "5551 Training Loss: tensor(0.0559)\n",
      "5552 Training Loss: tensor(0.0611)\n",
      "5553 Training Loss: tensor(0.0536)\n",
      "5554 Training Loss: tensor(0.0633)\n",
      "5555 Training Loss: tensor(0.0582)\n",
      "5556 Training Loss: tensor(0.0599)\n",
      "5557 Training Loss: tensor(0.0545)\n",
      "5558 Training Loss: tensor(0.0621)\n",
      "5559 Training Loss: tensor(0.0546)\n",
      "5560 Training Loss: tensor(0.0565)\n",
      "5561 Training Loss: tensor(0.0528)\n",
      "5562 Training Loss: tensor(0.0595)\n",
      "5563 Training Loss: tensor(0.0604)\n",
      "5564 Training Loss: tensor(0.0616)\n",
      "5565 Training Loss: tensor(0.0559)\n",
      "5566 Training Loss: tensor(0.0627)\n",
      "5567 Training Loss: tensor(0.0557)\n",
      "5568 Training Loss: tensor(0.0616)\n",
      "5569 Training Loss: tensor(0.0585)\n",
      "5570 Training Loss: tensor(0.0525)\n",
      "5571 Training Loss: tensor(0.0514)\n",
      "5572 Training Loss: tensor(0.0580)\n",
      "5573 Training Loss: tensor(0.0581)\n",
      "5574 Training Loss: tensor(0.0569)\n",
      "5575 Training Loss: tensor(0.0587)\n",
      "5576 Training Loss: tensor(0.0623)\n",
      "5577 Training Loss: tensor(0.0605)\n",
      "5578 Training Loss: tensor(0.0614)\n",
      "5579 Training Loss: tensor(0.0598)\n",
      "5580 Training Loss: tensor(0.0601)\n",
      "5581 Training Loss: tensor(0.0528)\n",
      "5582 Training Loss: tensor(0.0623)\n",
      "5583 Training Loss: tensor(0.0609)\n",
      "5584 Training Loss: tensor(0.0520)\n",
      "5585 Training Loss: tensor(0.0555)\n",
      "5586 Training Loss: tensor(0.0595)\n",
      "5587 Training Loss: tensor(0.0546)\n",
      "5588 Training Loss: tensor(0.0619)\n",
      "5589 Training Loss: tensor(0.0552)\n",
      "5590 Training Loss: tensor(0.0636)\n",
      "5591 Training Loss: tensor(0.0564)\n",
      "5592 Training Loss: tensor(0.0559)\n",
      "5593 Training Loss: tensor(0.0626)\n",
      "5594 Training Loss: tensor(0.0575)\n",
      "5595 Training Loss: tensor(0.0589)\n",
      "5596 Training Loss: tensor(0.0596)\n",
      "5597 Training Loss: tensor(0.0623)\n",
      "5598 Training Loss: tensor(0.0580)\n",
      "5599 Training Loss: tensor(0.0617)\n",
      "5600 Training Loss: tensor(0.0564)\n",
      "5601 Training Loss: tensor(0.0607)\n",
      "5602 Training Loss: tensor(0.0560)\n",
      "5603 Training Loss: tensor(0.0562)\n",
      "5604 Training Loss: tensor(0.0555)\n",
      "5605 Training Loss: tensor(0.0595)\n",
      "5606 Training Loss: tensor(0.0546)\n",
      "5607 Training Loss: tensor(0.0609)\n",
      "5608 Training Loss: tensor(0.0555)\n",
      "5609 Training Loss: tensor(0.0580)\n",
      "5610 Training Loss: tensor(0.0547)\n",
      "5611 Training Loss: tensor(0.0539)\n",
      "5612 Training Loss: tensor(0.0567)\n",
      "5613 Training Loss: tensor(0.0559)\n",
      "5614 Training Loss: tensor(0.0582)\n",
      "5615 Training Loss: tensor(0.0566)\n",
      "5616 Training Loss: tensor(0.0630)\n",
      "5617 Training Loss: tensor(0.0514)\n",
      "5618 Training Loss: tensor(0.0639)\n",
      "5619 Training Loss: tensor(0.0573)\n",
      "5620 Training Loss: tensor(0.0534)\n",
      "5621 Training Loss: tensor(0.0629)\n",
      "5622 Training Loss: tensor(0.0605)\n",
      "5623 Training Loss: tensor(0.0558)\n",
      "5624 Training Loss: tensor(0.0581)\n",
      "5625 Training Loss: tensor(0.0631)\n",
      "5626 Training Loss: tensor(0.0596)\n",
      "5627 Training Loss: tensor(0.0574)\n",
      "5628 Training Loss: tensor(0.0566)\n",
      "5629 Training Loss: tensor(0.0561)\n",
      "5630 Training Loss: tensor(0.0548)\n",
      "5631 Training Loss: tensor(0.0563)\n",
      "5632 Training Loss: tensor(0.0517)\n",
      "5633 Training Loss: tensor(0.0569)\n",
      "5634 Training Loss: tensor(0.0624)\n",
      "5635 Training Loss: tensor(0.0525)\n",
      "5636 Training Loss: tensor(0.0588)\n",
      "5637 Training Loss: tensor(0.0603)\n",
      "5638 Training Loss: tensor(0.0564)\n",
      "5639 Training Loss: tensor(0.0582)\n",
      "5640 Training Loss: tensor(0.0570)\n",
      "5641 Training Loss: tensor(0.0542)\n",
      "5642 Training Loss: tensor(0.0470)\n",
      "5643 Training Loss: tensor(0.0587)\n",
      "5644 Training Loss: tensor(0.0601)\n",
      "5645 Training Loss: tensor(0.0566)\n",
      "5646 Training Loss: tensor(0.0561)\n",
      "5647 Training Loss: tensor(0.0590)\n",
      "5648 Training Loss: tensor(0.0653)\n",
      "5649 Training Loss: tensor(0.0548)\n",
      "5650 Training Loss: tensor(0.0580)\n",
      "5651 Training Loss: tensor(0.0573)\n",
      "5652 Training Loss: tensor(0.0563)\n",
      "5653 Training Loss: tensor(0.0605)\n",
      "5654 Training Loss: tensor(0.0591)\n",
      "5655 Training Loss: tensor(0.0553)\n",
      "5656 Training Loss: tensor(0.0582)\n",
      "5657 Training Loss: tensor(0.0473)\n",
      "5658 Training Loss: tensor(0.0594)\n",
      "5659 Training Loss: tensor(0.0579)\n",
      "5660 Training Loss: tensor(0.0560)\n",
      "5661 Training Loss: tensor(0.0603)\n",
      "5662 Training Loss: tensor(0.0589)\n",
      "5663 Training Loss: tensor(0.0584)\n",
      "5664 Training Loss: tensor(0.0581)\n",
      "5665 Training Loss: tensor(0.0529)\n",
      "5666 Training Loss: tensor(0.0599)\n",
      "5667 Training Loss: tensor(0.0487)\n",
      "5668 Training Loss: tensor(0.0626)\n",
      "5669 Training Loss: tensor(0.0570)\n",
      "5670 Training Loss: tensor(0.0592)\n",
      "5671 Training Loss: tensor(0.0578)\n",
      "5672 Training Loss: tensor(0.0526)\n",
      "5673 Training Loss: tensor(0.0601)\n",
      "5674 Training Loss: tensor(0.0564)\n",
      "5675 Training Loss: tensor(0.0484)\n",
      "5676 Training Loss: tensor(0.0573)\n",
      "5677 Training Loss: tensor(0.0566)\n",
      "5678 Training Loss: tensor(0.0599)\n",
      "5679 Training Loss: tensor(0.0582)\n",
      "5680 Training Loss: tensor(0.0545)\n",
      "5681 Training Loss: tensor(0.0644)\n",
      "5682 Training Loss: tensor(0.0615)\n",
      "5683 Training Loss: tensor(0.0648)\n",
      "5684 Training Loss: tensor(0.0606)\n",
      "5685 Training Loss: tensor(0.0565)\n",
      "5686 Training Loss: tensor(0.0557)\n",
      "5687 Training Loss: tensor(0.0507)\n",
      "5688 Training Loss: tensor(0.0603)\n",
      "5689 Training Loss: tensor(0.0581)\n",
      "5690 Training Loss: tensor(0.0538)\n",
      "5691 Training Loss: tensor(0.0634)\n",
      "5692 Training Loss: tensor(0.0635)\n",
      "5693 Training Loss: tensor(0.0612)\n",
      "5694 Training Loss: tensor(0.0556)\n",
      "5695 Training Loss: tensor(0.0518)\n",
      "5696 Training Loss: tensor(0.0547)\n",
      "5697 Training Loss: tensor(0.0539)\n",
      "5698 Training Loss: tensor(0.0525)\n",
      "5699 Training Loss: tensor(0.0496)\n",
      "5700 Training Loss: tensor(0.0513)\n",
      "5701 Training Loss: tensor(0.0634)\n",
      "5702 Training Loss: tensor(0.0633)\n",
      "5703 Training Loss: tensor(0.0556)\n",
      "5704 Training Loss: tensor(0.0598)\n",
      "5705 Training Loss: tensor(0.0542)\n",
      "5706 Training Loss: tensor(0.0590)\n",
      "5707 Training Loss: tensor(0.0582)\n",
      "5708 Training Loss: tensor(0.0561)\n",
      "5709 Training Loss: tensor(0.0594)\n",
      "5710 Training Loss: tensor(0.0544)\n",
      "5711 Training Loss: tensor(0.0533)\n",
      "5712 Training Loss: tensor(0.0568)\n",
      "5713 Training Loss: tensor(0.0551)\n",
      "5714 Training Loss: tensor(0.0568)\n",
      "5715 Training Loss: tensor(0.0557)\n",
      "5716 Training Loss: tensor(0.0561)\n",
      "5717 Training Loss: tensor(0.0506)\n",
      "5718 Training Loss: tensor(0.0543)\n",
      "5719 Training Loss: tensor(0.0578)\n",
      "5720 Training Loss: tensor(0.0503)\n",
      "5721 Training Loss: tensor(0.0608)\n",
      "5722 Training Loss: tensor(0.0480)\n",
      "5723 Training Loss: tensor(0.0575)\n",
      "5724 Training Loss: tensor(0.0607)\n",
      "5725 Training Loss: tensor(0.0600)\n",
      "5726 Training Loss: tensor(0.0566)\n",
      "5727 Training Loss: tensor(0.0545)\n",
      "5728 Training Loss: tensor(0.0611)\n",
      "5729 Training Loss: tensor(0.0635)\n",
      "5730 Training Loss: tensor(0.0522)\n",
      "5731 Training Loss: tensor(0.0572)\n",
      "5732 Training Loss: tensor(0.0569)\n",
      "5733 Training Loss: tensor(0.0501)\n",
      "5734 Training Loss: tensor(0.0696)\n",
      "5735 Training Loss: tensor(0.0577)\n",
      "5736 Training Loss: tensor(0.0557)\n",
      "5737 Training Loss: tensor(0.0554)\n",
      "5738 Training Loss: tensor(0.0587)\n",
      "5739 Training Loss: tensor(0.0569)\n",
      "5740 Training Loss: tensor(0.0658)\n",
      "5741 Training Loss: tensor(0.0488)\n",
      "5742 Training Loss: tensor(0.0508)\n",
      "5743 Training Loss: tensor(0.0560)\n",
      "5744 Training Loss: tensor(0.0539)\n",
      "5745 Training Loss: tensor(0.0569)\n",
      "5746 Training Loss: tensor(0.0570)\n",
      "5747 Training Loss: tensor(0.0575)\n",
      "5748 Training Loss: tensor(0.0576)\n",
      "5749 Training Loss: tensor(0.0552)\n",
      "5750 Training Loss: tensor(0.0584)\n",
      "5751 Training Loss: tensor(0.0533)\n",
      "5752 Training Loss: tensor(0.0591)\n",
      "5753 Training Loss: tensor(0.0577)\n",
      "5754 Training Loss: tensor(0.0585)\n",
      "5755 Training Loss: tensor(0.0492)\n",
      "5756 Training Loss: tensor(0.0602)\n",
      "5757 Training Loss: tensor(0.0599)\n",
      "5758 Training Loss: tensor(0.0544)\n",
      "5759 Training Loss: tensor(0.0582)\n",
      "5760 Training Loss: tensor(0.0605)\n",
      "5761 Training Loss: tensor(0.0584)\n",
      "5762 Training Loss: tensor(0.0547)\n",
      "5763 Training Loss: tensor(0.0592)\n",
      "5764 Training Loss: tensor(0.0559)\n",
      "5765 Training Loss: tensor(0.0614)\n",
      "5766 Training Loss: tensor(0.0611)\n",
      "5767 Training Loss: tensor(0.0550)\n",
      "5768 Training Loss: tensor(0.0628)\n",
      "5769 Training Loss: tensor(0.0573)\n",
      "5770 Training Loss: tensor(0.0527)\n",
      "5771 Training Loss: tensor(0.0527)\n",
      "5772 Training Loss: tensor(0.0567)\n",
      "5773 Training Loss: tensor(0.0590)\n",
      "5774 Training Loss: tensor(0.0550)\n",
      "5775 Training Loss: tensor(0.0544)\n",
      "5776 Training Loss: tensor(0.0598)\n",
      "5777 Training Loss: tensor(0.0531)\n",
      "5778 Training Loss: tensor(0.0529)\n",
      "5779 Training Loss: tensor(0.0577)\n",
      "5780 Training Loss: tensor(0.0541)\n",
      "5781 Training Loss: tensor(0.0601)\n",
      "5782 Training Loss: tensor(0.0614)\n",
      "5783 Training Loss: tensor(0.0614)\n",
      "5784 Training Loss: tensor(0.0556)\n",
      "5785 Training Loss: tensor(0.0618)\n",
      "5786 Training Loss: tensor(0.0511)\n",
      "5787 Training Loss: tensor(0.0510)\n",
      "5788 Training Loss: tensor(0.0615)\n",
      "5789 Training Loss: tensor(0.0523)\n",
      "5790 Training Loss: tensor(0.0566)\n",
      "5791 Training Loss: tensor(0.0556)\n",
      "5792 Training Loss: tensor(0.0571)\n",
      "5793 Training Loss: tensor(0.0497)\n",
      "5794 Training Loss: tensor(0.0499)\n",
      "5795 Training Loss: tensor(0.0591)\n",
      "5796 Training Loss: tensor(0.0544)\n",
      "5797 Training Loss: tensor(0.0567)\n",
      "5798 Training Loss: tensor(0.0492)\n",
      "5799 Training Loss: tensor(0.0541)\n",
      "5800 Training Loss: tensor(0.0544)\n",
      "5801 Training Loss: tensor(0.0541)\n",
      "5802 Training Loss: tensor(0.0579)\n",
      "5803 Training Loss: tensor(0.0510)\n",
      "5804 Training Loss: tensor(0.0539)\n",
      "5805 Training Loss: tensor(0.0545)\n",
      "5806 Training Loss: tensor(0.0473)\n",
      "5807 Training Loss: tensor(0.0535)\n",
      "5808 Training Loss: tensor(0.0527)\n",
      "5809 Training Loss: tensor(0.0525)\n",
      "5810 Training Loss: tensor(0.0518)\n",
      "5811 Training Loss: tensor(0.0533)\n",
      "5812 Training Loss: tensor(0.0560)\n",
      "5813 Training Loss: tensor(0.0478)\n",
      "5814 Training Loss: tensor(0.0560)\n",
      "5815 Training Loss: tensor(0.0484)\n",
      "5816 Training Loss: tensor(0.0553)\n",
      "5817 Training Loss: tensor(0.0534)\n",
      "5818 Training Loss: tensor(0.0565)\n",
      "5819 Training Loss: tensor(0.0575)\n",
      "5820 Training Loss: tensor(0.0616)\n",
      "5821 Training Loss: tensor(0.0642)\n",
      "5822 Training Loss: tensor(0.0568)\n",
      "5823 Training Loss: tensor(0.0599)\n",
      "5824 Training Loss: tensor(0.0592)\n",
      "5825 Training Loss: tensor(0.0543)\n",
      "5826 Training Loss: tensor(0.0576)\n",
      "5827 Training Loss: tensor(0.0535)\n",
      "5828 Training Loss: tensor(0.0543)\n",
      "5829 Training Loss: tensor(0.0492)\n",
      "5830 Training Loss: tensor(0.0573)\n",
      "5831 Training Loss: tensor(0.0564)\n",
      "5832 Training Loss: tensor(0.0511)\n",
      "5833 Training Loss: tensor(0.0651)\n",
      "5834 Training Loss: tensor(0.0595)\n",
      "5835 Training Loss: tensor(0.0537)\n",
      "5836 Training Loss: tensor(0.0506)\n",
      "5837 Training Loss: tensor(0.0549)\n",
      "5838 Training Loss: tensor(0.0537)\n",
      "5839 Training Loss: tensor(0.0629)\n",
      "5840 Training Loss: tensor(0.0525)\n",
      "5841 Training Loss: tensor(0.0512)\n",
      "5842 Training Loss: tensor(0.0544)\n",
      "5843 Training Loss: tensor(0.0535)\n",
      "5844 Training Loss: tensor(0.0548)\n",
      "5845 Training Loss: tensor(0.0594)\n",
      "5846 Training Loss: tensor(0.0472)\n",
      "5847 Training Loss: tensor(0.0570)\n",
      "5848 Training Loss: tensor(0.0612)\n",
      "5849 Training Loss: tensor(0.0490)\n",
      "5850 Training Loss: tensor(0.0595)\n",
      "5851 Training Loss: tensor(0.0528)\n",
      "5852 Training Loss: tensor(0.0537)\n",
      "5853 Training Loss: tensor(0.0533)\n",
      "5854 Training Loss: tensor(0.0536)\n",
      "5855 Training Loss: tensor(0.0565)\n",
      "5856 Training Loss: tensor(0.0574)\n",
      "5857 Training Loss: tensor(0.0512)\n",
      "5858 Training Loss: tensor(0.0580)\n",
      "5859 Training Loss: tensor(0.0550)\n",
      "5860 Training Loss: tensor(0.0536)\n",
      "5861 Training Loss: tensor(0.0639)\n",
      "5862 Training Loss: tensor(0.0550)\n",
      "5863 Training Loss: tensor(0.0567)\n",
      "5864 Training Loss: tensor(0.0571)\n",
      "5865 Training Loss: tensor(0.0559)\n",
      "5866 Training Loss: tensor(0.0522)\n",
      "5867 Training Loss: tensor(0.0562)\n",
      "5868 Training Loss: tensor(0.0443)\n",
      "5869 Training Loss: tensor(0.0549)\n",
      "5870 Training Loss: tensor(0.0557)\n",
      "5871 Training Loss: tensor(0.0551)\n",
      "5872 Training Loss: tensor(0.0510)\n",
      "5873 Training Loss: tensor(0.0530)\n",
      "5874 Training Loss: tensor(0.0529)\n",
      "5875 Training Loss: tensor(0.0586)\n",
      "5876 Training Loss: tensor(0.0552)\n",
      "5877 Training Loss: tensor(0.0537)\n",
      "5878 Training Loss: tensor(0.0546)\n",
      "5879 Training Loss: tensor(0.0618)\n",
      "5880 Training Loss: tensor(0.0655)\n",
      "5881 Training Loss: tensor(0.0593)\n",
      "5882 Training Loss: tensor(0.0569)\n",
      "5883 Training Loss: tensor(0.0540)\n",
      "5884 Training Loss: tensor(0.0551)\n",
      "5885 Training Loss: tensor(0.0527)\n",
      "5886 Training Loss: tensor(0.0517)\n",
      "5887 Training Loss: tensor(0.0568)\n",
      "5888 Training Loss: tensor(0.0479)\n",
      "5889 Training Loss: tensor(0.0508)\n",
      "5890 Training Loss: tensor(0.0551)\n",
      "5891 Training Loss: tensor(0.0568)\n",
      "5892 Training Loss: tensor(0.0527)\n",
      "5893 Training Loss: tensor(0.0523)\n",
      "5894 Training Loss: tensor(0.0562)\n",
      "5895 Training Loss: tensor(0.0482)\n",
      "5896 Training Loss: tensor(0.0551)\n",
      "5897 Training Loss: tensor(0.0602)\n",
      "5898 Training Loss: tensor(0.0563)\n",
      "5899 Training Loss: tensor(0.0562)\n",
      "5900 Training Loss: tensor(0.0602)\n",
      "5901 Training Loss: tensor(0.0518)\n",
      "5902 Training Loss: tensor(0.0556)\n",
      "5903 Training Loss: tensor(0.0549)\n",
      "5904 Training Loss: tensor(0.0537)\n",
      "5905 Training Loss: tensor(0.0550)\n",
      "5906 Training Loss: tensor(0.0561)\n",
      "5907 Training Loss: tensor(0.0584)\n",
      "5908 Training Loss: tensor(0.0533)\n",
      "5909 Training Loss: tensor(0.0510)\n",
      "5910 Training Loss: tensor(0.0499)\n",
      "5911 Training Loss: tensor(0.0553)\n",
      "5912 Training Loss: tensor(0.0563)\n",
      "5913 Training Loss: tensor(0.0528)\n",
      "5914 Training Loss: tensor(0.0508)\n",
      "5915 Training Loss: tensor(0.0613)\n",
      "5916 Training Loss: tensor(0.0529)\n",
      "5917 Training Loss: tensor(0.0546)\n",
      "5918 Training Loss: tensor(0.0563)\n",
      "5919 Training Loss: tensor(0.0557)\n",
      "5920 Training Loss: tensor(0.0534)\n",
      "5921 Training Loss: tensor(0.0550)\n",
      "5922 Training Loss: tensor(0.0521)\n",
      "5923 Training Loss: tensor(0.0459)\n",
      "5924 Training Loss: tensor(0.0533)\n",
      "5925 Training Loss: tensor(0.0497)\n",
      "5926 Training Loss: tensor(0.0562)\n",
      "5927 Training Loss: tensor(0.0527)\n",
      "5928 Training Loss: tensor(0.0520)\n",
      "5929 Training Loss: tensor(0.0541)\n",
      "5930 Training Loss: tensor(0.0517)\n",
      "5931 Training Loss: tensor(0.0535)\n",
      "5932 Training Loss: tensor(0.0533)\n",
      "5933 Training Loss: tensor(0.0534)\n",
      "5934 Training Loss: tensor(0.0522)\n",
      "5935 Training Loss: tensor(0.0516)\n",
      "5936 Training Loss: tensor(0.0507)\n",
      "5937 Training Loss: tensor(0.0560)\n",
      "5938 Training Loss: tensor(0.0581)\n",
      "5939 Training Loss: tensor(0.0540)\n",
      "5940 Training Loss: tensor(0.0566)\n",
      "5941 Training Loss: tensor(0.0478)\n",
      "5942 Training Loss: tensor(0.0517)\n",
      "5943 Training Loss: tensor(0.0522)\n",
      "5944 Training Loss: tensor(0.0569)\n",
      "5945 Training Loss: tensor(0.0530)\n",
      "5946 Training Loss: tensor(0.0558)\n",
      "5947 Training Loss: tensor(0.0569)\n",
      "5948 Training Loss: tensor(0.0559)\n",
      "5949 Training Loss: tensor(0.0587)\n",
      "5950 Training Loss: tensor(0.0523)\n",
      "5951 Training Loss: tensor(0.0626)\n",
      "5952 Training Loss: tensor(0.0510)\n",
      "5953 Training Loss: tensor(0.0583)\n",
      "5954 Training Loss: tensor(0.0564)\n",
      "5955 Training Loss: tensor(0.0547)\n",
      "5956 Training Loss: tensor(0.0560)\n",
      "5957 Training Loss: tensor(0.0618)\n",
      "5958 Training Loss: tensor(0.0588)\n",
      "5959 Training Loss: tensor(0.0529)\n",
      "5960 Training Loss: tensor(0.0544)\n",
      "5961 Training Loss: tensor(0.0483)\n",
      "5962 Training Loss: tensor(0.0508)\n",
      "5963 Training Loss: tensor(0.0529)\n",
      "5964 Training Loss: tensor(0.0537)\n",
      "5965 Training Loss: tensor(0.0601)\n",
      "5966 Training Loss: tensor(0.0503)\n",
      "5967 Training Loss: tensor(0.0457)\n",
      "5968 Training Loss: tensor(0.0606)\n",
      "5969 Training Loss: tensor(0.0496)\n",
      "5970 Training Loss: tensor(0.0552)\n",
      "5971 Training Loss: tensor(0.0514)\n",
      "5972 Training Loss: tensor(0.0552)\n",
      "5973 Training Loss: tensor(0.0497)\n",
      "5974 Training Loss: tensor(0.0581)\n",
      "5975 Training Loss: tensor(0.0525)\n",
      "5976 Training Loss: tensor(0.0527)\n",
      "5977 Training Loss: tensor(0.0553)\n",
      "5978 Training Loss: tensor(0.0535)\n",
      "5979 Training Loss: tensor(0.0506)\n",
      "5980 Training Loss: tensor(0.0559)\n",
      "5981 Training Loss: tensor(0.0564)\n",
      "5982 Training Loss: tensor(0.0552)\n",
      "5983 Training Loss: tensor(0.0503)\n",
      "5984 Training Loss: tensor(0.0600)\n",
      "5985 Training Loss: tensor(0.0538)\n",
      "5986 Training Loss: tensor(0.0588)\n",
      "5987 Training Loss: tensor(0.0588)\n",
      "5988 Training Loss: tensor(0.0523)\n",
      "5989 Training Loss: tensor(0.0565)\n",
      "5990 Training Loss: tensor(0.0543)\n",
      "5991 Training Loss: tensor(0.0561)\n",
      "5992 Training Loss: tensor(0.0516)\n",
      "5993 Training Loss: tensor(0.0511)\n",
      "5994 Training Loss: tensor(0.0516)\n",
      "5995 Training Loss: tensor(0.0534)\n",
      "5996 Training Loss: tensor(0.0587)\n",
      "5997 Training Loss: tensor(0.0586)\n",
      "5998 Training Loss: tensor(0.0560)\n",
      "5999 Training Loss: tensor(0.0508)\n",
      "6000 Training Loss: tensor(0.0573)\n",
      "6001 Training Loss: tensor(0.0500)\n",
      "6002 Training Loss: tensor(0.0485)\n",
      "6003 Training Loss: tensor(0.0596)\n",
      "6004 Training Loss: tensor(0.0512)\n",
      "6005 Training Loss: tensor(0.0540)\n",
      "6006 Training Loss: tensor(0.0485)\n",
      "6007 Training Loss: tensor(0.0502)\n",
      "6008 Training Loss: tensor(0.0573)\n",
      "6009 Training Loss: tensor(0.0514)\n",
      "6010 Training Loss: tensor(0.0601)\n",
      "6011 Training Loss: tensor(0.0517)\n",
      "6012 Training Loss: tensor(0.0492)\n",
      "6013 Training Loss: tensor(0.0546)\n",
      "6014 Training Loss: tensor(0.0576)\n",
      "6015 Training Loss: tensor(0.0490)\n",
      "6016 Training Loss: tensor(0.0532)\n",
      "6017 Training Loss: tensor(0.0548)\n",
      "6018 Training Loss: tensor(0.0620)\n",
      "6019 Training Loss: tensor(0.0606)\n",
      "6020 Training Loss: tensor(0.0551)\n",
      "6021 Training Loss: tensor(0.0571)\n",
      "6022 Training Loss: tensor(0.0578)\n",
      "6023 Training Loss: tensor(0.0521)\n",
      "6024 Training Loss: tensor(0.0530)\n",
      "6025 Training Loss: tensor(0.0561)\n",
      "6026 Training Loss: tensor(0.0636)\n",
      "6027 Training Loss: tensor(0.0616)\n",
      "6028 Training Loss: tensor(0.0604)\n",
      "6029 Training Loss: tensor(0.0534)\n",
      "6030 Training Loss: tensor(0.0563)\n",
      "6031 Training Loss: tensor(0.0523)\n",
      "6032 Training Loss: tensor(0.0512)\n",
      "6033 Training Loss: tensor(0.0561)\n",
      "6034 Training Loss: tensor(0.0477)\n",
      "6035 Training Loss: tensor(0.0518)\n",
      "6036 Training Loss: tensor(0.0539)\n",
      "6037 Training Loss: tensor(0.0508)\n",
      "6038 Training Loss: tensor(0.0512)\n",
      "6039 Training Loss: tensor(0.0512)\n",
      "6040 Training Loss: tensor(0.0577)\n",
      "6041 Training Loss: tensor(0.0536)\n",
      "6042 Training Loss: tensor(0.0558)\n",
      "6043 Training Loss: tensor(0.0510)\n",
      "6044 Training Loss: tensor(0.0514)\n",
      "6045 Training Loss: tensor(0.0597)\n",
      "6046 Training Loss: tensor(0.0485)\n",
      "6047 Training Loss: tensor(0.0576)\n",
      "6048 Training Loss: tensor(0.0517)\n",
      "6049 Training Loss: tensor(0.0507)\n",
      "6050 Training Loss: tensor(0.0649)\n",
      "6051 Training Loss: tensor(0.0520)\n",
      "6052 Training Loss: tensor(0.0579)\n",
      "6053 Training Loss: tensor(0.0597)\n",
      "6054 Training Loss: tensor(0.0493)\n",
      "6055 Training Loss: tensor(0.0498)\n",
      "6056 Training Loss: tensor(0.0623)\n",
      "6057 Training Loss: tensor(0.0499)\n",
      "6058 Training Loss: tensor(0.0536)\n",
      "6059 Training Loss: tensor(0.0575)\n",
      "6060 Training Loss: tensor(0.0572)\n",
      "6061 Training Loss: tensor(0.0480)\n",
      "6062 Training Loss: tensor(0.0521)\n",
      "6063 Training Loss: tensor(0.0603)\n",
      "6064 Training Loss: tensor(0.0516)\n",
      "6065 Training Loss: tensor(0.0491)\n",
      "6066 Training Loss: tensor(0.0550)\n",
      "6067 Training Loss: tensor(0.0572)\n",
      "6068 Training Loss: tensor(0.0566)\n",
      "6069 Training Loss: tensor(0.0596)\n",
      "6070 Training Loss: tensor(0.0502)\n",
      "6071 Training Loss: tensor(0.0544)\n",
      "6072 Training Loss: tensor(0.0518)\n",
      "6073 Training Loss: tensor(0.0506)\n",
      "6074 Training Loss: tensor(0.0539)\n",
      "6075 Training Loss: tensor(0.0587)\n",
      "6076 Training Loss: tensor(0.0503)\n",
      "6077 Training Loss: tensor(0.0517)\n",
      "6078 Training Loss: tensor(0.0547)\n",
      "6079 Training Loss: tensor(0.0537)\n",
      "6080 Training Loss: tensor(0.0502)\n",
      "6081 Training Loss: tensor(0.0590)\n",
      "6082 Training Loss: tensor(0.0501)\n",
      "6083 Training Loss: tensor(0.0570)\n",
      "6084 Training Loss: tensor(0.0599)\n",
      "6085 Training Loss: tensor(0.0531)\n",
      "6086 Training Loss: tensor(0.0505)\n",
      "6087 Training Loss: tensor(0.0494)\n",
      "6088 Training Loss: tensor(0.0560)\n",
      "6089 Training Loss: tensor(0.0511)\n",
      "6090 Training Loss: tensor(0.0480)\n",
      "6091 Training Loss: tensor(0.0506)\n",
      "6092 Training Loss: tensor(0.0506)\n",
      "6093 Training Loss: tensor(0.0511)\n",
      "6094 Training Loss: tensor(0.0510)\n",
      "6095 Training Loss: tensor(0.0517)\n",
      "6096 Training Loss: tensor(0.0552)\n",
      "6097 Training Loss: tensor(0.0561)\n",
      "6098 Training Loss: tensor(0.0578)\n",
      "6099 Training Loss: tensor(0.0559)\n",
      "6100 Training Loss: tensor(0.0548)\n",
      "6101 Training Loss: tensor(0.0526)\n",
      "6102 Training Loss: tensor(0.0553)\n",
      "6103 Training Loss: tensor(0.0527)\n",
      "6104 Training Loss: tensor(0.0578)\n",
      "6105 Training Loss: tensor(0.0539)\n",
      "6106 Training Loss: tensor(0.0500)\n",
      "6107 Training Loss: tensor(0.0493)\n",
      "6108 Training Loss: tensor(0.0581)\n",
      "6109 Training Loss: tensor(0.0533)\n",
      "6110 Training Loss: tensor(0.0447)\n",
      "6111 Training Loss: tensor(0.0532)\n",
      "6112 Training Loss: tensor(0.0491)\n",
      "6113 Training Loss: tensor(0.0488)\n",
      "6114 Training Loss: tensor(0.0580)\n",
      "6115 Training Loss: tensor(0.0545)\n",
      "6116 Training Loss: tensor(0.0494)\n",
      "6117 Training Loss: tensor(0.0530)\n",
      "6118 Training Loss: tensor(0.0490)\n",
      "6119 Training Loss: tensor(0.0508)\n",
      "6120 Training Loss: tensor(0.0553)\n",
      "6121 Training Loss: tensor(0.0463)\n",
      "6122 Training Loss: tensor(0.0491)\n",
      "6123 Training Loss: tensor(0.0532)\n",
      "6124 Training Loss: tensor(0.0554)\n",
      "6125 Training Loss: tensor(0.0531)\n",
      "6126 Training Loss: tensor(0.0523)\n",
      "6127 Training Loss: tensor(0.0553)\n",
      "6128 Training Loss: tensor(0.0541)\n",
      "6129 Training Loss: tensor(0.0524)\n",
      "6130 Training Loss: tensor(0.0535)\n",
      "6131 Training Loss: tensor(0.0544)\n",
      "6132 Training Loss: tensor(0.0581)\n",
      "6133 Training Loss: tensor(0.0508)\n",
      "6134 Training Loss: tensor(0.0502)\n",
      "6135 Training Loss: tensor(0.0518)\n",
      "6136 Training Loss: tensor(0.0515)\n",
      "6137 Training Loss: tensor(0.0485)\n",
      "6138 Training Loss: tensor(0.0550)\n",
      "6139 Training Loss: tensor(0.0500)\n",
      "6140 Training Loss: tensor(0.0531)\n",
      "6141 Training Loss: tensor(0.0588)\n",
      "6142 Training Loss: tensor(0.0594)\n",
      "6143 Training Loss: tensor(0.0507)\n",
      "6144 Training Loss: tensor(0.0545)\n",
      "6145 Training Loss: tensor(0.0532)\n",
      "6146 Training Loss: tensor(0.0492)\n",
      "6147 Training Loss: tensor(0.0494)\n",
      "6148 Training Loss: tensor(0.0522)\n",
      "6149 Training Loss: tensor(0.0565)\n",
      "6150 Training Loss: tensor(0.0512)\n",
      "6151 Training Loss: tensor(0.0522)\n",
      "6152 Training Loss: tensor(0.0555)\n",
      "6153 Training Loss: tensor(0.0500)\n",
      "6154 Training Loss: tensor(0.0496)\n",
      "6155 Training Loss: tensor(0.0531)\n",
      "6156 Training Loss: tensor(0.0539)\n",
      "6157 Training Loss: tensor(0.0575)\n",
      "6158 Training Loss: tensor(0.0542)\n",
      "6159 Training Loss: tensor(0.0595)\n",
      "6160 Training Loss: tensor(0.0507)\n",
      "6161 Training Loss: tensor(0.0555)\n",
      "6162 Training Loss: tensor(0.0507)\n",
      "6163 Training Loss: tensor(0.0552)\n",
      "6164 Training Loss: tensor(0.0472)\n",
      "6165 Training Loss: tensor(0.0499)\n",
      "6166 Training Loss: tensor(0.0542)\n",
      "6167 Training Loss: tensor(0.0543)\n",
      "6168 Training Loss: tensor(0.0510)\n",
      "6169 Training Loss: tensor(0.0620)\n",
      "6170 Training Loss: tensor(0.0585)\n",
      "6171 Training Loss: tensor(0.0534)\n",
      "6172 Training Loss: tensor(0.0536)\n",
      "6173 Training Loss: tensor(0.0533)\n",
      "6174 Training Loss: tensor(0.0515)\n",
      "6175 Training Loss: tensor(0.0562)\n",
      "6176 Training Loss: tensor(0.0492)\n",
      "6177 Training Loss: tensor(0.0449)\n",
      "6178 Training Loss: tensor(0.0506)\n",
      "6179 Training Loss: tensor(0.0497)\n",
      "6180 Training Loss: tensor(0.0517)\n",
      "6181 Training Loss: tensor(0.0560)\n",
      "6182 Training Loss: tensor(0.0521)\n",
      "6183 Training Loss: tensor(0.0525)\n",
      "6184 Training Loss: tensor(0.0515)\n",
      "6185 Training Loss: tensor(0.0542)\n",
      "6186 Training Loss: tensor(0.0580)\n",
      "6187 Training Loss: tensor(0.0542)\n",
      "6188 Training Loss: tensor(0.0474)\n",
      "6189 Training Loss: tensor(0.0558)\n",
      "6190 Training Loss: tensor(0.0548)\n",
      "6191 Training Loss: tensor(0.0523)\n",
      "6192 Training Loss: tensor(0.0481)\n",
      "6193 Training Loss: tensor(0.0529)\n",
      "6194 Training Loss: tensor(0.0564)\n",
      "6195 Training Loss: tensor(0.0516)\n",
      "6196 Training Loss: tensor(0.0539)\n",
      "6197 Training Loss: tensor(0.0476)\n",
      "6198 Training Loss: tensor(0.0598)\n",
      "6199 Training Loss: tensor(0.0516)\n",
      "6200 Training Loss: tensor(0.0574)\n",
      "6201 Training Loss: tensor(0.0596)\n",
      "6202 Training Loss: tensor(0.0529)\n",
      "6203 Training Loss: tensor(0.0562)\n",
      "6204 Training Loss: tensor(0.0514)\n",
      "6205 Training Loss: tensor(0.0507)\n",
      "6206 Training Loss: tensor(0.0533)\n",
      "6207 Training Loss: tensor(0.0539)\n",
      "6208 Training Loss: tensor(0.0474)\n",
      "6209 Training Loss: tensor(0.0501)\n",
      "6210 Training Loss: tensor(0.0617)\n",
      "6211 Training Loss: tensor(0.0527)\n",
      "6212 Training Loss: tensor(0.0582)\n",
      "6213 Training Loss: tensor(0.0562)\n",
      "6214 Training Loss: tensor(0.0526)\n",
      "6215 Training Loss: tensor(0.0500)\n",
      "6216 Training Loss: tensor(0.0483)\n",
      "6217 Training Loss: tensor(0.0500)\n",
      "6218 Training Loss: tensor(0.0574)\n",
      "6219 Training Loss: tensor(0.0561)\n",
      "6220 Training Loss: tensor(0.0535)\n",
      "6221 Training Loss: tensor(0.0566)\n",
      "6222 Training Loss: tensor(0.0493)\n",
      "6223 Training Loss: tensor(0.0536)\n",
      "6224 Training Loss: tensor(0.0520)\n",
      "6225 Training Loss: tensor(0.0558)\n",
      "6226 Training Loss: tensor(0.0533)\n",
      "6227 Training Loss: tensor(0.0560)\n",
      "6228 Training Loss: tensor(0.0547)\n",
      "6229 Training Loss: tensor(0.0563)\n",
      "6230 Training Loss: tensor(0.0545)\n",
      "6231 Training Loss: tensor(0.0537)\n",
      "6232 Training Loss: tensor(0.0505)\n",
      "6233 Training Loss: tensor(0.0535)\n",
      "6234 Training Loss: tensor(0.0541)\n",
      "6235 Training Loss: tensor(0.0519)\n",
      "6236 Training Loss: tensor(0.0561)\n",
      "6237 Training Loss: tensor(0.0474)\n",
      "6238 Training Loss: tensor(0.0512)\n",
      "6239 Training Loss: tensor(0.0549)\n",
      "6240 Training Loss: tensor(0.0513)\n",
      "6241 Training Loss: tensor(0.0476)\n",
      "6242 Training Loss: tensor(0.0524)\n",
      "6243 Training Loss: tensor(0.0498)\n",
      "6244 Training Loss: tensor(0.0562)\n",
      "6245 Training Loss: tensor(0.0482)\n",
      "6246 Training Loss: tensor(0.0490)\n",
      "6247 Training Loss: tensor(0.0486)\n",
      "6248 Training Loss: tensor(0.0538)\n",
      "6249 Training Loss: tensor(0.0542)\n",
      "6250 Training Loss: tensor(0.0501)\n",
      "6251 Training Loss: tensor(0.0492)\n",
      "6252 Training Loss: tensor(0.0551)\n",
      "6253 Training Loss: tensor(0.0587)\n",
      "6254 Training Loss: tensor(0.0473)\n",
      "6255 Training Loss: tensor(0.0622)\n",
      "6256 Training Loss: tensor(0.0512)\n",
      "6257 Training Loss: tensor(0.0547)\n",
      "6258 Training Loss: tensor(0.0503)\n",
      "6259 Training Loss: tensor(0.0508)\n",
      "6260 Training Loss: tensor(0.0568)\n",
      "6261 Training Loss: tensor(0.0571)\n",
      "6262 Training Loss: tensor(0.0473)\n",
      "6263 Training Loss: tensor(0.0490)\n",
      "6264 Training Loss: tensor(0.0513)\n",
      "6265 Training Loss: tensor(0.0595)\n",
      "6266 Training Loss: tensor(0.0522)\n",
      "6267 Training Loss: tensor(0.0551)\n",
      "6268 Training Loss: tensor(0.0509)\n",
      "6269 Training Loss: tensor(0.0529)\n",
      "6270 Training Loss: tensor(0.0516)\n",
      "6271 Training Loss: tensor(0.0547)\n",
      "6272 Training Loss: tensor(0.0542)\n",
      "6273 Training Loss: tensor(0.0521)\n",
      "6274 Training Loss: tensor(0.0569)\n",
      "6275 Training Loss: tensor(0.0531)\n",
      "6276 Training Loss: tensor(0.0556)\n",
      "6277 Training Loss: tensor(0.0549)\n",
      "6278 Training Loss: tensor(0.0560)\n",
      "6279 Training Loss: tensor(0.0582)\n",
      "6280 Training Loss: tensor(0.0591)\n",
      "6281 Training Loss: tensor(0.0549)\n",
      "6282 Training Loss: tensor(0.0512)\n",
      "6283 Training Loss: tensor(0.0510)\n",
      "6284 Training Loss: tensor(0.0540)\n",
      "6285 Training Loss: tensor(0.0521)\n",
      "6286 Training Loss: tensor(0.0572)\n",
      "6287 Training Loss: tensor(0.0443)\n",
      "6288 Training Loss: tensor(0.0557)\n",
      "6289 Training Loss: tensor(0.0456)\n",
      "6290 Training Loss: tensor(0.0514)\n",
      "6291 Training Loss: tensor(0.0505)\n",
      "6292 Training Loss: tensor(0.0583)\n",
      "6293 Training Loss: tensor(0.0528)\n",
      "6294 Training Loss: tensor(0.0515)\n",
      "6295 Training Loss: tensor(0.0481)\n",
      "6296 Training Loss: tensor(0.0518)\n",
      "6297 Training Loss: tensor(0.0538)\n",
      "6298 Training Loss: tensor(0.0501)\n",
      "6299 Training Loss: tensor(0.0517)\n",
      "6300 Training Loss: tensor(0.0558)\n",
      "6301 Training Loss: tensor(0.0528)\n",
      "6302 Training Loss: tensor(0.0483)\n",
      "6303 Training Loss: tensor(0.0475)\n",
      "6304 Training Loss: tensor(0.0504)\n",
      "6305 Training Loss: tensor(0.0532)\n",
      "6306 Training Loss: tensor(0.0487)\n",
      "6307 Training Loss: tensor(0.0524)\n",
      "6308 Training Loss: tensor(0.0464)\n",
      "6309 Training Loss: tensor(0.0546)\n",
      "6310 Training Loss: tensor(0.0537)\n",
      "6311 Training Loss: tensor(0.0475)\n",
      "6312 Training Loss: tensor(0.0525)\n",
      "6313 Training Loss: tensor(0.0502)\n",
      "6314 Training Loss: tensor(0.0434)\n",
      "6315 Training Loss: tensor(0.0530)\n",
      "6316 Training Loss: tensor(0.0448)\n",
      "6317 Training Loss: tensor(0.0517)\n",
      "6318 Training Loss: tensor(0.0491)\n",
      "6319 Training Loss: tensor(0.0498)\n",
      "6320 Training Loss: tensor(0.0493)\n",
      "6321 Training Loss: tensor(0.0539)\n",
      "6322 Training Loss: tensor(0.0529)\n",
      "6323 Training Loss: tensor(0.0513)\n",
      "6324 Training Loss: tensor(0.0503)\n",
      "6325 Training Loss: tensor(0.0536)\n",
      "6326 Training Loss: tensor(0.0524)\n",
      "6327 Training Loss: tensor(0.0475)\n",
      "6328 Training Loss: tensor(0.0505)\n",
      "6329 Training Loss: tensor(0.0491)\n",
      "6330 Training Loss: tensor(0.0505)\n",
      "6331 Training Loss: tensor(0.0472)\n",
      "6332 Training Loss: tensor(0.0439)\n",
      "6333 Training Loss: tensor(0.0455)\n",
      "6334 Training Loss: tensor(0.0527)\n",
      "6335 Training Loss: tensor(0.0490)\n",
      "6336 Training Loss: tensor(0.0532)\n",
      "6337 Training Loss: tensor(0.0525)\n",
      "6338 Training Loss: tensor(0.0559)\n",
      "6339 Training Loss: tensor(0.0486)\n",
      "6340 Training Loss: tensor(0.0518)\n",
      "6341 Training Loss: tensor(0.0481)\n",
      "6342 Training Loss: tensor(0.0512)\n",
      "6343 Training Loss: tensor(0.0473)\n",
      "6344 Training Loss: tensor(0.0492)\n",
      "6345 Training Loss: tensor(0.0538)\n",
      "6346 Training Loss: tensor(0.0516)\n",
      "6347 Training Loss: tensor(0.0487)\n",
      "6348 Training Loss: tensor(0.0516)\n",
      "6349 Training Loss: tensor(0.0510)\n",
      "6350 Training Loss: tensor(0.0500)\n",
      "6351 Training Loss: tensor(0.0452)\n",
      "6352 Training Loss: tensor(0.0518)\n",
      "6353 Training Loss: tensor(0.0485)\n",
      "6354 Training Loss: tensor(0.0513)\n",
      "6355 Training Loss: tensor(0.0526)\n",
      "6356 Training Loss: tensor(0.0490)\n",
      "6357 Training Loss: tensor(0.0551)\n",
      "6358 Training Loss: tensor(0.0539)\n",
      "6359 Training Loss: tensor(0.0493)\n",
      "6360 Training Loss: tensor(0.0486)\n",
      "6361 Training Loss: tensor(0.0519)\n",
      "6362 Training Loss: tensor(0.0492)\n",
      "6363 Training Loss: tensor(0.0524)\n",
      "6364 Training Loss: tensor(0.0470)\n",
      "6365 Training Loss: tensor(0.0496)\n",
      "6366 Training Loss: tensor(0.0570)\n",
      "6367 Training Loss: tensor(0.0496)\n",
      "6368 Training Loss: tensor(0.0549)\n",
      "6369 Training Loss: tensor(0.0529)\n",
      "6370 Training Loss: tensor(0.0517)\n",
      "6371 Training Loss: tensor(0.0570)\n",
      "6372 Training Loss: tensor(0.0564)\n",
      "6373 Training Loss: tensor(0.0509)\n",
      "6374 Training Loss: tensor(0.0517)\n",
      "6375 Training Loss: tensor(0.0471)\n",
      "6376 Training Loss: tensor(0.0517)\n",
      "6377 Training Loss: tensor(0.0522)\n",
      "6378 Training Loss: tensor(0.0505)\n",
      "6379 Training Loss: tensor(0.0485)\n",
      "6380 Training Loss: tensor(0.0548)\n",
      "6381 Training Loss: tensor(0.0471)\n",
      "6382 Training Loss: tensor(0.0469)\n",
      "6383 Training Loss: tensor(0.0525)\n",
      "6384 Training Loss: tensor(0.0483)\n",
      "6385 Training Loss: tensor(0.0418)\n",
      "6386 Training Loss: tensor(0.0590)\n",
      "6387 Training Loss: tensor(0.0444)\n",
      "6388 Training Loss: tensor(0.0493)\n",
      "6389 Training Loss: tensor(0.0553)\n",
      "6390 Training Loss: tensor(0.0492)\n",
      "6391 Training Loss: tensor(0.0530)\n",
      "6392 Training Loss: tensor(0.0474)\n",
      "6393 Training Loss: tensor(0.0490)\n",
      "6394 Training Loss: tensor(0.0497)\n",
      "6395 Training Loss: tensor(0.0535)\n",
      "6396 Training Loss: tensor(0.0470)\n",
      "6397 Training Loss: tensor(0.0503)\n",
      "6398 Training Loss: tensor(0.0527)\n",
      "6399 Training Loss: tensor(0.0528)\n",
      "6400 Training Loss: tensor(0.0471)\n",
      "6401 Training Loss: tensor(0.0501)\n",
      "6402 Training Loss: tensor(0.0493)\n",
      "6403 Training Loss: tensor(0.0482)\n",
      "6404 Training Loss: tensor(0.0508)\n",
      "6405 Training Loss: tensor(0.0539)\n",
      "6406 Training Loss: tensor(0.0527)\n",
      "6407 Training Loss: tensor(0.0442)\n",
      "6408 Training Loss: tensor(0.0541)\n",
      "6409 Training Loss: tensor(0.0539)\n",
      "6410 Training Loss: tensor(0.0535)\n",
      "6411 Training Loss: tensor(0.0510)\n",
      "6412 Training Loss: tensor(0.0497)\n",
      "6413 Training Loss: tensor(0.0509)\n",
      "6414 Training Loss: tensor(0.0512)\n",
      "6415 Training Loss: tensor(0.0527)\n",
      "6416 Training Loss: tensor(0.0513)\n",
      "6417 Training Loss: tensor(0.0539)\n",
      "6418 Training Loss: tensor(0.0511)\n",
      "6419 Training Loss: tensor(0.0552)\n",
      "6420 Training Loss: tensor(0.0537)\n",
      "6421 Training Loss: tensor(0.0467)\n",
      "6422 Training Loss: tensor(0.0554)\n",
      "6423 Training Loss: tensor(0.0480)\n",
      "6424 Training Loss: tensor(0.0570)\n",
      "6425 Training Loss: tensor(0.0551)\n",
      "6426 Training Loss: tensor(0.0439)\n",
      "6427 Training Loss: tensor(0.0548)\n",
      "6428 Training Loss: tensor(0.0599)\n",
      "6429 Training Loss: tensor(0.0489)\n",
      "6430 Training Loss: tensor(0.0498)\n",
      "6431 Training Loss: tensor(0.0444)\n",
      "6432 Training Loss: tensor(0.0494)\n",
      "6433 Training Loss: tensor(0.0491)\n",
      "6434 Training Loss: tensor(0.0526)\n",
      "6435 Training Loss: tensor(0.0555)\n",
      "6436 Training Loss: tensor(0.0510)\n",
      "6437 Training Loss: tensor(0.0573)\n",
      "6438 Training Loss: tensor(0.0484)\n",
      "6439 Training Loss: tensor(0.0474)\n",
      "6440 Training Loss: tensor(0.0479)\n",
      "6441 Training Loss: tensor(0.0478)\n",
      "6442 Training Loss: tensor(0.0512)\n",
      "6443 Training Loss: tensor(0.0509)\n",
      "6444 Training Loss: tensor(0.0494)\n",
      "6445 Training Loss: tensor(0.0497)\n",
      "6446 Training Loss: tensor(0.0527)\n",
      "6447 Training Loss: tensor(0.0460)\n",
      "6448 Training Loss: tensor(0.0510)\n",
      "6449 Training Loss: tensor(0.0558)\n",
      "6450 Training Loss: tensor(0.0481)\n",
      "6451 Training Loss: tensor(0.0415)\n",
      "6452 Training Loss: tensor(0.0536)\n",
      "6453 Training Loss: tensor(0.0493)\n",
      "6454 Training Loss: tensor(0.0536)\n",
      "6455 Training Loss: tensor(0.0540)\n",
      "6456 Training Loss: tensor(0.0473)\n",
      "6457 Training Loss: tensor(0.0530)\n",
      "6458 Training Loss: tensor(0.0468)\n",
      "6459 Training Loss: tensor(0.0498)\n",
      "6460 Training Loss: tensor(0.0516)\n",
      "6461 Training Loss: tensor(0.0478)\n",
      "6462 Training Loss: tensor(0.0554)\n",
      "6463 Training Loss: tensor(0.0497)\n",
      "6464 Training Loss: tensor(0.0489)\n",
      "6465 Training Loss: tensor(0.0466)\n",
      "6466 Training Loss: tensor(0.0441)\n",
      "6467 Training Loss: tensor(0.0558)\n",
      "6468 Training Loss: tensor(0.0524)\n",
      "6469 Training Loss: tensor(0.0490)\n",
      "6470 Training Loss: tensor(0.0499)\n",
      "6471 Training Loss: tensor(0.0482)\n",
      "6472 Training Loss: tensor(0.0504)\n",
      "6473 Training Loss: tensor(0.0488)\n",
      "6474 Training Loss: tensor(0.0491)\n",
      "6475 Training Loss: tensor(0.0504)\n",
      "6476 Training Loss: tensor(0.0533)\n",
      "6477 Training Loss: tensor(0.0529)\n",
      "6478 Training Loss: tensor(0.0503)\n",
      "6479 Training Loss: tensor(0.0457)\n",
      "6480 Training Loss: tensor(0.0524)\n",
      "6481 Training Loss: tensor(0.0482)\n",
      "6482 Training Loss: tensor(0.0472)\n",
      "6483 Training Loss: tensor(0.0516)\n",
      "6484 Training Loss: tensor(0.0522)\n",
      "6485 Training Loss: tensor(0.0485)\n",
      "6486 Training Loss: tensor(0.0511)\n",
      "6487 Training Loss: tensor(0.0514)\n",
      "6488 Training Loss: tensor(0.0582)\n",
      "6489 Training Loss: tensor(0.0479)\n",
      "6490 Training Loss: tensor(0.0523)\n",
      "6491 Training Loss: tensor(0.0528)\n",
      "6492 Training Loss: tensor(0.0583)\n",
      "6493 Training Loss: tensor(0.0491)\n",
      "6494 Training Loss: tensor(0.0522)\n",
      "6495 Training Loss: tensor(0.0446)\n",
      "6496 Training Loss: tensor(0.0497)\n",
      "6497 Training Loss: tensor(0.0472)\n",
      "6498 Training Loss: tensor(0.0515)\n",
      "6499 Training Loss: tensor(0.0532)\n",
      "6500 Training Loss: tensor(0.0543)\n",
      "6501 Training Loss: tensor(0.0460)\n",
      "6502 Training Loss: tensor(0.0479)\n",
      "6503 Training Loss: tensor(0.0590)\n",
      "6504 Training Loss: tensor(0.0537)\n",
      "6505 Training Loss: tensor(0.0537)\n",
      "6506 Training Loss: tensor(0.0504)\n",
      "6507 Training Loss: tensor(0.0446)\n",
      "6508 Training Loss: tensor(0.0554)\n",
      "6509 Training Loss: tensor(0.0514)\n",
      "6510 Training Loss: tensor(0.0520)\n",
      "6511 Training Loss: tensor(0.0571)\n",
      "6512 Training Loss: tensor(0.0528)\n",
      "6513 Training Loss: tensor(0.0458)\n",
      "6514 Training Loss: tensor(0.0551)\n",
      "6515 Training Loss: tensor(0.0513)\n",
      "6516 Training Loss: tensor(0.0539)\n",
      "6517 Training Loss: tensor(0.0530)\n",
      "6518 Training Loss: tensor(0.0468)\n",
      "6519 Training Loss: tensor(0.0532)\n",
      "6520 Training Loss: tensor(0.0506)\n",
      "6521 Training Loss: tensor(0.0554)\n",
      "6522 Training Loss: tensor(0.0496)\n",
      "6523 Training Loss: tensor(0.0456)\n",
      "6524 Training Loss: tensor(0.0519)\n",
      "6525 Training Loss: tensor(0.0504)\n",
      "6526 Training Loss: tensor(0.0534)\n",
      "6527 Training Loss: tensor(0.0450)\n",
      "6528 Training Loss: tensor(0.0488)\n",
      "6529 Training Loss: tensor(0.0452)\n",
      "6530 Training Loss: tensor(0.0543)\n",
      "6531 Training Loss: tensor(0.0477)\n",
      "6532 Training Loss: tensor(0.0505)\n",
      "6533 Training Loss: tensor(0.0582)\n",
      "6534 Training Loss: tensor(0.0498)\n",
      "6535 Training Loss: tensor(0.0503)\n",
      "6536 Training Loss: tensor(0.0468)\n",
      "6537 Training Loss: tensor(0.0458)\n",
      "6538 Training Loss: tensor(0.0451)\n",
      "6539 Training Loss: tensor(0.0486)\n",
      "6540 Training Loss: tensor(0.0455)\n",
      "6541 Training Loss: tensor(0.0538)\n",
      "6542 Training Loss: tensor(0.0491)\n",
      "6543 Training Loss: tensor(0.0543)\n",
      "6544 Training Loss: tensor(0.0488)\n",
      "6545 Training Loss: tensor(0.0481)\n",
      "6546 Training Loss: tensor(0.0504)\n",
      "6547 Training Loss: tensor(0.0425)\n",
      "6548 Training Loss: tensor(0.0472)\n",
      "6549 Training Loss: tensor(0.0478)\n",
      "6550 Training Loss: tensor(0.0486)\n",
      "6551 Training Loss: tensor(0.0488)\n",
      "6552 Training Loss: tensor(0.0475)\n",
      "6553 Training Loss: tensor(0.0510)\n",
      "6554 Training Loss: tensor(0.0463)\n",
      "6555 Training Loss: tensor(0.0540)\n",
      "6556 Training Loss: tensor(0.0550)\n",
      "6557 Training Loss: tensor(0.0472)\n",
      "6558 Training Loss: tensor(0.0524)\n",
      "6559 Training Loss: tensor(0.0516)\n",
      "6560 Training Loss: tensor(0.0565)\n",
      "6561 Training Loss: tensor(0.0514)\n",
      "6562 Training Loss: tensor(0.0566)\n",
      "6563 Training Loss: tensor(0.0435)\n",
      "6564 Training Loss: tensor(0.0479)\n",
      "6565 Training Loss: tensor(0.0566)\n",
      "6566 Training Loss: tensor(0.0452)\n",
      "6567 Training Loss: tensor(0.0549)\n",
      "6568 Training Loss: tensor(0.0519)\n",
      "6569 Training Loss: tensor(0.0506)\n",
      "6570 Training Loss: tensor(0.0510)\n",
      "6571 Training Loss: tensor(0.0504)\n",
      "6572 Training Loss: tensor(0.0483)\n",
      "6573 Training Loss: tensor(0.0502)\n",
      "6574 Training Loss: tensor(0.0466)\n",
      "6575 Training Loss: tensor(0.0468)\n",
      "6576 Training Loss: tensor(0.0440)\n",
      "6577 Training Loss: tensor(0.0472)\n",
      "6578 Training Loss: tensor(0.0512)\n",
      "6579 Training Loss: tensor(0.0544)\n",
      "6580 Training Loss: tensor(0.0553)\n",
      "6581 Training Loss: tensor(0.0599)\n",
      "6582 Training Loss: tensor(0.0485)\n",
      "6583 Training Loss: tensor(0.0468)\n",
      "6584 Training Loss: tensor(0.0564)\n",
      "6585 Training Loss: tensor(0.0504)\n",
      "6586 Training Loss: tensor(0.0533)\n",
      "6587 Training Loss: tensor(0.0469)\n",
      "6588 Training Loss: tensor(0.0449)\n",
      "6589 Training Loss: tensor(0.0531)\n",
      "6590 Training Loss: tensor(0.0492)\n",
      "6591 Training Loss: tensor(0.0456)\n",
      "6592 Training Loss: tensor(0.0491)\n",
      "6593 Training Loss: tensor(0.0488)\n",
      "6594 Training Loss: tensor(0.0472)\n",
      "6595 Training Loss: tensor(0.0522)\n",
      "6596 Training Loss: tensor(0.0476)\n",
      "6597 Training Loss: tensor(0.0547)\n",
      "6598 Training Loss: tensor(0.0516)\n",
      "6599 Training Loss: tensor(0.0449)\n",
      "6600 Training Loss: tensor(0.0512)\n",
      "6601 Training Loss: tensor(0.0425)\n",
      "6602 Training Loss: tensor(0.0495)\n",
      "6603 Training Loss: tensor(0.0506)\n",
      "6604 Training Loss: tensor(0.0515)\n",
      "6605 Training Loss: tensor(0.0462)\n",
      "6606 Training Loss: tensor(0.0513)\n",
      "6607 Training Loss: tensor(0.0495)\n",
      "6608 Training Loss: tensor(0.0457)\n",
      "6609 Training Loss: tensor(0.0532)\n",
      "6610 Training Loss: tensor(0.0487)\n",
      "6611 Training Loss: tensor(0.0466)\n",
      "6612 Training Loss: tensor(0.0498)\n",
      "6613 Training Loss: tensor(0.0483)\n",
      "6614 Training Loss: tensor(0.0513)\n",
      "6615 Training Loss: tensor(0.0539)\n",
      "6616 Training Loss: tensor(0.0493)\n",
      "6617 Training Loss: tensor(0.0451)\n",
      "6618 Training Loss: tensor(0.0523)\n",
      "6619 Training Loss: tensor(0.0493)\n",
      "6620 Training Loss: tensor(0.0444)\n",
      "6621 Training Loss: tensor(0.0509)\n",
      "6622 Training Loss: tensor(0.0534)\n",
      "6623 Training Loss: tensor(0.0492)\n",
      "6624 Training Loss: tensor(0.0531)\n",
      "6625 Training Loss: tensor(0.0499)\n",
      "6626 Training Loss: tensor(0.0524)\n",
      "6627 Training Loss: tensor(0.0531)\n",
      "6628 Training Loss: tensor(0.0441)\n",
      "6629 Training Loss: tensor(0.0550)\n",
      "6630 Training Loss: tensor(0.0468)\n",
      "6631 Training Loss: tensor(0.0522)\n",
      "6632 Training Loss: tensor(0.0511)\n",
      "6633 Training Loss: tensor(0.0483)\n",
      "6634 Training Loss: tensor(0.0455)\n",
      "6635 Training Loss: tensor(0.0521)\n",
      "6636 Training Loss: tensor(0.0583)\n",
      "6637 Training Loss: tensor(0.0517)\n",
      "6638 Training Loss: tensor(0.0487)\n",
      "6639 Training Loss: tensor(0.0439)\n",
      "6640 Training Loss: tensor(0.0508)\n",
      "6641 Training Loss: tensor(0.0509)\n",
      "6642 Training Loss: tensor(0.0474)\n",
      "6643 Training Loss: tensor(0.0494)\n",
      "6644 Training Loss: tensor(0.0547)\n",
      "6645 Training Loss: tensor(0.0506)\n",
      "6646 Training Loss: tensor(0.0546)\n",
      "6647 Training Loss: tensor(0.0488)\n",
      "6648 Training Loss: tensor(0.0520)\n",
      "6649 Training Loss: tensor(0.0510)\n",
      "6650 Training Loss: tensor(0.0548)\n",
      "6651 Training Loss: tensor(0.0474)\n",
      "6652 Training Loss: tensor(0.0496)\n",
      "6653 Training Loss: tensor(0.0476)\n",
      "6654 Training Loss: tensor(0.0577)\n",
      "6655 Training Loss: tensor(0.0553)\n",
      "6656 Training Loss: tensor(0.0518)\n",
      "6657 Training Loss: tensor(0.0519)\n",
      "6658 Training Loss: tensor(0.0527)\n",
      "6659 Training Loss: tensor(0.0538)\n",
      "6660 Training Loss: tensor(0.0442)\n",
      "6661 Training Loss: tensor(0.0549)\n",
      "6662 Training Loss: tensor(0.0485)\n",
      "6663 Training Loss: tensor(0.0473)\n",
      "6664 Training Loss: tensor(0.0491)\n",
      "6665 Training Loss: tensor(0.0443)\n",
      "6666 Training Loss: tensor(0.0512)\n",
      "6667 Training Loss: tensor(0.0484)\n",
      "6668 Training Loss: tensor(0.0497)\n",
      "6669 Training Loss: tensor(0.0503)\n",
      "6670 Training Loss: tensor(0.0507)\n",
      "6671 Training Loss: tensor(0.0440)\n",
      "6672 Training Loss: tensor(0.0477)\n",
      "6673 Training Loss: tensor(0.0528)\n",
      "6674 Training Loss: tensor(0.0523)\n",
      "6675 Training Loss: tensor(0.0512)\n",
      "6676 Training Loss: tensor(0.0518)\n",
      "6677 Training Loss: tensor(0.0493)\n",
      "6678 Training Loss: tensor(0.0502)\n",
      "6679 Training Loss: tensor(0.0423)\n",
      "6680 Training Loss: tensor(0.0463)\n",
      "6681 Training Loss: tensor(0.0474)\n",
      "6682 Training Loss: tensor(0.0552)\n",
      "6683 Training Loss: tensor(0.0553)\n",
      "6684 Training Loss: tensor(0.0493)\n",
      "6685 Training Loss: tensor(0.0513)\n",
      "6686 Training Loss: tensor(0.0505)\n",
      "6687 Training Loss: tensor(0.0441)\n",
      "6688 Training Loss: tensor(0.0548)\n",
      "6689 Training Loss: tensor(0.0550)\n",
      "6690 Training Loss: tensor(0.0507)\n",
      "6691 Training Loss: tensor(0.0473)\n",
      "6692 Training Loss: tensor(0.0493)\n",
      "6693 Training Loss: tensor(0.0482)\n",
      "6694 Training Loss: tensor(0.0485)\n",
      "6695 Training Loss: tensor(0.0521)\n",
      "6696 Training Loss: tensor(0.0538)\n",
      "6697 Training Loss: tensor(0.0474)\n",
      "6698 Training Loss: tensor(0.0535)\n",
      "6699 Training Loss: tensor(0.0521)\n",
      "6700 Training Loss: tensor(0.0504)\n",
      "6701 Training Loss: tensor(0.0499)\n",
      "6702 Training Loss: tensor(0.0585)\n",
      "6703 Training Loss: tensor(0.0457)\n",
      "6704 Training Loss: tensor(0.0461)\n",
      "6705 Training Loss: tensor(0.0482)\n",
      "6706 Training Loss: tensor(0.0472)\n",
      "6707 Training Loss: tensor(0.0492)\n",
      "6708 Training Loss: tensor(0.0458)\n",
      "6709 Training Loss: tensor(0.0486)\n",
      "6710 Training Loss: tensor(0.0516)\n",
      "6711 Training Loss: tensor(0.0470)\n",
      "6712 Training Loss: tensor(0.0475)\n",
      "6713 Training Loss: tensor(0.0455)\n",
      "6714 Training Loss: tensor(0.0525)\n",
      "6715 Training Loss: tensor(0.0494)\n",
      "6716 Training Loss: tensor(0.0532)\n",
      "6717 Training Loss: tensor(0.0446)\n",
      "6718 Training Loss: tensor(0.0462)\n",
      "6719 Training Loss: tensor(0.0458)\n",
      "6720 Training Loss: tensor(0.0499)\n",
      "6721 Training Loss: tensor(0.0468)\n",
      "6722 Training Loss: tensor(0.0488)\n",
      "6723 Training Loss: tensor(0.0507)\n",
      "6724 Training Loss: tensor(0.0442)\n",
      "6725 Training Loss: tensor(0.0486)\n",
      "6726 Training Loss: tensor(0.0467)\n",
      "6727 Training Loss: tensor(0.0530)\n",
      "6728 Training Loss: tensor(0.0434)\n",
      "6729 Training Loss: tensor(0.0561)\n",
      "6730 Training Loss: tensor(0.0465)\n",
      "6731 Training Loss: tensor(0.0506)\n",
      "6732 Training Loss: tensor(0.0487)\n",
      "6733 Training Loss: tensor(0.0473)\n",
      "6734 Training Loss: tensor(0.0472)\n",
      "6735 Training Loss: tensor(0.0466)\n",
      "6736 Training Loss: tensor(0.0476)\n",
      "6737 Training Loss: tensor(0.0555)\n",
      "6738 Training Loss: tensor(0.0525)\n",
      "6739 Training Loss: tensor(0.0542)\n",
      "6740 Training Loss: tensor(0.0521)\n",
      "6741 Training Loss: tensor(0.0450)\n",
      "6742 Training Loss: tensor(0.0507)\n",
      "6743 Training Loss: tensor(0.0493)\n",
      "6744 Training Loss: tensor(0.0510)\n",
      "6745 Training Loss: tensor(0.0416)\n",
      "6746 Training Loss: tensor(0.0445)\n",
      "6747 Training Loss: tensor(0.0520)\n",
      "6748 Training Loss: tensor(0.0458)\n",
      "6749 Training Loss: tensor(0.0481)\n",
      "6750 Training Loss: tensor(0.0469)\n",
      "6751 Training Loss: tensor(0.0494)\n",
      "6752 Training Loss: tensor(0.0495)\n",
      "6753 Training Loss: tensor(0.0466)\n",
      "6754 Training Loss: tensor(0.0478)\n",
      "6755 Training Loss: tensor(0.0513)\n",
      "6756 Training Loss: tensor(0.0501)\n",
      "6757 Training Loss: tensor(0.0556)\n",
      "6758 Training Loss: tensor(0.0529)\n",
      "6759 Training Loss: tensor(0.0518)\n",
      "6760 Training Loss: tensor(0.0521)\n",
      "6761 Training Loss: tensor(0.0488)\n",
      "6762 Training Loss: tensor(0.0579)\n",
      "6763 Training Loss: tensor(0.0559)\n",
      "6764 Training Loss: tensor(0.0465)\n",
      "6765 Training Loss: tensor(0.0490)\n",
      "6766 Training Loss: tensor(0.0545)\n",
      "6767 Training Loss: tensor(0.0524)\n",
      "6768 Training Loss: tensor(0.0487)\n",
      "6769 Training Loss: tensor(0.0476)\n",
      "6770 Training Loss: tensor(0.0465)\n",
      "6771 Training Loss: tensor(0.0474)\n",
      "6772 Training Loss: tensor(0.0488)\n",
      "6773 Training Loss: tensor(0.0489)\n",
      "6774 Training Loss: tensor(0.0493)\n",
      "6775 Training Loss: tensor(0.0492)\n",
      "6776 Training Loss: tensor(0.0487)\n",
      "6777 Training Loss: tensor(0.0469)\n",
      "6778 Training Loss: tensor(0.0481)\n",
      "6779 Training Loss: tensor(0.0539)\n",
      "6780 Training Loss: tensor(0.0522)\n",
      "6781 Training Loss: tensor(0.0515)\n",
      "6782 Training Loss: tensor(0.0518)\n",
      "6783 Training Loss: tensor(0.0523)\n",
      "6784 Training Loss: tensor(0.0516)\n",
      "6785 Training Loss: tensor(0.0498)\n",
      "6786 Training Loss: tensor(0.0477)\n",
      "6787 Training Loss: tensor(0.0480)\n",
      "6788 Training Loss: tensor(0.0480)\n",
      "6789 Training Loss: tensor(0.0532)\n",
      "6790 Training Loss: tensor(0.0466)\n",
      "6791 Training Loss: tensor(0.0482)\n",
      "6792 Training Loss: tensor(0.0467)\n",
      "6793 Training Loss: tensor(0.0487)\n",
      "6794 Training Loss: tensor(0.0518)\n",
      "6795 Training Loss: tensor(0.0535)\n",
      "6796 Training Loss: tensor(0.0530)\n",
      "6797 Training Loss: tensor(0.0444)\n",
      "6798 Training Loss: tensor(0.0523)\n",
      "6799 Training Loss: tensor(0.0494)\n",
      "6800 Training Loss: tensor(0.0489)\n",
      "6801 Training Loss: tensor(0.0544)\n",
      "6802 Training Loss: tensor(0.0434)\n",
      "6803 Training Loss: tensor(0.0493)\n",
      "6804 Training Loss: tensor(0.0491)\n",
      "6805 Training Loss: tensor(0.0507)\n",
      "6806 Training Loss: tensor(0.0489)\n",
      "6807 Training Loss: tensor(0.0499)\n",
      "6808 Training Loss: tensor(0.0458)\n",
      "6809 Training Loss: tensor(0.0487)\n",
      "6810 Training Loss: tensor(0.0500)\n",
      "6811 Training Loss: tensor(0.0470)\n",
      "6812 Training Loss: tensor(0.0469)\n",
      "6813 Training Loss: tensor(0.0508)\n",
      "6814 Training Loss: tensor(0.0527)\n",
      "6815 Training Loss: tensor(0.0491)\n",
      "6816 Training Loss: tensor(0.0579)\n",
      "6817 Training Loss: tensor(0.0518)\n",
      "6818 Training Loss: tensor(0.0495)\n",
      "6819 Training Loss: tensor(0.0466)\n",
      "6820 Training Loss: tensor(0.0477)\n",
      "6821 Training Loss: tensor(0.0529)\n",
      "6822 Training Loss: tensor(0.0502)\n",
      "6823 Training Loss: tensor(0.0432)\n",
      "6824 Training Loss: tensor(0.0448)\n",
      "6825 Training Loss: tensor(0.0449)\n",
      "6826 Training Loss: tensor(0.0498)\n",
      "6827 Training Loss: tensor(0.0472)\n",
      "6828 Training Loss: tensor(0.0467)\n",
      "6829 Training Loss: tensor(0.0493)\n",
      "6830 Training Loss: tensor(0.0458)\n",
      "6831 Training Loss: tensor(0.0526)\n",
      "6832 Training Loss: tensor(0.0464)\n",
      "6833 Training Loss: tensor(0.0469)\n",
      "6834 Training Loss: tensor(0.0488)\n",
      "6835 Training Loss: tensor(0.0466)\n",
      "6836 Training Loss: tensor(0.0516)\n",
      "6837 Training Loss: tensor(0.0478)\n",
      "6838 Training Loss: tensor(0.0461)\n",
      "6839 Training Loss: tensor(0.0490)\n",
      "6840 Training Loss: tensor(0.0443)\n",
      "6841 Training Loss: tensor(0.0550)\n",
      "6842 Training Loss: tensor(0.0465)\n",
      "6843 Training Loss: tensor(0.0495)\n",
      "6844 Training Loss: tensor(0.0489)\n",
      "6845 Training Loss: tensor(0.0453)\n",
      "6846 Training Loss: tensor(0.0435)\n",
      "6847 Training Loss: tensor(0.0488)\n",
      "6848 Training Loss: tensor(0.0451)\n",
      "6849 Training Loss: tensor(0.0477)\n",
      "6850 Training Loss: tensor(0.0480)\n",
      "6851 Training Loss: tensor(0.0429)\n",
      "6852 Training Loss: tensor(0.0511)\n",
      "6853 Training Loss: tensor(0.0497)\n",
      "6854 Training Loss: tensor(0.0525)\n",
      "6855 Training Loss: tensor(0.0485)\n",
      "6856 Training Loss: tensor(0.0481)\n",
      "6857 Training Loss: tensor(0.0487)\n",
      "6858 Training Loss: tensor(0.0472)\n",
      "6859 Training Loss: tensor(0.0486)\n",
      "6860 Training Loss: tensor(0.0469)\n",
      "6861 Training Loss: tensor(0.0427)\n",
      "6862 Training Loss: tensor(0.0454)\n",
      "6863 Training Loss: tensor(0.0428)\n",
      "6864 Training Loss: tensor(0.0558)\n",
      "6865 Training Loss: tensor(0.0419)\n",
      "6866 Training Loss: tensor(0.0463)\n",
      "6867 Training Loss: tensor(0.0416)\n",
      "6868 Training Loss: tensor(0.0489)\n",
      "6869 Training Loss: tensor(0.0498)\n",
      "6870 Training Loss: tensor(0.0460)\n",
      "6871 Training Loss: tensor(0.0508)\n",
      "6872 Training Loss: tensor(0.0532)\n",
      "6873 Training Loss: tensor(0.0482)\n",
      "6874 Training Loss: tensor(0.0463)\n",
      "6875 Training Loss: tensor(0.0532)\n",
      "6876 Training Loss: tensor(0.0539)\n",
      "6877 Training Loss: tensor(0.0480)\n",
      "6878 Training Loss: tensor(0.0471)\n",
      "6879 Training Loss: tensor(0.0510)\n",
      "6880 Training Loss: tensor(0.0476)\n",
      "6881 Training Loss: tensor(0.0473)\n",
      "6882 Training Loss: tensor(0.0506)\n",
      "6883 Training Loss: tensor(0.0451)\n",
      "6884 Training Loss: tensor(0.0505)\n",
      "6885 Training Loss: tensor(0.0469)\n",
      "6886 Training Loss: tensor(0.0501)\n",
      "6887 Training Loss: tensor(0.0455)\n",
      "6888 Training Loss: tensor(0.0469)\n",
      "6889 Training Loss: tensor(0.0504)\n",
      "6890 Training Loss: tensor(0.0457)\n",
      "6891 Training Loss: tensor(0.0600)\n",
      "6892 Training Loss: tensor(0.0457)\n",
      "6893 Training Loss: tensor(0.0500)\n",
      "6894 Training Loss: tensor(0.0496)\n",
      "6895 Training Loss: tensor(0.0557)\n",
      "6896 Training Loss: tensor(0.0532)\n",
      "6897 Training Loss: tensor(0.0463)\n",
      "6898 Training Loss: tensor(0.0452)\n",
      "6899 Training Loss: tensor(0.0543)\n",
      "6900 Training Loss: tensor(0.0460)\n",
      "6901 Training Loss: tensor(0.0461)\n",
      "6902 Training Loss: tensor(0.0482)\n",
      "6903 Training Loss: tensor(0.0547)\n",
      "6904 Training Loss: tensor(0.0514)\n",
      "6905 Training Loss: tensor(0.0510)\n",
      "6906 Training Loss: tensor(0.0465)\n",
      "6907 Training Loss: tensor(0.0450)\n",
      "6908 Training Loss: tensor(0.0528)\n",
      "6909 Training Loss: tensor(0.0505)\n",
      "6910 Training Loss: tensor(0.0496)\n",
      "6911 Training Loss: tensor(0.0449)\n",
      "6912 Training Loss: tensor(0.0477)\n",
      "6913 Training Loss: tensor(0.0506)\n",
      "6914 Training Loss: tensor(0.0478)\n",
      "6915 Training Loss: tensor(0.0438)\n",
      "6916 Training Loss: tensor(0.0552)\n",
      "6917 Training Loss: tensor(0.0513)\n",
      "6918 Training Loss: tensor(0.0558)\n",
      "6919 Training Loss: tensor(0.0490)\n",
      "6920 Training Loss: tensor(0.0462)\n",
      "6921 Training Loss: tensor(0.0470)\n",
      "6922 Training Loss: tensor(0.0480)\n",
      "6923 Training Loss: tensor(0.0464)\n",
      "6924 Training Loss: tensor(0.0496)\n",
      "6925 Training Loss: tensor(0.0554)\n",
      "6926 Training Loss: tensor(0.0527)\n",
      "6927 Training Loss: tensor(0.0508)\n",
      "6928 Training Loss: tensor(0.0526)\n",
      "6929 Training Loss: tensor(0.0552)\n",
      "6930 Training Loss: tensor(0.0523)\n",
      "6931 Training Loss: tensor(0.0478)\n",
      "6932 Training Loss: tensor(0.0485)\n",
      "6933 Training Loss: tensor(0.0484)\n",
      "6934 Training Loss: tensor(0.0478)\n",
      "6935 Training Loss: tensor(0.0493)\n",
      "6936 Training Loss: tensor(0.0496)\n",
      "6937 Training Loss: tensor(0.0506)\n",
      "6938 Training Loss: tensor(0.0450)\n",
      "6939 Training Loss: tensor(0.0469)\n",
      "6940 Training Loss: tensor(0.0501)\n",
      "6941 Training Loss: tensor(0.0450)\n",
      "6942 Training Loss: tensor(0.0424)\n",
      "6943 Training Loss: tensor(0.0478)\n",
      "6944 Training Loss: tensor(0.0417)\n",
      "6945 Training Loss: tensor(0.0448)\n",
      "6946 Training Loss: tensor(0.0442)\n",
      "6947 Training Loss: tensor(0.0499)\n",
      "6948 Training Loss: tensor(0.0509)\n",
      "6949 Training Loss: tensor(0.0476)\n",
      "6950 Training Loss: tensor(0.0466)\n",
      "6951 Training Loss: tensor(0.0486)\n",
      "6952 Training Loss: tensor(0.0461)\n",
      "6953 Training Loss: tensor(0.0458)\n",
      "6954 Training Loss: tensor(0.0497)\n",
      "6955 Training Loss: tensor(0.0476)\n",
      "6956 Training Loss: tensor(0.0460)\n",
      "6957 Training Loss: tensor(0.0458)\n",
      "6958 Training Loss: tensor(0.0456)\n",
      "6959 Training Loss: tensor(0.0488)\n",
      "6960 Training Loss: tensor(0.0477)\n",
      "6961 Training Loss: tensor(0.0460)\n",
      "6962 Training Loss: tensor(0.0454)\n",
      "6963 Training Loss: tensor(0.0492)\n",
      "6964 Training Loss: tensor(0.0449)\n",
      "6965 Training Loss: tensor(0.0524)\n",
      "6966 Training Loss: tensor(0.0430)\n",
      "6967 Training Loss: tensor(0.0436)\n",
      "6968 Training Loss: tensor(0.0467)\n",
      "6969 Training Loss: tensor(0.0494)\n",
      "6970 Training Loss: tensor(0.0472)\n",
      "6971 Training Loss: tensor(0.0507)\n",
      "6972 Training Loss: tensor(0.0509)\n",
      "6973 Training Loss: tensor(0.0459)\n",
      "6974 Training Loss: tensor(0.0490)\n",
      "6975 Training Loss: tensor(0.0446)\n",
      "6976 Training Loss: tensor(0.0506)\n",
      "6977 Training Loss: tensor(0.0532)\n",
      "6978 Training Loss: tensor(0.0471)\n",
      "6979 Training Loss: tensor(0.0449)\n",
      "6980 Training Loss: tensor(0.0490)\n",
      "6981 Training Loss: tensor(0.0445)\n",
      "6982 Training Loss: tensor(0.0513)\n",
      "6983 Training Loss: tensor(0.0514)\n",
      "6984 Training Loss: tensor(0.0455)\n",
      "6985 Training Loss: tensor(0.0446)\n",
      "6986 Training Loss: tensor(0.0478)\n",
      "6987 Training Loss: tensor(0.0537)\n",
      "6988 Training Loss: tensor(0.0498)\n",
      "6989 Training Loss: tensor(0.0522)\n",
      "6990 Training Loss: tensor(0.0487)\n",
      "6991 Training Loss: tensor(0.0462)\n",
      "6992 Training Loss: tensor(0.0465)\n",
      "6993 Training Loss: tensor(0.0523)\n",
      "6994 Training Loss: tensor(0.0484)\n",
      "6995 Training Loss: tensor(0.0497)\n",
      "6996 Training Loss: tensor(0.0521)\n",
      "6997 Training Loss: tensor(0.0457)\n",
      "6998 Training Loss: tensor(0.0466)\n",
      "6999 Training Loss: tensor(0.0546)\n",
      "7000 Training Loss: tensor(0.0536)\n",
      "7001 Training Loss: tensor(0.0472)\n",
      "7002 Training Loss: tensor(0.0477)\n",
      "7003 Training Loss: tensor(0.0483)\n",
      "7004 Training Loss: tensor(0.0472)\n",
      "7005 Training Loss: tensor(0.0487)\n",
      "7006 Training Loss: tensor(0.0483)\n",
      "7007 Training Loss: tensor(0.0474)\n",
      "7008 Training Loss: tensor(0.0468)\n",
      "7009 Training Loss: tensor(0.0516)\n",
      "7010 Training Loss: tensor(0.0510)\n",
      "7011 Training Loss: tensor(0.0481)\n",
      "7012 Training Loss: tensor(0.0518)\n",
      "7013 Training Loss: tensor(0.0382)\n",
      "7014 Training Loss: tensor(0.0497)\n",
      "7015 Training Loss: tensor(0.0453)\n",
      "7016 Training Loss: tensor(0.0517)\n",
      "7017 Training Loss: tensor(0.0564)\n",
      "7018 Training Loss: tensor(0.0476)\n",
      "7019 Training Loss: tensor(0.0457)\n",
      "7020 Training Loss: tensor(0.0486)\n",
      "7021 Training Loss: tensor(0.0512)\n",
      "7022 Training Loss: tensor(0.0496)\n",
      "7023 Training Loss: tensor(0.0503)\n",
      "7024 Training Loss: tensor(0.0467)\n",
      "7025 Training Loss: tensor(0.0440)\n",
      "7026 Training Loss: tensor(0.0536)\n",
      "7027 Training Loss: tensor(0.0376)\n",
      "7028 Training Loss: tensor(0.0462)\n",
      "7029 Training Loss: tensor(0.0419)\n",
      "7030 Training Loss: tensor(0.0444)\n",
      "7031 Training Loss: tensor(0.0488)\n",
      "7032 Training Loss: tensor(0.0451)\n",
      "7033 Training Loss: tensor(0.0450)\n",
      "7034 Training Loss: tensor(0.0522)\n",
      "7035 Training Loss: tensor(0.0509)\n",
      "7036 Training Loss: tensor(0.0523)\n",
      "7037 Training Loss: tensor(0.0500)\n",
      "7038 Training Loss: tensor(0.0482)\n",
      "7039 Training Loss: tensor(0.0518)\n",
      "7040 Training Loss: tensor(0.0579)\n",
      "7041 Training Loss: tensor(0.0450)\n",
      "7042 Training Loss: tensor(0.0473)\n",
      "7043 Training Loss: tensor(0.0463)\n",
      "7044 Training Loss: tensor(0.0455)\n",
      "7045 Training Loss: tensor(0.0471)\n",
      "7046 Training Loss: tensor(0.0493)\n",
      "7047 Training Loss: tensor(0.0471)\n",
      "7048 Training Loss: tensor(0.0432)\n",
      "7049 Training Loss: tensor(0.0478)\n",
      "7050 Training Loss: tensor(0.0521)\n",
      "7051 Training Loss: tensor(0.0553)\n",
      "7052 Training Loss: tensor(0.0476)\n",
      "7053 Training Loss: tensor(0.0516)\n",
      "7054 Training Loss: tensor(0.0519)\n",
      "7055 Training Loss: tensor(0.0423)\n",
      "7056 Training Loss: tensor(0.0442)\n",
      "7057 Training Loss: tensor(0.0419)\n",
      "7058 Training Loss: tensor(0.0489)\n",
      "7059 Training Loss: tensor(0.0535)\n",
      "7060 Training Loss: tensor(0.0455)\n",
      "7061 Training Loss: tensor(0.0481)\n",
      "7062 Training Loss: tensor(0.0540)\n",
      "7063 Training Loss: tensor(0.0476)\n",
      "7064 Training Loss: tensor(0.0483)\n",
      "7065 Training Loss: tensor(0.0484)\n",
      "7066 Training Loss: tensor(0.0480)\n",
      "7067 Training Loss: tensor(0.0481)\n",
      "7068 Training Loss: tensor(0.0435)\n",
      "7069 Training Loss: tensor(0.0445)\n",
      "7070 Training Loss: tensor(0.0430)\n",
      "7071 Training Loss: tensor(0.0505)\n",
      "7072 Training Loss: tensor(0.0498)\n",
      "7073 Training Loss: tensor(0.0438)\n",
      "7074 Training Loss: tensor(0.0466)\n",
      "7075 Training Loss: tensor(0.0437)\n",
      "7076 Training Loss: tensor(0.0493)\n",
      "7077 Training Loss: tensor(0.0519)\n",
      "7078 Training Loss: tensor(0.0472)\n",
      "7079 Training Loss: tensor(0.0507)\n",
      "7080 Training Loss: tensor(0.0482)\n",
      "7081 Training Loss: tensor(0.0481)\n",
      "7082 Training Loss: tensor(0.0499)\n",
      "7083 Training Loss: tensor(0.0496)\n",
      "7084 Training Loss: tensor(0.0422)\n",
      "7085 Training Loss: tensor(0.0500)\n",
      "7086 Training Loss: tensor(0.0518)\n",
      "7087 Training Loss: tensor(0.0481)\n",
      "7088 Training Loss: tensor(0.0523)\n",
      "7089 Training Loss: tensor(0.0437)\n",
      "7090 Training Loss: tensor(0.0524)\n",
      "7091 Training Loss: tensor(0.0518)\n",
      "7092 Training Loss: tensor(0.0476)\n",
      "7093 Training Loss: tensor(0.0531)\n",
      "7094 Training Loss: tensor(0.0426)\n",
      "7095 Training Loss: tensor(0.0520)\n",
      "7096 Training Loss: tensor(0.0518)\n",
      "7097 Training Loss: tensor(0.0389)\n",
      "7098 Training Loss: tensor(0.0501)\n",
      "7099 Training Loss: tensor(0.0524)\n",
      "7100 Training Loss: tensor(0.0456)\n",
      "7101 Training Loss: tensor(0.0555)\n",
      "7102 Training Loss: tensor(0.0509)\n",
      "7103 Training Loss: tensor(0.0492)\n",
      "7104 Training Loss: tensor(0.0410)\n",
      "7105 Training Loss: tensor(0.0494)\n",
      "7106 Training Loss: tensor(0.0484)\n",
      "7107 Training Loss: tensor(0.0479)\n",
      "7108 Training Loss: tensor(0.0428)\n",
      "7109 Training Loss: tensor(0.0468)\n",
      "7110 Training Loss: tensor(0.0418)\n",
      "7111 Training Loss: tensor(0.0471)\n",
      "7112 Training Loss: tensor(0.0501)\n",
      "7113 Training Loss: tensor(0.0510)\n",
      "7114 Training Loss: tensor(0.0552)\n",
      "7115 Training Loss: tensor(0.0509)\n",
      "7116 Training Loss: tensor(0.0479)\n",
      "7117 Training Loss: tensor(0.0521)\n",
      "7118 Training Loss: tensor(0.0592)\n",
      "7119 Training Loss: tensor(0.0503)\n",
      "7120 Training Loss: tensor(0.0502)\n",
      "7121 Training Loss: tensor(0.0504)\n",
      "7122 Training Loss: tensor(0.0511)\n",
      "7123 Training Loss: tensor(0.0464)\n",
      "7124 Training Loss: tensor(0.0530)\n",
      "7125 Training Loss: tensor(0.0539)\n",
      "7126 Training Loss: tensor(0.0440)\n",
      "7127 Training Loss: tensor(0.0462)\n",
      "7128 Training Loss: tensor(0.0525)\n",
      "7129 Training Loss: tensor(0.0506)\n",
      "7130 Training Loss: tensor(0.0501)\n",
      "7131 Training Loss: tensor(0.0485)\n",
      "7132 Training Loss: tensor(0.0426)\n",
      "7133 Training Loss: tensor(0.0505)\n",
      "7134 Training Loss: tensor(0.0495)\n",
      "7135 Training Loss: tensor(0.0487)\n",
      "7136 Training Loss: tensor(0.0469)\n",
      "7137 Training Loss: tensor(0.0576)\n",
      "7138 Training Loss: tensor(0.0515)\n",
      "7139 Training Loss: tensor(0.0486)\n",
      "7140 Training Loss: tensor(0.0482)\n",
      "7141 Training Loss: tensor(0.0519)\n",
      "7142 Training Loss: tensor(0.0424)\n",
      "7143 Training Loss: tensor(0.0468)\n",
      "7144 Training Loss: tensor(0.0464)\n",
      "7145 Training Loss: tensor(0.0460)\n",
      "7146 Training Loss: tensor(0.0503)\n",
      "7147 Training Loss: tensor(0.0493)\n",
      "7148 Training Loss: tensor(0.0463)\n",
      "7149 Training Loss: tensor(0.0423)\n",
      "7150 Training Loss: tensor(0.0435)\n",
      "7151 Training Loss: tensor(0.0478)\n",
      "7152 Training Loss: tensor(0.0524)\n",
      "7153 Training Loss: tensor(0.0522)\n",
      "7154 Training Loss: tensor(0.0562)\n",
      "7155 Training Loss: tensor(0.0442)\n",
      "7156 Training Loss: tensor(0.0452)\n",
      "7157 Training Loss: tensor(0.0528)\n",
      "7158 Training Loss: tensor(0.0436)\n",
      "7159 Training Loss: tensor(0.0466)\n",
      "7160 Training Loss: tensor(0.0452)\n",
      "7161 Training Loss: tensor(0.0491)\n",
      "7162 Training Loss: tensor(0.0546)\n",
      "7163 Training Loss: tensor(0.0471)\n",
      "7164 Training Loss: tensor(0.0457)\n",
      "7165 Training Loss: tensor(0.0481)\n",
      "7166 Training Loss: tensor(0.0515)\n",
      "7167 Training Loss: tensor(0.0489)\n",
      "7168 Training Loss: tensor(0.0544)\n",
      "7169 Training Loss: tensor(0.0470)\n",
      "7170 Training Loss: tensor(0.0420)\n",
      "7171 Training Loss: tensor(0.0478)\n",
      "7172 Training Loss: tensor(0.0470)\n",
      "7173 Training Loss: tensor(0.0477)\n",
      "7174 Training Loss: tensor(0.0461)\n",
      "7175 Training Loss: tensor(0.0464)\n",
      "7176 Training Loss: tensor(0.0438)\n",
      "7177 Training Loss: tensor(0.0559)\n",
      "7178 Training Loss: tensor(0.0520)\n",
      "7179 Training Loss: tensor(0.0495)\n",
      "7180 Training Loss: tensor(0.0526)\n",
      "7181 Training Loss: tensor(0.0461)\n",
      "7182 Training Loss: tensor(0.0505)\n",
      "7183 Training Loss: tensor(0.0487)\n",
      "7184 Training Loss: tensor(0.0516)\n",
      "7185 Training Loss: tensor(0.0429)\n",
      "7186 Training Loss: tensor(0.0427)\n",
      "7187 Training Loss: tensor(0.0459)\n",
      "7188 Training Loss: tensor(0.0501)\n",
      "7189 Training Loss: tensor(0.0479)\n",
      "7190 Training Loss: tensor(0.0456)\n",
      "7191 Training Loss: tensor(0.0529)\n",
      "7192 Training Loss: tensor(0.0435)\n",
      "7193 Training Loss: tensor(0.0448)\n",
      "7194 Training Loss: tensor(0.0470)\n",
      "7195 Training Loss: tensor(0.0453)\n",
      "7196 Training Loss: tensor(0.0512)\n",
      "7197 Training Loss: tensor(0.0429)\n",
      "7198 Training Loss: tensor(0.0464)\n",
      "7199 Training Loss: tensor(0.0496)\n",
      "7200 Training Loss: tensor(0.0469)\n",
      "7201 Training Loss: tensor(0.0511)\n",
      "7202 Training Loss: tensor(0.0434)\n",
      "7203 Training Loss: tensor(0.0480)\n",
      "7204 Training Loss: tensor(0.0451)\n",
      "7205 Training Loss: tensor(0.0492)\n",
      "7206 Training Loss: tensor(0.0501)\n",
      "7207 Training Loss: tensor(0.0407)\n",
      "7208 Training Loss: tensor(0.0492)\n",
      "7209 Training Loss: tensor(0.0441)\n",
      "7210 Training Loss: tensor(0.0437)\n",
      "7211 Training Loss: tensor(0.0469)\n",
      "7212 Training Loss: tensor(0.0463)\n",
      "7213 Training Loss: tensor(0.0535)\n",
      "7214 Training Loss: tensor(0.0521)\n",
      "7215 Training Loss: tensor(0.0514)\n",
      "7216 Training Loss: tensor(0.0425)\n",
      "7217 Training Loss: tensor(0.0517)\n",
      "7218 Training Loss: tensor(0.0493)\n",
      "7219 Training Loss: tensor(0.0485)\n",
      "7220 Training Loss: tensor(0.0491)\n",
      "7221 Training Loss: tensor(0.0460)\n",
      "7222 Training Loss: tensor(0.0453)\n",
      "7223 Training Loss: tensor(0.0497)\n",
      "7224 Training Loss: tensor(0.0438)\n",
      "7225 Training Loss: tensor(0.0516)\n",
      "7226 Training Loss: tensor(0.0446)\n",
      "7227 Training Loss: tensor(0.0498)\n",
      "7228 Training Loss: tensor(0.0503)\n",
      "7229 Training Loss: tensor(0.0510)\n",
      "7230 Training Loss: tensor(0.0454)\n",
      "7231 Training Loss: tensor(0.0486)\n",
      "7232 Training Loss: tensor(0.0457)\n",
      "7233 Training Loss: tensor(0.0452)\n",
      "7234 Training Loss: tensor(0.0474)\n",
      "7235 Training Loss: tensor(0.0496)\n",
      "7236 Training Loss: tensor(0.0459)\n",
      "7237 Training Loss: tensor(0.0500)\n",
      "7238 Training Loss: tensor(0.0527)\n",
      "7239 Training Loss: tensor(0.0460)\n",
      "7240 Training Loss: tensor(0.0494)\n",
      "7241 Training Loss: tensor(0.0469)\n",
      "7242 Training Loss: tensor(0.0439)\n",
      "7243 Training Loss: tensor(0.0461)\n",
      "7244 Training Loss: tensor(0.0483)\n",
      "7245 Training Loss: tensor(0.0462)\n",
      "7246 Training Loss: tensor(0.0456)\n",
      "7247 Training Loss: tensor(0.0436)\n",
      "7248 Training Loss: tensor(0.0471)\n",
      "7249 Training Loss: tensor(0.0496)\n",
      "7250 Training Loss: tensor(0.0470)\n",
      "7251 Training Loss: tensor(0.0503)\n",
      "7252 Training Loss: tensor(0.0527)\n",
      "7253 Training Loss: tensor(0.0431)\n",
      "7254 Training Loss: tensor(0.0442)\n",
      "7255 Training Loss: tensor(0.0461)\n",
      "7256 Training Loss: tensor(0.0497)\n",
      "7257 Training Loss: tensor(0.0481)\n",
      "7258 Training Loss: tensor(0.0467)\n",
      "7259 Training Loss: tensor(0.0420)\n",
      "7260 Training Loss: tensor(0.0474)\n",
      "7261 Training Loss: tensor(0.0474)\n",
      "7262 Training Loss: tensor(0.0439)\n",
      "7263 Training Loss: tensor(0.0482)\n",
      "7264 Training Loss: tensor(0.0459)\n",
      "7265 Training Loss: tensor(0.0507)\n",
      "7266 Training Loss: tensor(0.0472)\n",
      "7267 Training Loss: tensor(0.0540)\n",
      "7268 Training Loss: tensor(0.0457)\n",
      "7269 Training Loss: tensor(0.0478)\n",
      "7270 Training Loss: tensor(0.0451)\n",
      "7271 Training Loss: tensor(0.0501)\n",
      "7272 Training Loss: tensor(0.0523)\n",
      "7273 Training Loss: tensor(0.0551)\n",
      "7274 Training Loss: tensor(0.0410)\n",
      "7275 Training Loss: tensor(0.0476)\n",
      "7276 Training Loss: tensor(0.0352)\n",
      "7277 Training Loss: tensor(0.0486)\n",
      "7278 Training Loss: tensor(0.0525)\n",
      "7279 Training Loss: tensor(0.0469)\n",
      "7280 Training Loss: tensor(0.0518)\n",
      "7281 Training Loss: tensor(0.0438)\n",
      "7282 Training Loss: tensor(0.0534)\n",
      "7283 Training Loss: tensor(0.0515)\n",
      "7284 Training Loss: tensor(0.0502)\n",
      "7285 Training Loss: tensor(0.0470)\n",
      "7286 Training Loss: tensor(0.0500)\n",
      "7287 Training Loss: tensor(0.0481)\n",
      "7288 Training Loss: tensor(0.0453)\n",
      "7289 Training Loss: tensor(0.0485)\n",
      "7290 Training Loss: tensor(0.0488)\n",
      "7291 Training Loss: tensor(0.0480)\n",
      "7292 Training Loss: tensor(0.0506)\n",
      "7293 Training Loss: tensor(0.0475)\n",
      "7294 Training Loss: tensor(0.0398)\n",
      "7295 Training Loss: tensor(0.0448)\n",
      "7296 Training Loss: tensor(0.0450)\n",
      "7297 Training Loss: tensor(0.0488)\n",
      "7298 Training Loss: tensor(0.0456)\n",
      "7299 Training Loss: tensor(0.0398)\n",
      "7300 Training Loss: tensor(0.0474)\n",
      "7301 Training Loss: tensor(0.0469)\n",
      "7302 Training Loss: tensor(0.0427)\n",
      "7303 Training Loss: tensor(0.0473)\n",
      "7304 Training Loss: tensor(0.0490)\n",
      "7305 Training Loss: tensor(0.0544)\n",
      "7306 Training Loss: tensor(0.0532)\n",
      "7307 Training Loss: tensor(0.0430)\n",
      "7308 Training Loss: tensor(0.0491)\n",
      "7309 Training Loss: tensor(0.0519)\n",
      "7310 Training Loss: tensor(0.0403)\n",
      "7311 Training Loss: tensor(0.0482)\n",
      "7312 Training Loss: tensor(0.0502)\n",
      "7313 Training Loss: tensor(0.0464)\n",
      "7314 Training Loss: tensor(0.0450)\n",
      "7315 Training Loss: tensor(0.0541)\n",
      "7316 Training Loss: tensor(0.0453)\n",
      "7317 Training Loss: tensor(0.0376)\n",
      "7318 Training Loss: tensor(0.0479)\n",
      "7319 Training Loss: tensor(0.0527)\n",
      "7320 Training Loss: tensor(0.0400)\n",
      "7321 Training Loss: tensor(0.0460)\n",
      "7322 Training Loss: tensor(0.0500)\n",
      "7323 Training Loss: tensor(0.0506)\n",
      "7324 Training Loss: tensor(0.0467)\n",
      "7325 Training Loss: tensor(0.0479)\n",
      "7326 Training Loss: tensor(0.0515)\n",
      "7327 Training Loss: tensor(0.0442)\n",
      "7328 Training Loss: tensor(0.0482)\n",
      "7329 Training Loss: tensor(0.0504)\n",
      "7330 Training Loss: tensor(0.0516)\n",
      "7331 Training Loss: tensor(0.0418)\n",
      "7332 Training Loss: tensor(0.0428)\n",
      "7333 Training Loss: tensor(0.0457)\n",
      "7334 Training Loss: tensor(0.0462)\n",
      "7335 Training Loss: tensor(0.0498)\n",
      "7336 Training Loss: tensor(0.0478)\n",
      "7337 Training Loss: tensor(0.0485)\n",
      "7338 Training Loss: tensor(0.0385)\n",
      "7339 Training Loss: tensor(0.0456)\n",
      "7340 Training Loss: tensor(0.0463)\n",
      "7341 Training Loss: tensor(0.0430)\n",
      "7342 Training Loss: tensor(0.0532)\n",
      "7343 Training Loss: tensor(0.0429)\n",
      "7344 Training Loss: tensor(0.0505)\n",
      "7345 Training Loss: tensor(0.0422)\n",
      "7346 Training Loss: tensor(0.0455)\n",
      "7347 Training Loss: tensor(0.0450)\n",
      "7348 Training Loss: tensor(0.0452)\n",
      "7349 Training Loss: tensor(0.0460)\n",
      "7350 Training Loss: tensor(0.0483)\n",
      "7351 Training Loss: tensor(0.0467)\n",
      "7352 Training Loss: tensor(0.0468)\n",
      "7353 Training Loss: tensor(0.0459)\n",
      "7354 Training Loss: tensor(0.0458)\n",
      "7355 Training Loss: tensor(0.0463)\n",
      "7356 Training Loss: tensor(0.0451)\n",
      "7357 Training Loss: tensor(0.0463)\n",
      "7358 Training Loss: tensor(0.0510)\n",
      "7359 Training Loss: tensor(0.0489)\n",
      "7360 Training Loss: tensor(0.0436)\n",
      "7361 Training Loss: tensor(0.0453)\n",
      "7362 Training Loss: tensor(0.0454)\n",
      "7363 Training Loss: tensor(0.0496)\n",
      "7364 Training Loss: tensor(0.0503)\n",
      "7365 Training Loss: tensor(0.0498)\n",
      "7366 Training Loss: tensor(0.0402)\n",
      "7367 Training Loss: tensor(0.0505)\n",
      "7368 Training Loss: tensor(0.0488)\n",
      "7369 Training Loss: tensor(0.0438)\n",
      "7370 Training Loss: tensor(0.0467)\n",
      "7371 Training Loss: tensor(0.0383)\n",
      "7372 Training Loss: tensor(0.0547)\n",
      "7373 Training Loss: tensor(0.0439)\n",
      "7374 Training Loss: tensor(0.0489)\n",
      "7375 Training Loss: tensor(0.0452)\n",
      "7376 Training Loss: tensor(0.0467)\n",
      "7377 Training Loss: tensor(0.0516)\n",
      "7378 Training Loss: tensor(0.0524)\n",
      "7379 Training Loss: tensor(0.0489)\n",
      "7380 Training Loss: tensor(0.0427)\n",
      "7381 Training Loss: tensor(0.0456)\n",
      "7382 Training Loss: tensor(0.0459)\n",
      "7383 Training Loss: tensor(0.0474)\n",
      "7384 Training Loss: tensor(0.0520)\n",
      "7385 Training Loss: tensor(0.0420)\n",
      "7386 Training Loss: tensor(0.0481)\n",
      "7387 Training Loss: tensor(0.0409)\n",
      "7388 Training Loss: tensor(0.0488)\n",
      "7389 Training Loss: tensor(0.0440)\n",
      "7390 Training Loss: tensor(0.0418)\n",
      "7391 Training Loss: tensor(0.0467)\n",
      "7392 Training Loss: tensor(0.0492)\n",
      "7393 Training Loss: tensor(0.0524)\n",
      "7394 Training Loss: tensor(0.0421)\n",
      "7395 Training Loss: tensor(0.0468)\n",
      "7396 Training Loss: tensor(0.0500)\n",
      "7397 Training Loss: tensor(0.0461)\n",
      "7398 Training Loss: tensor(0.0484)\n",
      "7399 Training Loss: tensor(0.0441)\n",
      "7400 Training Loss: tensor(0.0540)\n",
      "7401 Training Loss: tensor(0.0430)\n",
      "7402 Training Loss: tensor(0.0468)\n",
      "7403 Training Loss: tensor(0.0482)\n",
      "7404 Training Loss: tensor(0.0456)\n",
      "7405 Training Loss: tensor(0.0441)\n",
      "7406 Training Loss: tensor(0.0412)\n",
      "7407 Training Loss: tensor(0.0460)\n",
      "7408 Training Loss: tensor(0.0502)\n",
      "7409 Training Loss: tensor(0.0475)\n",
      "7410 Training Loss: tensor(0.0424)\n",
      "7411 Training Loss: tensor(0.0466)\n",
      "7412 Training Loss: tensor(0.0449)\n",
      "7413 Training Loss: tensor(0.0453)\n",
      "7414 Training Loss: tensor(0.0441)\n",
      "7415 Training Loss: tensor(0.0455)\n",
      "7416 Training Loss: tensor(0.0479)\n",
      "7417 Training Loss: tensor(0.0473)\n",
      "7418 Training Loss: tensor(0.0490)\n",
      "7419 Training Loss: tensor(0.0549)\n",
      "7420 Training Loss: tensor(0.0507)\n",
      "7421 Training Loss: tensor(0.0449)\n",
      "7422 Training Loss: tensor(0.0491)\n",
      "7423 Training Loss: tensor(0.0471)\n",
      "7424 Training Loss: tensor(0.0454)\n",
      "7425 Training Loss: tensor(0.0471)\n",
      "7426 Training Loss: tensor(0.0450)\n",
      "7427 Training Loss: tensor(0.0503)\n",
      "7428 Training Loss: tensor(0.0472)\n",
      "7429 Training Loss: tensor(0.0448)\n",
      "7430 Training Loss: tensor(0.0448)\n",
      "7431 Training Loss: tensor(0.0448)\n",
      "7432 Training Loss: tensor(0.0443)\n",
      "7433 Training Loss: tensor(0.0503)\n",
      "7434 Training Loss: tensor(0.0448)\n",
      "7435 Training Loss: tensor(0.0508)\n",
      "7436 Training Loss: tensor(0.0486)\n",
      "7437 Training Loss: tensor(0.0538)\n",
      "7438 Training Loss: tensor(0.0464)\n",
      "7439 Training Loss: tensor(0.0438)\n",
      "7440 Training Loss: tensor(0.0493)\n",
      "7441 Training Loss: tensor(0.0499)\n",
      "7442 Training Loss: tensor(0.0450)\n",
      "7443 Training Loss: tensor(0.0500)\n",
      "7444 Training Loss: tensor(0.0463)\n",
      "7445 Training Loss: tensor(0.0450)\n",
      "7446 Training Loss: tensor(0.0517)\n",
      "7447 Training Loss: tensor(0.0491)\n",
      "7448 Training Loss: tensor(0.0457)\n",
      "7449 Training Loss: tensor(0.0466)\n",
      "7450 Training Loss: tensor(0.0475)\n",
      "7451 Training Loss: tensor(0.0435)\n",
      "7452 Training Loss: tensor(0.0469)\n",
      "7453 Training Loss: tensor(0.0523)\n",
      "7454 Training Loss: tensor(0.0460)\n",
      "7455 Training Loss: tensor(0.0471)\n",
      "7456 Training Loss: tensor(0.0389)\n",
      "7457 Training Loss: tensor(0.0493)\n",
      "7458 Training Loss: tensor(0.0502)\n",
      "7459 Training Loss: tensor(0.0546)\n",
      "7460 Training Loss: tensor(0.0514)\n",
      "7461 Training Loss: tensor(0.0458)\n",
      "7462 Training Loss: tensor(0.0453)\n",
      "7463 Training Loss: tensor(0.0463)\n",
      "7464 Training Loss: tensor(0.0383)\n",
      "7465 Training Loss: tensor(0.0516)\n",
      "7466 Training Loss: tensor(0.0479)\n",
      "7467 Training Loss: tensor(0.0525)\n",
      "7468 Training Loss: tensor(0.0445)\n",
      "7469 Training Loss: tensor(0.0467)\n",
      "7470 Training Loss: tensor(0.0445)\n",
      "7471 Training Loss: tensor(0.0416)\n",
      "7472 Training Loss: tensor(0.0488)\n",
      "7473 Training Loss: tensor(0.0456)\n",
      "7474 Training Loss: tensor(0.0494)\n",
      "7475 Training Loss: tensor(0.0435)\n",
      "7476 Training Loss: tensor(0.0511)\n",
      "7477 Training Loss: tensor(0.0448)\n",
      "7478 Training Loss: tensor(0.0510)\n",
      "7479 Training Loss: tensor(0.0444)\n",
      "7480 Training Loss: tensor(0.0485)\n",
      "7481 Training Loss: tensor(0.0518)\n",
      "7482 Training Loss: tensor(0.0459)\n",
      "7483 Training Loss: tensor(0.0450)\n",
      "7484 Training Loss: tensor(0.0498)\n",
      "7485 Training Loss: tensor(0.0481)\n",
      "7486 Training Loss: tensor(0.0478)\n",
      "7487 Training Loss: tensor(0.0468)\n",
      "7488 Training Loss: tensor(0.0413)\n",
      "7489 Training Loss: tensor(0.0495)\n",
      "7490 Training Loss: tensor(0.0479)\n",
      "7491 Training Loss: tensor(0.0420)\n",
      "7492 Training Loss: tensor(0.0431)\n",
      "7493 Training Loss: tensor(0.0412)\n",
      "7494 Training Loss: tensor(0.0501)\n",
      "7495 Training Loss: tensor(0.0427)\n",
      "7496 Training Loss: tensor(0.0498)\n",
      "7497 Training Loss: tensor(0.0380)\n",
      "7498 Training Loss: tensor(0.0474)\n",
      "7499 Training Loss: tensor(0.0417)\n",
      "7500 Training Loss: tensor(0.0447)\n",
      "7501 Training Loss: tensor(0.0512)\n",
      "7502 Training Loss: tensor(0.0473)\n",
      "7503 Training Loss: tensor(0.0462)\n",
      "7504 Training Loss: tensor(0.0457)\n",
      "7505 Training Loss: tensor(0.0427)\n",
      "7506 Training Loss: tensor(0.0463)\n",
      "7507 Training Loss: tensor(0.0466)\n",
      "7508 Training Loss: tensor(0.0415)\n",
      "7509 Training Loss: tensor(0.0452)\n",
      "7510 Training Loss: tensor(0.0420)\n",
      "7511 Training Loss: tensor(0.0491)\n",
      "7512 Training Loss: tensor(0.0512)\n",
      "7513 Training Loss: tensor(0.0466)\n",
      "7514 Training Loss: tensor(0.0498)\n",
      "7515 Training Loss: tensor(0.0453)\n",
      "7516 Training Loss: tensor(0.0433)\n",
      "7517 Training Loss: tensor(0.0419)\n",
      "7518 Training Loss: tensor(0.0504)\n",
      "7519 Training Loss: tensor(0.0457)\n",
      "7520 Training Loss: tensor(0.0429)\n",
      "7521 Training Loss: tensor(0.0531)\n",
      "7522 Training Loss: tensor(0.0444)\n",
      "7523 Training Loss: tensor(0.0440)\n",
      "7524 Training Loss: tensor(0.0469)\n",
      "7525 Training Loss: tensor(0.0451)\n",
      "7526 Training Loss: tensor(0.0452)\n",
      "7527 Training Loss: tensor(0.0441)\n",
      "7528 Training Loss: tensor(0.0510)\n",
      "7529 Training Loss: tensor(0.0503)\n",
      "7530 Training Loss: tensor(0.0461)\n",
      "7531 Training Loss: tensor(0.0420)\n",
      "7532 Training Loss: tensor(0.0446)\n",
      "7533 Training Loss: tensor(0.0445)\n",
      "7534 Training Loss: tensor(0.0434)\n",
      "7535 Training Loss: tensor(0.0468)\n",
      "7536 Training Loss: tensor(0.0468)\n",
      "7537 Training Loss: tensor(0.0453)\n",
      "7538 Training Loss: tensor(0.0474)\n",
      "7539 Training Loss: tensor(0.0514)\n",
      "7540 Training Loss: tensor(0.0443)\n",
      "7541 Training Loss: tensor(0.0486)\n",
      "7542 Training Loss: tensor(0.0461)\n",
      "7543 Training Loss: tensor(0.0510)\n",
      "7544 Training Loss: tensor(0.0440)\n",
      "7545 Training Loss: tensor(0.0452)\n",
      "7546 Training Loss: tensor(0.0455)\n",
      "7547 Training Loss: tensor(0.0439)\n",
      "7548 Training Loss: tensor(0.0472)\n",
      "7549 Training Loss: tensor(0.0530)\n",
      "7550 Training Loss: tensor(0.0489)\n",
      "7551 Training Loss: tensor(0.0489)\n",
      "7552 Training Loss: tensor(0.0467)\n",
      "7553 Training Loss: tensor(0.0486)\n",
      "7554 Training Loss: tensor(0.0483)\n",
      "7555 Training Loss: tensor(0.0482)\n",
      "7556 Training Loss: tensor(0.0446)\n",
      "7557 Training Loss: tensor(0.0416)\n",
      "7558 Training Loss: tensor(0.0505)\n",
      "7559 Training Loss: tensor(0.0472)\n",
      "7560 Training Loss: tensor(0.0427)\n",
      "7561 Training Loss: tensor(0.0509)\n",
      "7562 Training Loss: tensor(0.0514)\n",
      "7563 Training Loss: tensor(0.0451)\n",
      "7564 Training Loss: tensor(0.0428)\n",
      "7565 Training Loss: tensor(0.0440)\n",
      "7566 Training Loss: tensor(0.0466)\n",
      "7567 Training Loss: tensor(0.0490)\n",
      "7568 Training Loss: tensor(0.0500)\n",
      "7569 Training Loss: tensor(0.0505)\n",
      "7570 Training Loss: tensor(0.0480)\n",
      "7571 Training Loss: tensor(0.0443)\n",
      "7572 Training Loss: tensor(0.0433)\n",
      "7573 Training Loss: tensor(0.0432)\n",
      "7574 Training Loss: tensor(0.0530)\n",
      "7575 Training Loss: tensor(0.0456)\n",
      "7576 Training Loss: tensor(0.0530)\n",
      "7577 Training Loss: tensor(0.0445)\n",
      "7578 Training Loss: tensor(0.0497)\n",
      "7579 Training Loss: tensor(0.0433)\n",
      "7580 Training Loss: tensor(0.0484)\n",
      "7581 Training Loss: tensor(0.0434)\n",
      "7582 Training Loss: tensor(0.0432)\n",
      "7583 Training Loss: tensor(0.0481)\n",
      "7584 Training Loss: tensor(0.0492)\n",
      "7585 Training Loss: tensor(0.0465)\n",
      "7586 Training Loss: tensor(0.0460)\n",
      "7587 Training Loss: tensor(0.0508)\n",
      "7588 Training Loss: tensor(0.0472)\n",
      "7589 Training Loss: tensor(0.0454)\n",
      "7590 Training Loss: tensor(0.0447)\n",
      "7591 Training Loss: tensor(0.0482)\n",
      "7592 Training Loss: tensor(0.0506)\n",
      "7593 Training Loss: tensor(0.0404)\n",
      "7594 Training Loss: tensor(0.0489)\n",
      "7595 Training Loss: tensor(0.0448)\n",
      "7596 Training Loss: tensor(0.0503)\n",
      "7597 Training Loss: tensor(0.0474)\n",
      "7598 Training Loss: tensor(0.0434)\n",
      "7599 Training Loss: tensor(0.0485)\n",
      "7600 Training Loss: tensor(0.0424)\n",
      "7601 Training Loss: tensor(0.0442)\n",
      "7602 Training Loss: tensor(0.0486)\n",
      "7603 Training Loss: tensor(0.0454)\n",
      "7604 Training Loss: tensor(0.0462)\n",
      "7605 Training Loss: tensor(0.0423)\n",
      "7606 Training Loss: tensor(0.0491)\n",
      "7607 Training Loss: tensor(0.0512)\n",
      "7608 Training Loss: tensor(0.0483)\n",
      "7609 Training Loss: tensor(0.0411)\n",
      "7610 Training Loss: tensor(0.0411)\n",
      "7611 Training Loss: tensor(0.0477)\n",
      "7612 Training Loss: tensor(0.0453)\n",
      "7613 Training Loss: tensor(0.0464)\n",
      "7614 Training Loss: tensor(0.0485)\n",
      "7615 Training Loss: tensor(0.0410)\n",
      "7616 Training Loss: tensor(0.0437)\n",
      "7617 Training Loss: tensor(0.0476)\n",
      "7618 Training Loss: tensor(0.0461)\n",
      "7619 Training Loss: tensor(0.0476)\n",
      "7620 Training Loss: tensor(0.0418)\n",
      "7621 Training Loss: tensor(0.0491)\n",
      "7622 Training Loss: tensor(0.0499)\n",
      "7623 Training Loss: tensor(0.0448)\n",
      "7624 Training Loss: tensor(0.0450)\n",
      "7625 Training Loss: tensor(0.0442)\n",
      "7626 Training Loss: tensor(0.0447)\n",
      "7627 Training Loss: tensor(0.0506)\n",
      "7628 Training Loss: tensor(0.0501)\n",
      "7629 Training Loss: tensor(0.0556)\n",
      "7630 Training Loss: tensor(0.0477)\n",
      "7631 Training Loss: tensor(0.0478)\n",
      "7632 Training Loss: tensor(0.0456)\n",
      "7633 Training Loss: tensor(0.0468)\n",
      "7634 Training Loss: tensor(0.0416)\n",
      "7635 Training Loss: tensor(0.0479)\n",
      "7636 Training Loss: tensor(0.0469)\n",
      "7637 Training Loss: tensor(0.0397)\n",
      "7638 Training Loss: tensor(0.0474)\n",
      "7639 Training Loss: tensor(0.0460)\n",
      "7640 Training Loss: tensor(0.0479)\n",
      "7641 Training Loss: tensor(0.0464)\n",
      "7642 Training Loss: tensor(0.0445)\n",
      "7643 Training Loss: tensor(0.0403)\n",
      "7644 Training Loss: tensor(0.0441)\n",
      "7645 Training Loss: tensor(0.0428)\n",
      "7646 Training Loss: tensor(0.0434)\n",
      "7647 Training Loss: tensor(0.0446)\n",
      "7648 Training Loss: tensor(0.0420)\n",
      "7649 Training Loss: tensor(0.0479)\n",
      "7650 Training Loss: tensor(0.0450)\n",
      "7651 Training Loss: tensor(0.0446)\n",
      "7652 Training Loss: tensor(0.0473)\n",
      "7653 Training Loss: tensor(0.0486)\n",
      "7654 Training Loss: tensor(0.0463)\n",
      "7655 Training Loss: tensor(0.0484)\n",
      "7656 Training Loss: tensor(0.0456)\n",
      "7657 Training Loss: tensor(0.0512)\n",
      "7658 Training Loss: tensor(0.0456)\n",
      "7659 Training Loss: tensor(0.0502)\n",
      "7660 Training Loss: tensor(0.0419)\n",
      "7661 Training Loss: tensor(0.0511)\n",
      "7662 Training Loss: tensor(0.0441)\n",
      "7663 Training Loss: tensor(0.0448)\n",
      "7664 Training Loss: tensor(0.0525)\n",
      "7665 Training Loss: tensor(0.0421)\n",
      "7666 Training Loss: tensor(0.0477)\n",
      "7667 Training Loss: tensor(0.0474)\n",
      "7668 Training Loss: tensor(0.0514)\n",
      "7669 Training Loss: tensor(0.0411)\n",
      "7670 Training Loss: tensor(0.0483)\n",
      "7671 Training Loss: tensor(0.0504)\n",
      "7672 Training Loss: tensor(0.0472)\n",
      "7673 Training Loss: tensor(0.0475)\n",
      "7674 Training Loss: tensor(0.0447)\n",
      "7675 Training Loss: tensor(0.0525)\n",
      "7676 Training Loss: tensor(0.0464)\n",
      "7677 Training Loss: tensor(0.0487)\n",
      "7678 Training Loss: tensor(0.0516)\n",
      "7679 Training Loss: tensor(0.0404)\n",
      "7680 Training Loss: tensor(0.0456)\n",
      "7681 Training Loss: tensor(0.0408)\n",
      "7682 Training Loss: tensor(0.0426)\n",
      "7683 Training Loss: tensor(0.0495)\n",
      "7684 Training Loss: tensor(0.0520)\n",
      "7685 Training Loss: tensor(0.0399)\n",
      "7686 Training Loss: tensor(0.0417)\n",
      "7687 Training Loss: tensor(0.0496)\n",
      "7688 Training Loss: tensor(0.0465)\n",
      "7689 Training Loss: tensor(0.0462)\n",
      "7690 Training Loss: tensor(0.0417)\n",
      "7691 Training Loss: tensor(0.0449)\n",
      "7692 Training Loss: tensor(0.0454)\n",
      "7693 Training Loss: tensor(0.0424)\n",
      "7694 Training Loss: tensor(0.0478)\n",
      "7695 Training Loss: tensor(0.0478)\n",
      "7696 Training Loss: tensor(0.0460)\n",
      "7697 Training Loss: tensor(0.0377)\n",
      "7698 Training Loss: tensor(0.0433)\n",
      "7699 Training Loss: tensor(0.0482)\n",
      "7700 Training Loss: tensor(0.0413)\n",
      "7701 Training Loss: tensor(0.0483)\n",
      "7702 Training Loss: tensor(0.0457)\n",
      "7703 Training Loss: tensor(0.0447)\n",
      "7704 Training Loss: tensor(0.0468)\n",
      "7705 Training Loss: tensor(0.0429)\n",
      "7706 Training Loss: tensor(0.0459)\n",
      "7707 Training Loss: tensor(0.0481)\n",
      "7708 Training Loss: tensor(0.0487)\n",
      "7709 Training Loss: tensor(0.0477)\n",
      "7710 Training Loss: tensor(0.0446)\n",
      "7711 Training Loss: tensor(0.0503)\n",
      "7712 Training Loss: tensor(0.0423)\n",
      "7713 Training Loss: tensor(0.0427)\n",
      "7714 Training Loss: tensor(0.0480)\n",
      "7715 Training Loss: tensor(0.0488)\n",
      "7716 Training Loss: tensor(0.0458)\n",
      "7717 Training Loss: tensor(0.0474)\n",
      "7718 Training Loss: tensor(0.0455)\n",
      "7719 Training Loss: tensor(0.0516)\n",
      "7720 Training Loss: tensor(0.0479)\n",
      "7721 Training Loss: tensor(0.0460)\n",
      "7722 Training Loss: tensor(0.0439)\n",
      "7723 Training Loss: tensor(0.0479)\n",
      "7724 Training Loss: tensor(0.0470)\n",
      "7725 Training Loss: tensor(0.0458)\n",
      "7726 Training Loss: tensor(0.0479)\n",
      "7727 Training Loss: tensor(0.0482)\n",
      "7728 Training Loss: tensor(0.0421)\n",
      "7729 Training Loss: tensor(0.0409)\n",
      "7730 Training Loss: tensor(0.0500)\n",
      "7731 Training Loss: tensor(0.0441)\n",
      "7732 Training Loss: tensor(0.0424)\n",
      "7733 Training Loss: tensor(0.0482)\n",
      "7734 Training Loss: tensor(0.0453)\n",
      "7735 Training Loss: tensor(0.0421)\n",
      "7736 Training Loss: tensor(0.0445)\n",
      "7737 Training Loss: tensor(0.0396)\n",
      "7738 Training Loss: tensor(0.0490)\n",
      "7739 Training Loss: tensor(0.0463)\n",
      "7740 Training Loss: tensor(0.0401)\n",
      "7741 Training Loss: tensor(0.0482)\n",
      "7742 Training Loss: tensor(0.0493)\n",
      "7743 Training Loss: tensor(0.0447)\n",
      "7744 Training Loss: tensor(0.0504)\n",
      "7745 Training Loss: tensor(0.0419)\n",
      "7746 Training Loss: tensor(0.0464)\n",
      "7747 Training Loss: tensor(0.0399)\n",
      "7748 Training Loss: tensor(0.0463)\n",
      "7749 Training Loss: tensor(0.0535)\n",
      "7750 Training Loss: tensor(0.0435)\n",
      "7751 Training Loss: tensor(0.0473)\n",
      "7752 Training Loss: tensor(0.0458)\n",
      "7753 Training Loss: tensor(0.0453)\n",
      "7754 Training Loss: tensor(0.0497)\n",
      "7755 Training Loss: tensor(0.0448)\n",
      "7756 Training Loss: tensor(0.0474)\n",
      "7757 Training Loss: tensor(0.0448)\n",
      "7758 Training Loss: tensor(0.0439)\n",
      "7759 Training Loss: tensor(0.0442)\n",
      "7760 Training Loss: tensor(0.0461)\n",
      "7761 Training Loss: tensor(0.0464)\n",
      "7762 Training Loss: tensor(0.0405)\n",
      "7763 Training Loss: tensor(0.0452)\n",
      "7764 Training Loss: tensor(0.0470)\n",
      "7765 Training Loss: tensor(0.0448)\n",
      "7766 Training Loss: tensor(0.0487)\n",
      "7767 Training Loss: tensor(0.0455)\n",
      "7768 Training Loss: tensor(0.0502)\n",
      "7769 Training Loss: tensor(0.0495)\n",
      "7770 Training Loss: tensor(0.0524)\n",
      "7771 Training Loss: tensor(0.0574)\n",
      "7772 Training Loss: tensor(0.0477)\n",
      "7773 Training Loss: tensor(0.0458)\n",
      "7774 Training Loss: tensor(0.0522)\n",
      "7775 Training Loss: tensor(0.0440)\n",
      "7776 Training Loss: tensor(0.0430)\n",
      "7777 Training Loss: tensor(0.0463)\n",
      "7778 Training Loss: tensor(0.0482)\n",
      "7779 Training Loss: tensor(0.0472)\n",
      "7780 Training Loss: tensor(0.0452)\n",
      "7781 Training Loss: tensor(0.0482)\n",
      "7782 Training Loss: tensor(0.0445)\n",
      "7783 Training Loss: tensor(0.0487)\n",
      "7784 Training Loss: tensor(0.0401)\n",
      "7785 Training Loss: tensor(0.0539)\n",
      "7786 Training Loss: tensor(0.0465)\n",
      "7787 Training Loss: tensor(0.0476)\n",
      "7788 Training Loss: tensor(0.0453)\n",
      "7789 Training Loss: tensor(0.0456)\n",
      "7790 Training Loss: tensor(0.0400)\n",
      "7791 Training Loss: tensor(0.0476)\n",
      "7792 Training Loss: tensor(0.0476)\n",
      "7793 Training Loss: tensor(0.0384)\n",
      "7794 Training Loss: tensor(0.0461)\n",
      "7795 Training Loss: tensor(0.0460)\n",
      "7796 Training Loss: tensor(0.0449)\n",
      "7797 Training Loss: tensor(0.0479)\n",
      "7798 Training Loss: tensor(0.0421)\n",
      "7799 Training Loss: tensor(0.0464)\n",
      "7800 Training Loss: tensor(0.0459)\n",
      "7801 Training Loss: tensor(0.0428)\n",
      "7802 Training Loss: tensor(0.0491)\n",
      "7803 Training Loss: tensor(0.0444)\n",
      "7804 Training Loss: tensor(0.0445)\n",
      "7805 Training Loss: tensor(0.0501)\n",
      "7806 Training Loss: tensor(0.0488)\n",
      "7807 Training Loss: tensor(0.0407)\n",
      "7808 Training Loss: tensor(0.0429)\n",
      "7809 Training Loss: tensor(0.0422)\n",
      "7810 Training Loss: tensor(0.0457)\n",
      "7811 Training Loss: tensor(0.0512)\n",
      "7812 Training Loss: tensor(0.0466)\n",
      "7813 Training Loss: tensor(0.0440)\n",
      "7814 Training Loss: tensor(0.0455)\n",
      "7815 Training Loss: tensor(0.0447)\n",
      "7816 Training Loss: tensor(0.0452)\n",
      "7817 Training Loss: tensor(0.0453)\n",
      "7818 Training Loss: tensor(0.0484)\n",
      "7819 Training Loss: tensor(0.0441)\n",
      "7820 Training Loss: tensor(0.0496)\n",
      "7821 Training Loss: tensor(0.0449)\n",
      "7822 Training Loss: tensor(0.0462)\n",
      "7823 Training Loss: tensor(0.0484)\n",
      "7824 Training Loss: tensor(0.0512)\n",
      "7825 Training Loss: tensor(0.0472)\n",
      "7826 Training Loss: tensor(0.0448)\n",
      "7827 Training Loss: tensor(0.0466)\n",
      "7828 Training Loss: tensor(0.0517)\n",
      "7829 Training Loss: tensor(0.0453)\n",
      "7830 Training Loss: tensor(0.0502)\n",
      "7831 Training Loss: tensor(0.0475)\n",
      "7832 Training Loss: tensor(0.0401)\n",
      "7833 Training Loss: tensor(0.0565)\n",
      "7834 Training Loss: tensor(0.0453)\n",
      "7835 Training Loss: tensor(0.0463)\n",
      "7836 Training Loss: tensor(0.0468)\n",
      "7837 Training Loss: tensor(0.0475)\n",
      "7838 Training Loss: tensor(0.0474)\n",
      "7839 Training Loss: tensor(0.0480)\n",
      "7840 Training Loss: tensor(0.0451)\n",
      "7841 Training Loss: tensor(0.0489)\n",
      "7842 Training Loss: tensor(0.0412)\n",
      "7843 Training Loss: tensor(0.0456)\n",
      "7844 Training Loss: tensor(0.0458)\n",
      "7845 Training Loss: tensor(0.0468)\n",
      "7846 Training Loss: tensor(0.0470)\n",
      "7847 Training Loss: tensor(0.0479)\n",
      "7848 Training Loss: tensor(0.0425)\n",
      "7849 Training Loss: tensor(0.0491)\n",
      "7850 Training Loss: tensor(0.0464)\n",
      "7851 Training Loss: tensor(0.0480)\n",
      "7852 Training Loss: tensor(0.0514)\n",
      "7853 Training Loss: tensor(0.0541)\n",
      "7854 Training Loss: tensor(0.0483)\n",
      "7855 Training Loss: tensor(0.0398)\n",
      "7856 Training Loss: tensor(0.0467)\n",
      "7857 Training Loss: tensor(0.0457)\n",
      "7858 Training Loss: tensor(0.0474)\n",
      "7859 Training Loss: tensor(0.0540)\n",
      "7860 Training Loss: tensor(0.0503)\n",
      "7861 Training Loss: tensor(0.0425)\n",
      "7862 Training Loss: tensor(0.0386)\n",
      "7863 Training Loss: tensor(0.0469)\n",
      "7864 Training Loss: tensor(0.0449)\n",
      "7865 Training Loss: tensor(0.0446)\n",
      "7866 Training Loss: tensor(0.0477)\n",
      "7867 Training Loss: tensor(0.0413)\n",
      "7868 Training Loss: tensor(0.0439)\n",
      "7869 Training Loss: tensor(0.0488)\n",
      "7870 Training Loss: tensor(0.0505)\n",
      "7871 Training Loss: tensor(0.0434)\n",
      "7872 Training Loss: tensor(0.0479)\n",
      "7873 Training Loss: tensor(0.0441)\n",
      "7874 Training Loss: tensor(0.0473)\n",
      "7875 Training Loss: tensor(0.0470)\n",
      "7876 Training Loss: tensor(0.0432)\n",
      "7877 Training Loss: tensor(0.0401)\n",
      "7878 Training Loss: tensor(0.0379)\n",
      "7879 Training Loss: tensor(0.0524)\n",
      "7880 Training Loss: tensor(0.0456)\n",
      "7881 Training Loss: tensor(0.0479)\n",
      "7882 Training Loss: tensor(0.0419)\n",
      "7883 Training Loss: tensor(0.0410)\n",
      "7884 Training Loss: tensor(0.0457)\n",
      "7885 Training Loss: tensor(0.0415)\n",
      "7886 Training Loss: tensor(0.0441)\n",
      "7887 Training Loss: tensor(0.0473)\n",
      "7888 Training Loss: tensor(0.0446)\n",
      "7889 Training Loss: tensor(0.0465)\n",
      "7890 Training Loss: tensor(0.0462)\n",
      "7891 Training Loss: tensor(0.0498)\n",
      "7892 Training Loss: tensor(0.0460)\n",
      "7893 Training Loss: tensor(0.0426)\n",
      "7894 Training Loss: tensor(0.0476)\n",
      "7895 Training Loss: tensor(0.0519)\n",
      "7896 Training Loss: tensor(0.0391)\n",
      "7897 Training Loss: tensor(0.0470)\n",
      "7898 Training Loss: tensor(0.0459)\n",
      "7899 Training Loss: tensor(0.0457)\n",
      "7900 Training Loss: tensor(0.0488)\n",
      "7901 Training Loss: tensor(0.0541)\n",
      "7902 Training Loss: tensor(0.0445)\n",
      "7903 Training Loss: tensor(0.0457)\n",
      "7904 Training Loss: tensor(0.0462)\n",
      "7905 Training Loss: tensor(0.0489)\n",
      "7906 Training Loss: tensor(0.0439)\n",
      "7907 Training Loss: tensor(0.0419)\n",
      "7908 Training Loss: tensor(0.0482)\n",
      "7909 Training Loss: tensor(0.0484)\n",
      "7910 Training Loss: tensor(0.0552)\n",
      "7911 Training Loss: tensor(0.0488)\n",
      "7912 Training Loss: tensor(0.0532)\n",
      "7913 Training Loss: tensor(0.0415)\n",
      "7914 Training Loss: tensor(0.0509)\n",
      "7915 Training Loss: tensor(0.0424)\n",
      "7916 Training Loss: tensor(0.0471)\n",
      "7917 Training Loss: tensor(0.0439)\n",
      "7918 Training Loss: tensor(0.0441)\n",
      "7919 Training Loss: tensor(0.0454)\n",
      "7920 Training Loss: tensor(0.0416)\n",
      "7921 Training Loss: tensor(0.0516)\n",
      "7922 Training Loss: tensor(0.0456)\n",
      "7923 Training Loss: tensor(0.0470)\n",
      "7924 Training Loss: tensor(0.0446)\n",
      "7925 Training Loss: tensor(0.0495)\n",
      "7926 Training Loss: tensor(0.0550)\n",
      "7927 Training Loss: tensor(0.0448)\n",
      "7928 Training Loss: tensor(0.0398)\n",
      "7929 Training Loss: tensor(0.0435)\n",
      "7930 Training Loss: tensor(0.0527)\n",
      "7931 Training Loss: tensor(0.0545)\n",
      "7932 Training Loss: tensor(0.0471)\n",
      "7933 Training Loss: tensor(0.0492)\n",
      "7934 Training Loss: tensor(0.0523)\n",
      "7935 Training Loss: tensor(0.0445)\n",
      "7936 Training Loss: tensor(0.0450)\n",
      "7937 Training Loss: tensor(0.0504)\n",
      "7938 Training Loss: tensor(0.0413)\n",
      "7939 Training Loss: tensor(0.0512)\n",
      "7940 Training Loss: tensor(0.0495)\n",
      "7941 Training Loss: tensor(0.0483)\n",
      "7942 Training Loss: tensor(0.0469)\n",
      "7943 Training Loss: tensor(0.0523)\n",
      "7944 Training Loss: tensor(0.0447)\n",
      "7945 Training Loss: tensor(0.0475)\n",
      "7946 Training Loss: tensor(0.0438)\n",
      "7947 Training Loss: tensor(0.0423)\n",
      "7948 Training Loss: tensor(0.0431)\n",
      "7949 Training Loss: tensor(0.0459)\n",
      "7950 Training Loss: tensor(0.0498)\n",
      "7951 Training Loss: tensor(0.0405)\n",
      "7952 Training Loss: tensor(0.0387)\n",
      "7953 Training Loss: tensor(0.0420)\n",
      "7954 Training Loss: tensor(0.0468)\n",
      "7955 Training Loss: tensor(0.0497)\n",
      "7956 Training Loss: tensor(0.0425)\n",
      "7957 Training Loss: tensor(0.0425)\n",
      "7958 Training Loss: tensor(0.0427)\n",
      "7959 Training Loss: tensor(0.0471)\n",
      "7960 Training Loss: tensor(0.0437)\n",
      "7961 Training Loss: tensor(0.0515)\n",
      "7962 Training Loss: tensor(0.0486)\n",
      "7963 Training Loss: tensor(0.0442)\n",
      "7964 Training Loss: tensor(0.0420)\n",
      "7965 Training Loss: tensor(0.0465)\n",
      "7966 Training Loss: tensor(0.0490)\n",
      "7967 Training Loss: tensor(0.0454)\n",
      "7968 Training Loss: tensor(0.0425)\n",
      "7969 Training Loss: tensor(0.0483)\n",
      "7970 Training Loss: tensor(0.0457)\n",
      "7971 Training Loss: tensor(0.0447)\n",
      "7972 Training Loss: tensor(0.0495)\n",
      "7973 Training Loss: tensor(0.0482)\n",
      "7974 Training Loss: tensor(0.0473)\n",
      "7975 Training Loss: tensor(0.0499)\n",
      "7976 Training Loss: tensor(0.0460)\n",
      "7977 Training Loss: tensor(0.0455)\n",
      "7978 Training Loss: tensor(0.0471)\n",
      "7979 Training Loss: tensor(0.0446)\n",
      "7980 Training Loss: tensor(0.0498)\n",
      "7981 Training Loss: tensor(0.0463)\n",
      "7982 Training Loss: tensor(0.0486)\n",
      "7983 Training Loss: tensor(0.0490)\n",
      "7984 Training Loss: tensor(0.0490)\n",
      "7985 Training Loss: tensor(0.0401)\n",
      "7986 Training Loss: tensor(0.0469)\n",
      "7987 Training Loss: tensor(0.0489)\n",
      "7988 Training Loss: tensor(0.0433)\n",
      "7989 Training Loss: tensor(0.0547)\n",
      "7990 Training Loss: tensor(0.0509)\n",
      "7991 Training Loss: tensor(0.0434)\n",
      "7992 Training Loss: tensor(0.0477)\n",
      "7993 Training Loss: tensor(0.0421)\n",
      "7994 Training Loss: tensor(0.0477)\n",
      "7995 Training Loss: tensor(0.0533)\n",
      "7996 Training Loss: tensor(0.0496)\n",
      "7997 Training Loss: tensor(0.0409)\n",
      "7998 Training Loss: tensor(0.0472)\n",
      "7999 Training Loss: tensor(0.0435)\n",
      "8000 Training Loss: tensor(0.0434)\n",
      "8001 Training Loss: tensor(0.0453)\n",
      "8002 Training Loss: tensor(0.0429)\n",
      "8003 Training Loss: tensor(0.0431)\n",
      "8004 Training Loss: tensor(0.0486)\n",
      "8005 Training Loss: tensor(0.0473)\n",
      "8006 Training Loss: tensor(0.0450)\n",
      "8007 Training Loss: tensor(0.0510)\n",
      "8008 Training Loss: tensor(0.0515)\n",
      "8009 Training Loss: tensor(0.0417)\n",
      "8010 Training Loss: tensor(0.0440)\n",
      "8011 Training Loss: tensor(0.0445)\n",
      "8012 Training Loss: tensor(0.0488)\n",
      "8013 Training Loss: tensor(0.0483)\n",
      "8014 Training Loss: tensor(0.0499)\n",
      "8015 Training Loss: tensor(0.0464)\n",
      "8016 Training Loss: tensor(0.0436)\n",
      "8017 Training Loss: tensor(0.0446)\n",
      "8018 Training Loss: tensor(0.0488)\n",
      "8019 Training Loss: tensor(0.0402)\n",
      "8020 Training Loss: tensor(0.0467)\n",
      "8021 Training Loss: tensor(0.0456)\n",
      "8022 Training Loss: tensor(0.0471)\n",
      "8023 Training Loss: tensor(0.0399)\n",
      "8024 Training Loss: tensor(0.0456)\n",
      "8025 Training Loss: tensor(0.0465)\n",
      "8026 Training Loss: tensor(0.0430)\n",
      "8027 Training Loss: tensor(0.0437)\n",
      "8028 Training Loss: tensor(0.0433)\n",
      "8029 Training Loss: tensor(0.0444)\n",
      "8030 Training Loss: tensor(0.0514)\n",
      "8031 Training Loss: tensor(0.0514)\n",
      "8032 Training Loss: tensor(0.0463)\n",
      "8033 Training Loss: tensor(0.0484)\n",
      "8034 Training Loss: tensor(0.0462)\n",
      "8035 Training Loss: tensor(0.0437)\n",
      "8036 Training Loss: tensor(0.0495)\n",
      "8037 Training Loss: tensor(0.0459)\n",
      "8038 Training Loss: tensor(0.0441)\n",
      "8039 Training Loss: tensor(0.0437)\n",
      "8040 Training Loss: tensor(0.0416)\n",
      "8041 Training Loss: tensor(0.0507)\n",
      "8042 Training Loss: tensor(0.0480)\n",
      "8043 Training Loss: tensor(0.0454)\n",
      "8044 Training Loss: tensor(0.0480)\n",
      "8045 Training Loss: tensor(0.0452)\n",
      "8046 Training Loss: tensor(0.0534)\n",
      "8047 Training Loss: tensor(0.0474)\n",
      "8048 Training Loss: tensor(0.0447)\n",
      "8049 Training Loss: tensor(0.0439)\n",
      "8050 Training Loss: tensor(0.0427)\n",
      "8051 Training Loss: tensor(0.0437)\n",
      "8052 Training Loss: tensor(0.0422)\n",
      "8053 Training Loss: tensor(0.0456)\n",
      "8054 Training Loss: tensor(0.0444)\n",
      "8055 Training Loss: tensor(0.0483)\n",
      "8056 Training Loss: tensor(0.0452)\n",
      "8057 Training Loss: tensor(0.0450)\n",
      "8058 Training Loss: tensor(0.0378)\n",
      "8059 Training Loss: tensor(0.0546)\n",
      "8060 Training Loss: tensor(0.0459)\n",
      "8061 Training Loss: tensor(0.0508)\n",
      "8062 Training Loss: tensor(0.0482)\n",
      "8063 Training Loss: tensor(0.0482)\n",
      "8064 Training Loss: tensor(0.0470)\n",
      "8065 Training Loss: tensor(0.0406)\n",
      "8066 Training Loss: tensor(0.0439)\n",
      "8067 Training Loss: tensor(0.0442)\n",
      "8068 Training Loss: tensor(0.0466)\n",
      "8069 Training Loss: tensor(0.0487)\n",
      "8070 Training Loss: tensor(0.0394)\n",
      "8071 Training Loss: tensor(0.0519)\n",
      "8072 Training Loss: tensor(0.0431)\n",
      "8073 Training Loss: tensor(0.0445)\n",
      "8074 Training Loss: tensor(0.0415)\n",
      "8075 Training Loss: tensor(0.0476)\n",
      "8076 Training Loss: tensor(0.0463)\n",
      "8077 Training Loss: tensor(0.0469)\n",
      "8078 Training Loss: tensor(0.0482)\n",
      "8079 Training Loss: tensor(0.0448)\n",
      "8080 Training Loss: tensor(0.0427)\n",
      "8081 Training Loss: tensor(0.0433)\n",
      "8082 Training Loss: tensor(0.0505)\n",
      "8083 Training Loss: tensor(0.0468)\n",
      "8084 Training Loss: tensor(0.0474)\n",
      "8085 Training Loss: tensor(0.0419)\n",
      "8086 Training Loss: tensor(0.0455)\n",
      "8087 Training Loss: tensor(0.0421)\n",
      "8088 Training Loss: tensor(0.0495)\n",
      "8089 Training Loss: tensor(0.0380)\n",
      "8090 Training Loss: tensor(0.0427)\n",
      "8091 Training Loss: tensor(0.0470)\n",
      "8092 Training Loss: tensor(0.0479)\n",
      "8093 Training Loss: tensor(0.0447)\n",
      "8094 Training Loss: tensor(0.0525)\n",
      "8095 Training Loss: tensor(0.0444)\n",
      "8096 Training Loss: tensor(0.0516)\n",
      "8097 Training Loss: tensor(0.0443)\n",
      "8098 Training Loss: tensor(0.0415)\n",
      "8099 Training Loss: tensor(0.0389)\n",
      "8100 Training Loss: tensor(0.0570)\n",
      "8101 Training Loss: tensor(0.0463)\n",
      "8102 Training Loss: tensor(0.0475)\n",
      "8103 Training Loss: tensor(0.0382)\n",
      "8104 Training Loss: tensor(0.0450)\n",
      "8105 Training Loss: tensor(0.0404)\n",
      "8106 Training Loss: tensor(0.0474)\n",
      "8107 Training Loss: tensor(0.0440)\n",
      "8108 Training Loss: tensor(0.0464)\n",
      "8109 Training Loss: tensor(0.0403)\n",
      "8110 Training Loss: tensor(0.0468)\n",
      "8111 Training Loss: tensor(0.0422)\n",
      "8112 Training Loss: tensor(0.0475)\n",
      "8113 Training Loss: tensor(0.0499)\n",
      "8114 Training Loss: tensor(0.0522)\n",
      "8115 Training Loss: tensor(0.0411)\n",
      "8116 Training Loss: tensor(0.0453)\n",
      "8117 Training Loss: tensor(0.0429)\n",
      "8118 Training Loss: tensor(0.0432)\n",
      "8119 Training Loss: tensor(0.0368)\n",
      "8120 Training Loss: tensor(0.0468)\n",
      "8121 Training Loss: tensor(0.0441)\n",
      "8122 Training Loss: tensor(0.0445)\n",
      "8123 Training Loss: tensor(0.0444)\n",
      "8124 Training Loss: tensor(0.0421)\n",
      "8125 Training Loss: tensor(0.0437)\n",
      "8126 Training Loss: tensor(0.0482)\n",
      "8127 Training Loss: tensor(0.0499)\n",
      "8128 Training Loss: tensor(0.0429)\n",
      "8129 Training Loss: tensor(0.0486)\n",
      "8130 Training Loss: tensor(0.0463)\n",
      "8131 Training Loss: tensor(0.0409)\n",
      "8132 Training Loss: tensor(0.0448)\n",
      "8133 Training Loss: tensor(0.0473)\n",
      "8134 Training Loss: tensor(0.0467)\n",
      "8135 Training Loss: tensor(0.0461)\n",
      "8136 Training Loss: tensor(0.0378)\n",
      "8137 Training Loss: tensor(0.0448)\n",
      "8138 Training Loss: tensor(0.0427)\n",
      "8139 Training Loss: tensor(0.0473)\n",
      "8140 Training Loss: tensor(0.0443)\n",
      "8141 Training Loss: tensor(0.0515)\n",
      "8142 Training Loss: tensor(0.0479)\n",
      "8143 Training Loss: tensor(0.0404)\n",
      "8144 Training Loss: tensor(0.0474)\n",
      "8145 Training Loss: tensor(0.0411)\n",
      "8146 Training Loss: tensor(0.0530)\n",
      "8147 Training Loss: tensor(0.0431)\n",
      "8148 Training Loss: tensor(0.0479)\n",
      "8149 Training Loss: tensor(0.0419)\n",
      "8150 Training Loss: tensor(0.0419)\n",
      "8151 Training Loss: tensor(0.0421)\n",
      "8152 Training Loss: tensor(0.0438)\n",
      "8153 Training Loss: tensor(0.0410)\n",
      "8154 Training Loss: tensor(0.0474)\n",
      "8155 Training Loss: tensor(0.0468)\n",
      "8156 Training Loss: tensor(0.0414)\n",
      "8157 Training Loss: tensor(0.0483)\n",
      "8158 Training Loss: tensor(0.0559)\n",
      "8159 Training Loss: tensor(0.0460)\n",
      "8160 Training Loss: tensor(0.0452)\n",
      "8161 Training Loss: tensor(0.0456)\n",
      "8162 Training Loss: tensor(0.0468)\n",
      "8163 Training Loss: tensor(0.0464)\n",
      "8164 Training Loss: tensor(0.0504)\n",
      "8165 Training Loss: tensor(0.0475)\n",
      "8166 Training Loss: tensor(0.0473)\n",
      "8167 Training Loss: tensor(0.0528)\n",
      "8168 Training Loss: tensor(0.0438)\n",
      "8169 Training Loss: tensor(0.0495)\n",
      "8170 Training Loss: tensor(0.0498)\n",
      "8171 Training Loss: tensor(0.0486)\n",
      "8172 Training Loss: tensor(0.0461)\n",
      "8173 Training Loss: tensor(0.0480)\n",
      "8174 Training Loss: tensor(0.0430)\n",
      "8175 Training Loss: tensor(0.0446)\n",
      "8176 Training Loss: tensor(0.0443)\n",
      "8177 Training Loss: tensor(0.0445)\n",
      "8178 Training Loss: tensor(0.0476)\n",
      "8179 Training Loss: tensor(0.0507)\n",
      "8180 Training Loss: tensor(0.0380)\n",
      "8181 Training Loss: tensor(0.0418)\n",
      "8182 Training Loss: tensor(0.0538)\n",
      "8183 Training Loss: tensor(0.0477)\n",
      "8184 Training Loss: tensor(0.0441)\n",
      "8185 Training Loss: tensor(0.0511)\n",
      "8186 Training Loss: tensor(0.0478)\n",
      "8187 Training Loss: tensor(0.0464)\n",
      "8188 Training Loss: tensor(0.0400)\n",
      "8189 Training Loss: tensor(0.0459)\n",
      "8190 Training Loss: tensor(0.0479)\n",
      "8191 Training Loss: tensor(0.0520)\n",
      "8192 Training Loss: tensor(0.0437)\n",
      "8193 Training Loss: tensor(0.0466)\n",
      "8194 Training Loss: tensor(0.0446)\n",
      "8195 Training Loss: tensor(0.0525)\n",
      "8196 Training Loss: tensor(0.0432)\n",
      "8197 Training Loss: tensor(0.0448)\n",
      "8198 Training Loss: tensor(0.0454)\n",
      "8199 Training Loss: tensor(0.0518)\n",
      "8200 Training Loss: tensor(0.0420)\n",
      "8201 Training Loss: tensor(0.0434)\n",
      "8202 Training Loss: tensor(0.0418)\n",
      "8203 Training Loss: tensor(0.0440)\n",
      "8204 Training Loss: tensor(0.0439)\n",
      "8205 Training Loss: tensor(0.0456)\n",
      "8206 Training Loss: tensor(0.0436)\n",
      "8207 Training Loss: tensor(0.0403)\n",
      "8208 Training Loss: tensor(0.0502)\n",
      "8209 Training Loss: tensor(0.0400)\n",
      "8210 Training Loss: tensor(0.0449)\n",
      "8211 Training Loss: tensor(0.0464)\n",
      "8212 Training Loss: tensor(0.0421)\n",
      "8213 Training Loss: tensor(0.0433)\n",
      "8214 Training Loss: tensor(0.0457)\n",
      "8215 Training Loss: tensor(0.0433)\n",
      "8216 Training Loss: tensor(0.0432)\n",
      "8217 Training Loss: tensor(0.0411)\n",
      "8218 Training Loss: tensor(0.0472)\n",
      "8219 Training Loss: tensor(0.0487)\n",
      "8220 Training Loss: tensor(0.0450)\n",
      "8221 Training Loss: tensor(0.0398)\n",
      "8222 Training Loss: tensor(0.0484)\n",
      "8223 Training Loss: tensor(0.0465)\n",
      "8224 Training Loss: tensor(0.0441)\n",
      "8225 Training Loss: tensor(0.0408)\n",
      "8226 Training Loss: tensor(0.0483)\n",
      "8227 Training Loss: tensor(0.0413)\n",
      "8228 Training Loss: tensor(0.0470)\n",
      "8229 Training Loss: tensor(0.0466)\n",
      "8230 Training Loss: tensor(0.0440)\n",
      "8231 Training Loss: tensor(0.0470)\n",
      "8232 Training Loss: tensor(0.0496)\n",
      "8233 Training Loss: tensor(0.0397)\n",
      "8234 Training Loss: tensor(0.0423)\n",
      "8235 Training Loss: tensor(0.0472)\n",
      "8236 Training Loss: tensor(0.0403)\n",
      "8237 Training Loss: tensor(0.0405)\n",
      "8238 Training Loss: tensor(0.0407)\n",
      "8239 Training Loss: tensor(0.0436)\n",
      "8240 Training Loss: tensor(0.0428)\n",
      "8241 Training Loss: tensor(0.0453)\n",
      "8242 Training Loss: tensor(0.0501)\n",
      "8243 Training Loss: tensor(0.0425)\n",
      "8244 Training Loss: tensor(0.0464)\n",
      "8245 Training Loss: tensor(0.0474)\n",
      "8246 Training Loss: tensor(0.0433)\n",
      "8247 Training Loss: tensor(0.0456)\n",
      "8248 Training Loss: tensor(0.0443)\n",
      "8249 Training Loss: tensor(0.0435)\n",
      "8250 Training Loss: tensor(0.0484)\n",
      "8251 Training Loss: tensor(0.0507)\n",
      "8252 Training Loss: tensor(0.0449)\n",
      "8253 Training Loss: tensor(0.0411)\n",
      "8254 Training Loss: tensor(0.0417)\n",
      "8255 Training Loss: tensor(0.0415)\n",
      "8256 Training Loss: tensor(0.0450)\n",
      "8257 Training Loss: tensor(0.0482)\n",
      "8258 Training Loss: tensor(0.0456)\n",
      "8259 Training Loss: tensor(0.0457)\n",
      "8260 Training Loss: tensor(0.0441)\n",
      "8261 Training Loss: tensor(0.0410)\n",
      "8262 Training Loss: tensor(0.0469)\n",
      "8263 Training Loss: tensor(0.0451)\n",
      "8264 Training Loss: tensor(0.0408)\n",
      "8265 Training Loss: tensor(0.0470)\n",
      "8266 Training Loss: tensor(0.0479)\n",
      "8267 Training Loss: tensor(0.0443)\n",
      "8268 Training Loss: tensor(0.0444)\n",
      "8269 Training Loss: tensor(0.0452)\n",
      "8270 Training Loss: tensor(0.0476)\n",
      "8271 Training Loss: tensor(0.0429)\n",
      "8272 Training Loss: tensor(0.0474)\n",
      "8273 Training Loss: tensor(0.0461)\n",
      "8274 Training Loss: tensor(0.0454)\n",
      "8275 Training Loss: tensor(0.0516)\n",
      "8276 Training Loss: tensor(0.0501)\n",
      "8277 Training Loss: tensor(0.0473)\n",
      "8278 Training Loss: tensor(0.0447)\n",
      "8279 Training Loss: tensor(0.0438)\n",
      "8280 Training Loss: tensor(0.0454)\n",
      "8281 Training Loss: tensor(0.0397)\n",
      "8282 Training Loss: tensor(0.0353)\n",
      "8283 Training Loss: tensor(0.0421)\n",
      "8284 Training Loss: tensor(0.0483)\n",
      "8285 Training Loss: tensor(0.0442)\n",
      "8286 Training Loss: tensor(0.0484)\n",
      "8287 Training Loss: tensor(0.0526)\n",
      "8288 Training Loss: tensor(0.0447)\n",
      "8289 Training Loss: tensor(0.0424)\n",
      "8290 Training Loss: tensor(0.0427)\n",
      "8291 Training Loss: tensor(0.0449)\n",
      "8292 Training Loss: tensor(0.0488)\n",
      "8293 Training Loss: tensor(0.0438)\n",
      "8294 Training Loss: tensor(0.0433)\n",
      "8295 Training Loss: tensor(0.0395)\n",
      "8296 Training Loss: tensor(0.0468)\n",
      "8297 Training Loss: tensor(0.0408)\n",
      "8298 Training Loss: tensor(0.0467)\n",
      "8299 Training Loss: tensor(0.0490)\n",
      "8300 Training Loss: tensor(0.0413)\n",
      "8301 Training Loss: tensor(0.0433)\n",
      "8302 Training Loss: tensor(0.0492)\n",
      "8303 Training Loss: tensor(0.0461)\n",
      "8304 Training Loss: tensor(0.0445)\n",
      "8305 Training Loss: tensor(0.0413)\n",
      "8306 Training Loss: tensor(0.0423)\n",
      "8307 Training Loss: tensor(0.0418)\n",
      "8308 Training Loss: tensor(0.0478)\n",
      "8309 Training Loss: tensor(0.0439)\n",
      "8310 Training Loss: tensor(0.0451)\n",
      "8311 Training Loss: tensor(0.0451)\n",
      "8312 Training Loss: tensor(0.0496)\n",
      "8313 Training Loss: tensor(0.0414)\n",
      "8314 Training Loss: tensor(0.0436)\n",
      "8315 Training Loss: tensor(0.0425)\n",
      "8316 Training Loss: tensor(0.0458)\n",
      "8317 Training Loss: tensor(0.0474)\n",
      "8318 Training Loss: tensor(0.0460)\n",
      "8319 Training Loss: tensor(0.0532)\n",
      "8320 Training Loss: tensor(0.0453)\n",
      "8321 Training Loss: tensor(0.0452)\n",
      "8322 Training Loss: tensor(0.0440)\n",
      "8323 Training Loss: tensor(0.0416)\n",
      "8324 Training Loss: tensor(0.0478)\n",
      "8325 Training Loss: tensor(0.0475)\n",
      "8326 Training Loss: tensor(0.0444)\n",
      "8327 Training Loss: tensor(0.0456)\n",
      "8328 Training Loss: tensor(0.0483)\n",
      "8329 Training Loss: tensor(0.0423)\n",
      "8330 Training Loss: tensor(0.0485)\n",
      "8331 Training Loss: tensor(0.0485)\n",
      "8332 Training Loss: tensor(0.0418)\n",
      "8333 Training Loss: tensor(0.0463)\n",
      "8334 Training Loss: tensor(0.0425)\n",
      "8335 Training Loss: tensor(0.0441)\n",
      "8336 Training Loss: tensor(0.0451)\n",
      "8337 Training Loss: tensor(0.0458)\n",
      "8338 Training Loss: tensor(0.0452)\n",
      "8339 Training Loss: tensor(0.0427)\n",
      "8340 Training Loss: tensor(0.0472)\n",
      "8341 Training Loss: tensor(0.0417)\n",
      "8342 Training Loss: tensor(0.0463)\n",
      "8343 Training Loss: tensor(0.0445)\n",
      "8344 Training Loss: tensor(0.0460)\n",
      "8345 Training Loss: tensor(0.0446)\n",
      "8346 Training Loss: tensor(0.0422)\n",
      "8347 Training Loss: tensor(0.0437)\n",
      "8348 Training Loss: tensor(0.0456)\n",
      "8349 Training Loss: tensor(0.0500)\n",
      "8350 Training Loss: tensor(0.0459)\n",
      "8351 Training Loss: tensor(0.0454)\n",
      "8352 Training Loss: tensor(0.0416)\n",
      "8353 Training Loss: tensor(0.0452)\n",
      "8354 Training Loss: tensor(0.0405)\n",
      "8355 Training Loss: tensor(0.0445)\n",
      "8356 Training Loss: tensor(0.0466)\n",
      "8357 Training Loss: tensor(0.0476)\n",
      "8358 Training Loss: tensor(0.0481)\n",
      "8359 Training Loss: tensor(0.0447)\n",
      "8360 Training Loss: tensor(0.0450)\n",
      "8361 Training Loss: tensor(0.0451)\n",
      "8362 Training Loss: tensor(0.0452)\n",
      "8363 Training Loss: tensor(0.0401)\n",
      "8364 Training Loss: tensor(0.0420)\n",
      "8365 Training Loss: tensor(0.0453)\n",
      "8366 Training Loss: tensor(0.0422)\n",
      "8367 Training Loss: tensor(0.0479)\n",
      "8368 Training Loss: tensor(0.0443)\n",
      "8369 Training Loss: tensor(0.0431)\n",
      "8370 Training Loss: tensor(0.0429)\n",
      "8371 Training Loss: tensor(0.0473)\n",
      "8372 Training Loss: tensor(0.0439)\n",
      "8373 Training Loss: tensor(0.0466)\n",
      "8374 Training Loss: tensor(0.0448)\n",
      "8375 Training Loss: tensor(0.0416)\n",
      "8376 Training Loss: tensor(0.0463)\n",
      "8377 Training Loss: tensor(0.0435)\n",
      "8378 Training Loss: tensor(0.0468)\n",
      "8379 Training Loss: tensor(0.0438)\n",
      "8380 Training Loss: tensor(0.0414)\n",
      "8381 Training Loss: tensor(0.0454)\n",
      "8382 Training Loss: tensor(0.0420)\n",
      "8383 Training Loss: tensor(0.0456)\n",
      "8384 Training Loss: tensor(0.0436)\n",
      "8385 Training Loss: tensor(0.0432)\n",
      "8386 Training Loss: tensor(0.0465)\n",
      "8387 Training Loss: tensor(0.0489)\n",
      "8388 Training Loss: tensor(0.0455)\n",
      "8389 Training Loss: tensor(0.0460)\n",
      "8390 Training Loss: tensor(0.0505)\n",
      "8391 Training Loss: tensor(0.0524)\n",
      "8392 Training Loss: tensor(0.0404)\n",
      "8393 Training Loss: tensor(0.0418)\n",
      "8394 Training Loss: tensor(0.0506)\n",
      "8395 Training Loss: tensor(0.0467)\n",
      "8396 Training Loss: tensor(0.0474)\n",
      "8397 Training Loss: tensor(0.0448)\n",
      "8398 Training Loss: tensor(0.0452)\n",
      "8399 Training Loss: tensor(0.0443)\n",
      "8400 Training Loss: tensor(0.0382)\n",
      "8401 Training Loss: tensor(0.0428)\n",
      "8402 Training Loss: tensor(0.0480)\n",
      "8403 Training Loss: tensor(0.0455)\n",
      "8404 Training Loss: tensor(0.0409)\n",
      "8405 Training Loss: tensor(0.0425)\n",
      "8406 Training Loss: tensor(0.0457)\n",
      "8407 Training Loss: tensor(0.0415)\n",
      "8408 Training Loss: tensor(0.0431)\n",
      "8409 Training Loss: tensor(0.0403)\n",
      "8410 Training Loss: tensor(0.0476)\n",
      "8411 Training Loss: tensor(0.0428)\n",
      "8412 Training Loss: tensor(0.0457)\n",
      "8413 Training Loss: tensor(0.0468)\n",
      "8414 Training Loss: tensor(0.0540)\n",
      "8415 Training Loss: tensor(0.0435)\n",
      "8416 Training Loss: tensor(0.0450)\n",
      "8417 Training Loss: tensor(0.0424)\n",
      "8418 Training Loss: tensor(0.0448)\n",
      "8419 Training Loss: tensor(0.0462)\n",
      "8420 Training Loss: tensor(0.0446)\n",
      "8421 Training Loss: tensor(0.0495)\n",
      "8422 Training Loss: tensor(0.0506)\n",
      "8423 Training Loss: tensor(0.0477)\n",
      "8424 Training Loss: tensor(0.0462)\n",
      "8425 Training Loss: tensor(0.0471)\n",
      "8426 Training Loss: tensor(0.0516)\n",
      "8427 Training Loss: tensor(0.0517)\n",
      "8428 Training Loss: tensor(0.0482)\n",
      "8429 Training Loss: tensor(0.0450)\n",
      "8430 Training Loss: tensor(0.0457)\n",
      "8431 Training Loss: tensor(0.0404)\n",
      "8432 Training Loss: tensor(0.0464)\n",
      "8433 Training Loss: tensor(0.0426)\n",
      "8434 Training Loss: tensor(0.0416)\n",
      "8435 Training Loss: tensor(0.0506)\n",
      "8436 Training Loss: tensor(0.0466)\n",
      "8437 Training Loss: tensor(0.0422)\n",
      "8438 Training Loss: tensor(0.0519)\n",
      "8439 Training Loss: tensor(0.0453)\n",
      "8440 Training Loss: tensor(0.0435)\n",
      "8441 Training Loss: tensor(0.0392)\n",
      "8442 Training Loss: tensor(0.0473)\n",
      "8443 Training Loss: tensor(0.0405)\n",
      "8444 Training Loss: tensor(0.0438)\n",
      "8445 Training Loss: tensor(0.0483)\n",
      "8446 Training Loss: tensor(0.0469)\n",
      "8447 Training Loss: tensor(0.0457)\n",
      "8448 Training Loss: tensor(0.0444)\n",
      "8449 Training Loss: tensor(0.0488)\n",
      "8450 Training Loss: tensor(0.0411)\n",
      "8451 Training Loss: tensor(0.0442)\n",
      "8452 Training Loss: tensor(0.0450)\n",
      "8453 Training Loss: tensor(0.0445)\n",
      "8454 Training Loss: tensor(0.0457)\n",
      "8455 Training Loss: tensor(0.0496)\n",
      "8456 Training Loss: tensor(0.0395)\n",
      "8457 Training Loss: tensor(0.0393)\n",
      "8458 Training Loss: tensor(0.0426)\n",
      "8459 Training Loss: tensor(0.0447)\n",
      "8460 Training Loss: tensor(0.0453)\n",
      "8461 Training Loss: tensor(0.0401)\n",
      "8462 Training Loss: tensor(0.0463)\n",
      "8463 Training Loss: tensor(0.0453)\n",
      "8464 Training Loss: tensor(0.0443)\n",
      "8465 Training Loss: tensor(0.0435)\n",
      "8466 Training Loss: tensor(0.0461)\n",
      "8467 Training Loss: tensor(0.0463)\n",
      "8468 Training Loss: tensor(0.0418)\n",
      "8469 Training Loss: tensor(0.0430)\n",
      "8470 Training Loss: tensor(0.0498)\n",
      "8471 Training Loss: tensor(0.0405)\n",
      "8472 Training Loss: tensor(0.0412)\n",
      "8473 Training Loss: tensor(0.0419)\n",
      "8474 Training Loss: tensor(0.0402)\n",
      "8475 Training Loss: tensor(0.0417)\n",
      "8476 Training Loss: tensor(0.0430)\n",
      "8477 Training Loss: tensor(0.0480)\n",
      "8478 Training Loss: tensor(0.0426)\n",
      "8479 Training Loss: tensor(0.0419)\n",
      "8480 Training Loss: tensor(0.0426)\n",
      "8481 Training Loss: tensor(0.0475)\n",
      "8482 Training Loss: tensor(0.0412)\n",
      "8483 Training Loss: tensor(0.0430)\n",
      "8484 Training Loss: tensor(0.0448)\n",
      "8485 Training Loss: tensor(0.0471)\n",
      "8486 Training Loss: tensor(0.0451)\n",
      "8487 Training Loss: tensor(0.0461)\n",
      "8488 Training Loss: tensor(0.0499)\n",
      "8489 Training Loss: tensor(0.0443)\n",
      "8490 Training Loss: tensor(0.0441)\n",
      "8491 Training Loss: tensor(0.0451)\n",
      "8492 Training Loss: tensor(0.0427)\n",
      "8493 Training Loss: tensor(0.0474)\n",
      "8494 Training Loss: tensor(0.0404)\n",
      "8495 Training Loss: tensor(0.0406)\n",
      "8496 Training Loss: tensor(0.0431)\n",
      "8497 Training Loss: tensor(0.0404)\n",
      "8498 Training Loss: tensor(0.0455)\n",
      "8499 Training Loss: tensor(0.0432)\n",
      "8500 Training Loss: tensor(0.0446)\n",
      "8501 Training Loss: tensor(0.0454)\n",
      "8502 Training Loss: tensor(0.0376)\n",
      "8503 Training Loss: tensor(0.0440)\n",
      "8504 Training Loss: tensor(0.0457)\n",
      "8505 Training Loss: tensor(0.0469)\n",
      "8506 Training Loss: tensor(0.0462)\n",
      "8507 Training Loss: tensor(0.0437)\n",
      "8508 Training Loss: tensor(0.0414)\n",
      "8509 Training Loss: tensor(0.0495)\n",
      "8510 Training Loss: tensor(0.0449)\n",
      "8511 Training Loss: tensor(0.0449)\n",
      "8512 Training Loss: tensor(0.0419)\n",
      "8513 Training Loss: tensor(0.0427)\n",
      "8514 Training Loss: tensor(0.0495)\n",
      "8515 Training Loss: tensor(0.0428)\n",
      "8516 Training Loss: tensor(0.0461)\n",
      "8517 Training Loss: tensor(0.0462)\n",
      "8518 Training Loss: tensor(0.0486)\n",
      "8519 Training Loss: tensor(0.0445)\n",
      "8520 Training Loss: tensor(0.0434)\n",
      "8521 Training Loss: tensor(0.0450)\n",
      "8522 Training Loss: tensor(0.0492)\n",
      "8523 Training Loss: tensor(0.0450)\n",
      "8524 Training Loss: tensor(0.0482)\n",
      "8525 Training Loss: tensor(0.0459)\n",
      "8526 Training Loss: tensor(0.0476)\n",
      "8527 Training Loss: tensor(0.0439)\n",
      "8528 Training Loss: tensor(0.0423)\n",
      "8529 Training Loss: tensor(0.0417)\n",
      "8530 Training Loss: tensor(0.0399)\n",
      "8531 Training Loss: tensor(0.0395)\n",
      "8532 Training Loss: tensor(0.0450)\n",
      "8533 Training Loss: tensor(0.0400)\n",
      "8534 Training Loss: tensor(0.0468)\n",
      "8535 Training Loss: tensor(0.0395)\n",
      "8536 Training Loss: tensor(0.0470)\n",
      "8537 Training Loss: tensor(0.0470)\n",
      "8538 Training Loss: tensor(0.0487)\n",
      "8539 Training Loss: tensor(0.0433)\n",
      "8540 Training Loss: tensor(0.0464)\n",
      "8541 Training Loss: tensor(0.0431)\n",
      "8542 Training Loss: tensor(0.0447)\n",
      "8543 Training Loss: tensor(0.0453)\n",
      "8544 Training Loss: tensor(0.0407)\n",
      "8545 Training Loss: tensor(0.0489)\n",
      "8546 Training Loss: tensor(0.0458)\n",
      "8547 Training Loss: tensor(0.0449)\n",
      "8548 Training Loss: tensor(0.0456)\n",
      "8549 Training Loss: tensor(0.0489)\n",
      "8550 Training Loss: tensor(0.0460)\n",
      "8551 Training Loss: tensor(0.0438)\n",
      "8552 Training Loss: tensor(0.0446)\n",
      "8553 Training Loss: tensor(0.0412)\n",
      "8554 Training Loss: tensor(0.0458)\n",
      "8555 Training Loss: tensor(0.0427)\n",
      "8556 Training Loss: tensor(0.0440)\n",
      "8557 Training Loss: tensor(0.0526)\n",
      "8558 Training Loss: tensor(0.0412)\n",
      "8559 Training Loss: tensor(0.0429)\n",
      "8560 Training Loss: tensor(0.0461)\n",
      "8561 Training Loss: tensor(0.0467)\n",
      "8562 Training Loss: tensor(0.0501)\n",
      "8563 Training Loss: tensor(0.0495)\n",
      "8564 Training Loss: tensor(0.0412)\n",
      "8565 Training Loss: tensor(0.0403)\n",
      "8566 Training Loss: tensor(0.0467)\n",
      "8567 Training Loss: tensor(0.0420)\n",
      "8568 Training Loss: tensor(0.0534)\n",
      "8569 Training Loss: tensor(0.0468)\n",
      "8570 Training Loss: tensor(0.0451)\n",
      "8571 Training Loss: tensor(0.0484)\n",
      "8572 Training Loss: tensor(0.0431)\n",
      "8573 Training Loss: tensor(0.0524)\n",
      "8574 Training Loss: tensor(0.0420)\n",
      "8575 Training Loss: tensor(0.0450)\n",
      "8576 Training Loss: tensor(0.0421)\n",
      "8577 Training Loss: tensor(0.0489)\n",
      "8578 Training Loss: tensor(0.0475)\n",
      "8579 Training Loss: tensor(0.0419)\n",
      "8580 Training Loss: tensor(0.0482)\n",
      "8581 Training Loss: tensor(0.0524)\n",
      "8582 Training Loss: tensor(0.0525)\n",
      "8583 Training Loss: tensor(0.0465)\n",
      "8584 Training Loss: tensor(0.0445)\n",
      "8585 Training Loss: tensor(0.0453)\n",
      "8586 Training Loss: tensor(0.0430)\n",
      "8587 Training Loss: tensor(0.0469)\n",
      "8588 Training Loss: tensor(0.0362)\n",
      "8589 Training Loss: tensor(0.0409)\n",
      "8590 Training Loss: tensor(0.0478)\n",
      "8591 Training Loss: tensor(0.0475)\n",
      "8592 Training Loss: tensor(0.0465)\n",
      "8593 Training Loss: tensor(0.0474)\n",
      "8594 Training Loss: tensor(0.0434)\n",
      "8595 Training Loss: tensor(0.0479)\n",
      "8596 Training Loss: tensor(0.0447)\n",
      "8597 Training Loss: tensor(0.0387)\n",
      "8598 Training Loss: tensor(0.0431)\n",
      "8599 Training Loss: tensor(0.0525)\n",
      "8600 Training Loss: tensor(0.0490)\n",
      "8601 Training Loss: tensor(0.0465)\n",
      "8602 Training Loss: tensor(0.0369)\n",
      "8603 Training Loss: tensor(0.0401)\n",
      "8604 Training Loss: tensor(0.0463)\n",
      "8605 Training Loss: tensor(0.0434)\n",
      "8606 Training Loss: tensor(0.0561)\n",
      "8607 Training Loss: tensor(0.0432)\n",
      "8608 Training Loss: tensor(0.0393)\n",
      "8609 Training Loss: tensor(0.0442)\n",
      "8610 Training Loss: tensor(0.0418)\n",
      "8611 Training Loss: tensor(0.0476)\n",
      "8612 Training Loss: tensor(0.0479)\n",
      "8613 Training Loss: tensor(0.0451)\n",
      "8614 Training Loss: tensor(0.0444)\n",
      "8615 Training Loss: tensor(0.0380)\n",
      "8616 Training Loss: tensor(0.0420)\n",
      "8617 Training Loss: tensor(0.0381)\n",
      "8618 Training Loss: tensor(0.0487)\n",
      "8619 Training Loss: tensor(0.0494)\n",
      "8620 Training Loss: tensor(0.0465)\n",
      "8621 Training Loss: tensor(0.0447)\n",
      "8622 Training Loss: tensor(0.0499)\n",
      "8623 Training Loss: tensor(0.0469)\n",
      "8624 Training Loss: tensor(0.0467)\n",
      "8625 Training Loss: tensor(0.0432)\n",
      "8626 Training Loss: tensor(0.0400)\n",
      "8627 Training Loss: tensor(0.0525)\n",
      "8628 Training Loss: tensor(0.0445)\n",
      "8629 Training Loss: tensor(0.0494)\n",
      "8630 Training Loss: tensor(0.0536)\n",
      "8631 Training Loss: tensor(0.0426)\n",
      "8632 Training Loss: tensor(0.0450)\n",
      "8633 Training Loss: tensor(0.0448)\n",
      "8634 Training Loss: tensor(0.0429)\n",
      "8635 Training Loss: tensor(0.0467)\n",
      "8636 Training Loss: tensor(0.0484)\n",
      "8637 Training Loss: tensor(0.0423)\n",
      "8638 Training Loss: tensor(0.0468)\n",
      "8639 Training Loss: tensor(0.0419)\n",
      "8640 Training Loss: tensor(0.0432)\n",
      "8641 Training Loss: tensor(0.0458)\n",
      "8642 Training Loss: tensor(0.0426)\n",
      "8643 Training Loss: tensor(0.0445)\n",
      "8644 Training Loss: tensor(0.0456)\n",
      "8645 Training Loss: tensor(0.0414)\n",
      "8646 Training Loss: tensor(0.0431)\n",
      "8647 Training Loss: tensor(0.0462)\n",
      "8648 Training Loss: tensor(0.0465)\n",
      "8649 Training Loss: tensor(0.0469)\n",
      "8650 Training Loss: tensor(0.0447)\n",
      "8651 Training Loss: tensor(0.0430)\n",
      "8652 Training Loss: tensor(0.0487)\n",
      "8653 Training Loss: tensor(0.0464)\n",
      "8654 Training Loss: tensor(0.0441)\n",
      "8655 Training Loss: tensor(0.0470)\n",
      "8656 Training Loss: tensor(0.0487)\n",
      "8657 Training Loss: tensor(0.0473)\n",
      "8658 Training Loss: tensor(0.0406)\n",
      "8659 Training Loss: tensor(0.0436)\n",
      "8660 Training Loss: tensor(0.0466)\n",
      "8661 Training Loss: tensor(0.0454)\n",
      "8662 Training Loss: tensor(0.0383)\n",
      "8663 Training Loss: tensor(0.0490)\n",
      "8664 Training Loss: tensor(0.0506)\n",
      "8665 Training Loss: tensor(0.0461)\n",
      "8666 Training Loss: tensor(0.0409)\n",
      "8667 Training Loss: tensor(0.0525)\n",
      "8668 Training Loss: tensor(0.0388)\n",
      "8669 Training Loss: tensor(0.0358)\n",
      "8670 Training Loss: tensor(0.0489)\n",
      "8671 Training Loss: tensor(0.0449)\n",
      "8672 Training Loss: tensor(0.0424)\n",
      "8673 Training Loss: tensor(0.0460)\n",
      "8674 Training Loss: tensor(0.0469)\n",
      "8675 Training Loss: tensor(0.0450)\n",
      "8676 Training Loss: tensor(0.0418)\n",
      "8677 Training Loss: tensor(0.0459)\n",
      "8678 Training Loss: tensor(0.0443)\n",
      "8679 Training Loss: tensor(0.0420)\n",
      "8680 Training Loss: tensor(0.0420)\n",
      "8681 Training Loss: tensor(0.0463)\n",
      "8682 Training Loss: tensor(0.0412)\n",
      "8683 Training Loss: tensor(0.0367)\n",
      "8684 Training Loss: tensor(0.0469)\n",
      "8685 Training Loss: tensor(0.0433)\n",
      "8686 Training Loss: tensor(0.0393)\n",
      "8687 Training Loss: tensor(0.0400)\n",
      "8688 Training Loss: tensor(0.0444)\n",
      "8689 Training Loss: tensor(0.0467)\n",
      "8690 Training Loss: tensor(0.0380)\n",
      "8691 Training Loss: tensor(0.0462)\n",
      "8692 Training Loss: tensor(0.0439)\n",
      "8693 Training Loss: tensor(0.0422)\n",
      "8694 Training Loss: tensor(0.0398)\n",
      "8695 Training Loss: tensor(0.0416)\n",
      "8696 Training Loss: tensor(0.0489)\n",
      "8697 Training Loss: tensor(0.0473)\n",
      "8698 Training Loss: tensor(0.0459)\n",
      "8699 Training Loss: tensor(0.0420)\n",
      "8700 Training Loss: tensor(0.0421)\n",
      "8701 Training Loss: tensor(0.0483)\n",
      "8702 Training Loss: tensor(0.0447)\n",
      "8703 Training Loss: tensor(0.0421)\n",
      "8704 Training Loss: tensor(0.0406)\n",
      "8705 Training Loss: tensor(0.0486)\n",
      "8706 Training Loss: tensor(0.0421)\n",
      "8707 Training Loss: tensor(0.0410)\n",
      "8708 Training Loss: tensor(0.0426)\n",
      "8709 Training Loss: tensor(0.0378)\n",
      "8710 Training Loss: tensor(0.0444)\n",
      "8711 Training Loss: tensor(0.0441)\n",
      "8712 Training Loss: tensor(0.0468)\n",
      "8713 Training Loss: tensor(0.0453)\n",
      "8714 Training Loss: tensor(0.0485)\n",
      "8715 Training Loss: tensor(0.0462)\n",
      "8716 Training Loss: tensor(0.0406)\n",
      "8717 Training Loss: tensor(0.0428)\n",
      "8718 Training Loss: tensor(0.0440)\n",
      "8719 Training Loss: tensor(0.0406)\n",
      "8720 Training Loss: tensor(0.0415)\n",
      "8721 Training Loss: tensor(0.0449)\n",
      "8722 Training Loss: tensor(0.0412)\n",
      "8723 Training Loss: tensor(0.0398)\n",
      "8724 Training Loss: tensor(0.0489)\n",
      "8725 Training Loss: tensor(0.0468)\n",
      "8726 Training Loss: tensor(0.0450)\n",
      "8727 Training Loss: tensor(0.0504)\n",
      "8728 Training Loss: tensor(0.0451)\n",
      "8729 Training Loss: tensor(0.0432)\n",
      "8730 Training Loss: tensor(0.0477)\n",
      "8731 Training Loss: tensor(0.0434)\n",
      "8732 Training Loss: tensor(0.0412)\n",
      "8733 Training Loss: tensor(0.0438)\n",
      "8734 Training Loss: tensor(0.0457)\n",
      "8735 Training Loss: tensor(0.0457)\n",
      "8736 Training Loss: tensor(0.0406)\n",
      "8737 Training Loss: tensor(0.0425)\n",
      "8738 Training Loss: tensor(0.0420)\n",
      "8739 Training Loss: tensor(0.0406)\n",
      "8740 Training Loss: tensor(0.0459)\n",
      "8741 Training Loss: tensor(0.0442)\n",
      "8742 Training Loss: tensor(0.0424)\n",
      "8743 Training Loss: tensor(0.0444)\n",
      "8744 Training Loss: tensor(0.0470)\n",
      "8745 Training Loss: tensor(0.0445)\n",
      "8746 Training Loss: tensor(0.0437)\n",
      "8747 Training Loss: tensor(0.0441)\n",
      "8748 Training Loss: tensor(0.0411)\n",
      "8749 Training Loss: tensor(0.0434)\n",
      "8750 Training Loss: tensor(0.0454)\n",
      "8751 Training Loss: tensor(0.0392)\n",
      "8752 Training Loss: tensor(0.0438)\n",
      "8753 Training Loss: tensor(0.0450)\n",
      "8754 Training Loss: tensor(0.0433)\n",
      "8755 Training Loss: tensor(0.0477)\n",
      "8756 Training Loss: tensor(0.0415)\n",
      "8757 Training Loss: tensor(0.0440)\n",
      "8758 Training Loss: tensor(0.0455)\n",
      "8759 Training Loss: tensor(0.0485)\n",
      "8760 Training Loss: tensor(0.0432)\n",
      "8761 Training Loss: tensor(0.0523)\n",
      "8762 Training Loss: tensor(0.0453)\n",
      "8763 Training Loss: tensor(0.0474)\n",
      "8764 Training Loss: tensor(0.0532)\n",
      "8765 Training Loss: tensor(0.0525)\n",
      "8766 Training Loss: tensor(0.0429)\n",
      "8767 Training Loss: tensor(0.0504)\n",
      "8768 Training Loss: tensor(0.0472)\n",
      "8769 Training Loss: tensor(0.0419)\n",
      "8770 Training Loss: tensor(0.0479)\n",
      "8771 Training Loss: tensor(0.0457)\n",
      "8772 Training Loss: tensor(0.0419)\n",
      "8773 Training Loss: tensor(0.0456)\n",
      "8774 Training Loss: tensor(0.0451)\n",
      "8775 Training Loss: tensor(0.0484)\n",
      "8776 Training Loss: tensor(0.0428)\n",
      "8777 Training Loss: tensor(0.0459)\n",
      "8778 Training Loss: tensor(0.0471)\n",
      "8779 Training Loss: tensor(0.0440)\n",
      "8780 Training Loss: tensor(0.0480)\n",
      "8781 Training Loss: tensor(0.0425)\n",
      "8782 Training Loss: tensor(0.0491)\n",
      "8783 Training Loss: tensor(0.0476)\n",
      "8784 Training Loss: tensor(0.0504)\n",
      "8785 Training Loss: tensor(0.0436)\n",
      "8786 Training Loss: tensor(0.0477)\n",
      "8787 Training Loss: tensor(0.0426)\n",
      "8788 Training Loss: tensor(0.0430)\n",
      "8789 Training Loss: tensor(0.0459)\n",
      "8790 Training Loss: tensor(0.0424)\n",
      "8791 Training Loss: tensor(0.0459)\n",
      "8792 Training Loss: tensor(0.0420)\n",
      "8793 Training Loss: tensor(0.0458)\n",
      "8794 Training Loss: tensor(0.0402)\n",
      "8795 Training Loss: tensor(0.0471)\n",
      "8796 Training Loss: tensor(0.0412)\n",
      "8797 Training Loss: tensor(0.0459)\n",
      "8798 Training Loss: tensor(0.0431)\n",
      "8799 Training Loss: tensor(0.0435)\n",
      "8800 Training Loss: tensor(0.0467)\n",
      "8801 Training Loss: tensor(0.0430)\n",
      "8802 Training Loss: tensor(0.0491)\n",
      "8803 Training Loss: tensor(0.0421)\n",
      "8804 Training Loss: tensor(0.0497)\n",
      "8805 Training Loss: tensor(0.0388)\n",
      "8806 Training Loss: tensor(0.0467)\n",
      "8807 Training Loss: tensor(0.0467)\n",
      "8808 Training Loss: tensor(0.0516)\n",
      "8809 Training Loss: tensor(0.0479)\n",
      "8810 Training Loss: tensor(0.0444)\n",
      "8811 Training Loss: tensor(0.0418)\n",
      "8812 Training Loss: tensor(0.0460)\n",
      "8813 Training Loss: tensor(0.0480)\n",
      "8814 Training Loss: tensor(0.0446)\n",
      "8815 Training Loss: tensor(0.0456)\n",
      "8816 Training Loss: tensor(0.0480)\n",
      "8817 Training Loss: tensor(0.0471)\n",
      "8818 Training Loss: tensor(0.0436)\n",
      "8819 Training Loss: tensor(0.0474)\n",
      "8820 Training Loss: tensor(0.0415)\n",
      "8821 Training Loss: tensor(0.0424)\n",
      "8822 Training Loss: tensor(0.0452)\n",
      "8823 Training Loss: tensor(0.0441)\n",
      "8824 Training Loss: tensor(0.0450)\n",
      "8825 Training Loss: tensor(0.0473)\n",
      "8826 Training Loss: tensor(0.0457)\n",
      "8827 Training Loss: tensor(0.0514)\n",
      "8828 Training Loss: tensor(0.0476)\n",
      "8829 Training Loss: tensor(0.0437)\n",
      "8830 Training Loss: tensor(0.0466)\n",
      "8831 Training Loss: tensor(0.0444)\n",
      "8832 Training Loss: tensor(0.0471)\n",
      "8833 Training Loss: tensor(0.0444)\n",
      "8834 Training Loss: tensor(0.0421)\n",
      "8835 Training Loss: tensor(0.0443)\n",
      "8836 Training Loss: tensor(0.0438)\n",
      "8837 Training Loss: tensor(0.0466)\n",
      "8838 Training Loss: tensor(0.0414)\n",
      "8839 Training Loss: tensor(0.0429)\n",
      "8840 Training Loss: tensor(0.0452)\n",
      "8841 Training Loss: tensor(0.0439)\n",
      "8842 Training Loss: tensor(0.0421)\n",
      "8843 Training Loss: tensor(0.0356)\n",
      "8844 Training Loss: tensor(0.0436)\n",
      "8845 Training Loss: tensor(0.0452)\n",
      "8846 Training Loss: tensor(0.0404)\n",
      "8847 Training Loss: tensor(0.0445)\n",
      "8848 Training Loss: tensor(0.0500)\n",
      "8849 Training Loss: tensor(0.0463)\n",
      "8850 Training Loss: tensor(0.0423)\n",
      "8851 Training Loss: tensor(0.0414)\n",
      "8852 Training Loss: tensor(0.0465)\n",
      "8853 Training Loss: tensor(0.0463)\n",
      "8854 Training Loss: tensor(0.0462)\n",
      "8855 Training Loss: tensor(0.0430)\n",
      "8856 Training Loss: tensor(0.0505)\n",
      "8857 Training Loss: tensor(0.0441)\n",
      "8858 Training Loss: tensor(0.0455)\n",
      "8859 Training Loss: tensor(0.0444)\n",
      "8860 Training Loss: tensor(0.0512)\n",
      "8861 Training Loss: tensor(0.0467)\n",
      "8862 Training Loss: tensor(0.0470)\n",
      "8863 Training Loss: tensor(0.0413)\n",
      "8864 Training Loss: tensor(0.0460)\n",
      "8865 Training Loss: tensor(0.0470)\n",
      "8866 Training Loss: tensor(0.0449)\n",
      "8867 Training Loss: tensor(0.0501)\n",
      "8868 Training Loss: tensor(0.0418)\n",
      "8869 Training Loss: tensor(0.0467)\n",
      "8870 Training Loss: tensor(0.0454)\n",
      "8871 Training Loss: tensor(0.0426)\n",
      "8872 Training Loss: tensor(0.0423)\n",
      "8873 Training Loss: tensor(0.0483)\n",
      "8874 Training Loss: tensor(0.0452)\n",
      "8875 Training Loss: tensor(0.0456)\n",
      "8876 Training Loss: tensor(0.0457)\n",
      "8877 Training Loss: tensor(0.0474)\n",
      "8878 Training Loss: tensor(0.0406)\n",
      "8879 Training Loss: tensor(0.0429)\n",
      "8880 Training Loss: tensor(0.0448)\n",
      "8881 Training Loss: tensor(0.0500)\n",
      "8882 Training Loss: tensor(0.0433)\n",
      "8883 Training Loss: tensor(0.0513)\n",
      "8884 Training Loss: tensor(0.0487)\n",
      "8885 Training Loss: tensor(0.0472)\n",
      "8886 Training Loss: tensor(0.0436)\n",
      "8887 Training Loss: tensor(0.0431)\n",
      "8888 Training Loss: tensor(0.0462)\n",
      "8889 Training Loss: tensor(0.0410)\n",
      "8890 Training Loss: tensor(0.0466)\n",
      "8891 Training Loss: tensor(0.0396)\n",
      "8892 Training Loss: tensor(0.0455)\n",
      "8893 Training Loss: tensor(0.0447)\n",
      "8894 Training Loss: tensor(0.0435)\n",
      "8895 Training Loss: tensor(0.0443)\n",
      "8896 Training Loss: tensor(0.0413)\n",
      "8897 Training Loss: tensor(0.0431)\n",
      "8898 Training Loss: tensor(0.0450)\n",
      "8899 Training Loss: tensor(0.0452)\n",
      "8900 Training Loss: tensor(0.0470)\n",
      "8901 Training Loss: tensor(0.0474)\n",
      "8902 Training Loss: tensor(0.0452)\n",
      "8903 Training Loss: tensor(0.0477)\n",
      "8904 Training Loss: tensor(0.0438)\n",
      "8905 Training Loss: tensor(0.0433)\n",
      "8906 Training Loss: tensor(0.0469)\n",
      "8907 Training Loss: tensor(0.0461)\n",
      "8908 Training Loss: tensor(0.0414)\n",
      "8909 Training Loss: tensor(0.0458)\n",
      "8910 Training Loss: tensor(0.0438)\n",
      "8911 Training Loss: tensor(0.0458)\n",
      "8912 Training Loss: tensor(0.0500)\n",
      "8913 Training Loss: tensor(0.0473)\n",
      "8914 Training Loss: tensor(0.0514)\n",
      "8915 Training Loss: tensor(0.0425)\n",
      "8916 Training Loss: tensor(0.0488)\n",
      "8917 Training Loss: tensor(0.0466)\n",
      "8918 Training Loss: tensor(0.0447)\n",
      "8919 Training Loss: tensor(0.0523)\n",
      "8920 Training Loss: tensor(0.0469)\n",
      "8921 Training Loss: tensor(0.0463)\n",
      "8922 Training Loss: tensor(0.0428)\n",
      "8923 Training Loss: tensor(0.0506)\n",
      "8924 Training Loss: tensor(0.0392)\n",
      "8925 Training Loss: tensor(0.0493)\n",
      "8926 Training Loss: tensor(0.0455)\n",
      "8927 Training Loss: tensor(0.0411)\n",
      "8928 Training Loss: tensor(0.0419)\n",
      "8929 Training Loss: tensor(0.0429)\n",
      "8930 Training Loss: tensor(0.0412)\n",
      "8931 Training Loss: tensor(0.0430)\n",
      "8932 Training Loss: tensor(0.0500)\n",
      "8933 Training Loss: tensor(0.0405)\n",
      "8934 Training Loss: tensor(0.0451)\n",
      "8935 Training Loss: tensor(0.0465)\n",
      "8936 Training Loss: tensor(0.0410)\n",
      "8937 Training Loss: tensor(0.0378)\n",
      "8938 Training Loss: tensor(0.0483)\n",
      "8939 Training Loss: tensor(0.0414)\n",
      "8940 Training Loss: tensor(0.0449)\n",
      "8941 Training Loss: tensor(0.0468)\n",
      "8942 Training Loss: tensor(0.0470)\n",
      "8943 Training Loss: tensor(0.0402)\n",
      "8944 Training Loss: tensor(0.0456)\n",
      "8945 Training Loss: tensor(0.0380)\n",
      "8946 Training Loss: tensor(0.0419)\n",
      "8947 Training Loss: tensor(0.0405)\n",
      "8948 Training Loss: tensor(0.0467)\n",
      "8949 Training Loss: tensor(0.0464)\n",
      "8950 Training Loss: tensor(0.0439)\n",
      "8951 Training Loss: tensor(0.0424)\n",
      "8952 Training Loss: tensor(0.0432)\n",
      "8953 Training Loss: tensor(0.0438)\n",
      "8954 Training Loss: tensor(0.0444)\n",
      "8955 Training Loss: tensor(0.0459)\n",
      "8956 Training Loss: tensor(0.0422)\n",
      "8957 Training Loss: tensor(0.0473)\n",
      "8958 Training Loss: tensor(0.0458)\n",
      "8959 Training Loss: tensor(0.0503)\n",
      "8960 Training Loss: tensor(0.0455)\n",
      "8961 Training Loss: tensor(0.0399)\n",
      "8962 Training Loss: tensor(0.0440)\n",
      "8963 Training Loss: tensor(0.0445)\n",
      "8964 Training Loss: tensor(0.0447)\n",
      "8965 Training Loss: tensor(0.0510)\n",
      "8966 Training Loss: tensor(0.0419)\n",
      "8967 Training Loss: tensor(0.0402)\n",
      "8968 Training Loss: tensor(0.0455)\n",
      "8969 Training Loss: tensor(0.0461)\n",
      "8970 Training Loss: tensor(0.0489)\n",
      "8971 Training Loss: tensor(0.0440)\n",
      "8972 Training Loss: tensor(0.0487)\n",
      "8973 Training Loss: tensor(0.0371)\n",
      "8974 Training Loss: tensor(0.0468)\n",
      "8975 Training Loss: tensor(0.0461)\n",
      "8976 Training Loss: tensor(0.0400)\n",
      "8977 Training Loss: tensor(0.0458)\n",
      "8978 Training Loss: tensor(0.0488)\n",
      "8979 Training Loss: tensor(0.0423)\n",
      "8980 Training Loss: tensor(0.0444)\n",
      "8981 Training Loss: tensor(0.0404)\n",
      "8982 Training Loss: tensor(0.0407)\n",
      "8983 Training Loss: tensor(0.0443)\n",
      "8984 Training Loss: tensor(0.0409)\n",
      "8985 Training Loss: tensor(0.0412)\n",
      "8986 Training Loss: tensor(0.0506)\n",
      "8987 Training Loss: tensor(0.0426)\n",
      "8988 Training Loss: tensor(0.0443)\n",
      "8989 Training Loss: tensor(0.0372)\n",
      "8990 Training Loss: tensor(0.0449)\n",
      "8991 Training Loss: tensor(0.0430)\n",
      "8992 Training Loss: tensor(0.0513)\n",
      "8993 Training Loss: tensor(0.0434)\n",
      "8994 Training Loss: tensor(0.0463)\n",
      "8995 Training Loss: tensor(0.0445)\n",
      "8996 Training Loss: tensor(0.0370)\n",
      "8997 Training Loss: tensor(0.0374)\n",
      "8998 Training Loss: tensor(0.0399)\n",
      "8999 Training Loss: tensor(0.0394)\n",
      "9000 Training Loss: tensor(0.0444)\n",
      "9001 Training Loss: tensor(0.0420)\n",
      "9002 Training Loss: tensor(0.0442)\n",
      "9003 Training Loss: tensor(0.0402)\n",
      "9004 Training Loss: tensor(0.0410)\n",
      "9005 Training Loss: tensor(0.0446)\n",
      "9006 Training Loss: tensor(0.0455)\n",
      "9007 Training Loss: tensor(0.0423)\n",
      "9008 Training Loss: tensor(0.0468)\n",
      "9009 Training Loss: tensor(0.0419)\n",
      "9010 Training Loss: tensor(0.0463)\n",
      "9011 Training Loss: tensor(0.0484)\n",
      "9012 Training Loss: tensor(0.0436)\n",
      "9013 Training Loss: tensor(0.0431)\n",
      "9014 Training Loss: tensor(0.0399)\n",
      "9015 Training Loss: tensor(0.0461)\n",
      "9016 Training Loss: tensor(0.0498)\n",
      "9017 Training Loss: tensor(0.0361)\n",
      "9018 Training Loss: tensor(0.0385)\n",
      "9019 Training Loss: tensor(0.0453)\n",
      "9020 Training Loss: tensor(0.0463)\n",
      "9021 Training Loss: tensor(0.0434)\n",
      "9022 Training Loss: tensor(0.0444)\n",
      "9023 Training Loss: tensor(0.0390)\n",
      "9024 Training Loss: tensor(0.0417)\n",
      "9025 Training Loss: tensor(0.0393)\n",
      "9026 Training Loss: tensor(0.0396)\n",
      "9027 Training Loss: tensor(0.0447)\n",
      "9028 Training Loss: tensor(0.0455)\n",
      "9029 Training Loss: tensor(0.0394)\n",
      "9030 Training Loss: tensor(0.0465)\n",
      "9031 Training Loss: tensor(0.0449)\n",
      "9032 Training Loss: tensor(0.0502)\n",
      "9033 Training Loss: tensor(0.0472)\n",
      "9034 Training Loss: tensor(0.0446)\n",
      "9035 Training Loss: tensor(0.0498)\n",
      "9036 Training Loss: tensor(0.0434)\n",
      "9037 Training Loss: tensor(0.0412)\n",
      "9038 Training Loss: tensor(0.0456)\n",
      "9039 Training Loss: tensor(0.0432)\n",
      "9040 Training Loss: tensor(0.0457)\n",
      "9041 Training Loss: tensor(0.0428)\n",
      "9042 Training Loss: tensor(0.0409)\n",
      "9043 Training Loss: tensor(0.0552)\n",
      "9044 Training Loss: tensor(0.0483)\n",
      "9045 Training Loss: tensor(0.0395)\n",
      "9046 Training Loss: tensor(0.0464)\n",
      "9047 Training Loss: tensor(0.0470)\n",
      "9048 Training Loss: tensor(0.0432)\n",
      "9049 Training Loss: tensor(0.0481)\n",
      "9050 Training Loss: tensor(0.0396)\n",
      "9051 Training Loss: tensor(0.0387)\n",
      "9052 Training Loss: tensor(0.0510)\n",
      "9053 Training Loss: tensor(0.0408)\n",
      "9054 Training Loss: tensor(0.0479)\n",
      "9055 Training Loss: tensor(0.0422)\n",
      "9056 Training Loss: tensor(0.0478)\n",
      "9057 Training Loss: tensor(0.0415)\n",
      "9058 Training Loss: tensor(0.0404)\n",
      "9059 Training Loss: tensor(0.0445)\n",
      "9060 Training Loss: tensor(0.0487)\n",
      "9061 Training Loss: tensor(0.0440)\n",
      "9062 Training Loss: tensor(0.0513)\n",
      "9063 Training Loss: tensor(0.0409)\n",
      "9064 Training Loss: tensor(0.0470)\n",
      "9065 Training Loss: tensor(0.0439)\n",
      "9066 Training Loss: tensor(0.0506)\n",
      "9067 Training Loss: tensor(0.0500)\n",
      "9068 Training Loss: tensor(0.0456)\n",
      "9069 Training Loss: tensor(0.0407)\n",
      "9070 Training Loss: tensor(0.0474)\n",
      "9071 Training Loss: tensor(0.0441)\n",
      "9072 Training Loss: tensor(0.0522)\n",
      "9073 Training Loss: tensor(0.0465)\n",
      "9074 Training Loss: tensor(0.0474)\n",
      "9075 Training Loss: tensor(0.0476)\n",
      "9076 Training Loss: tensor(0.0446)\n",
      "9077 Training Loss: tensor(0.0420)\n",
      "9078 Training Loss: tensor(0.0463)\n",
      "9079 Training Loss: tensor(0.0441)\n",
      "9080 Training Loss: tensor(0.0444)\n",
      "9081 Training Loss: tensor(0.0459)\n",
      "9082 Training Loss: tensor(0.0419)\n",
      "9083 Training Loss: tensor(0.0406)\n",
      "9084 Training Loss: tensor(0.0432)\n",
      "9085 Training Loss: tensor(0.0447)\n",
      "9086 Training Loss: tensor(0.0457)\n",
      "9087 Training Loss: tensor(0.0476)\n",
      "9088 Training Loss: tensor(0.0444)\n",
      "9089 Training Loss: tensor(0.0398)\n",
      "9090 Training Loss: tensor(0.0390)\n",
      "9091 Training Loss: tensor(0.0399)\n",
      "9092 Training Loss: tensor(0.0440)\n",
      "9093 Training Loss: tensor(0.0410)\n",
      "9094 Training Loss: tensor(0.0429)\n",
      "9095 Training Loss: tensor(0.0383)\n",
      "9096 Training Loss: tensor(0.0409)\n",
      "9097 Training Loss: tensor(0.0429)\n",
      "9098 Training Loss: tensor(0.0443)\n",
      "9099 Training Loss: tensor(0.0381)\n",
      "9100 Training Loss: tensor(0.0464)\n",
      "9101 Training Loss: tensor(0.0460)\n",
      "9102 Training Loss: tensor(0.0418)\n",
      "9103 Training Loss: tensor(0.0436)\n",
      "9104 Training Loss: tensor(0.0465)\n",
      "9105 Training Loss: tensor(0.0492)\n",
      "9106 Training Loss: tensor(0.0456)\n",
      "9107 Training Loss: tensor(0.0526)\n",
      "9108 Training Loss: tensor(0.0419)\n",
      "9109 Training Loss: tensor(0.0400)\n",
      "9110 Training Loss: tensor(0.0443)\n",
      "9111 Training Loss: tensor(0.0444)\n",
      "9112 Training Loss: tensor(0.0407)\n",
      "9113 Training Loss: tensor(0.0388)\n",
      "9114 Training Loss: tensor(0.0498)\n",
      "9115 Training Loss: tensor(0.0462)\n",
      "9116 Training Loss: tensor(0.0453)\n",
      "9117 Training Loss: tensor(0.0412)\n",
      "9118 Training Loss: tensor(0.0433)\n",
      "9119 Training Loss: tensor(0.0464)\n",
      "9120 Training Loss: tensor(0.0473)\n",
      "9121 Training Loss: tensor(0.0419)\n",
      "9122 Training Loss: tensor(0.0445)\n",
      "9123 Training Loss: tensor(0.0457)\n",
      "9124 Training Loss: tensor(0.0427)\n",
      "9125 Training Loss: tensor(0.0454)\n",
      "9126 Training Loss: tensor(0.0472)\n",
      "9127 Training Loss: tensor(0.0491)\n",
      "9128 Training Loss: tensor(0.0458)\n",
      "9129 Training Loss: tensor(0.0450)\n",
      "9130 Training Loss: tensor(0.0450)\n",
      "9131 Training Loss: tensor(0.0415)\n",
      "9132 Training Loss: tensor(0.0419)\n",
      "9133 Training Loss: tensor(0.0423)\n",
      "9134 Training Loss: tensor(0.0475)\n",
      "9135 Training Loss: tensor(0.0447)\n",
      "9136 Training Loss: tensor(0.0424)\n",
      "9137 Training Loss: tensor(0.0390)\n",
      "9138 Training Loss: tensor(0.0412)\n",
      "9139 Training Loss: tensor(0.0529)\n",
      "9140 Training Loss: tensor(0.0508)\n",
      "9141 Training Loss: tensor(0.0442)\n",
      "9142 Training Loss: tensor(0.0428)\n",
      "9143 Training Loss: tensor(0.0394)\n",
      "9144 Training Loss: tensor(0.0447)\n",
      "9145 Training Loss: tensor(0.0435)\n",
      "9146 Training Loss: tensor(0.0480)\n",
      "9147 Training Loss: tensor(0.0441)\n",
      "9148 Training Loss: tensor(0.0437)\n",
      "9149 Training Loss: tensor(0.0504)\n",
      "9150 Training Loss: tensor(0.0426)\n",
      "9151 Training Loss: tensor(0.0418)\n",
      "9152 Training Loss: tensor(0.0465)\n",
      "9153 Training Loss: tensor(0.0442)\n",
      "9154 Training Loss: tensor(0.0441)\n",
      "9155 Training Loss: tensor(0.0448)\n",
      "9156 Training Loss: tensor(0.0444)\n",
      "9157 Training Loss: tensor(0.0463)\n",
      "9158 Training Loss: tensor(0.0464)\n",
      "9159 Training Loss: tensor(0.0502)\n",
      "9160 Training Loss: tensor(0.0388)\n",
      "9161 Training Loss: tensor(0.0410)\n",
      "9162 Training Loss: tensor(0.0448)\n",
      "9163 Training Loss: tensor(0.0393)\n",
      "9164 Training Loss: tensor(0.0450)\n",
      "9165 Training Loss: tensor(0.0476)\n",
      "9166 Training Loss: tensor(0.0408)\n",
      "9167 Training Loss: tensor(0.0428)\n",
      "9168 Training Loss: tensor(0.0444)\n",
      "9169 Training Loss: tensor(0.0505)\n",
      "9170 Training Loss: tensor(0.0409)\n",
      "9171 Training Loss: tensor(0.0454)\n",
      "9172 Training Loss: tensor(0.0478)\n",
      "9173 Training Loss: tensor(0.0449)\n",
      "9174 Training Loss: tensor(0.0464)\n",
      "9175 Training Loss: tensor(0.0479)\n",
      "9176 Training Loss: tensor(0.0391)\n",
      "9177 Training Loss: tensor(0.0442)\n",
      "9178 Training Loss: tensor(0.0395)\n",
      "9179 Training Loss: tensor(0.0445)\n",
      "9180 Training Loss: tensor(0.0456)\n",
      "9181 Training Loss: tensor(0.0399)\n",
      "9182 Training Loss: tensor(0.0475)\n",
      "9183 Training Loss: tensor(0.0400)\n",
      "9184 Training Loss: tensor(0.0402)\n",
      "9185 Training Loss: tensor(0.0428)\n",
      "9186 Training Loss: tensor(0.0467)\n",
      "9187 Training Loss: tensor(0.0481)\n",
      "9188 Training Loss: tensor(0.0411)\n",
      "9189 Training Loss: tensor(0.0393)\n",
      "9190 Training Loss: tensor(0.0409)\n",
      "9191 Training Loss: tensor(0.0446)\n",
      "9192 Training Loss: tensor(0.0479)\n",
      "9193 Training Loss: tensor(0.0458)\n",
      "9194 Training Loss: tensor(0.0442)\n",
      "9195 Training Loss: tensor(0.0434)\n",
      "9196 Training Loss: tensor(0.0424)\n",
      "9197 Training Loss: tensor(0.0362)\n",
      "9198 Training Loss: tensor(0.0467)\n",
      "9199 Training Loss: tensor(0.0453)\n",
      "9200 Training Loss: tensor(0.0379)\n",
      "9201 Training Loss: tensor(0.0421)\n",
      "9202 Training Loss: tensor(0.0432)\n",
      "9203 Training Loss: tensor(0.0429)\n",
      "9204 Training Loss: tensor(0.0466)\n",
      "9205 Training Loss: tensor(0.0412)\n",
      "9206 Training Loss: tensor(0.0418)\n",
      "9207 Training Loss: tensor(0.0458)\n",
      "9208 Training Loss: tensor(0.0448)\n",
      "9209 Training Loss: tensor(0.0397)\n",
      "9210 Training Loss: tensor(0.0387)\n",
      "9211 Training Loss: tensor(0.0450)\n",
      "9212 Training Loss: tensor(0.0404)\n",
      "9213 Training Loss: tensor(0.0446)\n",
      "9214 Training Loss: tensor(0.0433)\n",
      "9215 Training Loss: tensor(0.0447)\n",
      "9216 Training Loss: tensor(0.0486)\n",
      "9217 Training Loss: tensor(0.0456)\n",
      "9218 Training Loss: tensor(0.0409)\n",
      "9219 Training Loss: tensor(0.0388)\n",
      "9220 Training Loss: tensor(0.0458)\n",
      "9221 Training Loss: tensor(0.0451)\n",
      "9222 Training Loss: tensor(0.0438)\n",
      "9223 Training Loss: tensor(0.0493)\n",
      "9224 Training Loss: tensor(0.0421)\n",
      "9225 Training Loss: tensor(0.0417)\n",
      "9226 Training Loss: tensor(0.0443)\n",
      "9227 Training Loss: tensor(0.0419)\n",
      "9228 Training Loss: tensor(0.0477)\n",
      "9229 Training Loss: tensor(0.0433)\n",
      "9230 Training Loss: tensor(0.0490)\n",
      "9231 Training Loss: tensor(0.0409)\n",
      "9232 Training Loss: tensor(0.0436)\n",
      "9233 Training Loss: tensor(0.0457)\n",
      "9234 Training Loss: tensor(0.0418)\n",
      "9235 Training Loss: tensor(0.0544)\n",
      "9236 Training Loss: tensor(0.0446)\n",
      "9237 Training Loss: tensor(0.0403)\n",
      "9238 Training Loss: tensor(0.0443)\n",
      "9239 Training Loss: tensor(0.0454)\n",
      "9240 Training Loss: tensor(0.0464)\n",
      "9241 Training Loss: tensor(0.0454)\n",
      "9242 Training Loss: tensor(0.0473)\n",
      "9243 Training Loss: tensor(0.0489)\n",
      "9244 Training Loss: tensor(0.0426)\n",
      "9245 Training Loss: tensor(0.0447)\n",
      "9246 Training Loss: tensor(0.0388)\n",
      "9247 Training Loss: tensor(0.0417)\n",
      "9248 Training Loss: tensor(0.0531)\n",
      "9249 Training Loss: tensor(0.0454)\n",
      "9250 Training Loss: tensor(0.0422)\n",
      "9251 Training Loss: tensor(0.0456)\n",
      "9252 Training Loss: tensor(0.0451)\n",
      "9253 Training Loss: tensor(0.0442)\n",
      "9254 Training Loss: tensor(0.0438)\n",
      "9255 Training Loss: tensor(0.0450)\n",
      "9256 Training Loss: tensor(0.0431)\n",
      "9257 Training Loss: tensor(0.0457)\n",
      "9258 Training Loss: tensor(0.0420)\n",
      "9259 Training Loss: tensor(0.0411)\n",
      "9260 Training Loss: tensor(0.0451)\n",
      "9261 Training Loss: tensor(0.0442)\n",
      "9262 Training Loss: tensor(0.0378)\n",
      "9263 Training Loss: tensor(0.0417)\n",
      "9264 Training Loss: tensor(0.0442)\n",
      "9265 Training Loss: tensor(0.0372)\n",
      "9266 Training Loss: tensor(0.0487)\n",
      "9267 Training Loss: tensor(0.0461)\n",
      "9268 Training Loss: tensor(0.0434)\n",
      "9269 Training Loss: tensor(0.0464)\n",
      "9270 Training Loss: tensor(0.0393)\n",
      "9271 Training Loss: tensor(0.0429)\n",
      "9272 Training Loss: tensor(0.0507)\n",
      "9273 Training Loss: tensor(0.0416)\n",
      "9274 Training Loss: tensor(0.0407)\n",
      "9275 Training Loss: tensor(0.0450)\n",
      "9276 Training Loss: tensor(0.0422)\n",
      "9277 Training Loss: tensor(0.0407)\n",
      "9278 Training Loss: tensor(0.0500)\n",
      "9279 Training Loss: tensor(0.0442)\n",
      "9280 Training Loss: tensor(0.0449)\n",
      "9281 Training Loss: tensor(0.0451)\n",
      "9282 Training Loss: tensor(0.0460)\n",
      "9283 Training Loss: tensor(0.0515)\n",
      "9284 Training Loss: tensor(0.0460)\n",
      "9285 Training Loss: tensor(0.0405)\n",
      "9286 Training Loss: tensor(0.0543)\n",
      "9287 Training Loss: tensor(0.0430)\n",
      "9288 Training Loss: tensor(0.0428)\n",
      "9289 Training Loss: tensor(0.0417)\n",
      "9290 Training Loss: tensor(0.0441)\n",
      "9291 Training Loss: tensor(0.0405)\n",
      "9292 Training Loss: tensor(0.0438)\n",
      "9293 Training Loss: tensor(0.0415)\n",
      "9294 Training Loss: tensor(0.0387)\n",
      "9295 Training Loss: tensor(0.0416)\n",
      "9296 Training Loss: tensor(0.0453)\n",
      "9297 Training Loss: tensor(0.0440)\n",
      "9298 Training Loss: tensor(0.0407)\n",
      "9299 Training Loss: tensor(0.0402)\n",
      "9300 Training Loss: tensor(0.0428)\n",
      "9301 Training Loss: tensor(0.0420)\n",
      "9302 Training Loss: tensor(0.0489)\n",
      "9303 Training Loss: tensor(0.0468)\n",
      "9304 Training Loss: tensor(0.0417)\n",
      "9305 Training Loss: tensor(0.0466)\n",
      "9306 Training Loss: tensor(0.0436)\n",
      "9307 Training Loss: tensor(0.0504)\n",
      "9308 Training Loss: tensor(0.0416)\n",
      "9309 Training Loss: tensor(0.0373)\n",
      "9310 Training Loss: tensor(0.0518)\n",
      "9311 Training Loss: tensor(0.0393)\n",
      "9312 Training Loss: tensor(0.0380)\n",
      "9313 Training Loss: tensor(0.0437)\n",
      "9314 Training Loss: tensor(0.0456)\n",
      "9315 Training Loss: tensor(0.0426)\n",
      "9316 Training Loss: tensor(0.0420)\n",
      "9317 Training Loss: tensor(0.0436)\n",
      "9318 Training Loss: tensor(0.0446)\n",
      "9319 Training Loss: tensor(0.0424)\n",
      "9320 Training Loss: tensor(0.0444)\n",
      "9321 Training Loss: tensor(0.0428)\n",
      "9322 Training Loss: tensor(0.0417)\n",
      "9323 Training Loss: tensor(0.0446)\n",
      "9324 Training Loss: tensor(0.0424)\n",
      "9325 Training Loss: tensor(0.0406)\n",
      "9326 Training Loss: tensor(0.0461)\n",
      "9327 Training Loss: tensor(0.0410)\n",
      "9328 Training Loss: tensor(0.0449)\n",
      "9329 Training Loss: tensor(0.0418)\n",
      "9330 Training Loss: tensor(0.0398)\n",
      "9331 Training Loss: tensor(0.0500)\n",
      "9332 Training Loss: tensor(0.0455)\n",
      "9333 Training Loss: tensor(0.0500)\n",
      "9334 Training Loss: tensor(0.0438)\n",
      "9335 Training Loss: tensor(0.0454)\n",
      "9336 Training Loss: tensor(0.0407)\n",
      "9337 Training Loss: tensor(0.0437)\n",
      "9338 Training Loss: tensor(0.0405)\n",
      "9339 Training Loss: tensor(0.0445)\n",
      "9340 Training Loss: tensor(0.0425)\n",
      "9341 Training Loss: tensor(0.0414)\n",
      "9342 Training Loss: tensor(0.0432)\n",
      "9343 Training Loss: tensor(0.0432)\n",
      "9344 Training Loss: tensor(0.0403)\n",
      "9345 Training Loss: tensor(0.0399)\n",
      "9346 Training Loss: tensor(0.0391)\n",
      "9347 Training Loss: tensor(0.0414)\n",
      "9348 Training Loss: tensor(0.0427)\n",
      "9349 Training Loss: tensor(0.0393)\n",
      "9350 Training Loss: tensor(0.0485)\n",
      "9351 Training Loss: tensor(0.0368)\n",
      "9352 Training Loss: tensor(0.0456)\n",
      "9353 Training Loss: tensor(0.0430)\n",
      "9354 Training Loss: tensor(0.0451)\n",
      "9355 Training Loss: tensor(0.0388)\n",
      "9356 Training Loss: tensor(0.0429)\n",
      "9357 Training Loss: tensor(0.0455)\n",
      "9358 Training Loss: tensor(0.0472)\n",
      "9359 Training Loss: tensor(0.0506)\n",
      "9360 Training Loss: tensor(0.0409)\n",
      "9361 Training Loss: tensor(0.0503)\n",
      "9362 Training Loss: tensor(0.0435)\n",
      "9363 Training Loss: tensor(0.0388)\n",
      "9364 Training Loss: tensor(0.0378)\n",
      "9365 Training Loss: tensor(0.0449)\n",
      "9366 Training Loss: tensor(0.0400)\n",
      "9367 Training Loss: tensor(0.0463)\n",
      "9368 Training Loss: tensor(0.0369)\n",
      "9369 Training Loss: tensor(0.0431)\n",
      "9370 Training Loss: tensor(0.0458)\n",
      "9371 Training Loss: tensor(0.0444)\n",
      "9372 Training Loss: tensor(0.0441)\n",
      "9373 Training Loss: tensor(0.0373)\n",
      "9374 Training Loss: tensor(0.0484)\n",
      "9375 Training Loss: tensor(0.0409)\n",
      "9376 Training Loss: tensor(0.0454)\n",
      "9377 Training Loss: tensor(0.0423)\n",
      "9378 Training Loss: tensor(0.0437)\n",
      "9379 Training Loss: tensor(0.0431)\n",
      "9380 Training Loss: tensor(0.0443)\n",
      "9381 Training Loss: tensor(0.0481)\n",
      "9382 Training Loss: tensor(0.0380)\n",
      "9383 Training Loss: tensor(0.0428)\n",
      "9384 Training Loss: tensor(0.0395)\n",
      "9385 Training Loss: tensor(0.0432)\n",
      "9386 Training Loss: tensor(0.0391)\n",
      "9387 Training Loss: tensor(0.0435)\n",
      "9388 Training Loss: tensor(0.0400)\n",
      "9389 Training Loss: tensor(0.0467)\n",
      "9390 Training Loss: tensor(0.0480)\n",
      "9391 Training Loss: tensor(0.0460)\n",
      "9392 Training Loss: tensor(0.0474)\n",
      "9393 Training Loss: tensor(0.0424)\n",
      "9394 Training Loss: tensor(0.0419)\n",
      "9395 Training Loss: tensor(0.0461)\n",
      "9396 Training Loss: tensor(0.0393)\n",
      "9397 Training Loss: tensor(0.0498)\n",
      "9398 Training Loss: tensor(0.0440)\n",
      "9399 Training Loss: tensor(0.0431)\n",
      "9400 Training Loss: tensor(0.0370)\n",
      "9401 Training Loss: tensor(0.0427)\n",
      "9402 Training Loss: tensor(0.0441)\n",
      "9403 Training Loss: tensor(0.0487)\n",
      "9404 Training Loss: tensor(0.0404)\n",
      "9405 Training Loss: tensor(0.0470)\n",
      "9406 Training Loss: tensor(0.0420)\n",
      "9407 Training Loss: tensor(0.0469)\n",
      "9408 Training Loss: tensor(0.0494)\n",
      "9409 Training Loss: tensor(0.0385)\n",
      "9410 Training Loss: tensor(0.0438)\n",
      "9411 Training Loss: tensor(0.0465)\n",
      "9412 Training Loss: tensor(0.0422)\n",
      "9413 Training Loss: tensor(0.0441)\n",
      "9414 Training Loss: tensor(0.0437)\n",
      "9415 Training Loss: tensor(0.0466)\n",
      "9416 Training Loss: tensor(0.0482)\n",
      "9417 Training Loss: tensor(0.0375)\n",
      "9418 Training Loss: tensor(0.0449)\n",
      "9419 Training Loss: tensor(0.0395)\n",
      "9420 Training Loss: tensor(0.0443)\n",
      "9421 Training Loss: tensor(0.0401)\n",
      "9422 Training Loss: tensor(0.0450)\n",
      "9423 Training Loss: tensor(0.0410)\n",
      "9424 Training Loss: tensor(0.0405)\n",
      "9425 Training Loss: tensor(0.0486)\n",
      "9426 Training Loss: tensor(0.0385)\n",
      "9427 Training Loss: tensor(0.0476)\n",
      "9428 Training Loss: tensor(0.0386)\n",
      "9429 Training Loss: tensor(0.0480)\n",
      "9430 Training Loss: tensor(0.0468)\n",
      "9431 Training Loss: tensor(0.0409)\n",
      "9432 Training Loss: tensor(0.0379)\n",
      "9433 Training Loss: tensor(0.0402)\n",
      "9434 Training Loss: tensor(0.0502)\n",
      "9435 Training Loss: tensor(0.0450)\n",
      "9436 Training Loss: tensor(0.0420)\n",
      "9437 Training Loss: tensor(0.0432)\n",
      "9438 Training Loss: tensor(0.0440)\n",
      "9439 Training Loss: tensor(0.0434)\n",
      "9440 Training Loss: tensor(0.0423)\n",
      "9441 Training Loss: tensor(0.0511)\n",
      "9442 Training Loss: tensor(0.0447)\n",
      "9443 Training Loss: tensor(0.0425)\n",
      "9444 Training Loss: tensor(0.0395)\n",
      "9445 Training Loss: tensor(0.0453)\n",
      "9446 Training Loss: tensor(0.0416)\n",
      "9447 Training Loss: tensor(0.0392)\n",
      "9448 Training Loss: tensor(0.0410)\n",
      "9449 Training Loss: tensor(0.0448)\n",
      "9450 Training Loss: tensor(0.0410)\n",
      "9451 Training Loss: tensor(0.0369)\n",
      "9452 Training Loss: tensor(0.0444)\n",
      "9453 Training Loss: tensor(0.0423)\n",
      "9454 Training Loss: tensor(0.0429)\n",
      "9455 Training Loss: tensor(0.0450)\n",
      "9456 Training Loss: tensor(0.0416)\n",
      "9457 Training Loss: tensor(0.0405)\n",
      "9458 Training Loss: tensor(0.0439)\n",
      "9459 Training Loss: tensor(0.0368)\n",
      "9460 Training Loss: tensor(0.0441)\n",
      "9461 Training Loss: tensor(0.0379)\n",
      "9462 Training Loss: tensor(0.0383)\n",
      "9463 Training Loss: tensor(0.0362)\n",
      "9464 Training Loss: tensor(0.0445)\n",
      "9465 Training Loss: tensor(0.0436)\n",
      "9466 Training Loss: tensor(0.0452)\n",
      "9467 Training Loss: tensor(0.0450)\n",
      "9468 Training Loss: tensor(0.0460)\n",
      "9469 Training Loss: tensor(0.0458)\n",
      "9470 Training Loss: tensor(0.0476)\n",
      "9471 Training Loss: tensor(0.0464)\n",
      "9472 Training Loss: tensor(0.0425)\n",
      "9473 Training Loss: tensor(0.0510)\n",
      "9474 Training Loss: tensor(0.0477)\n",
      "9475 Training Loss: tensor(0.0427)\n",
      "9476 Training Loss: tensor(0.0463)\n",
      "9477 Training Loss: tensor(0.0386)\n",
      "9478 Training Loss: tensor(0.0428)\n",
      "9479 Training Loss: tensor(0.0401)\n",
      "9480 Training Loss: tensor(0.0449)\n",
      "9481 Training Loss: tensor(0.0419)\n",
      "9482 Training Loss: tensor(0.0449)\n",
      "9483 Training Loss: tensor(0.0418)\n",
      "9484 Training Loss: tensor(0.0453)\n",
      "9485 Training Loss: tensor(0.0405)\n",
      "9486 Training Loss: tensor(0.0410)\n",
      "9487 Training Loss: tensor(0.0470)\n",
      "9488 Training Loss: tensor(0.0482)\n",
      "9489 Training Loss: tensor(0.0387)\n",
      "9490 Training Loss: tensor(0.0451)\n",
      "9491 Training Loss: tensor(0.0457)\n",
      "9492 Training Loss: tensor(0.0445)\n",
      "9493 Training Loss: tensor(0.0463)\n",
      "9494 Training Loss: tensor(0.0433)\n",
      "9495 Training Loss: tensor(0.0461)\n",
      "9496 Training Loss: tensor(0.0453)\n",
      "9497 Training Loss: tensor(0.0425)\n",
      "9498 Training Loss: tensor(0.0476)\n",
      "9499 Training Loss: tensor(0.0455)\n",
      "9500 Training Loss: tensor(0.0441)\n",
      "9501 Training Loss: tensor(0.0488)\n",
      "9502 Training Loss: tensor(0.0430)\n",
      "9503 Training Loss: tensor(0.0429)\n",
      "9504 Training Loss: tensor(0.0430)\n",
      "9505 Training Loss: tensor(0.0439)\n",
      "9506 Training Loss: tensor(0.0443)\n",
      "9507 Training Loss: tensor(0.0434)\n",
      "9508 Training Loss: tensor(0.0406)\n",
      "9509 Training Loss: tensor(0.0391)\n",
      "9510 Training Loss: tensor(0.0370)\n",
      "9511 Training Loss: tensor(0.0458)\n",
      "9512 Training Loss: tensor(0.0450)\n",
      "9513 Training Loss: tensor(0.0413)\n",
      "9514 Training Loss: tensor(0.0428)\n",
      "9515 Training Loss: tensor(0.0454)\n",
      "9516 Training Loss: tensor(0.0436)\n",
      "9517 Training Loss: tensor(0.0463)\n",
      "9518 Training Loss: tensor(0.0405)\n",
      "9519 Training Loss: tensor(0.0449)\n",
      "9520 Training Loss: tensor(0.0451)\n",
      "9521 Training Loss: tensor(0.0451)\n",
      "9522 Training Loss: tensor(0.0364)\n",
      "9523 Training Loss: tensor(0.0395)\n",
      "9524 Training Loss: tensor(0.0454)\n",
      "9525 Training Loss: tensor(0.0428)\n",
      "9526 Training Loss: tensor(0.0419)\n",
      "9527 Training Loss: tensor(0.0424)\n",
      "9528 Training Loss: tensor(0.0409)\n",
      "9529 Training Loss: tensor(0.0418)\n",
      "9530 Training Loss: tensor(0.0407)\n",
      "9531 Training Loss: tensor(0.0409)\n",
      "9532 Training Loss: tensor(0.0488)\n",
      "9533 Training Loss: tensor(0.0431)\n",
      "9534 Training Loss: tensor(0.0392)\n",
      "9535 Training Loss: tensor(0.0428)\n",
      "9536 Training Loss: tensor(0.0448)\n",
      "9537 Training Loss: tensor(0.0444)\n",
      "9538 Training Loss: tensor(0.0430)\n",
      "9539 Training Loss: tensor(0.0425)\n",
      "9540 Training Loss: tensor(0.0464)\n",
      "9541 Training Loss: tensor(0.0452)\n",
      "9542 Training Loss: tensor(0.0497)\n",
      "9543 Training Loss: tensor(0.0405)\n",
      "9544 Training Loss: tensor(0.0460)\n",
      "9545 Training Loss: tensor(0.0411)\n",
      "9546 Training Loss: tensor(0.0444)\n",
      "9547 Training Loss: tensor(0.0396)\n",
      "9548 Training Loss: tensor(0.0504)\n",
      "9549 Training Loss: tensor(0.0506)\n",
      "9550 Training Loss: tensor(0.0429)\n",
      "9551 Training Loss: tensor(0.0482)\n",
      "9552 Training Loss: tensor(0.0443)\n",
      "9553 Training Loss: tensor(0.0422)\n",
      "9554 Training Loss: tensor(0.0484)\n",
      "9555 Training Loss: tensor(0.0435)\n",
      "9556 Training Loss: tensor(0.0479)\n",
      "9557 Training Loss: tensor(0.0448)\n",
      "9558 Training Loss: tensor(0.0484)\n",
      "9559 Training Loss: tensor(0.0479)\n",
      "9560 Training Loss: tensor(0.0462)\n",
      "9561 Training Loss: tensor(0.0522)\n",
      "9562 Training Loss: tensor(0.0376)\n",
      "9563 Training Loss: tensor(0.0487)\n",
      "9564 Training Loss: tensor(0.0444)\n",
      "9565 Training Loss: tensor(0.0426)\n",
      "9566 Training Loss: tensor(0.0459)\n",
      "9567 Training Loss: tensor(0.0464)\n",
      "9568 Training Loss: tensor(0.0452)\n",
      "9569 Training Loss: tensor(0.0457)\n",
      "9570 Training Loss: tensor(0.0398)\n",
      "9571 Training Loss: tensor(0.0419)\n",
      "9572 Training Loss: tensor(0.0426)\n",
      "9573 Training Loss: tensor(0.0447)\n",
      "9574 Training Loss: tensor(0.0458)\n",
      "9575 Training Loss: tensor(0.0419)\n",
      "9576 Training Loss: tensor(0.0427)\n",
      "9577 Training Loss: tensor(0.0464)\n",
      "9578 Training Loss: tensor(0.0431)\n",
      "9579 Training Loss: tensor(0.0424)\n",
      "9580 Training Loss: tensor(0.0465)\n",
      "9581 Training Loss: tensor(0.0461)\n",
      "9582 Training Loss: tensor(0.0465)\n",
      "9583 Training Loss: tensor(0.0430)\n",
      "9584 Training Loss: tensor(0.0474)\n",
      "9585 Training Loss: tensor(0.0479)\n",
      "9586 Training Loss: tensor(0.0410)\n",
      "9587 Training Loss: tensor(0.0403)\n",
      "9588 Training Loss: tensor(0.0434)\n",
      "9589 Training Loss: tensor(0.0454)\n",
      "9590 Training Loss: tensor(0.0382)\n",
      "9591 Training Loss: tensor(0.0426)\n",
      "9592 Training Loss: tensor(0.0444)\n",
      "9593 Training Loss: tensor(0.0404)\n",
      "9594 Training Loss: tensor(0.0435)\n",
      "9595 Training Loss: tensor(0.0413)\n",
      "9596 Training Loss: tensor(0.0467)\n",
      "9597 Training Loss: tensor(0.0424)\n",
      "9598 Training Loss: tensor(0.0429)\n",
      "9599 Training Loss: tensor(0.0435)\n",
      "9600 Training Loss: tensor(0.0463)\n",
      "9601 Training Loss: tensor(0.0429)\n",
      "9602 Training Loss: tensor(0.0370)\n",
      "9603 Training Loss: tensor(0.0422)\n",
      "9604 Training Loss: tensor(0.0445)\n",
      "9605 Training Loss: tensor(0.0398)\n",
      "9606 Training Loss: tensor(0.0477)\n",
      "9607 Training Loss: tensor(0.0368)\n",
      "9608 Training Loss: tensor(0.0435)\n",
      "9609 Training Loss: tensor(0.0474)\n",
      "9610 Training Loss: tensor(0.0424)\n",
      "9611 Training Loss: tensor(0.0442)\n",
      "9612 Training Loss: tensor(0.0427)\n",
      "9613 Training Loss: tensor(0.0359)\n",
      "9614 Training Loss: tensor(0.0468)\n",
      "9615 Training Loss: tensor(0.0483)\n",
      "9616 Training Loss: tensor(0.0388)\n",
      "9617 Training Loss: tensor(0.0422)\n",
      "9618 Training Loss: tensor(0.0408)\n",
      "9619 Training Loss: tensor(0.0473)\n",
      "9620 Training Loss: tensor(0.0406)\n",
      "9621 Training Loss: tensor(0.0429)\n",
      "9622 Training Loss: tensor(0.0443)\n",
      "9623 Training Loss: tensor(0.0441)\n",
      "9624 Training Loss: tensor(0.0441)\n",
      "9625 Training Loss: tensor(0.0384)\n",
      "9626 Training Loss: tensor(0.0441)\n",
      "9627 Training Loss: tensor(0.0463)\n",
      "9628 Training Loss: tensor(0.0441)\n",
      "9629 Training Loss: tensor(0.0406)\n",
      "9630 Training Loss: tensor(0.0463)\n",
      "9631 Training Loss: tensor(0.0421)\n",
      "9632 Training Loss: tensor(0.0463)\n",
      "9633 Training Loss: tensor(0.0461)\n",
      "9634 Training Loss: tensor(0.0482)\n",
      "9635 Training Loss: tensor(0.0452)\n",
      "9636 Training Loss: tensor(0.0438)\n",
      "9637 Training Loss: tensor(0.0354)\n",
      "9638 Training Loss: tensor(0.0484)\n",
      "9639 Training Loss: tensor(0.0491)\n",
      "9640 Training Loss: tensor(0.0463)\n",
      "9641 Training Loss: tensor(0.0455)\n",
      "9642 Training Loss: tensor(0.0453)\n",
      "9643 Training Loss: tensor(0.0450)\n",
      "9644 Training Loss: tensor(0.0483)\n",
      "9645 Training Loss: tensor(0.0436)\n",
      "9646 Training Loss: tensor(0.0362)\n",
      "9647 Training Loss: tensor(0.0475)\n",
      "9648 Training Loss: tensor(0.0426)\n",
      "9649 Training Loss: tensor(0.0432)\n",
      "9650 Training Loss: tensor(0.0487)\n",
      "9651 Training Loss: tensor(0.0411)\n",
      "9652 Training Loss: tensor(0.0436)\n",
      "9653 Training Loss: tensor(0.0424)\n",
      "9654 Training Loss: tensor(0.0426)\n",
      "9655 Training Loss: tensor(0.0415)\n",
      "9656 Training Loss: tensor(0.0428)\n",
      "9657 Training Loss: tensor(0.0427)\n",
      "9658 Training Loss: tensor(0.0495)\n",
      "9659 Training Loss: tensor(0.0432)\n",
      "9660 Training Loss: tensor(0.0420)\n",
      "9661 Training Loss: tensor(0.0436)\n",
      "9662 Training Loss: tensor(0.0415)\n",
      "9663 Training Loss: tensor(0.0364)\n",
      "9664 Training Loss: tensor(0.0436)\n",
      "9665 Training Loss: tensor(0.0458)\n",
      "9666 Training Loss: tensor(0.0490)\n",
      "9667 Training Loss: tensor(0.0457)\n",
      "9668 Training Loss: tensor(0.0475)\n",
      "9669 Training Loss: tensor(0.0457)\n",
      "9670 Training Loss: tensor(0.0464)\n",
      "9671 Training Loss: tensor(0.0446)\n",
      "9672 Training Loss: tensor(0.0441)\n",
      "9673 Training Loss: tensor(0.0462)\n",
      "9674 Training Loss: tensor(0.0471)\n",
      "9675 Training Loss: tensor(0.0462)\n",
      "9676 Training Loss: tensor(0.0376)\n",
      "9677 Training Loss: tensor(0.0396)\n",
      "9678 Training Loss: tensor(0.0457)\n",
      "9679 Training Loss: tensor(0.0453)\n",
      "9680 Training Loss: tensor(0.0413)\n",
      "9681 Training Loss: tensor(0.0389)\n",
      "9682 Training Loss: tensor(0.0413)\n",
      "9683 Training Loss: tensor(0.0452)\n",
      "9684 Training Loss: tensor(0.0457)\n",
      "9685 Training Loss: tensor(0.0430)\n",
      "9686 Training Loss: tensor(0.0467)\n",
      "9687 Training Loss: tensor(0.0457)\n",
      "9688 Training Loss: tensor(0.0432)\n",
      "9689 Training Loss: tensor(0.0469)\n",
      "9690 Training Loss: tensor(0.0484)\n",
      "9691 Training Loss: tensor(0.0448)\n",
      "9692 Training Loss: tensor(0.0450)\n",
      "9693 Training Loss: tensor(0.0442)\n",
      "9694 Training Loss: tensor(0.0414)\n",
      "9695 Training Loss: tensor(0.0404)\n",
      "9696 Training Loss: tensor(0.0401)\n",
      "9697 Training Loss: tensor(0.0451)\n",
      "9698 Training Loss: tensor(0.0442)\n",
      "9699 Training Loss: tensor(0.0397)\n",
      "9700 Training Loss: tensor(0.0417)\n",
      "9701 Training Loss: tensor(0.0414)\n",
      "9702 Training Loss: tensor(0.0396)\n",
      "9703 Training Loss: tensor(0.0445)\n",
      "9704 Training Loss: tensor(0.0418)\n",
      "9705 Training Loss: tensor(0.0438)\n",
      "9706 Training Loss: tensor(0.0415)\n",
      "9707 Training Loss: tensor(0.0381)\n",
      "9708 Training Loss: tensor(0.0446)\n",
      "9709 Training Loss: tensor(0.0439)\n",
      "9710 Training Loss: tensor(0.0491)\n",
      "9711 Training Loss: tensor(0.0406)\n",
      "9712 Training Loss: tensor(0.0439)\n",
      "9713 Training Loss: tensor(0.0438)\n",
      "9714 Training Loss: tensor(0.0435)\n",
      "9715 Training Loss: tensor(0.0452)\n",
      "9716 Training Loss: tensor(0.0437)\n",
      "9717 Training Loss: tensor(0.0450)\n",
      "9718 Training Loss: tensor(0.0393)\n",
      "9719 Training Loss: tensor(0.0431)\n",
      "9720 Training Loss: tensor(0.0496)\n",
      "9721 Training Loss: tensor(0.0462)\n",
      "9722 Training Loss: tensor(0.0461)\n",
      "9723 Training Loss: tensor(0.0422)\n",
      "9724 Training Loss: tensor(0.0428)\n",
      "9725 Training Loss: tensor(0.0398)\n",
      "9726 Training Loss: tensor(0.0427)\n",
      "9727 Training Loss: tensor(0.0440)\n",
      "9728 Training Loss: tensor(0.0408)\n",
      "9729 Training Loss: tensor(0.0476)\n",
      "9730 Training Loss: tensor(0.0393)\n",
      "9731 Training Loss: tensor(0.0389)\n",
      "9732 Training Loss: tensor(0.0391)\n",
      "9733 Training Loss: tensor(0.0456)\n",
      "9734 Training Loss: tensor(0.0430)\n",
      "9735 Training Loss: tensor(0.0421)\n",
      "9736 Training Loss: tensor(0.0485)\n",
      "9737 Training Loss: tensor(0.0434)\n",
      "9738 Training Loss: tensor(0.0393)\n",
      "9739 Training Loss: tensor(0.0416)\n",
      "9740 Training Loss: tensor(0.0447)\n",
      "9741 Training Loss: tensor(0.0413)\n",
      "9742 Training Loss: tensor(0.0408)\n",
      "9743 Training Loss: tensor(0.0419)\n",
      "9744 Training Loss: tensor(0.0397)\n",
      "9745 Training Loss: tensor(0.0408)\n",
      "9746 Training Loss: tensor(0.0441)\n",
      "9747 Training Loss: tensor(0.0435)\n",
      "9748 Training Loss: tensor(0.0400)\n",
      "9749 Training Loss: tensor(0.0391)\n",
      "9750 Training Loss: tensor(0.0419)\n",
      "9751 Training Loss: tensor(0.0438)\n",
      "9752 Training Loss: tensor(0.0453)\n",
      "9753 Training Loss: tensor(0.0410)\n",
      "9754 Training Loss: tensor(0.0466)\n",
      "9755 Training Loss: tensor(0.0444)\n",
      "9756 Training Loss: tensor(0.0475)\n",
      "9757 Training Loss: tensor(0.0397)\n",
      "9758 Training Loss: tensor(0.0372)\n",
      "9759 Training Loss: tensor(0.0442)\n",
      "9760 Training Loss: tensor(0.0411)\n",
      "9761 Training Loss: tensor(0.0427)\n",
      "9762 Training Loss: tensor(0.0414)\n",
      "9763 Training Loss: tensor(0.0468)\n",
      "9764 Training Loss: tensor(0.0426)\n",
      "9765 Training Loss: tensor(0.0412)\n",
      "9766 Training Loss: tensor(0.0400)\n",
      "9767 Training Loss: tensor(0.0435)\n",
      "9768 Training Loss: tensor(0.0464)\n",
      "9769 Training Loss: tensor(0.0411)\n",
      "9770 Training Loss: tensor(0.0423)\n",
      "9771 Training Loss: tensor(0.0392)\n",
      "9772 Training Loss: tensor(0.0426)\n",
      "9773 Training Loss: tensor(0.0443)\n",
      "9774 Training Loss: tensor(0.0487)\n",
      "9775 Training Loss: tensor(0.0467)\n",
      "9776 Training Loss: tensor(0.0429)\n",
      "9777 Training Loss: tensor(0.0391)\n",
      "9778 Training Loss: tensor(0.0389)\n",
      "9779 Training Loss: tensor(0.0412)\n",
      "9780 Training Loss: tensor(0.0403)\n",
      "9781 Training Loss: tensor(0.0412)\n",
      "9782 Training Loss: tensor(0.0396)\n",
      "9783 Training Loss: tensor(0.0433)\n",
      "9784 Training Loss: tensor(0.0402)\n",
      "9785 Training Loss: tensor(0.0467)\n",
      "9786 Training Loss: tensor(0.0416)\n",
      "9787 Training Loss: tensor(0.0424)\n",
      "9788 Training Loss: tensor(0.0419)\n",
      "9789 Training Loss: tensor(0.0359)\n",
      "9790 Training Loss: tensor(0.0381)\n",
      "9791 Training Loss: tensor(0.0461)\n",
      "9792 Training Loss: tensor(0.0470)\n",
      "9793 Training Loss: tensor(0.0419)\n",
      "9794 Training Loss: tensor(0.0397)\n",
      "9795 Training Loss: tensor(0.0454)\n",
      "9796 Training Loss: tensor(0.0468)\n",
      "9797 Training Loss: tensor(0.0408)\n",
      "9798 Training Loss: tensor(0.0365)\n",
      "9799 Training Loss: tensor(0.0421)\n",
      "9800 Training Loss: tensor(0.0434)\n",
      "9801 Training Loss: tensor(0.0383)\n",
      "9802 Training Loss: tensor(0.0412)\n",
      "9803 Training Loss: tensor(0.0468)\n",
      "9804 Training Loss: tensor(0.0416)\n",
      "9805 Training Loss: tensor(0.0432)\n",
      "9806 Training Loss: tensor(0.0435)\n",
      "9807 Training Loss: tensor(0.0436)\n",
      "9808 Training Loss: tensor(0.0440)\n",
      "9809 Training Loss: tensor(0.0435)\n",
      "9810 Training Loss: tensor(0.0462)\n",
      "9811 Training Loss: tensor(0.0470)\n",
      "9812 Training Loss: tensor(0.0459)\n",
      "9813 Training Loss: tensor(0.0476)\n",
      "9814 Training Loss: tensor(0.0443)\n",
      "9815 Training Loss: tensor(0.0412)\n",
      "9816 Training Loss: tensor(0.0438)\n",
      "9817 Training Loss: tensor(0.0511)\n",
      "9818 Training Loss: tensor(0.0426)\n",
      "9819 Training Loss: tensor(0.0451)\n",
      "9820 Training Loss: tensor(0.0439)\n",
      "9821 Training Loss: tensor(0.0443)\n",
      "9822 Training Loss: tensor(0.0519)\n",
      "9823 Training Loss: tensor(0.0409)\n",
      "9824 Training Loss: tensor(0.0402)\n",
      "9825 Training Loss: tensor(0.0469)\n",
      "9826 Training Loss: tensor(0.0435)\n",
      "9827 Training Loss: tensor(0.0480)\n",
      "9828 Training Loss: tensor(0.0458)\n",
      "9829 Training Loss: tensor(0.0449)\n",
      "9830 Training Loss: tensor(0.0401)\n",
      "9831 Training Loss: tensor(0.0487)\n",
      "9832 Training Loss: tensor(0.0453)\n",
      "9833 Training Loss: tensor(0.0470)\n",
      "9834 Training Loss: tensor(0.0445)\n",
      "9835 Training Loss: tensor(0.0448)\n",
      "9836 Training Loss: tensor(0.0446)\n",
      "9837 Training Loss: tensor(0.0443)\n",
      "9838 Training Loss: tensor(0.0382)\n",
      "9839 Training Loss: tensor(0.0418)\n",
      "9840 Training Loss: tensor(0.0419)\n",
      "9841 Training Loss: tensor(0.0436)\n",
      "9842 Training Loss: tensor(0.0475)\n",
      "9843 Training Loss: tensor(0.0436)\n",
      "9844 Training Loss: tensor(0.0431)\n",
      "9845 Training Loss: tensor(0.0415)\n",
      "9846 Training Loss: tensor(0.0411)\n",
      "9847 Training Loss: tensor(0.0425)\n",
      "9848 Training Loss: tensor(0.0397)\n",
      "9849 Training Loss: tensor(0.0443)\n",
      "9850 Training Loss: tensor(0.0437)\n",
      "9851 Training Loss: tensor(0.0434)\n",
      "9852 Training Loss: tensor(0.0416)\n",
      "9853 Training Loss: tensor(0.0378)\n",
      "9854 Training Loss: tensor(0.0535)\n",
      "9855 Training Loss: tensor(0.0471)\n",
      "9856 Training Loss: tensor(0.0411)\n",
      "9857 Training Loss: tensor(0.0441)\n",
      "9858 Training Loss: tensor(0.0407)\n",
      "9859 Training Loss: tensor(0.0391)\n",
      "9860 Training Loss: tensor(0.0448)\n",
      "9861 Training Loss: tensor(0.0471)\n",
      "9862 Training Loss: tensor(0.0473)\n",
      "9863 Training Loss: tensor(0.0419)\n",
      "9864 Training Loss: tensor(0.0427)\n",
      "9865 Training Loss: tensor(0.0460)\n",
      "9866 Training Loss: tensor(0.0458)\n",
      "9867 Training Loss: tensor(0.0405)\n",
      "9868 Training Loss: tensor(0.0417)\n",
      "9869 Training Loss: tensor(0.0441)\n",
      "9870 Training Loss: tensor(0.0477)\n",
      "9871 Training Loss: tensor(0.0481)\n",
      "9872 Training Loss: tensor(0.0384)\n",
      "9873 Training Loss: tensor(0.0451)\n",
      "9874 Training Loss: tensor(0.0478)\n",
      "9875 Training Loss: tensor(0.0444)\n",
      "9876 Training Loss: tensor(0.0429)\n",
      "9877 Training Loss: tensor(0.0425)\n",
      "9878 Training Loss: tensor(0.0470)\n",
      "9879 Training Loss: tensor(0.0447)\n",
      "9880 Training Loss: tensor(0.0432)\n",
      "9881 Training Loss: tensor(0.0435)\n",
      "9882 Training Loss: tensor(0.0427)\n",
      "9883 Training Loss: tensor(0.0475)\n",
      "9884 Training Loss: tensor(0.0412)\n",
      "9885 Training Loss: tensor(0.0455)\n",
      "9886 Training Loss: tensor(0.0482)\n",
      "9887 Training Loss: tensor(0.0405)\n",
      "9888 Training Loss: tensor(0.0380)\n",
      "9889 Training Loss: tensor(0.0437)\n",
      "9890 Training Loss: tensor(0.0463)\n",
      "9891 Training Loss: tensor(0.0438)\n",
      "9892 Training Loss: tensor(0.0399)\n",
      "9893 Training Loss: tensor(0.0452)\n",
      "9894 Training Loss: tensor(0.0419)\n",
      "9895 Training Loss: tensor(0.0414)\n",
      "9896 Training Loss: tensor(0.0449)\n",
      "9897 Training Loss: tensor(0.0418)\n",
      "9898 Training Loss: tensor(0.0411)\n",
      "9899 Training Loss: tensor(0.0479)\n",
      "9900 Training Loss: tensor(0.0428)\n",
      "9901 Training Loss: tensor(0.0445)\n",
      "9902 Training Loss: tensor(0.0470)\n",
      "9903 Training Loss: tensor(0.0438)\n",
      "9904 Training Loss: tensor(0.0427)\n",
      "9905 Training Loss: tensor(0.0395)\n",
      "9906 Training Loss: tensor(0.0425)\n",
      "9907 Training Loss: tensor(0.0457)\n",
      "9908 Training Loss: tensor(0.0434)\n",
      "9909 Training Loss: tensor(0.0404)\n",
      "9910 Training Loss: tensor(0.0475)\n",
      "9911 Training Loss: tensor(0.0395)\n",
      "9912 Training Loss: tensor(0.0386)\n",
      "9913 Training Loss: tensor(0.0431)\n",
      "9914 Training Loss: tensor(0.0457)\n",
      "9915 Training Loss: tensor(0.0430)\n",
      "9916 Training Loss: tensor(0.0479)\n",
      "9917 Training Loss: tensor(0.0484)\n",
      "9918 Training Loss: tensor(0.0445)\n",
      "9919 Training Loss: tensor(0.0452)\n",
      "9920 Training Loss: tensor(0.0377)\n",
      "9921 Training Loss: tensor(0.0402)\n",
      "9922 Training Loss: tensor(0.0449)\n",
      "9923 Training Loss: tensor(0.0408)\n",
      "9924 Training Loss: tensor(0.0422)\n",
      "9925 Training Loss: tensor(0.0427)\n",
      "9926 Training Loss: tensor(0.0402)\n",
      "9927 Training Loss: tensor(0.0498)\n",
      "9928 Training Loss: tensor(0.0414)\n",
      "9929 Training Loss: tensor(0.0420)\n",
      "9930 Training Loss: tensor(0.0483)\n",
      "9931 Training Loss: tensor(0.0438)\n",
      "9932 Training Loss: tensor(0.0405)\n",
      "9933 Training Loss: tensor(0.0452)\n",
      "9934 Training Loss: tensor(0.0486)\n",
      "9935 Training Loss: tensor(0.0399)\n",
      "9936 Training Loss: tensor(0.0469)\n",
      "9937 Training Loss: tensor(0.0437)\n",
      "9938 Training Loss: tensor(0.0486)\n",
      "9939 Training Loss: tensor(0.0428)\n",
      "9940 Training Loss: tensor(0.0446)\n",
      "9941 Training Loss: tensor(0.0387)\n",
      "9942 Training Loss: tensor(0.0434)\n",
      "9943 Training Loss: tensor(0.0405)\n",
      "9944 Training Loss: tensor(0.0422)\n",
      "9945 Training Loss: tensor(0.0423)\n",
      "9946 Training Loss: tensor(0.0473)\n",
      "9947 Training Loss: tensor(0.0435)\n",
      "9948 Training Loss: tensor(0.0438)\n",
      "9949 Training Loss: tensor(0.0415)\n",
      "9950 Training Loss: tensor(0.0431)\n",
      "9951 Training Loss: tensor(0.0505)\n",
      "9952 Training Loss: tensor(0.0441)\n",
      "9953 Training Loss: tensor(0.0395)\n",
      "9954 Training Loss: tensor(0.0508)\n",
      "9955 Training Loss: tensor(0.0468)\n",
      "9956 Training Loss: tensor(0.0481)\n",
      "9957 Training Loss: tensor(0.0402)\n",
      "9958 Training Loss: tensor(0.0452)\n",
      "9959 Training Loss: tensor(0.0438)\n",
      "9960 Training Loss: tensor(0.0467)\n",
      "9961 Training Loss: tensor(0.0434)\n",
      "9962 Training Loss: tensor(0.0476)\n",
      "9963 Training Loss: tensor(0.0452)\n",
      "9964 Training Loss: tensor(0.0439)\n",
      "9965 Training Loss: tensor(0.0458)\n",
      "9966 Training Loss: tensor(0.0391)\n",
      "9967 Training Loss: tensor(0.0476)\n",
      "9968 Training Loss: tensor(0.0445)\n",
      "9969 Training Loss: tensor(0.0426)\n",
      "9970 Training Loss: tensor(0.0457)\n",
      "9971 Training Loss: tensor(0.0391)\n",
      "9972 Training Loss: tensor(0.0447)\n",
      "9973 Training Loss: tensor(0.0451)\n",
      "9974 Training Loss: tensor(0.0446)\n",
      "9975 Training Loss: tensor(0.0427)\n",
      "9976 Training Loss: tensor(0.0406)\n",
      "9977 Training Loss: tensor(0.0406)\n",
      "9978 Training Loss: tensor(0.0401)\n",
      "9979 Training Loss: tensor(0.0465)\n",
      "9980 Training Loss: tensor(0.0437)\n",
      "9981 Training Loss: tensor(0.0467)\n",
      "9982 Training Loss: tensor(0.0391)\n",
      "9983 Training Loss: tensor(0.0390)\n",
      "9984 Training Loss: tensor(0.0474)\n",
      "9985 Training Loss: tensor(0.0430)\n",
      "9986 Training Loss: tensor(0.0388)\n",
      "9987 Training Loss: tensor(0.0426)\n",
      "9988 Training Loss: tensor(0.0423)\n",
      "9989 Training Loss: tensor(0.0444)\n",
      "9990 Training Loss: tensor(0.0365)\n",
      "9991 Training Loss: tensor(0.0481)\n",
      "9992 Training Loss: tensor(0.0423)\n",
      "9993 Training Loss: tensor(0.0438)\n",
      "9994 Training Loss: tensor(0.0400)\n",
      "9995 Training Loss: tensor(0.0456)\n",
      "9996 Training Loss: tensor(0.0428)\n",
      "9997 Training Loss: tensor(0.0412)\n",
      "9998 Training Loss: tensor(0.0401)\n",
      "9999 Training Loss: tensor(0.0391)\n",
      "10000 Training Loss: tensor(0.0470)\n",
      "10001 Training Loss: tensor(0.0438)\n",
      "10002 Training Loss: tensor(0.0415)\n",
      "10003 Training Loss: tensor(0.0383)\n",
      "10004 Training Loss: tensor(0.0389)\n",
      "10005 Training Loss: tensor(0.0402)\n",
      "10006 Training Loss: tensor(0.0419)\n",
      "10007 Training Loss: tensor(0.0464)\n",
      "10008 Training Loss: tensor(0.0462)\n",
      "10009 Training Loss: tensor(0.0466)\n",
      "10010 Training Loss: tensor(0.0412)\n",
      "10011 Training Loss: tensor(0.0454)\n",
      "10012 Training Loss: tensor(0.0381)\n",
      "10013 Training Loss: tensor(0.0483)\n",
      "10014 Training Loss: tensor(0.0447)\n",
      "10015 Training Loss: tensor(0.0453)\n",
      "10016 Training Loss: tensor(0.0478)\n",
      "10017 Training Loss: tensor(0.0405)\n",
      "10018 Training Loss: tensor(0.0427)\n",
      "10019 Training Loss: tensor(0.0407)\n",
      "10020 Training Loss: tensor(0.0450)\n",
      "10021 Training Loss: tensor(0.0401)\n",
      "10022 Training Loss: tensor(0.0417)\n",
      "10023 Training Loss: tensor(0.0501)\n",
      "10024 Training Loss: tensor(0.0420)\n",
      "10025 Training Loss: tensor(0.0389)\n",
      "10026 Training Loss: tensor(0.0480)\n",
      "10027 Training Loss: tensor(0.0428)\n",
      "10028 Training Loss: tensor(0.0418)\n",
      "10029 Training Loss: tensor(0.0392)\n",
      "10030 Training Loss: tensor(0.0406)\n",
      "10031 Training Loss: tensor(0.0467)\n",
      "10032 Training Loss: tensor(0.0432)\n",
      "10033 Training Loss: tensor(0.0492)\n",
      "10034 Training Loss: tensor(0.0448)\n",
      "10035 Training Loss: tensor(0.0398)\n",
      "10036 Training Loss: tensor(0.0456)\n",
      "10037 Training Loss: tensor(0.0447)\n",
      "10038 Training Loss: tensor(0.0426)\n",
      "10039 Training Loss: tensor(0.0502)\n",
      "10040 Training Loss: tensor(0.0429)\n",
      "10041 Training Loss: tensor(0.0446)\n",
      "10042 Training Loss: tensor(0.0410)\n",
      "10043 Training Loss: tensor(0.0428)\n",
      "10044 Training Loss: tensor(0.0418)\n",
      "10045 Training Loss: tensor(0.0430)\n",
      "10046 Training Loss: tensor(0.0437)\n",
      "10047 Training Loss: tensor(0.0407)\n",
      "10048 Training Loss: tensor(0.0400)\n",
      "10049 Training Loss: tensor(0.0404)\n",
      "10050 Training Loss: tensor(0.0474)\n",
      "10051 Training Loss: tensor(0.0455)\n",
      "10052 Training Loss: tensor(0.0415)\n",
      "10053 Training Loss: tensor(0.0442)\n",
      "10054 Training Loss: tensor(0.0429)\n",
      "10055 Training Loss: tensor(0.0432)\n",
      "10056 Training Loss: tensor(0.0399)\n",
      "10057 Training Loss: tensor(0.0474)\n",
      "10058 Training Loss: tensor(0.0526)\n",
      "10059 Training Loss: tensor(0.0410)\n",
      "10060 Training Loss: tensor(0.0449)\n",
      "10061 Training Loss: tensor(0.0460)\n",
      "10062 Training Loss: tensor(0.0450)\n",
      "10063 Training Loss: tensor(0.0494)\n",
      "10064 Training Loss: tensor(0.0485)\n",
      "10065 Training Loss: tensor(0.0415)\n",
      "10066 Training Loss: tensor(0.0491)\n",
      "10067 Training Loss: tensor(0.0393)\n",
      "10068 Training Loss: tensor(0.0400)\n",
      "10069 Training Loss: tensor(0.0520)\n",
      "10070 Training Loss: tensor(0.0469)\n",
      "10071 Training Loss: tensor(0.0459)\n",
      "10072 Training Loss: tensor(0.0487)\n",
      "10073 Training Loss: tensor(0.0397)\n",
      "10074 Training Loss: tensor(0.0349)\n",
      "10075 Training Loss: tensor(0.0428)\n",
      "10076 Training Loss: tensor(0.0453)\n",
      "10077 Training Loss: tensor(0.0409)\n",
      "10078 Training Loss: tensor(0.0414)\n",
      "10079 Training Loss: tensor(0.0454)\n",
      "10080 Training Loss: tensor(0.0455)\n",
      "10081 Training Loss: tensor(0.0444)\n",
      "10082 Training Loss: tensor(0.0435)\n",
      "10083 Training Loss: tensor(0.0421)\n",
      "10084 Training Loss: tensor(0.0434)\n",
      "10085 Training Loss: tensor(0.0430)\n",
      "10086 Training Loss: tensor(0.0464)\n",
      "10087 Training Loss: tensor(0.0415)\n",
      "10088 Training Loss: tensor(0.0437)\n",
      "10089 Training Loss: tensor(0.0375)\n",
      "10090 Training Loss: tensor(0.0422)\n",
      "10091 Training Loss: tensor(0.0453)\n",
      "10092 Training Loss: tensor(0.0490)\n",
      "10093 Training Loss: tensor(0.0479)\n",
      "10094 Training Loss: tensor(0.0392)\n",
      "10095 Training Loss: tensor(0.0433)\n",
      "10096 Training Loss: tensor(0.0366)\n",
      "10097 Training Loss: tensor(0.0412)\n",
      "10098 Training Loss: tensor(0.0428)\n",
      "10099 Training Loss: tensor(0.0399)\n",
      "10100 Training Loss: tensor(0.0388)\n",
      "10101 Training Loss: tensor(0.0398)\n",
      "10102 Training Loss: tensor(0.0465)\n",
      "10103 Training Loss: tensor(0.0384)\n",
      "10104 Training Loss: tensor(0.0408)\n",
      "10105 Training Loss: tensor(0.0376)\n",
      "10106 Training Loss: tensor(0.0456)\n",
      "10107 Training Loss: tensor(0.0451)\n",
      "10108 Training Loss: tensor(0.0427)\n",
      "10109 Training Loss: tensor(0.0361)\n",
      "10110 Training Loss: tensor(0.0438)\n",
      "10111 Training Loss: tensor(0.0410)\n",
      "10112 Training Loss: tensor(0.0465)\n",
      "10113 Training Loss: tensor(0.0396)\n",
      "10114 Training Loss: tensor(0.0470)\n",
      "10115 Training Loss: tensor(0.0381)\n",
      "10116 Training Loss: tensor(0.0425)\n",
      "10117 Training Loss: tensor(0.0490)\n",
      "10118 Training Loss: tensor(0.0408)\n",
      "10119 Training Loss: tensor(0.0366)\n",
      "10120 Training Loss: tensor(0.0376)\n",
      "10121 Training Loss: tensor(0.0476)\n",
      "10122 Training Loss: tensor(0.0426)\n",
      "10123 Training Loss: tensor(0.0405)\n",
      "10124 Training Loss: tensor(0.0436)\n",
      "10125 Training Loss: tensor(0.0430)\n",
      "10126 Training Loss: tensor(0.0452)\n",
      "10127 Training Loss: tensor(0.0410)\n",
      "10128 Training Loss: tensor(0.0438)\n",
      "10129 Training Loss: tensor(0.0435)\n",
      "10130 Training Loss: tensor(0.0450)\n",
      "10131 Training Loss: tensor(0.0399)\n",
      "10132 Training Loss: tensor(0.0389)\n",
      "10133 Training Loss: tensor(0.0465)\n",
      "10134 Training Loss: tensor(0.0394)\n",
      "10135 Training Loss: tensor(0.0374)\n",
      "10136 Training Loss: tensor(0.0400)\n",
      "10137 Training Loss: tensor(0.0408)\n",
      "10138 Training Loss: tensor(0.0442)\n",
      "10139 Training Loss: tensor(0.0399)\n",
      "10140 Training Loss: tensor(0.0393)\n",
      "10141 Training Loss: tensor(0.0486)\n",
      "10142 Training Loss: tensor(0.0441)\n",
      "10143 Training Loss: tensor(0.0399)\n",
      "10144 Training Loss: tensor(0.0410)\n",
      "10145 Training Loss: tensor(0.0450)\n",
      "10146 Training Loss: tensor(0.0392)\n",
      "10147 Training Loss: tensor(0.0412)\n",
      "10148 Training Loss: tensor(0.0442)\n",
      "10149 Training Loss: tensor(0.0492)\n",
      "10150 Training Loss: tensor(0.0413)\n",
      "10151 Training Loss: tensor(0.0437)\n",
      "10152 Training Loss: tensor(0.0447)\n",
      "10153 Training Loss: tensor(0.0461)\n",
      "10154 Training Loss: tensor(0.0442)\n",
      "10155 Training Loss: tensor(0.0443)\n",
      "10156 Training Loss: tensor(0.0449)\n",
      "10157 Training Loss: tensor(0.0426)\n",
      "10158 Training Loss: tensor(0.0429)\n",
      "10159 Training Loss: tensor(0.0385)\n",
      "10160 Training Loss: tensor(0.0418)\n",
      "10161 Training Loss: tensor(0.0423)\n",
      "10162 Training Loss: tensor(0.0381)\n",
      "10163 Training Loss: tensor(0.0382)\n",
      "10164 Training Loss: tensor(0.0402)\n",
      "10165 Training Loss: tensor(0.0461)\n",
      "10166 Training Loss: tensor(0.0426)\n",
      "10167 Training Loss: tensor(0.0466)\n",
      "10168 Training Loss: tensor(0.0427)\n",
      "10169 Training Loss: tensor(0.0451)\n",
      "10170 Training Loss: tensor(0.0403)\n",
      "10171 Training Loss: tensor(0.0419)\n",
      "10172 Training Loss: tensor(0.0464)\n",
      "10173 Training Loss: tensor(0.0454)\n",
      "10174 Training Loss: tensor(0.0470)\n",
      "10175 Training Loss: tensor(0.0414)\n",
      "10176 Training Loss: tensor(0.0409)\n",
      "10177 Training Loss: tensor(0.0428)\n",
      "10178 Training Loss: tensor(0.0456)\n",
      "10179 Training Loss: tensor(0.0423)\n",
      "10180 Training Loss: tensor(0.0434)\n",
      "10181 Training Loss: tensor(0.0429)\n",
      "10182 Training Loss: tensor(0.0396)\n",
      "10183 Training Loss: tensor(0.0439)\n",
      "10184 Training Loss: tensor(0.0411)\n",
      "10185 Training Loss: tensor(0.0455)\n",
      "10186 Training Loss: tensor(0.0438)\n",
      "10187 Training Loss: tensor(0.0365)\n",
      "10188 Training Loss: tensor(0.0380)\n",
      "10189 Training Loss: tensor(0.0508)\n",
      "10190 Training Loss: tensor(0.0437)\n",
      "10191 Training Loss: tensor(0.0398)\n",
      "10192 Training Loss: tensor(0.0423)\n",
      "10193 Training Loss: tensor(0.0410)\n",
      "10194 Training Loss: tensor(0.0480)\n",
      "10195 Training Loss: tensor(0.0416)\n",
      "10196 Training Loss: tensor(0.0410)\n",
      "10197 Training Loss: tensor(0.0445)\n",
      "10198 Training Loss: tensor(0.0483)\n",
      "10199 Training Loss: tensor(0.0428)\n",
      "10200 Training Loss: tensor(0.0402)\n",
      "10201 Training Loss: tensor(0.0427)\n",
      "10202 Training Loss: tensor(0.0474)\n",
      "10203 Training Loss: tensor(0.0437)\n",
      "10204 Training Loss: tensor(0.0410)\n",
      "10205 Training Loss: tensor(0.0389)\n",
      "10206 Training Loss: tensor(0.0403)\n",
      "10207 Training Loss: tensor(0.0424)\n",
      "10208 Training Loss: tensor(0.0493)\n",
      "10209 Training Loss: tensor(0.0379)\n",
      "10210 Training Loss: tensor(0.0365)\n",
      "10211 Training Loss: tensor(0.0443)\n",
      "10212 Training Loss: tensor(0.0356)\n",
      "10213 Training Loss: tensor(0.0416)\n",
      "10214 Training Loss: tensor(0.0457)\n",
      "10215 Training Loss: tensor(0.0396)\n",
      "10216 Training Loss: tensor(0.0385)\n",
      "10217 Training Loss: tensor(0.0441)\n",
      "10218 Training Loss: tensor(0.0465)\n",
      "10219 Training Loss: tensor(0.0444)\n",
      "10220 Training Loss: tensor(0.0398)\n",
      "10221 Training Loss: tensor(0.0368)\n",
      "10222 Training Loss: tensor(0.0473)\n",
      "10223 Training Loss: tensor(0.0400)\n",
      "10224 Training Loss: tensor(0.0473)\n",
      "10225 Training Loss: tensor(0.0491)\n",
      "10226 Training Loss: tensor(0.0448)\n",
      "10227 Training Loss: tensor(0.0448)\n",
      "10228 Training Loss: tensor(0.0405)\n",
      "10229 Training Loss: tensor(0.0401)\n",
      "10230 Training Loss: tensor(0.0441)\n",
      "10231 Training Loss: tensor(0.0430)\n",
      "10232 Training Loss: tensor(0.0464)\n",
      "10233 Training Loss: tensor(0.0466)\n",
      "10234 Training Loss: tensor(0.0472)\n",
      "10235 Training Loss: tensor(0.0377)\n",
      "10236 Training Loss: tensor(0.0445)\n",
      "10237 Training Loss: tensor(0.0368)\n",
      "10238 Training Loss: tensor(0.0446)\n",
      "10239 Training Loss: tensor(0.0424)\n",
      "10240 Training Loss: tensor(0.0449)\n",
      "10241 Training Loss: tensor(0.0446)\n",
      "10242 Training Loss: tensor(0.0427)\n",
      "10243 Training Loss: tensor(0.0441)\n",
      "10244 Training Loss: tensor(0.0471)\n",
      "10245 Training Loss: tensor(0.0437)\n",
      "10246 Training Loss: tensor(0.0371)\n",
      "10247 Training Loss: tensor(0.0441)\n",
      "10248 Training Loss: tensor(0.0376)\n",
      "10249 Training Loss: tensor(0.0380)\n",
      "10250 Training Loss: tensor(0.0484)\n",
      "10251 Training Loss: tensor(0.0424)\n",
      "10252 Training Loss: tensor(0.0365)\n",
      "10253 Training Loss: tensor(0.0387)\n",
      "10254 Training Loss: tensor(0.0411)\n",
      "10255 Training Loss: tensor(0.0433)\n",
      "10256 Training Loss: tensor(0.0404)\n",
      "10257 Training Loss: tensor(0.0422)\n",
      "10258 Training Loss: tensor(0.0434)\n",
      "10259 Training Loss: tensor(0.0396)\n",
      "10260 Training Loss: tensor(0.0404)\n",
      "10261 Training Loss: tensor(0.0468)\n",
      "10262 Training Loss: tensor(0.0417)\n",
      "10263 Training Loss: tensor(0.0420)\n",
      "10264 Training Loss: tensor(0.0393)\n",
      "10265 Training Loss: tensor(0.0422)\n",
      "10266 Training Loss: tensor(0.0372)\n",
      "10267 Training Loss: tensor(0.0447)\n",
      "10268 Training Loss: tensor(0.0411)\n",
      "10269 Training Loss: tensor(0.0377)\n",
      "10270 Training Loss: tensor(0.0412)\n",
      "10271 Training Loss: tensor(0.0493)\n",
      "10272 Training Loss: tensor(0.0392)\n",
      "10273 Training Loss: tensor(0.0454)\n",
      "10274 Training Loss: tensor(0.0376)\n",
      "10275 Training Loss: tensor(0.0412)\n",
      "10276 Training Loss: tensor(0.0405)\n",
      "10277 Training Loss: tensor(0.0435)\n",
      "10278 Training Loss: tensor(0.0382)\n",
      "10279 Training Loss: tensor(0.0458)\n",
      "10280 Training Loss: tensor(0.0402)\n",
      "10281 Training Loss: tensor(0.0402)\n",
      "10282 Training Loss: tensor(0.0434)\n",
      "10283 Training Loss: tensor(0.0405)\n",
      "10284 Training Loss: tensor(0.0421)\n",
      "10285 Training Loss: tensor(0.0410)\n",
      "10286 Training Loss: tensor(0.0438)\n",
      "10287 Training Loss: tensor(0.0491)\n",
      "10288 Training Loss: tensor(0.0418)\n",
      "10289 Training Loss: tensor(0.0416)\n",
      "10290 Training Loss: tensor(0.0421)\n",
      "10291 Training Loss: tensor(0.0437)\n",
      "10292 Training Loss: tensor(0.0416)\n",
      "10293 Training Loss: tensor(0.0436)\n",
      "10294 Training Loss: tensor(0.0482)\n",
      "10295 Training Loss: tensor(0.0445)\n",
      "10296 Training Loss: tensor(0.0450)\n",
      "10297 Training Loss: tensor(0.0442)\n",
      "10298 Training Loss: tensor(0.0434)\n",
      "10299 Training Loss: tensor(0.0460)\n",
      "10300 Training Loss: tensor(0.0421)\n",
      "10301 Training Loss: tensor(0.0469)\n",
      "10302 Training Loss: tensor(0.0428)\n",
      "10303 Training Loss: tensor(0.0399)\n",
      "10304 Training Loss: tensor(0.0407)\n",
      "10305 Training Loss: tensor(0.0466)\n",
      "10306 Training Loss: tensor(0.0434)\n",
      "10307 Training Loss: tensor(0.0411)\n",
      "10308 Training Loss: tensor(0.0460)\n",
      "10309 Training Loss: tensor(0.0424)\n",
      "10310 Training Loss: tensor(0.0435)\n",
      "10311 Training Loss: tensor(0.0439)\n",
      "10312 Training Loss: tensor(0.0447)\n",
      "10313 Training Loss: tensor(0.0433)\n",
      "10314 Training Loss: tensor(0.0450)\n",
      "10315 Training Loss: tensor(0.0447)\n",
      "10316 Training Loss: tensor(0.0468)\n",
      "10317 Training Loss: tensor(0.0421)\n",
      "10318 Training Loss: tensor(0.0419)\n",
      "10319 Training Loss: tensor(0.0482)\n",
      "10320 Training Loss: tensor(0.0387)\n",
      "10321 Training Loss: tensor(0.0414)\n",
      "10322 Training Loss: tensor(0.0493)\n",
      "10323 Training Loss: tensor(0.0436)\n",
      "10324 Training Loss: tensor(0.0403)\n",
      "10325 Training Loss: tensor(0.0450)\n",
      "10326 Training Loss: tensor(0.0440)\n",
      "10327 Training Loss: tensor(0.0408)\n",
      "10328 Training Loss: tensor(0.0417)\n",
      "10329 Training Loss: tensor(0.0452)\n",
      "10330 Training Loss: tensor(0.0469)\n",
      "10331 Training Loss: tensor(0.0378)\n",
      "10332 Training Loss: tensor(0.0378)\n",
      "10333 Training Loss: tensor(0.0410)\n",
      "10334 Training Loss: tensor(0.0431)\n",
      "10335 Training Loss: tensor(0.0408)\n",
      "10336 Training Loss: tensor(0.0457)\n",
      "10337 Training Loss: tensor(0.0396)\n",
      "10338 Training Loss: tensor(0.0458)\n",
      "10339 Training Loss: tensor(0.0439)\n",
      "10340 Training Loss: tensor(0.0393)\n",
      "10341 Training Loss: tensor(0.0429)\n",
      "10342 Training Loss: tensor(0.0426)\n",
      "10343 Training Loss: tensor(0.0433)\n",
      "10344 Training Loss: tensor(0.0430)\n",
      "10345 Training Loss: tensor(0.0362)\n",
      "10346 Training Loss: tensor(0.0366)\n",
      "10347 Training Loss: tensor(0.0447)\n",
      "10348 Training Loss: tensor(0.0401)\n",
      "10349 Training Loss: tensor(0.0373)\n",
      "10350 Training Loss: tensor(0.0406)\n",
      "10351 Training Loss: tensor(0.0416)\n",
      "10352 Training Loss: tensor(0.0404)\n",
      "10353 Training Loss: tensor(0.0389)\n",
      "10354 Training Loss: tensor(0.0434)\n",
      "10355 Training Loss: tensor(0.0412)\n",
      "10356 Training Loss: tensor(0.0414)\n",
      "10357 Training Loss: tensor(0.0455)\n",
      "10358 Training Loss: tensor(0.0433)\n",
      "10359 Training Loss: tensor(0.0424)\n",
      "10360 Training Loss: tensor(0.0397)\n",
      "10361 Training Loss: tensor(0.0440)\n",
      "10362 Training Loss: tensor(0.0353)\n",
      "10363 Training Loss: tensor(0.0454)\n",
      "10364 Training Loss: tensor(0.0434)\n",
      "10365 Training Loss: tensor(0.0442)\n",
      "10366 Training Loss: tensor(0.0427)\n",
      "10367 Training Loss: tensor(0.0413)\n",
      "10368 Training Loss: tensor(0.0399)\n",
      "10369 Training Loss: tensor(0.0418)\n",
      "10370 Training Loss: tensor(0.0392)\n",
      "10371 Training Loss: tensor(0.0419)\n",
      "10372 Training Loss: tensor(0.0422)\n",
      "10373 Training Loss: tensor(0.0398)\n",
      "10374 Training Loss: tensor(0.0420)\n",
      "10375 Training Loss: tensor(0.0434)\n",
      "10376 Training Loss: tensor(0.0419)\n",
      "10377 Training Loss: tensor(0.0414)\n",
      "10378 Training Loss: tensor(0.0388)\n",
      "10379 Training Loss: tensor(0.0379)\n",
      "10380 Training Loss: tensor(0.0412)\n",
      "10381 Training Loss: tensor(0.0440)\n",
      "10382 Training Loss: tensor(0.0402)\n",
      "10383 Training Loss: tensor(0.0461)\n",
      "10384 Training Loss: tensor(0.0423)\n",
      "10385 Training Loss: tensor(0.0389)\n",
      "10386 Training Loss: tensor(0.0459)\n",
      "10387 Training Loss: tensor(0.0371)\n",
      "10388 Training Loss: tensor(0.0399)\n",
      "10389 Training Loss: tensor(0.0379)\n",
      "10390 Training Loss: tensor(0.0448)\n",
      "10391 Training Loss: tensor(0.0392)\n",
      "10392 Training Loss: tensor(0.0428)\n",
      "10393 Training Loss: tensor(0.0402)\n",
      "10394 Training Loss: tensor(0.0439)\n",
      "10395 Training Loss: tensor(0.0374)\n",
      "10396 Training Loss: tensor(0.0428)\n",
      "10397 Training Loss: tensor(0.0410)\n",
      "10398 Training Loss: tensor(0.0430)\n",
      "10399 Training Loss: tensor(0.0389)\n",
      "10400 Training Loss: tensor(0.0388)\n",
      "10401 Training Loss: tensor(0.0440)\n",
      "10402 Training Loss: tensor(0.0393)\n",
      "10403 Training Loss: tensor(0.0433)\n",
      "10404 Training Loss: tensor(0.0427)\n",
      "10405 Training Loss: tensor(0.0411)\n",
      "10406 Training Loss: tensor(0.0417)\n",
      "10407 Training Loss: tensor(0.0405)\n",
      "10408 Training Loss: tensor(0.0388)\n",
      "10409 Training Loss: tensor(0.0409)\n",
      "10410 Training Loss: tensor(0.0432)\n",
      "10411 Training Loss: tensor(0.0401)\n",
      "10412 Training Loss: tensor(0.0488)\n",
      "10413 Training Loss: tensor(0.0396)\n",
      "10414 Training Loss: tensor(0.0411)\n",
      "10415 Training Loss: tensor(0.0452)\n",
      "10416 Training Loss: tensor(0.0413)\n",
      "10417 Training Loss: tensor(0.0381)\n",
      "10418 Training Loss: tensor(0.0424)\n",
      "10419 Training Loss: tensor(0.0384)\n",
      "10420 Training Loss: tensor(0.0457)\n",
      "10421 Training Loss: tensor(0.0459)\n",
      "10422 Training Loss: tensor(0.0475)\n",
      "10423 Training Loss: tensor(0.0445)\n",
      "10424 Training Loss: tensor(0.0443)\n",
      "10425 Training Loss: tensor(0.0399)\n",
      "10426 Training Loss: tensor(0.0394)\n",
      "10427 Training Loss: tensor(0.0433)\n",
      "10428 Training Loss: tensor(0.0432)\n",
      "10429 Training Loss: tensor(0.0400)\n",
      "10430 Training Loss: tensor(0.0433)\n",
      "10431 Training Loss: tensor(0.0431)\n",
      "10432 Training Loss: tensor(0.0498)\n",
      "10433 Training Loss: tensor(0.0434)\n",
      "10434 Training Loss: tensor(0.0454)\n",
      "10435 Training Loss: tensor(0.0490)\n",
      "10436 Training Loss: tensor(0.0407)\n",
      "10437 Training Loss: tensor(0.0428)\n",
      "10438 Training Loss: tensor(0.0445)\n",
      "10439 Training Loss: tensor(0.0415)\n",
      "10440 Training Loss: tensor(0.0411)\n",
      "10441 Training Loss: tensor(0.0455)\n",
      "10442 Training Loss: tensor(0.0357)\n",
      "10443 Training Loss: tensor(0.0445)\n",
      "10444 Training Loss: tensor(0.0381)\n",
      "10445 Training Loss: tensor(0.0388)\n",
      "10446 Training Loss: tensor(0.0443)\n",
      "10447 Training Loss: tensor(0.0478)\n",
      "10448 Training Loss: tensor(0.0450)\n",
      "10449 Training Loss: tensor(0.0417)\n",
      "10450 Training Loss: tensor(0.0481)\n",
      "10451 Training Loss: tensor(0.0427)\n",
      "10452 Training Loss: tensor(0.0458)\n",
      "10453 Training Loss: tensor(0.0427)\n",
      "10454 Training Loss: tensor(0.0474)\n",
      "10455 Training Loss: tensor(0.0412)\n",
      "10456 Training Loss: tensor(0.0465)\n",
      "10457 Training Loss: tensor(0.0471)\n",
      "10458 Training Loss: tensor(0.0463)\n",
      "10459 Training Loss: tensor(0.0447)\n",
      "10460 Training Loss: tensor(0.0443)\n",
      "10461 Training Loss: tensor(0.0455)\n",
      "10462 Training Loss: tensor(0.0423)\n",
      "10463 Training Loss: tensor(0.0421)\n",
      "10464 Training Loss: tensor(0.0421)\n",
      "10465 Training Loss: tensor(0.0455)\n",
      "10466 Training Loss: tensor(0.0418)\n",
      "10467 Training Loss: tensor(0.0421)\n",
      "10468 Training Loss: tensor(0.0467)\n",
      "10469 Training Loss: tensor(0.0431)\n",
      "10470 Training Loss: tensor(0.0434)\n",
      "10471 Training Loss: tensor(0.0390)\n",
      "10472 Training Loss: tensor(0.0403)\n",
      "10473 Training Loss: tensor(0.0438)\n",
      "10474 Training Loss: tensor(0.0404)\n",
      "10475 Training Loss: tensor(0.0387)\n",
      "10476 Training Loss: tensor(0.0404)\n",
      "10477 Training Loss: tensor(0.0368)\n",
      "10478 Training Loss: tensor(0.0446)\n",
      "10479 Training Loss: tensor(0.0398)\n",
      "10480 Training Loss: tensor(0.0430)\n",
      "10481 Training Loss: tensor(0.0429)\n",
      "10482 Training Loss: tensor(0.0407)\n",
      "10483 Training Loss: tensor(0.0417)\n",
      "10484 Training Loss: tensor(0.0399)\n",
      "10485 Training Loss: tensor(0.0412)\n",
      "10486 Training Loss: tensor(0.0467)\n",
      "10487 Training Loss: tensor(0.0406)\n",
      "10488 Training Loss: tensor(0.0395)\n",
      "10489 Training Loss: tensor(0.0377)\n",
      "10490 Training Loss: tensor(0.0434)\n",
      "10491 Training Loss: tensor(0.0398)\n",
      "10492 Training Loss: tensor(0.0428)\n",
      "10493 Training Loss: tensor(0.0388)\n",
      "10494 Training Loss: tensor(0.0417)\n",
      "10495 Training Loss: tensor(0.0465)\n",
      "10496 Training Loss: tensor(0.0457)\n",
      "10497 Training Loss: tensor(0.0401)\n",
      "10498 Training Loss: tensor(0.0414)\n",
      "10499 Training Loss: tensor(0.0402)\n",
      "10500 Training Loss: tensor(0.0439)\n",
      "10501 Training Loss: tensor(0.0390)\n",
      "10502 Training Loss: tensor(0.0400)\n",
      "10503 Training Loss: tensor(0.0395)\n",
      "10504 Training Loss: tensor(0.0439)\n",
      "10505 Training Loss: tensor(0.0438)\n",
      "10506 Training Loss: tensor(0.0415)\n",
      "10507 Training Loss: tensor(0.0380)\n",
      "10508 Training Loss: tensor(0.0412)\n",
      "10509 Training Loss: tensor(0.0444)\n",
      "10510 Training Loss: tensor(0.0417)\n",
      "10511 Training Loss: tensor(0.0449)\n",
      "10512 Training Loss: tensor(0.0445)\n",
      "10513 Training Loss: tensor(0.0364)\n",
      "10514 Training Loss: tensor(0.0385)\n",
      "10515 Training Loss: tensor(0.0430)\n",
      "10516 Training Loss: tensor(0.0420)\n",
      "10517 Training Loss: tensor(0.0460)\n",
      "10518 Training Loss: tensor(0.0450)\n",
      "10519 Training Loss: tensor(0.0461)\n",
      "10520 Training Loss: tensor(0.0433)\n",
      "10521 Training Loss: tensor(0.0435)\n",
      "10522 Training Loss: tensor(0.0406)\n",
      "10523 Training Loss: tensor(0.0438)\n",
      "10524 Training Loss: tensor(0.0439)\n",
      "10525 Training Loss: tensor(0.0466)\n",
      "10526 Training Loss: tensor(0.0387)\n",
      "10527 Training Loss: tensor(0.0413)\n",
      "10528 Training Loss: tensor(0.0434)\n",
      "10529 Training Loss: tensor(0.0395)\n",
      "10530 Training Loss: tensor(0.0436)\n",
      "10531 Training Loss: tensor(0.0403)\n",
      "10532 Training Loss: tensor(0.0463)\n",
      "10533 Training Loss: tensor(0.0370)\n",
      "10534 Training Loss: tensor(0.0428)\n",
      "10535 Training Loss: tensor(0.0383)\n",
      "10536 Training Loss: tensor(0.0403)\n",
      "10537 Training Loss: tensor(0.0434)\n",
      "10538 Training Loss: tensor(0.0369)\n",
      "10539 Training Loss: tensor(0.0431)\n",
      "10540 Training Loss: tensor(0.0472)\n",
      "10541 Training Loss: tensor(0.0387)\n",
      "10542 Training Loss: tensor(0.0409)\n",
      "10543 Training Loss: tensor(0.0377)\n",
      "10544 Training Loss: tensor(0.0408)\n",
      "10545 Training Loss: tensor(0.0448)\n",
      "10546 Training Loss: tensor(0.0420)\n",
      "10547 Training Loss: tensor(0.0393)\n",
      "10548 Training Loss: tensor(0.0393)\n",
      "10549 Training Loss: tensor(0.0452)\n",
      "10550 Training Loss: tensor(0.0446)\n",
      "10551 Training Loss: tensor(0.0455)\n",
      "10552 Training Loss: tensor(0.0440)\n",
      "10553 Training Loss: tensor(0.0462)\n",
      "10554 Training Loss: tensor(0.0435)\n",
      "10555 Training Loss: tensor(0.0415)\n",
      "10556 Training Loss: tensor(0.0406)\n",
      "10557 Training Loss: tensor(0.0371)\n",
      "10558 Training Loss: tensor(0.0448)\n",
      "10559 Training Loss: tensor(0.0388)\n",
      "10560 Training Loss: tensor(0.0431)\n",
      "10561 Training Loss: tensor(0.0382)\n",
      "10562 Training Loss: tensor(0.0461)\n",
      "10563 Training Loss: tensor(0.0414)\n",
      "10564 Training Loss: tensor(0.0394)\n",
      "10565 Training Loss: tensor(0.0424)\n",
      "10566 Training Loss: tensor(0.0412)\n",
      "10567 Training Loss: tensor(0.0424)\n",
      "10568 Training Loss: tensor(0.0450)\n",
      "10569 Training Loss: tensor(0.0406)\n",
      "10570 Training Loss: tensor(0.0402)\n",
      "10571 Training Loss: tensor(0.0442)\n",
      "10572 Training Loss: tensor(0.0401)\n",
      "10573 Training Loss: tensor(0.0401)\n",
      "10574 Training Loss: tensor(0.0359)\n",
      "10575 Training Loss: tensor(0.0381)\n",
      "10576 Training Loss: tensor(0.0390)\n",
      "10577 Training Loss: tensor(0.0437)\n",
      "10578 Training Loss: tensor(0.0458)\n",
      "10579 Training Loss: tensor(0.0415)\n",
      "10580 Training Loss: tensor(0.0483)\n",
      "10581 Training Loss: tensor(0.0463)\n",
      "10582 Training Loss: tensor(0.0401)\n",
      "10583 Training Loss: tensor(0.0408)\n",
      "10584 Training Loss: tensor(0.0450)\n",
      "10585 Training Loss: tensor(0.0396)\n",
      "10586 Training Loss: tensor(0.0421)\n",
      "10587 Training Loss: tensor(0.0425)\n",
      "10588 Training Loss: tensor(0.0489)\n",
      "10589 Training Loss: tensor(0.0435)\n",
      "10590 Training Loss: tensor(0.0445)\n",
      "10591 Training Loss: tensor(0.0486)\n",
      "10592 Training Loss: tensor(0.0420)\n",
      "10593 Training Loss: tensor(0.0487)\n",
      "10594 Training Loss: tensor(0.0455)\n",
      "10595 Training Loss: tensor(0.0381)\n",
      "10596 Training Loss: tensor(0.0433)\n",
      "10597 Training Loss: tensor(0.0436)\n",
      "10598 Training Loss: tensor(0.0469)\n",
      "10599 Training Loss: tensor(0.0414)\n",
      "10600 Training Loss: tensor(0.0406)\n",
      "10601 Training Loss: tensor(0.0385)\n",
      "10602 Training Loss: tensor(0.0408)\n",
      "10603 Training Loss: tensor(0.0406)\n",
      "10604 Training Loss: tensor(0.0413)\n",
      "10605 Training Loss: tensor(0.0442)\n",
      "10606 Training Loss: tensor(0.0378)\n",
      "10607 Training Loss: tensor(0.0417)\n",
      "10608 Training Loss: tensor(0.0397)\n",
      "10609 Training Loss: tensor(0.0416)\n",
      "10610 Training Loss: tensor(0.0468)\n",
      "10611 Training Loss: tensor(0.0484)\n",
      "10612 Training Loss: tensor(0.0394)\n",
      "10613 Training Loss: tensor(0.0401)\n",
      "10614 Training Loss: tensor(0.0389)\n",
      "10615 Training Loss: tensor(0.0447)\n",
      "10616 Training Loss: tensor(0.0413)\n",
      "10617 Training Loss: tensor(0.0400)\n",
      "10618 Training Loss: tensor(0.0470)\n",
      "10619 Training Loss: tensor(0.0456)\n",
      "10620 Training Loss: tensor(0.0432)\n",
      "10621 Training Loss: tensor(0.0522)\n",
      "10622 Training Loss: tensor(0.0334)\n",
      "10623 Training Loss: tensor(0.0359)\n",
      "10624 Training Loss: tensor(0.0451)\n",
      "10625 Training Loss: tensor(0.0418)\n",
      "10626 Training Loss: tensor(0.0393)\n",
      "10627 Training Loss: tensor(0.0413)\n",
      "10628 Training Loss: tensor(0.0440)\n",
      "10629 Training Loss: tensor(0.0423)\n",
      "10630 Training Loss: tensor(0.0402)\n",
      "10631 Training Loss: tensor(0.0460)\n",
      "10632 Training Loss: tensor(0.0436)\n",
      "10633 Training Loss: tensor(0.0446)\n",
      "10634 Training Loss: tensor(0.0433)\n",
      "10635 Training Loss: tensor(0.0467)\n",
      "10636 Training Loss: tensor(0.0378)\n",
      "10637 Training Loss: tensor(0.0458)\n",
      "10638 Training Loss: tensor(0.0440)\n",
      "10639 Training Loss: tensor(0.0414)\n",
      "10640 Training Loss: tensor(0.0425)\n",
      "10641 Training Loss: tensor(0.0456)\n",
      "10642 Training Loss: tensor(0.0445)\n",
      "10643 Training Loss: tensor(0.0463)\n",
      "10644 Training Loss: tensor(0.0370)\n",
      "10645 Training Loss: tensor(0.0388)\n",
      "10646 Training Loss: tensor(0.0498)\n",
      "10647 Training Loss: tensor(0.0436)\n",
      "10648 Training Loss: tensor(0.0436)\n",
      "10649 Training Loss: tensor(0.0466)\n",
      "10650 Training Loss: tensor(0.0453)\n",
      "10651 Training Loss: tensor(0.0403)\n",
      "10652 Training Loss: tensor(0.0464)\n",
      "10653 Training Loss: tensor(0.0436)\n",
      "10654 Training Loss: tensor(0.0468)\n",
      "10655 Training Loss: tensor(0.0478)\n",
      "10656 Training Loss: tensor(0.0484)\n",
      "10657 Training Loss: tensor(0.0475)\n",
      "10658 Training Loss: tensor(0.0447)\n",
      "10659 Training Loss: tensor(0.0482)\n",
      "10660 Training Loss: tensor(0.0391)\n",
      "10661 Training Loss: tensor(0.0400)\n",
      "10662 Training Loss: tensor(0.0408)\n",
      "10663 Training Loss: tensor(0.0410)\n",
      "10664 Training Loss: tensor(0.0385)\n",
      "10665 Training Loss: tensor(0.0459)\n",
      "10666 Training Loss: tensor(0.0423)\n",
      "10667 Training Loss: tensor(0.0407)\n",
      "10668 Training Loss: tensor(0.0359)\n",
      "10669 Training Loss: tensor(0.0380)\n",
      "10670 Training Loss: tensor(0.0417)\n",
      "10671 Training Loss: tensor(0.0427)\n",
      "10672 Training Loss: tensor(0.0430)\n",
      "10673 Training Loss: tensor(0.0382)\n",
      "10674 Training Loss: tensor(0.0446)\n",
      "10675 Training Loss: tensor(0.0408)\n",
      "10676 Training Loss: tensor(0.0404)\n",
      "10677 Training Loss: tensor(0.0417)\n",
      "10678 Training Loss: tensor(0.0418)\n",
      "10679 Training Loss: tensor(0.0387)\n",
      "10680 Training Loss: tensor(0.0430)\n",
      "10681 Training Loss: tensor(0.0421)\n",
      "10682 Training Loss: tensor(0.0406)\n",
      "10683 Training Loss: tensor(0.0416)\n",
      "10684 Training Loss: tensor(0.0470)\n",
      "10685 Training Loss: tensor(0.0398)\n",
      "10686 Training Loss: tensor(0.0471)\n",
      "10687 Training Loss: tensor(0.0427)\n",
      "10688 Training Loss: tensor(0.0495)\n",
      "10689 Training Loss: tensor(0.0438)\n",
      "10690 Training Loss: tensor(0.0433)\n",
      "10691 Training Loss: tensor(0.0425)\n",
      "10692 Training Loss: tensor(0.0429)\n",
      "10693 Training Loss: tensor(0.0429)\n",
      "10694 Training Loss: tensor(0.0420)\n",
      "10695 Training Loss: tensor(0.0431)\n",
      "10696 Training Loss: tensor(0.0443)\n",
      "10697 Training Loss: tensor(0.0408)\n",
      "10698 Training Loss: tensor(0.0390)\n",
      "10699 Training Loss: tensor(0.0373)\n",
      "10700 Training Loss: tensor(0.0386)\n",
      "10701 Training Loss: tensor(0.0381)\n",
      "10702 Training Loss: tensor(0.0358)\n",
      "10703 Training Loss: tensor(0.0437)\n",
      "10704 Training Loss: tensor(0.0420)\n",
      "10705 Training Loss: tensor(0.0410)\n",
      "10706 Training Loss: tensor(0.0402)\n",
      "10707 Training Loss: tensor(0.0419)\n",
      "10708 Training Loss: tensor(0.0382)\n",
      "10709 Training Loss: tensor(0.0429)\n",
      "10710 Training Loss: tensor(0.0474)\n",
      "10711 Training Loss: tensor(0.0405)\n",
      "10712 Training Loss: tensor(0.0466)\n",
      "10713 Training Loss: tensor(0.0384)\n",
      "10714 Training Loss: tensor(0.0350)\n",
      "10715 Training Loss: tensor(0.0401)\n",
      "10716 Training Loss: tensor(0.0432)\n",
      "10717 Training Loss: tensor(0.0417)\n",
      "10718 Training Loss: tensor(0.0442)\n",
      "10719 Training Loss: tensor(0.0435)\n",
      "10720 Training Loss: tensor(0.0367)\n",
      "10721 Training Loss: tensor(0.0428)\n",
      "10722 Training Loss: tensor(0.0413)\n",
      "10723 Training Loss: tensor(0.0453)\n",
      "10724 Training Loss: tensor(0.0432)\n",
      "10725 Training Loss: tensor(0.0377)\n",
      "10726 Training Loss: tensor(0.0443)\n",
      "10727 Training Loss: tensor(0.0435)\n",
      "10728 Training Loss: tensor(0.0441)\n",
      "10729 Training Loss: tensor(0.0442)\n",
      "10730 Training Loss: tensor(0.0435)\n",
      "10731 Training Loss: tensor(0.0451)\n",
      "10732 Training Loss: tensor(0.0427)\n",
      "10733 Training Loss: tensor(0.0444)\n",
      "10734 Training Loss: tensor(0.0488)\n",
      "10735 Training Loss: tensor(0.0438)\n",
      "10736 Training Loss: tensor(0.0476)\n",
      "10737 Training Loss: tensor(0.0404)\n",
      "10738 Training Loss: tensor(0.0451)\n",
      "10739 Training Loss: tensor(0.0474)\n",
      "10740 Training Loss: tensor(0.0413)\n",
      "10741 Training Loss: tensor(0.0467)\n",
      "10742 Training Loss: tensor(0.0380)\n",
      "10743 Training Loss: tensor(0.0450)\n",
      "10744 Training Loss: tensor(0.0423)\n",
      "10745 Training Loss: tensor(0.0415)\n",
      "10746 Training Loss: tensor(0.0428)\n",
      "10747 Training Loss: tensor(0.0445)\n",
      "10748 Training Loss: tensor(0.0471)\n",
      "10749 Training Loss: tensor(0.0425)\n",
      "10750 Training Loss: tensor(0.0370)\n",
      "10751 Training Loss: tensor(0.0443)\n",
      "10752 Training Loss: tensor(0.0460)\n",
      "10753 Training Loss: tensor(0.0399)\n",
      "10754 Training Loss: tensor(0.0434)\n",
      "10755 Training Loss: tensor(0.0419)\n",
      "10756 Training Loss: tensor(0.0411)\n",
      "10757 Training Loss: tensor(0.0366)\n",
      "10758 Training Loss: tensor(0.0476)\n",
      "10759 Training Loss: tensor(0.0381)\n",
      "10760 Training Loss: tensor(0.0445)\n",
      "10761 Training Loss: tensor(0.0486)\n",
      "10762 Training Loss: tensor(0.0422)\n",
      "10763 Training Loss: tensor(0.0455)\n",
      "10764 Training Loss: tensor(0.0418)\n",
      "10765 Training Loss: tensor(0.0428)\n",
      "10766 Training Loss: tensor(0.0428)\n",
      "10767 Training Loss: tensor(0.0425)\n",
      "10768 Training Loss: tensor(0.0393)\n",
      "10769 Training Loss: tensor(0.0469)\n",
      "10770 Training Loss: tensor(0.0416)\n",
      "10771 Training Loss: tensor(0.0407)\n",
      "10772 Training Loss: tensor(0.0416)\n",
      "10773 Training Loss: tensor(0.0505)\n",
      "10774 Training Loss: tensor(0.0438)\n",
      "10775 Training Loss: tensor(0.0417)\n",
      "10776 Training Loss: tensor(0.0428)\n",
      "10777 Training Loss: tensor(0.0428)\n",
      "10778 Training Loss: tensor(0.0444)\n",
      "10779 Training Loss: tensor(0.0367)\n",
      "10780 Training Loss: tensor(0.0379)\n",
      "10781 Training Loss: tensor(0.0450)\n",
      "10782 Training Loss: tensor(0.0403)\n",
      "10783 Training Loss: tensor(0.0425)\n",
      "10784 Training Loss: tensor(0.0451)\n",
      "10785 Training Loss: tensor(0.0438)\n",
      "10786 Training Loss: tensor(0.0462)\n",
      "10787 Training Loss: tensor(0.0411)\n",
      "10788 Training Loss: tensor(0.0499)\n",
      "10789 Training Loss: tensor(0.0458)\n",
      "10790 Training Loss: tensor(0.0383)\n",
      "10791 Training Loss: tensor(0.0425)\n",
      "10792 Training Loss: tensor(0.0453)\n",
      "10793 Training Loss: tensor(0.0366)\n",
      "10794 Training Loss: tensor(0.0411)\n",
      "10795 Training Loss: tensor(0.0444)\n",
      "10796 Training Loss: tensor(0.0414)\n",
      "10797 Training Loss: tensor(0.0378)\n",
      "10798 Training Loss: tensor(0.0413)\n",
      "10799 Training Loss: tensor(0.0409)\n",
      "10800 Training Loss: tensor(0.0435)\n",
      "10801 Training Loss: tensor(0.0401)\n",
      "10802 Training Loss: tensor(0.0430)\n",
      "10803 Training Loss: tensor(0.0431)\n",
      "10804 Training Loss: tensor(0.0437)\n",
      "10805 Training Loss: tensor(0.0408)\n",
      "10806 Training Loss: tensor(0.0404)\n",
      "10807 Training Loss: tensor(0.0410)\n",
      "10808 Training Loss: tensor(0.0407)\n",
      "10809 Training Loss: tensor(0.0397)\n",
      "10810 Training Loss: tensor(0.0353)\n",
      "10811 Training Loss: tensor(0.0402)\n",
      "10812 Training Loss: tensor(0.0398)\n",
      "10813 Training Loss: tensor(0.0435)\n",
      "10814 Training Loss: tensor(0.0435)\n",
      "10815 Training Loss: tensor(0.0428)\n",
      "10816 Training Loss: tensor(0.0397)\n",
      "10817 Training Loss: tensor(0.0443)\n",
      "10818 Training Loss: tensor(0.0387)\n",
      "10819 Training Loss: tensor(0.0408)\n",
      "10820 Training Loss: tensor(0.0411)\n",
      "10821 Training Loss: tensor(0.0462)\n",
      "10822 Training Loss: tensor(0.0416)\n",
      "10823 Training Loss: tensor(0.0419)\n",
      "10824 Training Loss: tensor(0.0441)\n",
      "10825 Training Loss: tensor(0.0405)\n",
      "10826 Training Loss: tensor(0.0406)\n",
      "10827 Training Loss: tensor(0.0386)\n",
      "10828 Training Loss: tensor(0.0374)\n",
      "10829 Training Loss: tensor(0.0471)\n",
      "10830 Training Loss: tensor(0.0440)\n",
      "10831 Training Loss: tensor(0.0407)\n",
      "10832 Training Loss: tensor(0.0422)\n",
      "10833 Training Loss: tensor(0.0408)\n",
      "10834 Training Loss: tensor(0.0368)\n",
      "10835 Training Loss: tensor(0.0364)\n",
      "10836 Training Loss: tensor(0.0393)\n",
      "10837 Training Loss: tensor(0.0400)\n",
      "10838 Training Loss: tensor(0.0394)\n",
      "10839 Training Loss: tensor(0.0461)\n",
      "10840 Training Loss: tensor(0.0449)\n",
      "10841 Training Loss: tensor(0.0406)\n",
      "10842 Training Loss: tensor(0.0430)\n",
      "10843 Training Loss: tensor(0.0444)\n",
      "10844 Training Loss: tensor(0.0429)\n",
      "10845 Training Loss: tensor(0.0375)\n",
      "10846 Training Loss: tensor(0.0415)\n",
      "10847 Training Loss: tensor(0.0420)\n",
      "10848 Training Loss: tensor(0.0470)\n",
      "10849 Training Loss: tensor(0.0379)\n",
      "10850 Training Loss: tensor(0.0364)\n",
      "10851 Training Loss: tensor(0.0398)\n",
      "10852 Training Loss: tensor(0.0484)\n",
      "10853 Training Loss: tensor(0.0426)\n",
      "10854 Training Loss: tensor(0.0415)\n",
      "10855 Training Loss: tensor(0.0419)\n",
      "10856 Training Loss: tensor(0.0391)\n",
      "10857 Training Loss: tensor(0.0371)\n",
      "10858 Training Loss: tensor(0.0375)\n",
      "10859 Training Loss: tensor(0.0382)\n",
      "10860 Training Loss: tensor(0.0439)\n",
      "10861 Training Loss: tensor(0.0376)\n",
      "10862 Training Loss: tensor(0.0480)\n",
      "10863 Training Loss: tensor(0.0381)\n",
      "10864 Training Loss: tensor(0.0464)\n",
      "10865 Training Loss: tensor(0.0455)\n",
      "10866 Training Loss: tensor(0.0471)\n",
      "10867 Training Loss: tensor(0.0466)\n",
      "10868 Training Loss: tensor(0.0403)\n",
      "10869 Training Loss: tensor(0.0440)\n",
      "10870 Training Loss: tensor(0.0470)\n",
      "10871 Training Loss: tensor(0.0458)\n",
      "10872 Training Loss: tensor(0.0408)\n",
      "10873 Training Loss: tensor(0.0463)\n",
      "10874 Training Loss: tensor(0.0463)\n",
      "10875 Training Loss: tensor(0.0457)\n",
      "10876 Training Loss: tensor(0.0427)\n",
      "10877 Training Loss: tensor(0.0541)\n",
      "10878 Training Loss: tensor(0.0400)\n",
      "10879 Training Loss: tensor(0.0478)\n",
      "10880 Training Loss: tensor(0.0472)\n",
      "10881 Training Loss: tensor(0.0411)\n",
      "10882 Training Loss: tensor(0.0392)\n",
      "10883 Training Loss: tensor(0.0474)\n",
      "10884 Training Loss: tensor(0.0440)\n",
      "10885 Training Loss: tensor(0.0403)\n",
      "10886 Training Loss: tensor(0.0457)\n",
      "10887 Training Loss: tensor(0.0403)\n",
      "10888 Training Loss: tensor(0.0482)\n",
      "10889 Training Loss: tensor(0.0355)\n",
      "10890 Training Loss: tensor(0.0424)\n",
      "10891 Training Loss: tensor(0.0443)\n",
      "10892 Training Loss: tensor(0.0436)\n",
      "10893 Training Loss: tensor(0.0407)\n",
      "10894 Training Loss: tensor(0.0430)\n",
      "10895 Training Loss: tensor(0.0472)\n",
      "10896 Training Loss: tensor(0.0393)\n",
      "10897 Training Loss: tensor(0.0391)\n",
      "10898 Training Loss: tensor(0.0436)\n",
      "10899 Training Loss: tensor(0.0405)\n",
      "10900 Training Loss: tensor(0.0450)\n",
      "10901 Training Loss: tensor(0.0382)\n",
      "10902 Training Loss: tensor(0.0424)\n",
      "10903 Training Loss: tensor(0.0420)\n",
      "10904 Training Loss: tensor(0.0401)\n",
      "10905 Training Loss: tensor(0.0408)\n",
      "10906 Training Loss: tensor(0.0414)\n",
      "10907 Training Loss: tensor(0.0413)\n",
      "10908 Training Loss: tensor(0.0427)\n",
      "10909 Training Loss: tensor(0.0376)\n",
      "10910 Training Loss: tensor(0.0453)\n",
      "10911 Training Loss: tensor(0.0397)\n",
      "10912 Training Loss: tensor(0.0374)\n",
      "10913 Training Loss: tensor(0.0428)\n",
      "10914 Training Loss: tensor(0.0434)\n",
      "10915 Training Loss: tensor(0.0431)\n",
      "10916 Training Loss: tensor(0.0434)\n",
      "10917 Training Loss: tensor(0.0433)\n",
      "10918 Training Loss: tensor(0.0396)\n",
      "10919 Training Loss: tensor(0.0447)\n",
      "10920 Training Loss: tensor(0.0439)\n",
      "10921 Training Loss: tensor(0.0385)\n",
      "10922 Training Loss: tensor(0.0456)\n",
      "10923 Training Loss: tensor(0.0460)\n",
      "10924 Training Loss: tensor(0.0407)\n",
      "10925 Training Loss: tensor(0.0375)\n",
      "10926 Training Loss: tensor(0.0437)\n",
      "10927 Training Loss: tensor(0.0365)\n",
      "10928 Training Loss: tensor(0.0432)\n",
      "10929 Training Loss: tensor(0.0475)\n",
      "10930 Training Loss: tensor(0.0442)\n",
      "10931 Training Loss: tensor(0.0422)\n",
      "10932 Training Loss: tensor(0.0478)\n",
      "10933 Training Loss: tensor(0.0458)\n",
      "10934 Training Loss: tensor(0.0430)\n",
      "10935 Training Loss: tensor(0.0433)\n",
      "10936 Training Loss: tensor(0.0393)\n",
      "10937 Training Loss: tensor(0.0410)\n",
      "10938 Training Loss: tensor(0.0431)\n",
      "10939 Training Loss: tensor(0.0384)\n",
      "10940 Training Loss: tensor(0.0387)\n",
      "10941 Training Loss: tensor(0.0402)\n",
      "10942 Training Loss: tensor(0.0404)\n",
      "10943 Training Loss: tensor(0.0427)\n",
      "10944 Training Loss: tensor(0.0404)\n",
      "10945 Training Loss: tensor(0.0384)\n",
      "10946 Training Loss: tensor(0.0422)\n",
      "10947 Training Loss: tensor(0.0393)\n",
      "10948 Training Loss: tensor(0.0462)\n",
      "10949 Training Loss: tensor(0.0436)\n",
      "10950 Training Loss: tensor(0.0413)\n",
      "10951 Training Loss: tensor(0.0408)\n",
      "10952 Training Loss: tensor(0.0417)\n",
      "10953 Training Loss: tensor(0.0422)\n",
      "10954 Training Loss: tensor(0.0430)\n",
      "10955 Training Loss: tensor(0.0390)\n",
      "10956 Training Loss: tensor(0.0457)\n",
      "10957 Training Loss: tensor(0.0424)\n",
      "10958 Training Loss: tensor(0.0422)\n",
      "10959 Training Loss: tensor(0.0456)\n",
      "10960 Training Loss: tensor(0.0444)\n",
      "10961 Training Loss: tensor(0.0412)\n",
      "10962 Training Loss: tensor(0.0435)\n",
      "10963 Training Loss: tensor(0.0360)\n",
      "10964 Training Loss: tensor(0.0418)\n",
      "10965 Training Loss: tensor(0.0437)\n",
      "10966 Training Loss: tensor(0.0439)\n",
      "10967 Training Loss: tensor(0.0406)\n",
      "10968 Training Loss: tensor(0.0408)\n",
      "10969 Training Loss: tensor(0.0399)\n",
      "10970 Training Loss: tensor(0.0404)\n",
      "10971 Training Loss: tensor(0.0438)\n",
      "10972 Training Loss: tensor(0.0450)\n",
      "10973 Training Loss: tensor(0.0471)\n",
      "10974 Training Loss: tensor(0.0413)\n",
      "10975 Training Loss: tensor(0.0475)\n",
      "10976 Training Loss: tensor(0.0414)\n",
      "10977 Training Loss: tensor(0.0392)\n",
      "10978 Training Loss: tensor(0.0398)\n",
      "10979 Training Loss: tensor(0.0459)\n",
      "10980 Training Loss: tensor(0.0441)\n",
      "10981 Training Loss: tensor(0.0432)\n",
      "10982 Training Loss: tensor(0.0452)\n",
      "10983 Training Loss: tensor(0.0386)\n",
      "10984 Training Loss: tensor(0.0318)\n",
      "10985 Training Loss: tensor(0.0450)\n",
      "10986 Training Loss: tensor(0.0406)\n",
      "10987 Training Loss: tensor(0.0395)\n",
      "10988 Training Loss: tensor(0.0423)\n",
      "10989 Training Loss: tensor(0.0489)\n",
      "10990 Training Loss: tensor(0.0383)\n",
      "10991 Training Loss: tensor(0.0447)\n",
      "10992 Training Loss: tensor(0.0392)\n",
      "10993 Training Loss: tensor(0.0452)\n",
      "10994 Training Loss: tensor(0.0396)\n",
      "10995 Training Loss: tensor(0.0395)\n",
      "10996 Training Loss: tensor(0.0427)\n",
      "10997 Training Loss: tensor(0.0427)\n",
      "10998 Training Loss: tensor(0.0425)\n",
      "10999 Training Loss: tensor(0.0449)\n",
      "11000 Training Loss: tensor(0.0470)\n",
      "11001 Training Loss: tensor(0.0427)\n",
      "11002 Training Loss: tensor(0.0404)\n",
      "11003 Training Loss: tensor(0.0390)\n",
      "11004 Training Loss: tensor(0.0429)\n",
      "11005 Training Loss: tensor(0.0366)\n",
      "11006 Training Loss: tensor(0.0409)\n",
      "11007 Training Loss: tensor(0.0414)\n",
      "11008 Training Loss: tensor(0.0405)\n",
      "11009 Training Loss: tensor(0.0419)\n",
      "11010 Training Loss: tensor(0.0377)\n",
      "11011 Training Loss: tensor(0.0443)\n",
      "11012 Training Loss: tensor(0.0452)\n",
      "11013 Training Loss: tensor(0.0455)\n",
      "11014 Training Loss: tensor(0.0455)\n",
      "11015 Training Loss: tensor(0.0411)\n",
      "11016 Training Loss: tensor(0.0448)\n",
      "11017 Training Loss: tensor(0.0459)\n",
      "11018 Training Loss: tensor(0.0388)\n",
      "11019 Training Loss: tensor(0.0384)\n",
      "11020 Training Loss: tensor(0.0430)\n",
      "11021 Training Loss: tensor(0.0422)\n",
      "11022 Training Loss: tensor(0.0457)\n",
      "11023 Training Loss: tensor(0.0434)\n",
      "11024 Training Loss: tensor(0.0432)\n",
      "11025 Training Loss: tensor(0.0479)\n",
      "11026 Training Loss: tensor(0.0438)\n",
      "11027 Training Loss: tensor(0.0426)\n",
      "11028 Training Loss: tensor(0.0446)\n",
      "11029 Training Loss: tensor(0.0422)\n",
      "11030 Training Loss: tensor(0.0336)\n",
      "11031 Training Loss: tensor(0.0418)\n",
      "11032 Training Loss: tensor(0.0402)\n",
      "11033 Training Loss: tensor(0.0506)\n",
      "11034 Training Loss: tensor(0.0403)\n",
      "11035 Training Loss: tensor(0.0432)\n",
      "11036 Training Loss: tensor(0.0445)\n",
      "11037 Training Loss: tensor(0.0429)\n",
      "11038 Training Loss: tensor(0.0402)\n",
      "11039 Training Loss: tensor(0.0458)\n",
      "11040 Training Loss: tensor(0.0421)\n",
      "11041 Training Loss: tensor(0.0413)\n",
      "11042 Training Loss: tensor(0.0422)\n",
      "11043 Training Loss: tensor(0.0420)\n",
      "11044 Training Loss: tensor(0.0387)\n",
      "11045 Training Loss: tensor(0.0460)\n",
      "11046 Training Loss: tensor(0.0496)\n",
      "11047 Training Loss: tensor(0.0456)\n",
      "11048 Training Loss: tensor(0.0373)\n",
      "11049 Training Loss: tensor(0.0428)\n",
      "11050 Training Loss: tensor(0.0443)\n",
      "11051 Training Loss: tensor(0.0386)\n",
      "11052 Training Loss: tensor(0.0416)\n",
      "11053 Training Loss: tensor(0.0443)\n",
      "11054 Training Loss: tensor(0.0418)\n",
      "11055 Training Loss: tensor(0.0436)\n",
      "11056 Training Loss: tensor(0.0405)\n",
      "11057 Training Loss: tensor(0.0416)\n",
      "11058 Training Loss: tensor(0.0391)\n",
      "11059 Training Loss: tensor(0.0458)\n",
      "11060 Training Loss: tensor(0.0421)\n",
      "11061 Training Loss: tensor(0.0432)\n",
      "11062 Training Loss: tensor(0.0379)\n",
      "11063 Training Loss: tensor(0.0424)\n",
      "11064 Training Loss: tensor(0.0382)\n",
      "11065 Training Loss: tensor(0.0415)\n",
      "11066 Training Loss: tensor(0.0433)\n",
      "11067 Training Loss: tensor(0.0391)\n",
      "11068 Training Loss: tensor(0.0348)\n",
      "11069 Training Loss: tensor(0.0392)\n",
      "11070 Training Loss: tensor(0.0432)\n",
      "11071 Training Loss: tensor(0.0425)\n",
      "11072 Training Loss: tensor(0.0469)\n",
      "11073 Training Loss: tensor(0.0411)\n",
      "11074 Training Loss: tensor(0.0430)\n",
      "11075 Training Loss: tensor(0.0480)\n",
      "11076 Training Loss: tensor(0.0413)\n",
      "11077 Training Loss: tensor(0.0395)\n",
      "11078 Training Loss: tensor(0.0439)\n",
      "11079 Training Loss: tensor(0.0391)\n",
      "11080 Training Loss: tensor(0.0345)\n",
      "11081 Training Loss: tensor(0.0468)\n",
      "11082 Training Loss: tensor(0.0453)\n",
      "11083 Training Loss: tensor(0.0382)\n",
      "11084 Training Loss: tensor(0.0459)\n",
      "11085 Training Loss: tensor(0.0409)\n",
      "11086 Training Loss: tensor(0.0405)\n",
      "11087 Training Loss: tensor(0.0464)\n",
      "11088 Training Loss: tensor(0.0408)\n",
      "11089 Training Loss: tensor(0.0413)\n",
      "11090 Training Loss: tensor(0.0452)\n",
      "11091 Training Loss: tensor(0.0458)\n",
      "11092 Training Loss: tensor(0.0448)\n",
      "11093 Training Loss: tensor(0.0428)\n",
      "11094 Training Loss: tensor(0.0429)\n",
      "11095 Training Loss: tensor(0.0440)\n",
      "11096 Training Loss: tensor(0.0426)\n",
      "11097 Training Loss: tensor(0.0400)\n",
      "11098 Training Loss: tensor(0.0422)\n",
      "11099 Training Loss: tensor(0.0399)\n",
      "11100 Training Loss: tensor(0.0418)\n",
      "11101 Training Loss: tensor(0.0451)\n",
      "11102 Training Loss: tensor(0.0465)\n",
      "11103 Training Loss: tensor(0.0466)\n",
      "11104 Training Loss: tensor(0.0436)\n",
      "11105 Training Loss: tensor(0.0459)\n",
      "11106 Training Loss: tensor(0.0427)\n",
      "11107 Training Loss: tensor(0.0436)\n",
      "11108 Training Loss: tensor(0.0401)\n",
      "11109 Training Loss: tensor(0.0466)\n",
      "11110 Training Loss: tensor(0.0376)\n",
      "11111 Training Loss: tensor(0.0427)\n",
      "11112 Training Loss: tensor(0.0423)\n",
      "11113 Training Loss: tensor(0.0469)\n",
      "11114 Training Loss: tensor(0.0418)\n",
      "11115 Training Loss: tensor(0.0430)\n",
      "11116 Training Loss: tensor(0.0397)\n",
      "11117 Training Loss: tensor(0.0404)\n",
      "11118 Training Loss: tensor(0.0379)\n",
      "11119 Training Loss: tensor(0.0376)\n",
      "11120 Training Loss: tensor(0.0374)\n",
      "11121 Training Loss: tensor(0.0411)\n",
      "11122 Training Loss: tensor(0.0410)\n",
      "11123 Training Loss: tensor(0.0432)\n",
      "11124 Training Loss: tensor(0.0415)\n",
      "11125 Training Loss: tensor(0.0397)\n",
      "11126 Training Loss: tensor(0.0436)\n",
      "11127 Training Loss: tensor(0.0456)\n",
      "11128 Training Loss: tensor(0.0439)\n",
      "11129 Training Loss: tensor(0.0369)\n",
      "11130 Training Loss: tensor(0.0451)\n",
      "11131 Training Loss: tensor(0.0414)\n",
      "11132 Training Loss: tensor(0.0406)\n",
      "11133 Training Loss: tensor(0.0503)\n",
      "11134 Training Loss: tensor(0.0462)\n",
      "11135 Training Loss: tensor(0.0442)\n",
      "11136 Training Loss: tensor(0.0446)\n",
      "11137 Training Loss: tensor(0.0423)\n",
      "11138 Training Loss: tensor(0.0390)\n",
      "11139 Training Loss: tensor(0.0445)\n",
      "11140 Training Loss: tensor(0.0416)\n",
      "11141 Training Loss: tensor(0.0426)\n",
      "11142 Training Loss: tensor(0.0464)\n",
      "11143 Training Loss: tensor(0.0397)\n",
      "11144 Training Loss: tensor(0.0393)\n",
      "11145 Training Loss: tensor(0.0402)\n",
      "11146 Training Loss: tensor(0.0418)\n",
      "11147 Training Loss: tensor(0.0405)\n",
      "11148 Training Loss: tensor(0.0400)\n",
      "11149 Training Loss: tensor(0.0421)\n",
      "11150 Training Loss: tensor(0.0361)\n",
      "11151 Training Loss: tensor(0.0382)\n",
      "11152 Training Loss: tensor(0.0378)\n",
      "11153 Training Loss: tensor(0.0421)\n",
      "11154 Training Loss: tensor(0.0399)\n",
      "11155 Training Loss: tensor(0.0425)\n",
      "11156 Training Loss: tensor(0.0377)\n",
      "11157 Training Loss: tensor(0.0350)\n",
      "11158 Training Loss: tensor(0.0401)\n",
      "11159 Training Loss: tensor(0.0398)\n",
      "11160 Training Loss: tensor(0.0437)\n",
      "11161 Training Loss: tensor(0.0426)\n",
      "11162 Training Loss: tensor(0.0368)\n",
      "11163 Training Loss: tensor(0.0376)\n",
      "11164 Training Loss: tensor(0.0391)\n",
      "11165 Training Loss: tensor(0.0395)\n",
      "11166 Training Loss: tensor(0.0424)\n",
      "11167 Training Loss: tensor(0.0428)\n",
      "11168 Training Loss: tensor(0.0393)\n",
      "11169 Training Loss: tensor(0.0379)\n",
      "11170 Training Loss: tensor(0.0410)\n",
      "11171 Training Loss: tensor(0.0377)\n",
      "11172 Training Loss: tensor(0.0399)\n",
      "11173 Training Loss: tensor(0.0409)\n",
      "11174 Training Loss: tensor(0.0404)\n",
      "11175 Training Loss: tensor(0.0418)\n",
      "11176 Training Loss: tensor(0.0409)\n",
      "11177 Training Loss: tensor(0.0400)\n",
      "11178 Training Loss: tensor(0.0394)\n",
      "11179 Training Loss: tensor(0.0395)\n",
      "11180 Training Loss: tensor(0.0386)\n",
      "11181 Training Loss: tensor(0.0421)\n",
      "11182 Training Loss: tensor(0.0445)\n",
      "11183 Training Loss: tensor(0.0402)\n",
      "11184 Training Loss: tensor(0.0385)\n",
      "11185 Training Loss: tensor(0.0401)\n",
      "11186 Training Loss: tensor(0.0417)\n",
      "11187 Training Loss: tensor(0.0395)\n",
      "11188 Training Loss: tensor(0.0428)\n",
      "11189 Training Loss: tensor(0.0404)\n",
      "11190 Training Loss: tensor(0.0393)\n",
      "11191 Training Loss: tensor(0.0386)\n",
      "11192 Training Loss: tensor(0.0389)\n",
      "11193 Training Loss: tensor(0.0399)\n",
      "11194 Training Loss: tensor(0.0381)\n",
      "11195 Training Loss: tensor(0.0449)\n",
      "11196 Training Loss: tensor(0.0460)\n",
      "11197 Training Loss: tensor(0.0392)\n",
      "11198 Training Loss: tensor(0.0504)\n",
      "11199 Training Loss: tensor(0.0400)\n",
      "11200 Training Loss: tensor(0.0449)\n",
      "11201 Training Loss: tensor(0.0421)\n",
      "11202 Training Loss: tensor(0.0423)\n",
      "11203 Training Loss: tensor(0.0442)\n",
      "11204 Training Loss: tensor(0.0427)\n",
      "11205 Training Loss: tensor(0.0417)\n",
      "11206 Training Loss: tensor(0.0407)\n",
      "11207 Training Loss: tensor(0.0408)\n",
      "11208 Training Loss: tensor(0.0424)\n",
      "11209 Training Loss: tensor(0.0494)\n",
      "11210 Training Loss: tensor(0.0379)\n",
      "11211 Training Loss: tensor(0.0454)\n",
      "11212 Training Loss: tensor(0.0380)\n",
      "11213 Training Loss: tensor(0.0452)\n",
      "11214 Training Loss: tensor(0.0413)\n",
      "11215 Training Loss: tensor(0.0426)\n",
      "11216 Training Loss: tensor(0.0392)\n",
      "11217 Training Loss: tensor(0.0416)\n",
      "11218 Training Loss: tensor(0.0456)\n",
      "11219 Training Loss: tensor(0.0413)\n",
      "11220 Training Loss: tensor(0.0455)\n",
      "11221 Training Loss: tensor(0.0443)\n",
      "11222 Training Loss: tensor(0.0444)\n",
      "11223 Training Loss: tensor(0.0389)\n",
      "11224 Training Loss: tensor(0.0418)\n",
      "11225 Training Loss: tensor(0.0473)\n",
      "11226 Training Loss: tensor(0.0364)\n",
      "11227 Training Loss: tensor(0.0404)\n",
      "11228 Training Loss: tensor(0.0368)\n",
      "11229 Training Loss: tensor(0.0450)\n",
      "11230 Training Loss: tensor(0.0432)\n",
      "11231 Training Loss: tensor(0.0445)\n",
      "11232 Training Loss: tensor(0.0393)\n",
      "11233 Training Loss: tensor(0.0399)\n",
      "11234 Training Loss: tensor(0.0432)\n",
      "11235 Training Loss: tensor(0.0403)\n",
      "11236 Training Loss: tensor(0.0433)\n",
      "11237 Training Loss: tensor(0.0487)\n",
      "11238 Training Loss: tensor(0.0442)\n",
      "11239 Training Loss: tensor(0.0429)\n",
      "11240 Training Loss: tensor(0.0443)\n",
      "11241 Training Loss: tensor(0.0477)\n",
      "11242 Training Loss: tensor(0.0414)\n",
      "11243 Training Loss: tensor(0.0424)\n",
      "11244 Training Loss: tensor(0.0422)\n",
      "11245 Training Loss: tensor(0.0491)\n",
      "11246 Training Loss: tensor(0.0440)\n",
      "11247 Training Loss: tensor(0.0407)\n",
      "11248 Training Loss: tensor(0.0374)\n",
      "11249 Training Loss: tensor(0.0395)\n",
      "11250 Training Loss: tensor(0.0374)\n",
      "11251 Training Loss: tensor(0.0464)\n",
      "11252 Training Loss: tensor(0.0386)\n",
      "11253 Training Loss: tensor(0.0349)\n",
      "11254 Training Loss: tensor(0.0450)\n",
      "11255 Training Loss: tensor(0.0444)\n",
      "11256 Training Loss: tensor(0.0428)\n",
      "11257 Training Loss: tensor(0.0422)\n",
      "11258 Training Loss: tensor(0.0443)\n",
      "11259 Training Loss: tensor(0.0410)\n",
      "11260 Training Loss: tensor(0.0409)\n",
      "11261 Training Loss: tensor(0.0406)\n",
      "11262 Training Loss: tensor(0.0419)\n",
      "11263 Training Loss: tensor(0.0345)\n",
      "11264 Training Loss: tensor(0.0407)\n",
      "11265 Training Loss: tensor(0.0425)\n",
      "11266 Training Loss: tensor(0.0418)\n",
      "11267 Training Loss: tensor(0.0462)\n",
      "11268 Training Loss: tensor(0.0410)\n",
      "11269 Training Loss: tensor(0.0416)\n",
      "11270 Training Loss: tensor(0.0424)\n",
      "11271 Training Loss: tensor(0.0398)\n",
      "11272 Training Loss: tensor(0.0361)\n",
      "11273 Training Loss: tensor(0.0463)\n",
      "11274 Training Loss: tensor(0.0369)\n",
      "11275 Training Loss: tensor(0.0416)\n",
      "11276 Training Loss: tensor(0.0434)\n",
      "11277 Training Loss: tensor(0.0453)\n",
      "11278 Training Loss: tensor(0.0408)\n",
      "11279 Training Loss: tensor(0.0489)\n",
      "11280 Training Loss: tensor(0.0397)\n",
      "11281 Training Loss: tensor(0.0397)\n",
      "11282 Training Loss: tensor(0.0418)\n",
      "11283 Training Loss: tensor(0.0383)\n",
      "11284 Training Loss: tensor(0.0418)\n",
      "11285 Training Loss: tensor(0.0438)\n",
      "11286 Training Loss: tensor(0.0472)\n",
      "11287 Training Loss: tensor(0.0425)\n",
      "11288 Training Loss: tensor(0.0386)\n",
      "11289 Training Loss: tensor(0.0389)\n",
      "11290 Training Loss: tensor(0.0384)\n",
      "11291 Training Loss: tensor(0.0501)\n",
      "11292 Training Loss: tensor(0.0408)\n",
      "11293 Training Loss: tensor(0.0432)\n",
      "11294 Training Loss: tensor(0.0435)\n",
      "11295 Training Loss: tensor(0.0414)\n",
      "11296 Training Loss: tensor(0.0423)\n",
      "11297 Training Loss: tensor(0.0418)\n",
      "11298 Training Loss: tensor(0.0487)\n",
      "11299 Training Loss: tensor(0.0363)\n",
      "11300 Training Loss: tensor(0.0433)\n",
      "11301 Training Loss: tensor(0.0400)\n",
      "11302 Training Loss: tensor(0.0413)\n",
      "11303 Training Loss: tensor(0.0392)\n",
      "11304 Training Loss: tensor(0.0377)\n",
      "11305 Training Loss: tensor(0.0429)\n",
      "11306 Training Loss: tensor(0.0427)\n",
      "11307 Training Loss: tensor(0.0436)\n",
      "11308 Training Loss: tensor(0.0426)\n",
      "11309 Training Loss: tensor(0.0388)\n",
      "11310 Training Loss: tensor(0.0441)\n",
      "11311 Training Loss: tensor(0.0455)\n",
      "11312 Training Loss: tensor(0.0403)\n",
      "11313 Training Loss: tensor(0.0401)\n",
      "11314 Training Loss: tensor(0.0443)\n",
      "11315 Training Loss: tensor(0.0401)\n",
      "11316 Training Loss: tensor(0.0423)\n",
      "11317 Training Loss: tensor(0.0384)\n",
      "11318 Training Loss: tensor(0.0435)\n",
      "11319 Training Loss: tensor(0.0451)\n",
      "11320 Training Loss: tensor(0.0448)\n",
      "11321 Training Loss: tensor(0.0396)\n",
      "11322 Training Loss: tensor(0.0423)\n",
      "11323 Training Loss: tensor(0.0440)\n",
      "11324 Training Loss: tensor(0.0442)\n",
      "11325 Training Loss: tensor(0.0409)\n",
      "11326 Training Loss: tensor(0.0445)\n",
      "11327 Training Loss: tensor(0.0398)\n",
      "11328 Training Loss: tensor(0.0368)\n",
      "11329 Training Loss: tensor(0.0393)\n",
      "11330 Training Loss: tensor(0.0434)\n",
      "11331 Training Loss: tensor(0.0429)\n",
      "11332 Training Loss: tensor(0.0445)\n",
      "11333 Training Loss: tensor(0.0435)\n",
      "11334 Training Loss: tensor(0.0369)\n",
      "11335 Training Loss: tensor(0.0362)\n",
      "11336 Training Loss: tensor(0.0406)\n",
      "11337 Training Loss: tensor(0.0463)\n",
      "11338 Training Loss: tensor(0.0420)\n",
      "11339 Training Loss: tensor(0.0402)\n",
      "11340 Training Loss: tensor(0.0404)\n",
      "11341 Training Loss: tensor(0.0416)\n",
      "11342 Training Loss: tensor(0.0403)\n",
      "11343 Training Loss: tensor(0.0456)\n",
      "11344 Training Loss: tensor(0.0364)\n",
      "11345 Training Loss: tensor(0.0400)\n",
      "11346 Training Loss: tensor(0.0431)\n",
      "11347 Training Loss: tensor(0.0382)\n",
      "11348 Training Loss: tensor(0.0418)\n",
      "11349 Training Loss: tensor(0.0428)\n",
      "11350 Training Loss: tensor(0.0439)\n",
      "11351 Training Loss: tensor(0.0414)\n",
      "11352 Training Loss: tensor(0.0452)\n",
      "11353 Training Loss: tensor(0.0406)\n",
      "11354 Training Loss: tensor(0.0419)\n",
      "11355 Training Loss: tensor(0.0430)\n",
      "11356 Training Loss: tensor(0.0429)\n",
      "11357 Training Loss: tensor(0.0453)\n",
      "11358 Training Loss: tensor(0.0420)\n",
      "11359 Training Loss: tensor(0.0420)\n",
      "11360 Training Loss: tensor(0.0453)\n",
      "11361 Training Loss: tensor(0.0436)\n",
      "11362 Training Loss: tensor(0.0420)\n",
      "11363 Training Loss: tensor(0.0414)\n",
      "11364 Training Loss: tensor(0.0359)\n",
      "11365 Training Loss: tensor(0.0426)\n",
      "11366 Training Loss: tensor(0.0398)\n",
      "11367 Training Loss: tensor(0.0331)\n",
      "11368 Training Loss: tensor(0.0457)\n",
      "11369 Training Loss: tensor(0.0368)\n",
      "11370 Training Loss: tensor(0.0431)\n",
      "11371 Training Loss: tensor(0.0385)\n",
      "11372 Training Loss: tensor(0.0375)\n",
      "11373 Training Loss: tensor(0.0411)\n",
      "11374 Training Loss: tensor(0.0341)\n",
      "11375 Training Loss: tensor(0.0442)\n",
      "11376 Training Loss: tensor(0.0453)\n",
      "11377 Training Loss: tensor(0.0380)\n",
      "11378 Training Loss: tensor(0.0412)\n",
      "11379 Training Loss: tensor(0.0392)\n",
      "11380 Training Loss: tensor(0.0419)\n",
      "11381 Training Loss: tensor(0.0422)\n",
      "11382 Training Loss: tensor(0.0391)\n",
      "11383 Training Loss: tensor(0.0424)\n",
      "11384 Training Loss: tensor(0.0414)\n",
      "11385 Training Loss: tensor(0.0363)\n",
      "11386 Training Loss: tensor(0.0350)\n",
      "11387 Training Loss: tensor(0.0384)\n",
      "11388 Training Loss: tensor(0.0413)\n",
      "11389 Training Loss: tensor(0.0426)\n",
      "11390 Training Loss: tensor(0.0433)\n",
      "11391 Training Loss: tensor(0.0449)\n",
      "11392 Training Loss: tensor(0.0405)\n",
      "11393 Training Loss: tensor(0.0397)\n",
      "11394 Training Loss: tensor(0.0472)\n",
      "11395 Training Loss: tensor(0.0359)\n",
      "11396 Training Loss: tensor(0.0399)\n",
      "11397 Training Loss: tensor(0.0429)\n",
      "11398 Training Loss: tensor(0.0445)\n",
      "11399 Training Loss: tensor(0.0442)\n",
      "11400 Training Loss: tensor(0.0352)\n",
      "11401 Training Loss: tensor(0.0461)\n",
      "11402 Training Loss: tensor(0.0397)\n",
      "11403 Training Loss: tensor(0.0439)\n",
      "11404 Training Loss: tensor(0.0421)\n",
      "11405 Training Loss: tensor(0.0395)\n",
      "11406 Training Loss: tensor(0.0413)\n",
      "11407 Training Loss: tensor(0.0451)\n",
      "11408 Training Loss: tensor(0.0427)\n",
      "11409 Training Loss: tensor(0.0453)\n",
      "11410 Training Loss: tensor(0.0357)\n",
      "11411 Training Loss: tensor(0.0420)\n",
      "11412 Training Loss: tensor(0.0469)\n",
      "11413 Training Loss: tensor(0.0346)\n",
      "11414 Training Loss: tensor(0.0451)\n",
      "11415 Training Loss: tensor(0.0435)\n",
      "11416 Training Loss: tensor(0.0427)\n",
      "11417 Training Loss: tensor(0.0424)\n",
      "11418 Training Loss: tensor(0.0449)\n",
      "11419 Training Loss: tensor(0.0413)\n",
      "11420 Training Loss: tensor(0.0413)\n",
      "11421 Training Loss: tensor(0.0420)\n",
      "11422 Training Loss: tensor(0.0397)\n",
      "11423 Training Loss: tensor(0.0412)\n",
      "11424 Training Loss: tensor(0.0337)\n",
      "11425 Training Loss: tensor(0.0409)\n",
      "11426 Training Loss: tensor(0.0432)\n",
      "11427 Training Loss: tensor(0.0465)\n",
      "11428 Training Loss: tensor(0.0453)\n",
      "11429 Training Loss: tensor(0.0439)\n",
      "11430 Training Loss: tensor(0.0407)\n",
      "11431 Training Loss: tensor(0.0455)\n",
      "11432 Training Loss: tensor(0.0383)\n",
      "11433 Training Loss: tensor(0.0480)\n",
      "11434 Training Loss: tensor(0.0410)\n",
      "11435 Training Loss: tensor(0.0380)\n",
      "11436 Training Loss: tensor(0.0372)\n",
      "11437 Training Loss: tensor(0.0410)\n",
      "11438 Training Loss: tensor(0.0439)\n",
      "11439 Training Loss: tensor(0.0403)\n",
      "11440 Training Loss: tensor(0.0358)\n",
      "11441 Training Loss: tensor(0.0375)\n",
      "11442 Training Loss: tensor(0.0429)\n",
      "11443 Training Loss: tensor(0.0420)\n",
      "11444 Training Loss: tensor(0.0396)\n",
      "11445 Training Loss: tensor(0.0384)\n",
      "11446 Training Loss: tensor(0.0493)\n",
      "11447 Training Loss: tensor(0.0443)\n",
      "11448 Training Loss: tensor(0.0378)\n",
      "11449 Training Loss: tensor(0.0422)\n",
      "11450 Training Loss: tensor(0.0420)\n",
      "11451 Training Loss: tensor(0.0388)\n",
      "11452 Training Loss: tensor(0.0407)\n",
      "11453 Training Loss: tensor(0.0385)\n",
      "11454 Training Loss: tensor(0.0366)\n",
      "11455 Training Loss: tensor(0.0454)\n",
      "11456 Training Loss: tensor(0.0438)\n",
      "11457 Training Loss: tensor(0.0428)\n",
      "11458 Training Loss: tensor(0.0455)\n",
      "11459 Training Loss: tensor(0.0427)\n",
      "11460 Training Loss: tensor(0.0475)\n",
      "11461 Training Loss: tensor(0.0396)\n",
      "11462 Training Loss: tensor(0.0372)\n",
      "11463 Training Loss: tensor(0.0441)\n",
      "11464 Training Loss: tensor(0.0437)\n",
      "11465 Training Loss: tensor(0.0464)\n",
      "11466 Training Loss: tensor(0.0385)\n",
      "11467 Training Loss: tensor(0.0388)\n",
      "11468 Training Loss: tensor(0.0417)\n",
      "11469 Training Loss: tensor(0.0394)\n",
      "11470 Training Loss: tensor(0.0377)\n",
      "11471 Training Loss: tensor(0.0401)\n",
      "11472 Training Loss: tensor(0.0414)\n",
      "11473 Training Loss: tensor(0.0481)\n",
      "11474 Training Loss: tensor(0.0434)\n",
      "11475 Training Loss: tensor(0.0441)\n",
      "11476 Training Loss: tensor(0.0495)\n",
      "11477 Training Loss: tensor(0.0437)\n",
      "11478 Training Loss: tensor(0.0432)\n",
      "11479 Training Loss: tensor(0.0367)\n",
      "11480 Training Loss: tensor(0.0385)\n",
      "11481 Training Loss: tensor(0.0419)\n",
      "11482 Training Loss: tensor(0.0427)\n",
      "11483 Training Loss: tensor(0.0369)\n",
      "11484 Training Loss: tensor(0.0447)\n",
      "11485 Training Loss: tensor(0.0433)\n",
      "11486 Training Loss: tensor(0.0394)\n",
      "11487 Training Loss: tensor(0.0435)\n",
      "11488 Training Loss: tensor(0.0411)\n",
      "11489 Training Loss: tensor(0.0384)\n",
      "11490 Training Loss: tensor(0.0417)\n",
      "11491 Training Loss: tensor(0.0398)\n",
      "11492 Training Loss: tensor(0.0382)\n",
      "11493 Training Loss: tensor(0.0467)\n",
      "11494 Training Loss: tensor(0.0412)\n",
      "11495 Training Loss: tensor(0.0419)\n",
      "11496 Training Loss: tensor(0.0525)\n",
      "11497 Training Loss: tensor(0.0356)\n",
      "11498 Training Loss: tensor(0.0399)\n",
      "11499 Training Loss: tensor(0.0448)\n",
      "11500 Training Loss: tensor(0.0449)\n",
      "11501 Training Loss: tensor(0.0400)\n",
      "11502 Training Loss: tensor(0.0465)\n",
      "11503 Training Loss: tensor(0.0424)\n",
      "11504 Training Loss: tensor(0.0480)\n",
      "11505 Training Loss: tensor(0.0420)\n",
      "11506 Training Loss: tensor(0.0376)\n",
      "11507 Training Loss: tensor(0.0421)\n",
      "11508 Training Loss: tensor(0.0405)\n",
      "11509 Training Loss: tensor(0.0418)\n",
      "11510 Training Loss: tensor(0.0412)\n",
      "11511 Training Loss: tensor(0.0428)\n",
      "11512 Training Loss: tensor(0.0342)\n",
      "11513 Training Loss: tensor(0.0404)\n",
      "11514 Training Loss: tensor(0.0396)\n",
      "11515 Training Loss: tensor(0.0429)\n",
      "11516 Training Loss: tensor(0.0380)\n",
      "11517 Training Loss: tensor(0.0392)\n",
      "11518 Training Loss: tensor(0.0395)\n",
      "11519 Training Loss: tensor(0.0471)\n",
      "11520 Training Loss: tensor(0.0455)\n",
      "11521 Training Loss: tensor(0.0425)\n",
      "11522 Training Loss: tensor(0.0456)\n",
      "11523 Training Loss: tensor(0.0372)\n",
      "11524 Training Loss: tensor(0.0426)\n",
      "11525 Training Loss: tensor(0.0406)\n",
      "11526 Training Loss: tensor(0.0435)\n",
      "11527 Training Loss: tensor(0.0454)\n",
      "11528 Training Loss: tensor(0.0368)\n",
      "11529 Training Loss: tensor(0.0358)\n",
      "11530 Training Loss: tensor(0.0466)\n",
      "11531 Training Loss: tensor(0.0473)\n",
      "11532 Training Loss: tensor(0.0373)\n",
      "11533 Training Loss: tensor(0.0468)\n",
      "11534 Training Loss: tensor(0.0466)\n",
      "11535 Training Loss: tensor(0.0418)\n",
      "11536 Training Loss: tensor(0.0431)\n",
      "11537 Training Loss: tensor(0.0391)\n",
      "11538 Training Loss: tensor(0.0453)\n",
      "11539 Training Loss: tensor(0.0412)\n",
      "11540 Training Loss: tensor(0.0468)\n",
      "11541 Training Loss: tensor(0.0396)\n",
      "11542 Training Loss: tensor(0.0400)\n",
      "11543 Training Loss: tensor(0.0417)\n",
      "11544 Training Loss: tensor(0.0361)\n",
      "11545 Training Loss: tensor(0.0436)\n",
      "11546 Training Loss: tensor(0.0349)\n",
      "11547 Training Loss: tensor(0.0467)\n",
      "11548 Training Loss: tensor(0.0431)\n",
      "11549 Training Loss: tensor(0.0380)\n",
      "11550 Training Loss: tensor(0.0422)\n",
      "11551 Training Loss: tensor(0.0387)\n",
      "11552 Training Loss: tensor(0.0387)\n",
      "11553 Training Loss: tensor(0.0385)\n",
      "11554 Training Loss: tensor(0.0364)\n",
      "11555 Training Loss: tensor(0.0353)\n",
      "11556 Training Loss: tensor(0.0435)\n",
      "11557 Training Loss: tensor(0.0444)\n",
      "11558 Training Loss: tensor(0.0439)\n",
      "11559 Training Loss: tensor(0.0409)\n",
      "11560 Training Loss: tensor(0.0427)\n",
      "11561 Training Loss: tensor(0.0388)\n",
      "11562 Training Loss: tensor(0.0400)\n",
      "11563 Training Loss: tensor(0.0465)\n",
      "11564 Training Loss: tensor(0.0460)\n",
      "11565 Training Loss: tensor(0.0453)\n",
      "11566 Training Loss: tensor(0.0421)\n",
      "11567 Training Loss: tensor(0.0396)\n",
      "11568 Training Loss: tensor(0.0387)\n",
      "11569 Training Loss: tensor(0.0455)\n",
      "11570 Training Loss: tensor(0.0391)\n",
      "11571 Training Loss: tensor(0.0435)\n",
      "11572 Training Loss: tensor(0.0380)\n",
      "11573 Training Loss: tensor(0.0423)\n",
      "11574 Training Loss: tensor(0.0392)\n",
      "11575 Training Loss: tensor(0.0371)\n",
      "11576 Training Loss: tensor(0.0433)\n",
      "11577 Training Loss: tensor(0.0437)\n",
      "11578 Training Loss: tensor(0.0416)\n",
      "11579 Training Loss: tensor(0.0433)\n",
      "11580 Training Loss: tensor(0.0509)\n",
      "11581 Training Loss: tensor(0.0394)\n",
      "11582 Training Loss: tensor(0.0406)\n",
      "11583 Training Loss: tensor(0.0405)\n",
      "11584 Training Loss: tensor(0.0431)\n",
      "11585 Training Loss: tensor(0.0442)\n",
      "11586 Training Loss: tensor(0.0401)\n",
      "11587 Training Loss: tensor(0.0368)\n",
      "11588 Training Loss: tensor(0.0442)\n",
      "11589 Training Loss: tensor(0.0469)\n",
      "11590 Training Loss: tensor(0.0445)\n",
      "11591 Training Loss: tensor(0.0428)\n",
      "11592 Training Loss: tensor(0.0398)\n",
      "11593 Training Loss: tensor(0.0462)\n",
      "11594 Training Loss: tensor(0.0438)\n",
      "11595 Training Loss: tensor(0.0525)\n",
      "11596 Training Loss: tensor(0.0404)\n",
      "11597 Training Loss: tensor(0.0385)\n",
      "11598 Training Loss: tensor(0.0419)\n",
      "11599 Training Loss: tensor(0.0410)\n",
      "11600 Training Loss: tensor(0.0480)\n",
      "11601 Training Loss: tensor(0.0397)\n",
      "11602 Training Loss: tensor(0.0398)\n",
      "11603 Training Loss: tensor(0.0411)\n",
      "11604 Training Loss: tensor(0.0394)\n",
      "11605 Training Loss: tensor(0.0374)\n",
      "11606 Training Loss: tensor(0.0415)\n",
      "11607 Training Loss: tensor(0.0445)\n",
      "11608 Training Loss: tensor(0.0435)\n",
      "11609 Training Loss: tensor(0.0380)\n",
      "11610 Training Loss: tensor(0.0430)\n",
      "11611 Training Loss: tensor(0.0413)\n",
      "11612 Training Loss: tensor(0.0426)\n",
      "11613 Training Loss: tensor(0.0423)\n",
      "11614 Training Loss: tensor(0.0460)\n",
      "11615 Training Loss: tensor(0.0415)\n",
      "11616 Training Loss: tensor(0.0387)\n",
      "11617 Training Loss: tensor(0.0397)\n",
      "11618 Training Loss: tensor(0.0409)\n",
      "11619 Training Loss: tensor(0.0433)\n",
      "11620 Training Loss: tensor(0.0398)\n",
      "11621 Training Loss: tensor(0.0408)\n",
      "11622 Training Loss: tensor(0.0427)\n",
      "11623 Training Loss: tensor(0.0442)\n",
      "11624 Training Loss: tensor(0.0445)\n",
      "11625 Training Loss: tensor(0.0396)\n",
      "11626 Training Loss: tensor(0.0421)\n",
      "11627 Training Loss: tensor(0.0391)\n",
      "11628 Training Loss: tensor(0.0357)\n",
      "11629 Training Loss: tensor(0.0455)\n",
      "11630 Training Loss: tensor(0.0433)\n",
      "11631 Training Loss: tensor(0.0408)\n",
      "11632 Training Loss: tensor(0.0419)\n",
      "11633 Training Loss: tensor(0.0390)\n",
      "11634 Training Loss: tensor(0.0391)\n",
      "11635 Training Loss: tensor(0.0362)\n",
      "11636 Training Loss: tensor(0.0419)\n",
      "11637 Training Loss: tensor(0.0409)\n",
      "11638 Training Loss: tensor(0.0407)\n",
      "11639 Training Loss: tensor(0.0434)\n",
      "11640 Training Loss: tensor(0.0410)\n",
      "11641 Training Loss: tensor(0.0376)\n",
      "11642 Training Loss: tensor(0.0416)\n",
      "11643 Training Loss: tensor(0.0358)\n",
      "11644 Training Loss: tensor(0.0416)\n",
      "11645 Training Loss: tensor(0.0391)\n",
      "11646 Training Loss: tensor(0.0400)\n",
      "11647 Training Loss: tensor(0.0386)\n",
      "11648 Training Loss: tensor(0.0463)\n",
      "11649 Training Loss: tensor(0.0421)\n",
      "11650 Training Loss: tensor(0.0447)\n",
      "11651 Training Loss: tensor(0.0418)\n",
      "11652 Training Loss: tensor(0.0366)\n",
      "11653 Training Loss: tensor(0.0418)\n",
      "11654 Training Loss: tensor(0.0387)\n",
      "11655 Training Loss: tensor(0.0356)\n",
      "11656 Training Loss: tensor(0.0413)\n",
      "11657 Training Loss: tensor(0.0402)\n",
      "11658 Training Loss: tensor(0.0388)\n",
      "11659 Training Loss: tensor(0.0435)\n",
      "11660 Training Loss: tensor(0.0449)\n",
      "11661 Training Loss: tensor(0.0374)\n",
      "11662 Training Loss: tensor(0.0435)\n",
      "11663 Training Loss: tensor(0.0400)\n",
      "11664 Training Loss: tensor(0.0408)\n",
      "11665 Training Loss: tensor(0.0431)\n",
      "11666 Training Loss: tensor(0.0433)\n",
      "11667 Training Loss: tensor(0.0465)\n",
      "11668 Training Loss: tensor(0.0402)\n",
      "11669 Training Loss: tensor(0.0410)\n",
      "11670 Training Loss: tensor(0.0440)\n",
      "11671 Training Loss: tensor(0.0404)\n",
      "11672 Training Loss: tensor(0.0414)\n",
      "11673 Training Loss: tensor(0.0399)\n",
      "11674 Training Loss: tensor(0.0407)\n",
      "11675 Training Loss: tensor(0.0457)\n",
      "11676 Training Loss: tensor(0.0397)\n",
      "11677 Training Loss: tensor(0.0405)\n",
      "11678 Training Loss: tensor(0.0402)\n",
      "11679 Training Loss: tensor(0.0432)\n",
      "11680 Training Loss: tensor(0.0443)\n",
      "11681 Training Loss: tensor(0.0456)\n",
      "11682 Training Loss: tensor(0.0385)\n",
      "11683 Training Loss: tensor(0.0426)\n",
      "11684 Training Loss: tensor(0.0430)\n",
      "11685 Training Loss: tensor(0.0335)\n",
      "11686 Training Loss: tensor(0.0442)\n",
      "11687 Training Loss: tensor(0.0408)\n",
      "11688 Training Loss: tensor(0.0396)\n",
      "11689 Training Loss: tensor(0.0443)\n",
      "11690 Training Loss: tensor(0.0424)\n",
      "11691 Training Loss: tensor(0.0385)\n",
      "11692 Training Loss: tensor(0.0434)\n",
      "11693 Training Loss: tensor(0.0414)\n",
      "11694 Training Loss: tensor(0.0419)\n",
      "11695 Training Loss: tensor(0.0379)\n",
      "11696 Training Loss: tensor(0.0388)\n",
      "11697 Training Loss: tensor(0.0372)\n",
      "11698 Training Loss: tensor(0.0434)\n",
      "11699 Training Loss: tensor(0.0384)\n",
      "11700 Training Loss: tensor(0.0396)\n",
      "11701 Training Loss: tensor(0.0407)\n",
      "11702 Training Loss: tensor(0.0351)\n",
      "11703 Training Loss: tensor(0.0436)\n",
      "11704 Training Loss: tensor(0.0427)\n",
      "11705 Training Loss: tensor(0.0429)\n",
      "11706 Training Loss: tensor(0.0402)\n",
      "11707 Training Loss: tensor(0.0419)\n",
      "11708 Training Loss: tensor(0.0376)\n",
      "11709 Training Loss: tensor(0.0452)\n",
      "11710 Training Loss: tensor(0.0418)\n",
      "11711 Training Loss: tensor(0.0398)\n",
      "11712 Training Loss: tensor(0.0423)\n",
      "11713 Training Loss: tensor(0.0410)\n",
      "11714 Training Loss: tensor(0.0409)\n",
      "11715 Training Loss: tensor(0.0385)\n",
      "11716 Training Loss: tensor(0.0401)\n",
      "11717 Training Loss: tensor(0.0363)\n",
      "11718 Training Loss: tensor(0.0386)\n",
      "11719 Training Loss: tensor(0.0416)\n",
      "11720 Training Loss: tensor(0.0408)\n",
      "11721 Training Loss: tensor(0.0404)\n",
      "11722 Training Loss: tensor(0.0452)\n",
      "11723 Training Loss: tensor(0.0377)\n",
      "11724 Training Loss: tensor(0.0430)\n",
      "11725 Training Loss: tensor(0.0371)\n",
      "11726 Training Loss: tensor(0.0404)\n",
      "11727 Training Loss: tensor(0.0392)\n",
      "11728 Training Loss: tensor(0.0386)\n",
      "11729 Training Loss: tensor(0.0438)\n",
      "11730 Training Loss: tensor(0.0394)\n",
      "11731 Training Loss: tensor(0.0392)\n",
      "11732 Training Loss: tensor(0.0438)\n",
      "11733 Training Loss: tensor(0.0433)\n",
      "11734 Training Loss: tensor(0.0375)\n",
      "11735 Training Loss: tensor(0.0393)\n",
      "11736 Training Loss: tensor(0.0339)\n",
      "11737 Training Loss: tensor(0.0416)\n",
      "11738 Training Loss: tensor(0.0460)\n",
      "11739 Training Loss: tensor(0.0482)\n",
      "11740 Training Loss: tensor(0.0403)\n",
      "11741 Training Loss: tensor(0.0413)\n",
      "11742 Training Loss: tensor(0.0388)\n",
      "11743 Training Loss: tensor(0.0401)\n",
      "11744 Training Loss: tensor(0.0437)\n",
      "11745 Training Loss: tensor(0.0364)\n",
      "11746 Training Loss: tensor(0.0428)\n",
      "11747 Training Loss: tensor(0.0455)\n",
      "11748 Training Loss: tensor(0.0379)\n",
      "11749 Training Loss: tensor(0.0386)\n",
      "11750 Training Loss: tensor(0.0398)\n",
      "11751 Training Loss: tensor(0.0392)\n",
      "11752 Training Loss: tensor(0.0415)\n",
      "11753 Training Loss: tensor(0.0405)\n",
      "11754 Training Loss: tensor(0.0415)\n",
      "11755 Training Loss: tensor(0.0393)\n",
      "11756 Training Loss: tensor(0.0395)\n",
      "11757 Training Loss: tensor(0.0423)\n",
      "11758 Training Loss: tensor(0.0393)\n",
      "11759 Training Loss: tensor(0.0392)\n",
      "11760 Training Loss: tensor(0.0416)\n",
      "11761 Training Loss: tensor(0.0442)\n",
      "11762 Training Loss: tensor(0.0324)\n",
      "11763 Training Loss: tensor(0.0426)\n",
      "11764 Training Loss: tensor(0.0367)\n",
      "11765 Training Loss: tensor(0.0357)\n",
      "11766 Training Loss: tensor(0.0365)\n",
      "11767 Training Loss: tensor(0.0429)\n",
      "11768 Training Loss: tensor(0.0427)\n",
      "11769 Training Loss: tensor(0.0380)\n",
      "11770 Training Loss: tensor(0.0421)\n",
      "11771 Training Loss: tensor(0.0389)\n",
      "11772 Training Loss: tensor(0.0455)\n",
      "11773 Training Loss: tensor(0.0401)\n",
      "11774 Training Loss: tensor(0.0423)\n",
      "11775 Training Loss: tensor(0.0399)\n",
      "11776 Training Loss: tensor(0.0405)\n",
      "11777 Training Loss: tensor(0.0428)\n",
      "11778 Training Loss: tensor(0.0431)\n",
      "11779 Training Loss: tensor(0.0411)\n",
      "11780 Training Loss: tensor(0.0358)\n",
      "11781 Training Loss: tensor(0.0427)\n",
      "11782 Training Loss: tensor(0.0442)\n",
      "11783 Training Loss: tensor(0.0419)\n",
      "11784 Training Loss: tensor(0.0419)\n",
      "11785 Training Loss: tensor(0.0381)\n",
      "11786 Training Loss: tensor(0.0383)\n",
      "11787 Training Loss: tensor(0.0376)\n",
      "11788 Training Loss: tensor(0.0455)\n",
      "11789 Training Loss: tensor(0.0423)\n",
      "11790 Training Loss: tensor(0.0398)\n",
      "11791 Training Loss: tensor(0.0379)\n",
      "11792 Training Loss: tensor(0.0424)\n",
      "11793 Training Loss: tensor(0.0435)\n",
      "11794 Training Loss: tensor(0.0448)\n",
      "11795 Training Loss: tensor(0.0415)\n",
      "11796 Training Loss: tensor(0.0372)\n",
      "11797 Training Loss: tensor(0.0392)\n",
      "11798 Training Loss: tensor(0.0425)\n",
      "11799 Training Loss: tensor(0.0469)\n",
      "11800 Training Loss: tensor(0.0406)\n",
      "11801 Training Loss: tensor(0.0462)\n",
      "11802 Training Loss: tensor(0.0476)\n",
      "11803 Training Loss: tensor(0.0365)\n",
      "11804 Training Loss: tensor(0.0405)\n",
      "11805 Training Loss: tensor(0.0382)\n",
      "11806 Training Loss: tensor(0.0442)\n",
      "11807 Training Loss: tensor(0.0431)\n",
      "11808 Training Loss: tensor(0.0439)\n",
      "11809 Training Loss: tensor(0.0367)\n",
      "11810 Training Loss: tensor(0.0409)\n",
      "11811 Training Loss: tensor(0.0434)\n",
      "11812 Training Loss: tensor(0.0395)\n",
      "11813 Training Loss: tensor(0.0493)\n",
      "11814 Training Loss: tensor(0.0413)\n",
      "11815 Training Loss: tensor(0.0404)\n",
      "11816 Training Loss: tensor(0.0380)\n",
      "11817 Training Loss: tensor(0.0394)\n",
      "11818 Training Loss: tensor(0.0368)\n",
      "11819 Training Loss: tensor(0.0416)\n",
      "11820 Training Loss: tensor(0.0457)\n",
      "11821 Training Loss: tensor(0.0416)\n",
      "11822 Training Loss: tensor(0.0443)\n",
      "11823 Training Loss: tensor(0.0415)\n",
      "11824 Training Loss: tensor(0.0400)\n",
      "11825 Training Loss: tensor(0.0397)\n",
      "11826 Training Loss: tensor(0.0442)\n",
      "11827 Training Loss: tensor(0.0433)\n",
      "11828 Training Loss: tensor(0.0415)\n",
      "11829 Training Loss: tensor(0.0445)\n",
      "11830 Training Loss: tensor(0.0423)\n",
      "11831 Training Loss: tensor(0.0372)\n",
      "11832 Training Loss: tensor(0.0398)\n",
      "11833 Training Loss: tensor(0.0407)\n",
      "11834 Training Loss: tensor(0.0441)\n",
      "11835 Training Loss: tensor(0.0460)\n",
      "11836 Training Loss: tensor(0.0434)\n",
      "11837 Training Loss: tensor(0.0452)\n",
      "11838 Training Loss: tensor(0.0433)\n",
      "11839 Training Loss: tensor(0.0479)\n",
      "11840 Training Loss: tensor(0.0459)\n",
      "11841 Training Loss: tensor(0.0468)\n",
      "11842 Training Loss: tensor(0.0426)\n",
      "11843 Training Loss: tensor(0.0407)\n",
      "11844 Training Loss: tensor(0.0385)\n",
      "11845 Training Loss: tensor(0.0431)\n",
      "11846 Training Loss: tensor(0.0427)\n",
      "11847 Training Loss: tensor(0.0394)\n",
      "11848 Training Loss: tensor(0.0418)\n",
      "11849 Training Loss: tensor(0.0416)\n",
      "11850 Training Loss: tensor(0.0442)\n",
      "11851 Training Loss: tensor(0.0424)\n",
      "11852 Training Loss: tensor(0.0392)\n",
      "11853 Training Loss: tensor(0.0402)\n",
      "11854 Training Loss: tensor(0.0412)\n",
      "11855 Training Loss: tensor(0.0421)\n",
      "11856 Training Loss: tensor(0.0436)\n",
      "11857 Training Loss: tensor(0.0484)\n",
      "11858 Training Loss: tensor(0.0443)\n",
      "11859 Training Loss: tensor(0.0367)\n",
      "11860 Training Loss: tensor(0.0336)\n",
      "11861 Training Loss: tensor(0.0413)\n",
      "11862 Training Loss: tensor(0.0404)\n",
      "11863 Training Loss: tensor(0.0403)\n",
      "11864 Training Loss: tensor(0.0489)\n",
      "11865 Training Loss: tensor(0.0390)\n",
      "11866 Training Loss: tensor(0.0373)\n",
      "11867 Training Loss: tensor(0.0445)\n",
      "11868 Training Loss: tensor(0.0397)\n",
      "11869 Training Loss: tensor(0.0388)\n",
      "11870 Training Loss: tensor(0.0452)\n",
      "11871 Training Loss: tensor(0.0448)\n",
      "11872 Training Loss: tensor(0.0464)\n",
      "11873 Training Loss: tensor(0.0395)\n",
      "11874 Training Loss: tensor(0.0463)\n",
      "11875 Training Loss: tensor(0.0408)\n",
      "11876 Training Loss: tensor(0.0401)\n",
      "11877 Training Loss: tensor(0.0381)\n",
      "11878 Training Loss: tensor(0.0389)\n",
      "11879 Training Loss: tensor(0.0475)\n",
      "11880 Training Loss: tensor(0.0443)\n",
      "11881 Training Loss: tensor(0.0465)\n",
      "11882 Training Loss: tensor(0.0366)\n",
      "11883 Training Loss: tensor(0.0362)\n",
      "11884 Training Loss: tensor(0.0427)\n",
      "11885 Training Loss: tensor(0.0381)\n",
      "11886 Training Loss: tensor(0.0369)\n",
      "11887 Training Loss: tensor(0.0375)\n",
      "11888 Training Loss: tensor(0.0404)\n",
      "11889 Training Loss: tensor(0.0383)\n",
      "11890 Training Loss: tensor(0.0395)\n",
      "11891 Training Loss: tensor(0.0359)\n",
      "11892 Training Loss: tensor(0.0379)\n",
      "11893 Training Loss: tensor(0.0430)\n",
      "11894 Training Loss: tensor(0.0457)\n",
      "11895 Training Loss: tensor(0.0416)\n",
      "11896 Training Loss: tensor(0.0404)\n",
      "11897 Training Loss: tensor(0.0438)\n",
      "11898 Training Loss: tensor(0.0429)\n",
      "11899 Training Loss: tensor(0.0421)\n",
      "11900 Training Loss: tensor(0.0367)\n",
      "11901 Training Loss: tensor(0.0405)\n",
      "11902 Training Loss: tensor(0.0468)\n",
      "11903 Training Loss: tensor(0.0437)\n",
      "11904 Training Loss: tensor(0.0398)\n",
      "11905 Training Loss: tensor(0.0410)\n",
      "11906 Training Loss: tensor(0.0370)\n",
      "11907 Training Loss: tensor(0.0413)\n",
      "11908 Training Loss: tensor(0.0363)\n",
      "11909 Training Loss: tensor(0.0452)\n",
      "11910 Training Loss: tensor(0.0417)\n",
      "11911 Training Loss: tensor(0.0351)\n",
      "11912 Training Loss: tensor(0.0454)\n",
      "11913 Training Loss: tensor(0.0341)\n",
      "11914 Training Loss: tensor(0.0448)\n",
      "11915 Training Loss: tensor(0.0391)\n",
      "11916 Training Loss: tensor(0.0357)\n",
      "11917 Training Loss: tensor(0.0345)\n",
      "11918 Training Loss: tensor(0.0373)\n",
      "11919 Training Loss: tensor(0.0419)\n",
      "11920 Training Loss: tensor(0.0399)\n",
      "11921 Training Loss: tensor(0.0423)\n",
      "11922 Training Loss: tensor(0.0390)\n",
      "11923 Training Loss: tensor(0.0384)\n",
      "11924 Training Loss: tensor(0.0377)\n",
      "11925 Training Loss: tensor(0.0418)\n",
      "11926 Training Loss: tensor(0.0423)\n",
      "11927 Training Loss: tensor(0.0425)\n",
      "11928 Training Loss: tensor(0.0424)\n",
      "11929 Training Loss: tensor(0.0390)\n",
      "11930 Training Loss: tensor(0.0373)\n",
      "11931 Training Loss: tensor(0.0404)\n",
      "11932 Training Loss: tensor(0.0426)\n",
      "11933 Training Loss: tensor(0.0416)\n",
      "11934 Training Loss: tensor(0.0427)\n",
      "11935 Training Loss: tensor(0.0415)\n",
      "11936 Training Loss: tensor(0.0393)\n",
      "11937 Training Loss: tensor(0.0375)\n",
      "11938 Training Loss: tensor(0.0378)\n",
      "11939 Training Loss: tensor(0.0379)\n",
      "11940 Training Loss: tensor(0.0445)\n",
      "11941 Training Loss: tensor(0.0422)\n",
      "11942 Training Loss: tensor(0.0398)\n",
      "11943 Training Loss: tensor(0.0460)\n",
      "11944 Training Loss: tensor(0.0405)\n",
      "11945 Training Loss: tensor(0.0421)\n",
      "11946 Training Loss: tensor(0.0425)\n",
      "11947 Training Loss: tensor(0.0391)\n",
      "11948 Training Loss: tensor(0.0360)\n",
      "11949 Training Loss: tensor(0.0394)\n",
      "11950 Training Loss: tensor(0.0442)\n",
      "11951 Training Loss: tensor(0.0443)\n",
      "11952 Training Loss: tensor(0.0441)\n",
      "11953 Training Loss: tensor(0.0459)\n",
      "11954 Training Loss: tensor(0.0377)\n",
      "11955 Training Loss: tensor(0.0420)\n",
      "11956 Training Loss: tensor(0.0433)\n",
      "11957 Training Loss: tensor(0.0414)\n",
      "11958 Training Loss: tensor(0.0394)\n",
      "11959 Training Loss: tensor(0.0431)\n",
      "11960 Training Loss: tensor(0.0388)\n",
      "11961 Training Loss: tensor(0.0449)\n",
      "11962 Training Loss: tensor(0.0413)\n",
      "11963 Training Loss: tensor(0.0466)\n",
      "11964 Training Loss: tensor(0.0411)\n",
      "11965 Training Loss: tensor(0.0433)\n",
      "11966 Training Loss: tensor(0.0449)\n",
      "11967 Training Loss: tensor(0.0412)\n",
      "11968 Training Loss: tensor(0.0398)\n",
      "11969 Training Loss: tensor(0.0421)\n",
      "11970 Training Loss: tensor(0.0403)\n",
      "11971 Training Loss: tensor(0.0389)\n",
      "11972 Training Loss: tensor(0.0380)\n",
      "11973 Training Loss: tensor(0.0389)\n",
      "11974 Training Loss: tensor(0.0451)\n",
      "11975 Training Loss: tensor(0.0406)\n",
      "11976 Training Loss: tensor(0.0436)\n",
      "11977 Training Loss: tensor(0.0432)\n",
      "11978 Training Loss: tensor(0.0349)\n",
      "11979 Training Loss: tensor(0.0427)\n",
      "11980 Training Loss: tensor(0.0382)\n",
      "11981 Training Loss: tensor(0.0379)\n",
      "11982 Training Loss: tensor(0.0422)\n",
      "11983 Training Loss: tensor(0.0426)\n",
      "11984 Training Loss: tensor(0.0431)\n",
      "11985 Training Loss: tensor(0.0382)\n",
      "11986 Training Loss: tensor(0.0415)\n",
      "11987 Training Loss: tensor(0.0416)\n",
      "11988 Training Loss: tensor(0.0361)\n",
      "11989 Training Loss: tensor(0.0386)\n",
      "11990 Training Loss: tensor(0.0396)\n",
      "11991 Training Loss: tensor(0.0397)\n",
      "11992 Training Loss: tensor(0.0479)\n",
      "11993 Training Loss: tensor(0.0391)\n",
      "11994 Training Loss: tensor(0.0394)\n",
      "11995 Training Loss: tensor(0.0403)\n",
      "11996 Training Loss: tensor(0.0429)\n",
      "11997 Training Loss: tensor(0.0495)\n",
      "11998 Training Loss: tensor(0.0387)\n",
      "11999 Training Loss: tensor(0.0424)\n",
      "12000 Training Loss: tensor(0.0397)\n",
      "12001 Training Loss: tensor(0.0434)\n",
      "12002 Training Loss: tensor(0.0396)\n",
      "12003 Training Loss: tensor(0.0436)\n",
      "12004 Training Loss: tensor(0.0424)\n",
      "12005 Training Loss: tensor(0.0353)\n",
      "12006 Training Loss: tensor(0.0360)\n",
      "12007 Training Loss: tensor(0.0401)\n",
      "12008 Training Loss: tensor(0.0468)\n",
      "12009 Training Loss: tensor(0.0413)\n",
      "12010 Training Loss: tensor(0.0441)\n",
      "12011 Training Loss: tensor(0.0441)\n",
      "12012 Training Loss: tensor(0.0445)\n",
      "12013 Training Loss: tensor(0.0402)\n",
      "12014 Training Loss: tensor(0.0407)\n",
      "12015 Training Loss: tensor(0.0377)\n",
      "12016 Training Loss: tensor(0.0412)\n",
      "12017 Training Loss: tensor(0.0408)\n",
      "12018 Training Loss: tensor(0.0428)\n",
      "12019 Training Loss: tensor(0.0442)\n",
      "12020 Training Loss: tensor(0.0417)\n",
      "12021 Training Loss: tensor(0.0376)\n",
      "12022 Training Loss: tensor(0.0455)\n",
      "12023 Training Loss: tensor(0.0393)\n",
      "12024 Training Loss: tensor(0.0406)\n",
      "12025 Training Loss: tensor(0.0424)\n",
      "12026 Training Loss: tensor(0.0371)\n",
      "12027 Training Loss: tensor(0.0398)\n",
      "12028 Training Loss: tensor(0.0386)\n",
      "12029 Training Loss: tensor(0.0450)\n",
      "12030 Training Loss: tensor(0.0375)\n",
      "12031 Training Loss: tensor(0.0399)\n",
      "12032 Training Loss: tensor(0.0373)\n",
      "12033 Training Loss: tensor(0.0413)\n",
      "12034 Training Loss: tensor(0.0427)\n",
      "12035 Training Loss: tensor(0.0382)\n",
      "12036 Training Loss: tensor(0.0454)\n",
      "12037 Training Loss: tensor(0.0402)\n",
      "12038 Training Loss: tensor(0.0407)\n",
      "12039 Training Loss: tensor(0.0461)\n",
      "12040 Training Loss: tensor(0.0429)\n",
      "12041 Training Loss: tensor(0.0396)\n",
      "12042 Training Loss: tensor(0.0471)\n",
      "12043 Training Loss: tensor(0.0431)\n",
      "12044 Training Loss: tensor(0.0399)\n",
      "12045 Training Loss: tensor(0.0438)\n",
      "12046 Training Loss: tensor(0.0403)\n",
      "12047 Training Loss: tensor(0.0420)\n",
      "12048 Training Loss: tensor(0.0438)\n",
      "12049 Training Loss: tensor(0.0428)\n",
      "12050 Training Loss: tensor(0.0396)\n",
      "12051 Training Loss: tensor(0.0364)\n",
      "12052 Training Loss: tensor(0.0483)\n",
      "12053 Training Loss: tensor(0.0445)\n",
      "12054 Training Loss: tensor(0.0424)\n",
      "12055 Training Loss: tensor(0.0473)\n",
      "12056 Training Loss: tensor(0.0390)\n",
      "12057 Training Loss: tensor(0.0472)\n",
      "12058 Training Loss: tensor(0.0419)\n",
      "12059 Training Loss: tensor(0.0414)\n",
      "12060 Training Loss: tensor(0.0452)\n",
      "12061 Training Loss: tensor(0.0427)\n",
      "12062 Training Loss: tensor(0.0420)\n",
      "12063 Training Loss: tensor(0.0413)\n",
      "12064 Training Loss: tensor(0.0437)\n",
      "12065 Training Loss: tensor(0.0403)\n",
      "12066 Training Loss: tensor(0.0399)\n",
      "12067 Training Loss: tensor(0.0458)\n",
      "12068 Training Loss: tensor(0.0443)\n",
      "12069 Training Loss: tensor(0.0393)\n",
      "12070 Training Loss: tensor(0.0423)\n",
      "12071 Training Loss: tensor(0.0407)\n",
      "12072 Training Loss: tensor(0.0422)\n",
      "12073 Training Loss: tensor(0.0379)\n",
      "12074 Training Loss: tensor(0.0341)\n",
      "12075 Training Loss: tensor(0.0390)\n",
      "12076 Training Loss: tensor(0.0401)\n",
      "12077 Training Loss: tensor(0.0402)\n",
      "12078 Training Loss: tensor(0.0452)\n",
      "12079 Training Loss: tensor(0.0371)\n",
      "12080 Training Loss: tensor(0.0410)\n",
      "12081 Training Loss: tensor(0.0430)\n",
      "12082 Training Loss: tensor(0.0388)\n",
      "12083 Training Loss: tensor(0.0398)\n",
      "12084 Training Loss: tensor(0.0425)\n",
      "12085 Training Loss: tensor(0.0415)\n",
      "12086 Training Loss: tensor(0.0438)\n",
      "12087 Training Loss: tensor(0.0460)\n",
      "12088 Training Loss: tensor(0.0428)\n",
      "12089 Training Loss: tensor(0.0415)\n",
      "12090 Training Loss: tensor(0.0448)\n",
      "12091 Training Loss: tensor(0.0416)\n",
      "12092 Training Loss: tensor(0.0354)\n",
      "12093 Training Loss: tensor(0.0388)\n",
      "12094 Training Loss: tensor(0.0423)\n",
      "12095 Training Loss: tensor(0.0381)\n",
      "12096 Training Loss: tensor(0.0474)\n",
      "12097 Training Loss: tensor(0.0407)\n",
      "12098 Training Loss: tensor(0.0456)\n",
      "12099 Training Loss: tensor(0.0386)\n",
      "12100 Training Loss: tensor(0.0434)\n",
      "12101 Training Loss: tensor(0.0408)\n",
      "12102 Training Loss: tensor(0.0388)\n",
      "12103 Training Loss: tensor(0.0428)\n",
      "12104 Training Loss: tensor(0.0407)\n",
      "12105 Training Loss: tensor(0.0424)\n",
      "12106 Training Loss: tensor(0.0419)\n",
      "12107 Training Loss: tensor(0.0367)\n",
      "12108 Training Loss: tensor(0.0411)\n",
      "12109 Training Loss: tensor(0.0401)\n",
      "12110 Training Loss: tensor(0.0404)\n",
      "12111 Training Loss: tensor(0.0435)\n",
      "12112 Training Loss: tensor(0.0381)\n",
      "12113 Training Loss: tensor(0.0377)\n",
      "12114 Training Loss: tensor(0.0462)\n",
      "12115 Training Loss: tensor(0.0405)\n",
      "12116 Training Loss: tensor(0.0396)\n",
      "12117 Training Loss: tensor(0.0438)\n",
      "12118 Training Loss: tensor(0.0473)\n",
      "12119 Training Loss: tensor(0.0450)\n",
      "12120 Training Loss: tensor(0.0469)\n",
      "12121 Training Loss: tensor(0.0410)\n",
      "12122 Training Loss: tensor(0.0455)\n",
      "12123 Training Loss: tensor(0.0377)\n",
      "12124 Training Loss: tensor(0.0419)\n",
      "12125 Training Loss: tensor(0.0455)\n",
      "12126 Training Loss: tensor(0.0435)\n",
      "12127 Training Loss: tensor(0.0467)\n",
      "12128 Training Loss: tensor(0.0426)\n",
      "12129 Training Loss: tensor(0.0392)\n",
      "12130 Training Loss: tensor(0.0401)\n",
      "12131 Training Loss: tensor(0.0434)\n",
      "12132 Training Loss: tensor(0.0416)\n",
      "12133 Training Loss: tensor(0.0405)\n",
      "12134 Training Loss: tensor(0.0382)\n",
      "12135 Training Loss: tensor(0.0437)\n",
      "12136 Training Loss: tensor(0.0357)\n",
      "12137 Training Loss: tensor(0.0420)\n",
      "12138 Training Loss: tensor(0.0381)\n",
      "12139 Training Loss: tensor(0.0394)\n",
      "12140 Training Loss: tensor(0.0438)\n",
      "12141 Training Loss: tensor(0.0434)\n",
      "12142 Training Loss: tensor(0.0399)\n",
      "12143 Training Loss: tensor(0.0430)\n",
      "12144 Training Loss: tensor(0.0441)\n",
      "12145 Training Loss: tensor(0.0390)\n",
      "12146 Training Loss: tensor(0.0368)\n",
      "12147 Training Loss: tensor(0.0415)\n",
      "12148 Training Loss: tensor(0.0383)\n",
      "12149 Training Loss: tensor(0.0432)\n",
      "12150 Training Loss: tensor(0.0402)\n",
      "12151 Training Loss: tensor(0.0457)\n",
      "12152 Training Loss: tensor(0.0373)\n",
      "12153 Training Loss: tensor(0.0342)\n",
      "12154 Training Loss: tensor(0.0375)\n",
      "12155 Training Loss: tensor(0.0431)\n",
      "12156 Training Loss: tensor(0.0393)\n",
      "12157 Training Loss: tensor(0.0422)\n",
      "12158 Training Loss: tensor(0.0432)\n",
      "12159 Training Loss: tensor(0.0416)\n",
      "12160 Training Loss: tensor(0.0404)\n",
      "12161 Training Loss: tensor(0.0422)\n",
      "12162 Training Loss: tensor(0.0380)\n",
      "12163 Training Loss: tensor(0.0452)\n",
      "12164 Training Loss: tensor(0.0433)\n",
      "12165 Training Loss: tensor(0.0421)\n",
      "12166 Training Loss: tensor(0.0440)\n",
      "12167 Training Loss: tensor(0.0388)\n",
      "12168 Training Loss: tensor(0.0355)\n",
      "12169 Training Loss: tensor(0.0396)\n",
      "12170 Training Loss: tensor(0.0412)\n",
      "12171 Training Loss: tensor(0.0428)\n",
      "12172 Training Loss: tensor(0.0388)\n",
      "12173 Training Loss: tensor(0.0413)\n",
      "12174 Training Loss: tensor(0.0415)\n",
      "12175 Training Loss: tensor(0.0424)\n",
      "12176 Training Loss: tensor(0.0383)\n",
      "12177 Training Loss: tensor(0.0369)\n",
      "12178 Training Loss: tensor(0.0419)\n",
      "12179 Training Loss: tensor(0.0405)\n",
      "12180 Training Loss: tensor(0.0388)\n",
      "12181 Training Loss: tensor(0.0450)\n",
      "12182 Training Loss: tensor(0.0376)\n",
      "12183 Training Loss: tensor(0.0433)\n",
      "12184 Training Loss: tensor(0.0395)\n",
      "12185 Training Loss: tensor(0.0394)\n",
      "12186 Training Loss: tensor(0.0410)\n",
      "12187 Training Loss: tensor(0.0458)\n",
      "12188 Training Loss: tensor(0.0421)\n",
      "12189 Training Loss: tensor(0.0377)\n",
      "12190 Training Loss: tensor(0.0341)\n",
      "12191 Training Loss: tensor(0.0356)\n",
      "12192 Training Loss: tensor(0.0426)\n",
      "12193 Training Loss: tensor(0.0405)\n",
      "12194 Training Loss: tensor(0.0405)\n",
      "12195 Training Loss: tensor(0.0395)\n",
      "12196 Training Loss: tensor(0.0457)\n",
      "12197 Training Loss: tensor(0.0369)\n",
      "12198 Training Loss: tensor(0.0470)\n",
      "12199 Training Loss: tensor(0.0426)\n",
      "12200 Training Loss: tensor(0.0394)\n",
      "12201 Training Loss: tensor(0.0406)\n",
      "12202 Training Loss: tensor(0.0364)\n",
      "12203 Training Loss: tensor(0.0377)\n",
      "12204 Training Loss: tensor(0.0420)\n",
      "12205 Training Loss: tensor(0.0365)\n",
      "12206 Training Loss: tensor(0.0406)\n",
      "12207 Training Loss: tensor(0.0426)\n",
      "12208 Training Loss: tensor(0.0407)\n",
      "12209 Training Loss: tensor(0.0399)\n",
      "12210 Training Loss: tensor(0.0405)\n",
      "12211 Training Loss: tensor(0.0421)\n",
      "12212 Training Loss: tensor(0.0432)\n",
      "12213 Training Loss: tensor(0.0459)\n",
      "12214 Training Loss: tensor(0.0401)\n",
      "12215 Training Loss: tensor(0.0404)\n",
      "12216 Training Loss: tensor(0.0360)\n",
      "12217 Training Loss: tensor(0.0459)\n",
      "12218 Training Loss: tensor(0.0412)\n",
      "12219 Training Loss: tensor(0.0448)\n",
      "12220 Training Loss: tensor(0.0487)\n",
      "12221 Training Loss: tensor(0.0406)\n",
      "12222 Training Loss: tensor(0.0421)\n",
      "12223 Training Loss: tensor(0.0391)\n",
      "12224 Training Loss: tensor(0.0352)\n",
      "12225 Training Loss: tensor(0.0422)\n",
      "12226 Training Loss: tensor(0.0444)\n",
      "12227 Training Loss: tensor(0.0414)\n",
      "12228 Training Loss: tensor(0.0379)\n",
      "12229 Training Loss: tensor(0.0361)\n",
      "12230 Training Loss: tensor(0.0429)\n",
      "12231 Training Loss: tensor(0.0409)\n",
      "12232 Training Loss: tensor(0.0413)\n",
      "12233 Training Loss: tensor(0.0431)\n",
      "12234 Training Loss: tensor(0.0412)\n",
      "12235 Training Loss: tensor(0.0363)\n",
      "12236 Training Loss: tensor(0.0391)\n",
      "12237 Training Loss: tensor(0.0464)\n",
      "12238 Training Loss: tensor(0.0386)\n",
      "12239 Training Loss: tensor(0.0377)\n",
      "12240 Training Loss: tensor(0.0391)\n",
      "12241 Training Loss: tensor(0.0338)\n",
      "12242 Training Loss: tensor(0.0411)\n",
      "12243 Training Loss: tensor(0.0460)\n",
      "12244 Training Loss: tensor(0.0419)\n",
      "12245 Training Loss: tensor(0.0399)\n",
      "12246 Training Loss: tensor(0.0420)\n",
      "12247 Training Loss: tensor(0.0395)\n",
      "12248 Training Loss: tensor(0.0410)\n",
      "12249 Training Loss: tensor(0.0444)\n",
      "12250 Training Loss: tensor(0.0447)\n",
      "12251 Training Loss: tensor(0.0466)\n",
      "12252 Training Loss: tensor(0.0385)\n",
      "12253 Training Loss: tensor(0.0456)\n",
      "12254 Training Loss: tensor(0.0428)\n",
      "12255 Training Loss: tensor(0.0355)\n",
      "12256 Training Loss: tensor(0.0428)\n",
      "12257 Training Loss: tensor(0.0411)\n",
      "12258 Training Loss: tensor(0.0412)\n",
      "12259 Training Loss: tensor(0.0487)\n",
      "12260 Training Loss: tensor(0.0356)\n",
      "12261 Training Loss: tensor(0.0399)\n",
      "12262 Training Loss: tensor(0.0406)\n",
      "12263 Training Loss: tensor(0.0400)\n",
      "12264 Training Loss: tensor(0.0403)\n",
      "12265 Training Loss: tensor(0.0429)\n",
      "12266 Training Loss: tensor(0.0395)\n",
      "12267 Training Loss: tensor(0.0371)\n",
      "12268 Training Loss: tensor(0.0404)\n",
      "12269 Training Loss: tensor(0.0412)\n",
      "12270 Training Loss: tensor(0.0404)\n",
      "12271 Training Loss: tensor(0.0404)\n",
      "12272 Training Loss: tensor(0.0422)\n",
      "12273 Training Loss: tensor(0.0377)\n",
      "12274 Training Loss: tensor(0.0380)\n",
      "12275 Training Loss: tensor(0.0404)\n",
      "12276 Training Loss: tensor(0.0430)\n",
      "12277 Training Loss: tensor(0.0386)\n",
      "12278 Training Loss: tensor(0.0437)\n",
      "12279 Training Loss: tensor(0.0408)\n",
      "12280 Training Loss: tensor(0.0410)\n",
      "12281 Training Loss: tensor(0.0413)\n",
      "12282 Training Loss: tensor(0.0418)\n",
      "12283 Training Loss: tensor(0.0450)\n",
      "12284 Training Loss: tensor(0.0436)\n",
      "12285 Training Loss: tensor(0.0386)\n",
      "12286 Training Loss: tensor(0.0409)\n",
      "12287 Training Loss: tensor(0.0443)\n",
      "12288 Training Loss: tensor(0.0402)\n",
      "12289 Training Loss: tensor(0.0412)\n",
      "12290 Training Loss: tensor(0.0391)\n",
      "12291 Training Loss: tensor(0.0388)\n",
      "12292 Training Loss: tensor(0.0416)\n",
      "12293 Training Loss: tensor(0.0388)\n",
      "12294 Training Loss: tensor(0.0395)\n",
      "12295 Training Loss: tensor(0.0400)\n",
      "12296 Training Loss: tensor(0.0415)\n",
      "12297 Training Loss: tensor(0.0436)\n",
      "12298 Training Loss: tensor(0.0431)\n",
      "12299 Training Loss: tensor(0.0342)\n",
      "12300 Training Loss: tensor(0.0404)\n",
      "12301 Training Loss: tensor(0.0392)\n",
      "12302 Training Loss: tensor(0.0418)\n",
      "12303 Training Loss: tensor(0.0341)\n",
      "12304 Training Loss: tensor(0.0435)\n",
      "12305 Training Loss: tensor(0.0348)\n",
      "12306 Training Loss: tensor(0.0413)\n",
      "12307 Training Loss: tensor(0.0394)\n",
      "12308 Training Loss: tensor(0.0379)\n",
      "12309 Training Loss: tensor(0.0416)\n",
      "12310 Training Loss: tensor(0.0403)\n",
      "12311 Training Loss: tensor(0.0376)\n",
      "12312 Training Loss: tensor(0.0401)\n",
      "12313 Training Loss: tensor(0.0338)\n",
      "12314 Training Loss: tensor(0.0340)\n",
      "12315 Training Loss: tensor(0.0417)\n",
      "12316 Training Loss: tensor(0.0414)\n",
      "12317 Training Loss: tensor(0.0362)\n",
      "12318 Training Loss: tensor(0.0398)\n",
      "12319 Training Loss: tensor(0.0427)\n",
      "12320 Training Loss: tensor(0.0382)\n",
      "12321 Training Loss: tensor(0.0428)\n",
      "12322 Training Loss: tensor(0.0430)\n",
      "12323 Training Loss: tensor(0.0392)\n",
      "12324 Training Loss: tensor(0.0376)\n",
      "12325 Training Loss: tensor(0.0413)\n",
      "12326 Training Loss: tensor(0.0395)\n",
      "12327 Training Loss: tensor(0.0362)\n",
      "12328 Training Loss: tensor(0.0450)\n",
      "12329 Training Loss: tensor(0.0419)\n",
      "12330 Training Loss: tensor(0.0442)\n",
      "12331 Training Loss: tensor(0.0410)\n",
      "12332 Training Loss: tensor(0.0386)\n",
      "12333 Training Loss: tensor(0.0423)\n",
      "12334 Training Loss: tensor(0.0425)\n",
      "12335 Training Loss: tensor(0.0377)\n",
      "12336 Training Loss: tensor(0.0390)\n",
      "12337 Training Loss: tensor(0.0393)\n",
      "12338 Training Loss: tensor(0.0388)\n",
      "12339 Training Loss: tensor(0.0454)\n",
      "12340 Training Loss: tensor(0.0430)\n",
      "12341 Training Loss: tensor(0.0464)\n",
      "12342 Training Loss: tensor(0.0416)\n",
      "12343 Training Loss: tensor(0.0360)\n",
      "12344 Training Loss: tensor(0.0432)\n",
      "12345 Training Loss: tensor(0.0394)\n",
      "12346 Training Loss: tensor(0.0387)\n",
      "12347 Training Loss: tensor(0.0359)\n",
      "12348 Training Loss: tensor(0.0454)\n",
      "12349 Training Loss: tensor(0.0472)\n",
      "12350 Training Loss: tensor(0.0443)\n",
      "12351 Training Loss: tensor(0.0420)\n",
      "12352 Training Loss: tensor(0.0368)\n",
      "12353 Training Loss: tensor(0.0423)\n",
      "12354 Training Loss: tensor(0.0434)\n",
      "12355 Training Loss: tensor(0.0430)\n",
      "12356 Training Loss: tensor(0.0408)\n",
      "12357 Training Loss: tensor(0.0392)\n",
      "12358 Training Loss: tensor(0.0363)\n",
      "12359 Training Loss: tensor(0.0415)\n",
      "12360 Training Loss: tensor(0.0440)\n",
      "12361 Training Loss: tensor(0.0403)\n",
      "12362 Training Loss: tensor(0.0398)\n",
      "12363 Training Loss: tensor(0.0379)\n",
      "12364 Training Loss: tensor(0.0395)\n",
      "12365 Training Loss: tensor(0.0430)\n",
      "12366 Training Loss: tensor(0.0451)\n",
      "12367 Training Loss: tensor(0.0417)\n",
      "12368 Training Loss: tensor(0.0399)\n",
      "12369 Training Loss: tensor(0.0432)\n",
      "12370 Training Loss: tensor(0.0384)\n",
      "12371 Training Loss: tensor(0.0477)\n",
      "12372 Training Loss: tensor(0.0409)\n",
      "12373 Training Loss: tensor(0.0365)\n",
      "12374 Training Loss: tensor(0.0437)\n",
      "12375 Training Loss: tensor(0.0411)\n",
      "12376 Training Loss: tensor(0.0370)\n",
      "12377 Training Loss: tensor(0.0429)\n",
      "12378 Training Loss: tensor(0.0395)\n",
      "12379 Training Loss: tensor(0.0430)\n",
      "12380 Training Loss: tensor(0.0372)\n",
      "12381 Training Loss: tensor(0.0431)\n",
      "12382 Training Loss: tensor(0.0375)\n",
      "12383 Training Loss: tensor(0.0448)\n",
      "12384 Training Loss: tensor(0.0364)\n",
      "12385 Training Loss: tensor(0.0390)\n",
      "12386 Training Loss: tensor(0.0437)\n",
      "12387 Training Loss: tensor(0.0435)\n",
      "12388 Training Loss: tensor(0.0424)\n",
      "12389 Training Loss: tensor(0.0408)\n",
      "12390 Training Loss: tensor(0.0396)\n",
      "12391 Training Loss: tensor(0.0396)\n",
      "12392 Training Loss: tensor(0.0419)\n",
      "12393 Training Loss: tensor(0.0357)\n",
      "12394 Training Loss: tensor(0.0394)\n",
      "12395 Training Loss: tensor(0.0452)\n",
      "12396 Training Loss: tensor(0.0463)\n",
      "12397 Training Loss: tensor(0.0395)\n",
      "12398 Training Loss: tensor(0.0411)\n",
      "12399 Training Loss: tensor(0.0412)\n",
      "12400 Training Loss: tensor(0.0417)\n",
      "12401 Training Loss: tensor(0.0395)\n",
      "12402 Training Loss: tensor(0.0391)\n",
      "12403 Training Loss: tensor(0.0414)\n",
      "12404 Training Loss: tensor(0.0417)\n",
      "12405 Training Loss: tensor(0.0385)\n",
      "12406 Training Loss: tensor(0.0397)\n",
      "12407 Training Loss: tensor(0.0428)\n",
      "12408 Training Loss: tensor(0.0422)\n",
      "12409 Training Loss: tensor(0.0379)\n",
      "12410 Training Loss: tensor(0.0491)\n",
      "12411 Training Loss: tensor(0.0433)\n",
      "12412 Training Loss: tensor(0.0369)\n",
      "12413 Training Loss: tensor(0.0420)\n",
      "12414 Training Loss: tensor(0.0426)\n",
      "12415 Training Loss: tensor(0.0467)\n",
      "12416 Training Loss: tensor(0.0393)\n",
      "12417 Training Loss: tensor(0.0423)\n",
      "12418 Training Loss: tensor(0.0407)\n",
      "12419 Training Loss: tensor(0.0433)\n",
      "12420 Training Loss: tensor(0.0374)\n",
      "12421 Training Loss: tensor(0.0468)\n",
      "12422 Training Loss: tensor(0.0390)\n",
      "12423 Training Loss: tensor(0.0367)\n",
      "12424 Training Loss: tensor(0.0411)\n",
      "12425 Training Loss: tensor(0.0400)\n",
      "12426 Training Loss: tensor(0.0423)\n",
      "12427 Training Loss: tensor(0.0416)\n",
      "12428 Training Loss: tensor(0.0403)\n",
      "12429 Training Loss: tensor(0.0437)\n",
      "12430 Training Loss: tensor(0.0389)\n",
      "12431 Training Loss: tensor(0.0398)\n",
      "12432 Training Loss: tensor(0.0452)\n",
      "12433 Training Loss: tensor(0.0398)\n",
      "12434 Training Loss: tensor(0.0469)\n",
      "12435 Training Loss: tensor(0.0392)\n",
      "12436 Training Loss: tensor(0.0380)\n",
      "12437 Training Loss: tensor(0.0440)\n",
      "12438 Training Loss: tensor(0.0439)\n",
      "12439 Training Loss: tensor(0.0411)\n",
      "12440 Training Loss: tensor(0.0406)\n",
      "12441 Training Loss: tensor(0.0380)\n",
      "12442 Training Loss: tensor(0.0455)\n",
      "12443 Training Loss: tensor(0.0381)\n",
      "12444 Training Loss: tensor(0.0428)\n",
      "12445 Training Loss: tensor(0.0412)\n",
      "12446 Training Loss: tensor(0.0466)\n",
      "12447 Training Loss: tensor(0.0405)\n",
      "12448 Training Loss: tensor(0.0443)\n",
      "12449 Training Loss: tensor(0.0382)\n",
      "12450 Training Loss: tensor(0.0376)\n",
      "12451 Training Loss: tensor(0.0451)\n",
      "12452 Training Loss: tensor(0.0481)\n",
      "12453 Training Loss: tensor(0.0379)\n",
      "12454 Training Loss: tensor(0.0389)\n",
      "12455 Training Loss: tensor(0.0457)\n",
      "12456 Training Loss: tensor(0.0475)\n",
      "12457 Training Loss: tensor(0.0403)\n",
      "12458 Training Loss: tensor(0.0341)\n",
      "12459 Training Loss: tensor(0.0402)\n",
      "12460 Training Loss: tensor(0.0338)\n",
      "12461 Training Loss: tensor(0.0453)\n",
      "12462 Training Loss: tensor(0.0415)\n",
      "12463 Training Loss: tensor(0.0409)\n",
      "12464 Training Loss: tensor(0.0445)\n",
      "12465 Training Loss: tensor(0.0408)\n",
      "12466 Training Loss: tensor(0.0408)\n",
      "12467 Training Loss: tensor(0.0370)\n",
      "12468 Training Loss: tensor(0.0336)\n",
      "12469 Training Loss: tensor(0.0391)\n",
      "12470 Training Loss: tensor(0.0411)\n",
      "12471 Training Loss: tensor(0.0394)\n",
      "12472 Training Loss: tensor(0.0419)\n",
      "12473 Training Loss: tensor(0.0420)\n",
      "12474 Training Loss: tensor(0.0447)\n",
      "12475 Training Loss: tensor(0.0426)\n",
      "12476 Training Loss: tensor(0.0438)\n",
      "12477 Training Loss: tensor(0.0387)\n",
      "12478 Training Loss: tensor(0.0423)\n",
      "12479 Training Loss: tensor(0.0396)\n",
      "12480 Training Loss: tensor(0.0449)\n",
      "12481 Training Loss: tensor(0.0408)\n",
      "12482 Training Loss: tensor(0.0430)\n",
      "12483 Training Loss: tensor(0.0382)\n",
      "12484 Training Loss: tensor(0.0420)\n",
      "12485 Training Loss: tensor(0.0410)\n",
      "12486 Training Loss: tensor(0.0438)\n",
      "12487 Training Loss: tensor(0.0383)\n",
      "12488 Training Loss: tensor(0.0354)\n",
      "12489 Training Loss: tensor(0.0410)\n",
      "12490 Training Loss: tensor(0.0385)\n",
      "12491 Training Loss: tensor(0.0434)\n",
      "12492 Training Loss: tensor(0.0425)\n",
      "12493 Training Loss: tensor(0.0413)\n",
      "12494 Training Loss: tensor(0.0437)\n",
      "12495 Training Loss: tensor(0.0427)\n",
      "12496 Training Loss: tensor(0.0378)\n",
      "12497 Training Loss: tensor(0.0387)\n",
      "12498 Training Loss: tensor(0.0393)\n",
      "12499 Training Loss: tensor(0.0439)\n",
      "12500 Training Loss: tensor(0.0477)\n",
      "12501 Training Loss: tensor(0.0445)\n",
      "12502 Training Loss: tensor(0.0308)\n",
      "12503 Training Loss: tensor(0.0416)\n",
      "12504 Training Loss: tensor(0.0420)\n",
      "12505 Training Loss: tensor(0.0403)\n",
      "12506 Training Loss: tensor(0.0388)\n",
      "12507 Training Loss: tensor(0.0375)\n",
      "12508 Training Loss: tensor(0.0385)\n",
      "12509 Training Loss: tensor(0.0371)\n",
      "12510 Training Loss: tensor(0.0375)\n",
      "12511 Training Loss: tensor(0.0371)\n",
      "12512 Training Loss: tensor(0.0347)\n",
      "12513 Training Loss: tensor(0.0431)\n",
      "12514 Training Loss: tensor(0.0368)\n",
      "12515 Training Loss: tensor(0.0390)\n",
      "12516 Training Loss: tensor(0.0433)\n",
      "12517 Training Loss: tensor(0.0398)\n",
      "12518 Training Loss: tensor(0.0382)\n",
      "12519 Training Loss: tensor(0.0339)\n",
      "12520 Training Loss: tensor(0.0370)\n",
      "12521 Training Loss: tensor(0.0438)\n",
      "12522 Training Loss: tensor(0.0394)\n",
      "12523 Training Loss: tensor(0.0349)\n",
      "12524 Training Loss: tensor(0.0356)\n",
      "12525 Training Loss: tensor(0.0457)\n",
      "12526 Training Loss: tensor(0.0353)\n",
      "12527 Training Loss: tensor(0.0448)\n",
      "12528 Training Loss: tensor(0.0414)\n",
      "12529 Training Loss: tensor(0.0361)\n",
      "12530 Training Loss: tensor(0.0460)\n",
      "12531 Training Loss: tensor(0.0460)\n",
      "12532 Training Loss: tensor(0.0413)\n",
      "12533 Training Loss: tensor(0.0541)\n",
      "12534 Training Loss: tensor(0.0330)\n",
      "12535 Training Loss: tensor(0.0475)\n",
      "12536 Training Loss: tensor(0.0418)\n",
      "12537 Training Loss: tensor(0.0392)\n",
      "12538 Training Loss: tensor(0.0426)\n",
      "12539 Training Loss: tensor(0.0377)\n",
      "12540 Training Loss: tensor(0.0375)\n",
      "12541 Training Loss: tensor(0.0396)\n",
      "12542 Training Loss: tensor(0.0429)\n",
      "12543 Training Loss: tensor(0.0416)\n",
      "12544 Training Loss: tensor(0.0378)\n",
      "12545 Training Loss: tensor(0.0418)\n",
      "12546 Training Loss: tensor(0.0472)\n",
      "12547 Training Loss: tensor(0.0345)\n",
      "12548 Training Loss: tensor(0.0479)\n",
      "12549 Training Loss: tensor(0.0377)\n",
      "12550 Training Loss: tensor(0.0402)\n",
      "12551 Training Loss: tensor(0.0443)\n",
      "12552 Training Loss: tensor(0.0400)\n",
      "12553 Training Loss: tensor(0.0392)\n",
      "12554 Training Loss: tensor(0.0412)\n",
      "12555 Training Loss: tensor(0.0428)\n",
      "12556 Training Loss: tensor(0.0374)\n",
      "12557 Training Loss: tensor(0.0407)\n",
      "12558 Training Loss: tensor(0.0394)\n",
      "12559 Training Loss: tensor(0.0373)\n",
      "12560 Training Loss: tensor(0.0495)\n",
      "12561 Training Loss: tensor(0.0409)\n",
      "12562 Training Loss: tensor(0.0453)\n",
      "12563 Training Loss: tensor(0.0399)\n",
      "12564 Training Loss: tensor(0.0431)\n",
      "12565 Training Loss: tensor(0.0456)\n",
      "12566 Training Loss: tensor(0.0409)\n",
      "12567 Training Loss: tensor(0.0369)\n",
      "12568 Training Loss: tensor(0.0436)\n",
      "12569 Training Loss: tensor(0.0414)\n",
      "12570 Training Loss: tensor(0.0323)\n",
      "12571 Training Loss: tensor(0.0405)\n",
      "12572 Training Loss: tensor(0.0405)\n",
      "12573 Training Loss: tensor(0.0380)\n",
      "12574 Training Loss: tensor(0.0407)\n",
      "12575 Training Loss: tensor(0.0457)\n",
      "12576 Training Loss: tensor(0.0442)\n",
      "12577 Training Loss: tensor(0.0421)\n",
      "12578 Training Loss: tensor(0.0413)\n",
      "12579 Training Loss: tensor(0.0407)\n",
      "12580 Training Loss: tensor(0.0389)\n",
      "12581 Training Loss: tensor(0.0395)\n",
      "12582 Training Loss: tensor(0.0414)\n",
      "12583 Training Loss: tensor(0.0396)\n",
      "12584 Training Loss: tensor(0.0416)\n",
      "12585 Training Loss: tensor(0.0414)\n",
      "12586 Training Loss: tensor(0.0407)\n",
      "12587 Training Loss: tensor(0.0406)\n",
      "12588 Training Loss: tensor(0.0433)\n",
      "12589 Training Loss: tensor(0.0442)\n",
      "12590 Training Loss: tensor(0.0394)\n",
      "12591 Training Loss: tensor(0.0432)\n",
      "12592 Training Loss: tensor(0.0413)\n",
      "12593 Training Loss: tensor(0.0447)\n",
      "12594 Training Loss: tensor(0.0388)\n",
      "12595 Training Loss: tensor(0.0427)\n",
      "12596 Training Loss: tensor(0.0412)\n",
      "12597 Training Loss: tensor(0.0415)\n",
      "12598 Training Loss: tensor(0.0370)\n",
      "12599 Training Loss: tensor(0.0437)\n",
      "12600 Training Loss: tensor(0.0458)\n",
      "12601 Training Loss: tensor(0.0427)\n",
      "12602 Training Loss: tensor(0.0424)\n",
      "12603 Training Loss: tensor(0.0385)\n",
      "12604 Training Loss: tensor(0.0398)\n",
      "12605 Training Loss: tensor(0.0446)\n",
      "12606 Training Loss: tensor(0.0393)\n",
      "12607 Training Loss: tensor(0.0489)\n",
      "12608 Training Loss: tensor(0.0405)\n",
      "12609 Training Loss: tensor(0.0385)\n",
      "12610 Training Loss: tensor(0.0396)\n",
      "12611 Training Loss: tensor(0.0444)\n",
      "12612 Training Loss: tensor(0.0413)\n",
      "12613 Training Loss: tensor(0.0374)\n",
      "12614 Training Loss: tensor(0.0399)\n",
      "12615 Training Loss: tensor(0.0385)\n",
      "12616 Training Loss: tensor(0.0399)\n",
      "12617 Training Loss: tensor(0.0449)\n",
      "12618 Training Loss: tensor(0.0372)\n",
      "12619 Training Loss: tensor(0.0361)\n",
      "12620 Training Loss: tensor(0.0422)\n",
      "12621 Training Loss: tensor(0.0406)\n",
      "12622 Training Loss: tensor(0.0372)\n",
      "12623 Training Loss: tensor(0.0390)\n",
      "12624 Training Loss: tensor(0.0395)\n",
      "12625 Training Loss: tensor(0.0424)\n",
      "12626 Training Loss: tensor(0.0363)\n",
      "12627 Training Loss: tensor(0.0465)\n",
      "12628 Training Loss: tensor(0.0394)\n",
      "12629 Training Loss: tensor(0.0427)\n",
      "12630 Training Loss: tensor(0.0431)\n",
      "12631 Training Loss: tensor(0.0402)\n",
      "12632 Training Loss: tensor(0.0388)\n",
      "12633 Training Loss: tensor(0.0412)\n",
      "12634 Training Loss: tensor(0.0424)\n",
      "12635 Training Loss: tensor(0.0380)\n",
      "12636 Training Loss: tensor(0.0397)\n",
      "12637 Training Loss: tensor(0.0377)\n",
      "12638 Training Loss: tensor(0.0406)\n",
      "12639 Training Loss: tensor(0.0431)\n",
      "12640 Training Loss: tensor(0.0347)\n",
      "12641 Training Loss: tensor(0.0396)\n",
      "12642 Training Loss: tensor(0.0392)\n",
      "12643 Training Loss: tensor(0.0426)\n",
      "12644 Training Loss: tensor(0.0452)\n",
      "12645 Training Loss: tensor(0.0315)\n",
      "12646 Training Loss: tensor(0.0400)\n",
      "12647 Training Loss: tensor(0.0342)\n",
      "12648 Training Loss: tensor(0.0376)\n",
      "12649 Training Loss: tensor(0.0392)\n",
      "12650 Training Loss: tensor(0.0440)\n",
      "12651 Training Loss: tensor(0.0396)\n",
      "12652 Training Loss: tensor(0.0358)\n",
      "12653 Training Loss: tensor(0.0388)\n",
      "12654 Training Loss: tensor(0.0407)\n",
      "12655 Training Loss: tensor(0.0443)\n",
      "12656 Training Loss: tensor(0.0392)\n",
      "12657 Training Loss: tensor(0.0402)\n",
      "12658 Training Loss: tensor(0.0419)\n",
      "12659 Training Loss: tensor(0.0402)\n",
      "12660 Training Loss: tensor(0.0433)\n",
      "12661 Training Loss: tensor(0.0435)\n",
      "12662 Training Loss: tensor(0.0420)\n",
      "12663 Training Loss: tensor(0.0407)\n",
      "12664 Training Loss: tensor(0.0376)\n",
      "12665 Training Loss: tensor(0.0389)\n",
      "12666 Training Loss: tensor(0.0373)\n",
      "12667 Training Loss: tensor(0.0396)\n",
      "12668 Training Loss: tensor(0.0362)\n",
      "12669 Training Loss: tensor(0.0421)\n",
      "12670 Training Loss: tensor(0.0346)\n",
      "12671 Training Loss: tensor(0.0423)\n",
      "12672 Training Loss: tensor(0.0433)\n",
      "12673 Training Loss: tensor(0.0371)\n",
      "12674 Training Loss: tensor(0.0451)\n",
      "12675 Training Loss: tensor(0.0409)\n",
      "12676 Training Loss: tensor(0.0428)\n",
      "12677 Training Loss: tensor(0.0457)\n",
      "12678 Training Loss: tensor(0.0451)\n",
      "12679 Training Loss: tensor(0.0331)\n",
      "12680 Training Loss: tensor(0.0451)\n",
      "12681 Training Loss: tensor(0.0425)\n",
      "12682 Training Loss: tensor(0.0403)\n",
      "12683 Training Loss: tensor(0.0464)\n",
      "12684 Training Loss: tensor(0.0456)\n",
      "12685 Training Loss: tensor(0.0417)\n",
      "12686 Training Loss: tensor(0.0434)\n",
      "12687 Training Loss: tensor(0.0456)\n",
      "12688 Training Loss: tensor(0.0410)\n",
      "12689 Training Loss: tensor(0.0391)\n",
      "12690 Training Loss: tensor(0.0373)\n",
      "12691 Training Loss: tensor(0.0423)\n",
      "12692 Training Loss: tensor(0.0402)\n",
      "12693 Training Loss: tensor(0.0358)\n",
      "12694 Training Loss: tensor(0.0388)\n",
      "12695 Training Loss: tensor(0.0462)\n",
      "12696 Training Loss: tensor(0.0431)\n",
      "12697 Training Loss: tensor(0.0384)\n",
      "12698 Training Loss: tensor(0.0386)\n",
      "12699 Training Loss: tensor(0.0430)\n",
      "12700 Training Loss: tensor(0.0385)\n",
      "12701 Training Loss: tensor(0.0398)\n",
      "12702 Training Loss: tensor(0.0389)\n",
      "12703 Training Loss: tensor(0.0411)\n",
      "12704 Training Loss: tensor(0.0455)\n",
      "12705 Training Loss: tensor(0.0367)\n",
      "12706 Training Loss: tensor(0.0402)\n",
      "12707 Training Loss: tensor(0.0430)\n",
      "12708 Training Loss: tensor(0.0464)\n",
      "12709 Training Loss: tensor(0.0391)\n",
      "12710 Training Loss: tensor(0.0440)\n",
      "12711 Training Loss: tensor(0.0379)\n",
      "12712 Training Loss: tensor(0.0368)\n",
      "12713 Training Loss: tensor(0.0346)\n",
      "12714 Training Loss: tensor(0.0412)\n",
      "12715 Training Loss: tensor(0.0415)\n",
      "12716 Training Loss: tensor(0.0397)\n",
      "12717 Training Loss: tensor(0.0370)\n",
      "12718 Training Loss: tensor(0.0370)\n",
      "12719 Training Loss: tensor(0.0371)\n",
      "12720 Training Loss: tensor(0.0384)\n",
      "12721 Training Loss: tensor(0.0422)\n",
      "12722 Training Loss: tensor(0.0429)\n",
      "12723 Training Loss: tensor(0.0390)\n",
      "12724 Training Loss: tensor(0.0425)\n",
      "12725 Training Loss: tensor(0.0434)\n",
      "12726 Training Loss: tensor(0.0403)\n",
      "12727 Training Loss: tensor(0.0406)\n",
      "12728 Training Loss: tensor(0.0421)\n",
      "12729 Training Loss: tensor(0.0424)\n",
      "12730 Training Loss: tensor(0.0418)\n",
      "12731 Training Loss: tensor(0.0378)\n",
      "12732 Training Loss: tensor(0.0456)\n",
      "12733 Training Loss: tensor(0.0449)\n",
      "12734 Training Loss: tensor(0.0439)\n",
      "12735 Training Loss: tensor(0.0415)\n",
      "12736 Training Loss: tensor(0.0424)\n",
      "12737 Training Loss: tensor(0.0407)\n",
      "12738 Training Loss: tensor(0.0380)\n",
      "12739 Training Loss: tensor(0.0400)\n",
      "12740 Training Loss: tensor(0.0372)\n",
      "12741 Training Loss: tensor(0.0396)\n",
      "12742 Training Loss: tensor(0.0463)\n",
      "12743 Training Loss: tensor(0.0419)\n",
      "12744 Training Loss: tensor(0.0423)\n",
      "12745 Training Loss: tensor(0.0402)\n",
      "12746 Training Loss: tensor(0.0418)\n",
      "12747 Training Loss: tensor(0.0416)\n",
      "12748 Training Loss: tensor(0.0424)\n",
      "12749 Training Loss: tensor(0.0392)\n",
      "12750 Training Loss: tensor(0.0378)\n",
      "12751 Training Loss: tensor(0.0438)\n",
      "12752 Training Loss: tensor(0.0388)\n",
      "12753 Training Loss: tensor(0.0384)\n",
      "12754 Training Loss: tensor(0.0413)\n",
      "12755 Training Loss: tensor(0.0410)\n",
      "12756 Training Loss: tensor(0.0370)\n",
      "12757 Training Loss: tensor(0.0466)\n",
      "12758 Training Loss: tensor(0.0387)\n",
      "12759 Training Loss: tensor(0.0400)\n",
      "12760 Training Loss: tensor(0.0379)\n",
      "12761 Training Loss: tensor(0.0385)\n",
      "12762 Training Loss: tensor(0.0390)\n",
      "12763 Training Loss: tensor(0.0434)\n",
      "12764 Training Loss: tensor(0.0430)\n",
      "12765 Training Loss: tensor(0.0349)\n",
      "12766 Training Loss: tensor(0.0369)\n",
      "12767 Training Loss: tensor(0.0383)\n",
      "12768 Training Loss: tensor(0.0392)\n",
      "12769 Training Loss: tensor(0.0406)\n",
      "12770 Training Loss: tensor(0.0357)\n",
      "12771 Training Loss: tensor(0.0466)\n",
      "12772 Training Loss: tensor(0.0376)\n",
      "12773 Training Loss: tensor(0.0413)\n",
      "12774 Training Loss: tensor(0.0398)\n",
      "12775 Training Loss: tensor(0.0448)\n",
      "12776 Training Loss: tensor(0.0377)\n",
      "12777 Training Loss: tensor(0.0394)\n",
      "12778 Training Loss: tensor(0.0463)\n",
      "12779 Training Loss: tensor(0.0363)\n",
      "12780 Training Loss: tensor(0.0380)\n",
      "12781 Training Loss: tensor(0.0380)\n",
      "12782 Training Loss: tensor(0.0432)\n",
      "12783 Training Loss: tensor(0.0408)\n",
      "12784 Training Loss: tensor(0.0433)\n",
      "12785 Training Loss: tensor(0.0406)\n",
      "12786 Training Loss: tensor(0.0397)\n",
      "12787 Training Loss: tensor(0.0423)\n",
      "12788 Training Loss: tensor(0.0374)\n",
      "12789 Training Loss: tensor(0.0393)\n",
      "12790 Training Loss: tensor(0.0397)\n",
      "12791 Training Loss: tensor(0.0442)\n",
      "12792 Training Loss: tensor(0.0414)\n",
      "12793 Training Loss: tensor(0.0437)\n",
      "12794 Training Loss: tensor(0.0360)\n",
      "12795 Training Loss: tensor(0.0388)\n",
      "12796 Training Loss: tensor(0.0441)\n",
      "12797 Training Loss: tensor(0.0389)\n",
      "12798 Training Loss: tensor(0.0402)\n",
      "12799 Training Loss: tensor(0.0400)\n",
      "12800 Training Loss: tensor(0.0374)\n",
      "12801 Training Loss: tensor(0.0363)\n",
      "12802 Training Loss: tensor(0.0384)\n",
      "12803 Training Loss: tensor(0.0392)\n",
      "12804 Training Loss: tensor(0.0369)\n",
      "12805 Training Loss: tensor(0.0416)\n",
      "12806 Training Loss: tensor(0.0428)\n",
      "12807 Training Loss: tensor(0.0342)\n",
      "12808 Training Loss: tensor(0.0424)\n",
      "12809 Training Loss: tensor(0.0416)\n",
      "12810 Training Loss: tensor(0.0444)\n",
      "12811 Training Loss: tensor(0.0379)\n",
      "12812 Training Loss: tensor(0.0424)\n",
      "12813 Training Loss: tensor(0.0379)\n",
      "12814 Training Loss: tensor(0.0436)\n",
      "12815 Training Loss: tensor(0.0428)\n",
      "12816 Training Loss: tensor(0.0404)\n",
      "12817 Training Loss: tensor(0.0403)\n",
      "12818 Training Loss: tensor(0.0480)\n",
      "12819 Training Loss: tensor(0.0416)\n",
      "12820 Training Loss: tensor(0.0392)\n",
      "12821 Training Loss: tensor(0.0386)\n",
      "12822 Training Loss: tensor(0.0392)\n",
      "12823 Training Loss: tensor(0.0378)\n",
      "12824 Training Loss: tensor(0.0365)\n",
      "12825 Training Loss: tensor(0.0433)\n",
      "12826 Training Loss: tensor(0.0359)\n",
      "12827 Training Loss: tensor(0.0393)\n",
      "12828 Training Loss: tensor(0.0441)\n",
      "12829 Training Loss: tensor(0.0414)\n",
      "12830 Training Loss: tensor(0.0383)\n",
      "12831 Training Loss: tensor(0.0446)\n",
      "12832 Training Loss: tensor(0.0398)\n",
      "12833 Training Loss: tensor(0.0360)\n",
      "12834 Training Loss: tensor(0.0364)\n",
      "12835 Training Loss: tensor(0.0413)\n",
      "12836 Training Loss: tensor(0.0465)\n",
      "12837 Training Loss: tensor(0.0430)\n",
      "12838 Training Loss: tensor(0.0393)\n",
      "12839 Training Loss: tensor(0.0379)\n",
      "12840 Training Loss: tensor(0.0420)\n",
      "12841 Training Loss: tensor(0.0372)\n",
      "12842 Training Loss: tensor(0.0420)\n",
      "12843 Training Loss: tensor(0.0398)\n",
      "12844 Training Loss: tensor(0.0474)\n",
      "12845 Training Loss: tensor(0.0461)\n",
      "12846 Training Loss: tensor(0.0417)\n",
      "12847 Training Loss: tensor(0.0427)\n",
      "12848 Training Loss: tensor(0.0436)\n",
      "12849 Training Loss: tensor(0.0423)\n",
      "12850 Training Loss: tensor(0.0364)\n",
      "12851 Training Loss: tensor(0.0452)\n",
      "12852 Training Loss: tensor(0.0428)\n",
      "12853 Training Loss: tensor(0.0373)\n",
      "12854 Training Loss: tensor(0.0403)\n",
      "12855 Training Loss: tensor(0.0369)\n",
      "12856 Training Loss: tensor(0.0374)\n",
      "12857 Training Loss: tensor(0.0366)\n",
      "12858 Training Loss: tensor(0.0374)\n",
      "12859 Training Loss: tensor(0.0384)\n",
      "12860 Training Loss: tensor(0.0363)\n",
      "12861 Training Loss: tensor(0.0387)\n",
      "12862 Training Loss: tensor(0.0382)\n",
      "12863 Training Loss: tensor(0.0404)\n",
      "12864 Training Loss: tensor(0.0458)\n",
      "12865 Training Loss: tensor(0.0393)\n",
      "12866 Training Loss: tensor(0.0390)\n",
      "12867 Training Loss: tensor(0.0433)\n",
      "12868 Training Loss: tensor(0.0395)\n",
      "12869 Training Loss: tensor(0.0400)\n",
      "12870 Training Loss: tensor(0.0352)\n",
      "12871 Training Loss: tensor(0.0365)\n",
      "12872 Training Loss: tensor(0.0353)\n",
      "12873 Training Loss: tensor(0.0501)\n",
      "12874 Training Loss: tensor(0.0395)\n",
      "12875 Training Loss: tensor(0.0372)\n",
      "12876 Training Loss: tensor(0.0416)\n",
      "12877 Training Loss: tensor(0.0382)\n",
      "12878 Training Loss: tensor(0.0392)\n",
      "12879 Training Loss: tensor(0.0468)\n",
      "12880 Training Loss: tensor(0.0407)\n",
      "12881 Training Loss: tensor(0.0472)\n",
      "12882 Training Loss: tensor(0.0388)\n",
      "12883 Training Loss: tensor(0.0423)\n",
      "12884 Training Loss: tensor(0.0435)\n",
      "12885 Training Loss: tensor(0.0489)\n",
      "12886 Training Loss: tensor(0.0378)\n",
      "12887 Training Loss: tensor(0.0337)\n",
      "12888 Training Loss: tensor(0.0400)\n",
      "12889 Training Loss: tensor(0.0413)\n",
      "12890 Training Loss: tensor(0.0400)\n",
      "12891 Training Loss: tensor(0.0477)\n",
      "12892 Training Loss: tensor(0.0380)\n",
      "12893 Training Loss: tensor(0.0404)\n",
      "12894 Training Loss: tensor(0.0477)\n",
      "12895 Training Loss: tensor(0.0398)\n",
      "12896 Training Loss: tensor(0.0520)\n",
      "12897 Training Loss: tensor(0.0420)\n",
      "12898 Training Loss: tensor(0.0474)\n",
      "12899 Training Loss: tensor(0.0399)\n",
      "12900 Training Loss: tensor(0.0418)\n",
      "12901 Training Loss: tensor(0.0407)\n",
      "12902 Training Loss: tensor(0.0391)\n",
      "12903 Training Loss: tensor(0.0415)\n",
      "12904 Training Loss: tensor(0.0377)\n",
      "12905 Training Loss: tensor(0.0507)\n",
      "12906 Training Loss: tensor(0.0396)\n",
      "12907 Training Loss: tensor(0.0377)\n",
      "12908 Training Loss: tensor(0.0389)\n",
      "12909 Training Loss: tensor(0.0452)\n",
      "12910 Training Loss: tensor(0.0370)\n",
      "12911 Training Loss: tensor(0.0493)\n",
      "12912 Training Loss: tensor(0.0358)\n",
      "12913 Training Loss: tensor(0.0396)\n",
      "12914 Training Loss: tensor(0.0445)\n",
      "12915 Training Loss: tensor(0.0423)\n",
      "12916 Training Loss: tensor(0.0422)\n",
      "12917 Training Loss: tensor(0.0425)\n",
      "12918 Training Loss: tensor(0.0423)\n",
      "12919 Training Loss: tensor(0.0398)\n",
      "12920 Training Loss: tensor(0.0474)\n",
      "12921 Training Loss: tensor(0.0418)\n",
      "12922 Training Loss: tensor(0.0404)\n",
      "12923 Training Loss: tensor(0.0351)\n",
      "12924 Training Loss: tensor(0.0354)\n",
      "12925 Training Loss: tensor(0.0413)\n",
      "12926 Training Loss: tensor(0.0411)\n",
      "12927 Training Loss: tensor(0.0368)\n",
      "12928 Training Loss: tensor(0.0412)\n",
      "12929 Training Loss: tensor(0.0412)\n",
      "12930 Training Loss: tensor(0.0427)\n",
      "12931 Training Loss: tensor(0.0399)\n",
      "12932 Training Loss: tensor(0.0444)\n",
      "12933 Training Loss: tensor(0.0380)\n",
      "12934 Training Loss: tensor(0.0384)\n",
      "12935 Training Loss: tensor(0.0436)\n",
      "12936 Training Loss: tensor(0.0400)\n",
      "12937 Training Loss: tensor(0.0391)\n",
      "12938 Training Loss: tensor(0.0400)\n",
      "12939 Training Loss: tensor(0.0493)\n",
      "12940 Training Loss: tensor(0.0382)\n",
      "12941 Training Loss: tensor(0.0381)\n",
      "12942 Training Loss: tensor(0.0399)\n",
      "12943 Training Loss: tensor(0.0395)\n",
      "12944 Training Loss: tensor(0.0377)\n",
      "12945 Training Loss: tensor(0.0376)\n",
      "12946 Training Loss: tensor(0.0419)\n",
      "12947 Training Loss: tensor(0.0398)\n",
      "12948 Training Loss: tensor(0.0381)\n",
      "12949 Training Loss: tensor(0.0392)\n",
      "12950 Training Loss: tensor(0.0414)\n",
      "12951 Training Loss: tensor(0.0391)\n",
      "12952 Training Loss: tensor(0.0420)\n",
      "12953 Training Loss: tensor(0.0394)\n",
      "12954 Training Loss: tensor(0.0417)\n",
      "12955 Training Loss: tensor(0.0393)\n",
      "12956 Training Loss: tensor(0.0399)\n",
      "12957 Training Loss: tensor(0.0456)\n",
      "12958 Training Loss: tensor(0.0373)\n",
      "12959 Training Loss: tensor(0.0409)\n",
      "12960 Training Loss: tensor(0.0440)\n",
      "12961 Training Loss: tensor(0.0388)\n",
      "12962 Training Loss: tensor(0.0424)\n",
      "12963 Training Loss: tensor(0.0432)\n",
      "12964 Training Loss: tensor(0.0413)\n",
      "12965 Training Loss: tensor(0.0419)\n",
      "12966 Training Loss: tensor(0.0418)\n",
      "12967 Training Loss: tensor(0.0424)\n",
      "12968 Training Loss: tensor(0.0424)\n",
      "12969 Training Loss: tensor(0.0431)\n",
      "12970 Training Loss: tensor(0.0427)\n",
      "12971 Training Loss: tensor(0.0402)\n",
      "12972 Training Loss: tensor(0.0398)\n",
      "12973 Training Loss: tensor(0.0468)\n",
      "12974 Training Loss: tensor(0.0393)\n",
      "12975 Training Loss: tensor(0.0401)\n",
      "12976 Training Loss: tensor(0.0361)\n",
      "12977 Training Loss: tensor(0.0425)\n",
      "12978 Training Loss: tensor(0.0389)\n",
      "12979 Training Loss: tensor(0.0360)\n",
      "12980 Training Loss: tensor(0.0441)\n",
      "12981 Training Loss: tensor(0.0427)\n",
      "12982 Training Loss: tensor(0.0394)\n",
      "12983 Training Loss: tensor(0.0433)\n",
      "12984 Training Loss: tensor(0.0483)\n",
      "12985 Training Loss: tensor(0.0357)\n",
      "12986 Training Loss: tensor(0.0426)\n",
      "12987 Training Loss: tensor(0.0425)\n",
      "12988 Training Loss: tensor(0.0443)\n",
      "12989 Training Loss: tensor(0.0425)\n",
      "12990 Training Loss: tensor(0.0427)\n",
      "12991 Training Loss: tensor(0.0357)\n",
      "12992 Training Loss: tensor(0.0423)\n",
      "12993 Training Loss: tensor(0.0461)\n",
      "12994 Training Loss: tensor(0.0427)\n",
      "12995 Training Loss: tensor(0.0360)\n",
      "12996 Training Loss: tensor(0.0384)\n",
      "12997 Training Loss: tensor(0.0410)\n",
      "12998 Training Loss: tensor(0.0381)\n",
      "12999 Training Loss: tensor(0.0369)\n",
      "13000 Training Loss: tensor(0.0361)\n",
      "13001 Training Loss: tensor(0.0418)\n",
      "13002 Training Loss: tensor(0.0415)\n",
      "13003 Training Loss: tensor(0.0436)\n",
      "13004 Training Loss: tensor(0.0406)\n",
      "13005 Training Loss: tensor(0.0458)\n",
      "13006 Training Loss: tensor(0.0406)\n",
      "13007 Training Loss: tensor(0.0426)\n",
      "13008 Training Loss: tensor(0.0488)\n",
      "13009 Training Loss: tensor(0.0345)\n",
      "13010 Training Loss: tensor(0.0426)\n",
      "13011 Training Loss: tensor(0.0423)\n",
      "13012 Training Loss: tensor(0.0389)\n",
      "13013 Training Loss: tensor(0.0407)\n",
      "13014 Training Loss: tensor(0.0378)\n",
      "13015 Training Loss: tensor(0.0424)\n",
      "13016 Training Loss: tensor(0.0429)\n",
      "13017 Training Loss: tensor(0.0424)\n",
      "13018 Training Loss: tensor(0.0404)\n",
      "13019 Training Loss: tensor(0.0458)\n",
      "13020 Training Loss: tensor(0.0381)\n",
      "13021 Training Loss: tensor(0.0386)\n",
      "13022 Training Loss: tensor(0.0413)\n",
      "13023 Training Loss: tensor(0.0381)\n",
      "13024 Training Loss: tensor(0.0389)\n",
      "13025 Training Loss: tensor(0.0369)\n",
      "13026 Training Loss: tensor(0.0360)\n",
      "13027 Training Loss: tensor(0.0452)\n",
      "13028 Training Loss: tensor(0.0413)\n",
      "13029 Training Loss: tensor(0.0395)\n",
      "13030 Training Loss: tensor(0.0411)\n",
      "13031 Training Loss: tensor(0.0423)\n",
      "13032 Training Loss: tensor(0.0396)\n",
      "13033 Training Loss: tensor(0.0432)\n",
      "13034 Training Loss: tensor(0.0396)\n",
      "13035 Training Loss: tensor(0.0364)\n",
      "13036 Training Loss: tensor(0.0417)\n",
      "13037 Training Loss: tensor(0.0403)\n",
      "13038 Training Loss: tensor(0.0432)\n",
      "13039 Training Loss: tensor(0.0394)\n",
      "13040 Training Loss: tensor(0.0377)\n",
      "13041 Training Loss: tensor(0.0446)\n",
      "13042 Training Loss: tensor(0.0415)\n",
      "13043 Training Loss: tensor(0.0407)\n",
      "13044 Training Loss: tensor(0.0324)\n",
      "13045 Training Loss: tensor(0.0413)\n",
      "13046 Training Loss: tensor(0.0393)\n",
      "13047 Training Loss: tensor(0.0451)\n",
      "13048 Training Loss: tensor(0.0401)\n",
      "13049 Training Loss: tensor(0.0402)\n",
      "13050 Training Loss: tensor(0.0393)\n",
      "13051 Training Loss: tensor(0.0429)\n",
      "13052 Training Loss: tensor(0.0455)\n",
      "13053 Training Loss: tensor(0.0410)\n",
      "13054 Training Loss: tensor(0.0391)\n",
      "13055 Training Loss: tensor(0.0462)\n",
      "13056 Training Loss: tensor(0.0427)\n",
      "13057 Training Loss: tensor(0.0415)\n",
      "13058 Training Loss: tensor(0.0388)\n",
      "13059 Training Loss: tensor(0.0462)\n",
      "13060 Training Loss: tensor(0.0409)\n",
      "13061 Training Loss: tensor(0.0347)\n",
      "13062 Training Loss: tensor(0.0423)\n",
      "13063 Training Loss: tensor(0.0430)\n",
      "13064 Training Loss: tensor(0.0391)\n",
      "13065 Training Loss: tensor(0.0406)\n",
      "13066 Training Loss: tensor(0.0409)\n",
      "13067 Training Loss: tensor(0.0377)\n",
      "13068 Training Loss: tensor(0.0437)\n",
      "13069 Training Loss: tensor(0.0365)\n",
      "13070 Training Loss: tensor(0.0427)\n",
      "13071 Training Loss: tensor(0.0418)\n",
      "13072 Training Loss: tensor(0.0386)\n",
      "13073 Training Loss: tensor(0.0384)\n",
      "13074 Training Loss: tensor(0.0396)\n",
      "13075 Training Loss: tensor(0.0427)\n",
      "13076 Training Loss: tensor(0.0429)\n",
      "13077 Training Loss: tensor(0.0442)\n",
      "13078 Training Loss: tensor(0.0369)\n",
      "13079 Training Loss: tensor(0.0397)\n",
      "13080 Training Loss: tensor(0.0368)\n",
      "13081 Training Loss: tensor(0.0391)\n",
      "13082 Training Loss: tensor(0.0391)\n",
      "13083 Training Loss: tensor(0.0408)\n",
      "13084 Training Loss: tensor(0.0402)\n",
      "13085 Training Loss: tensor(0.0363)\n",
      "13086 Training Loss: tensor(0.0474)\n",
      "13087 Training Loss: tensor(0.0378)\n",
      "13088 Training Loss: tensor(0.0369)\n",
      "13089 Training Loss: tensor(0.0357)\n",
      "13090 Training Loss: tensor(0.0410)\n",
      "13091 Training Loss: tensor(0.0417)\n",
      "13092 Training Loss: tensor(0.0443)\n",
      "13093 Training Loss: tensor(0.0416)\n",
      "13094 Training Loss: tensor(0.0484)\n",
      "13095 Training Loss: tensor(0.0400)\n",
      "13096 Training Loss: tensor(0.0352)\n",
      "13097 Training Loss: tensor(0.0386)\n",
      "13098 Training Loss: tensor(0.0405)\n",
      "13099 Training Loss: tensor(0.0452)\n",
      "13100 Training Loss: tensor(0.0412)\n",
      "13101 Training Loss: tensor(0.0447)\n",
      "13102 Training Loss: tensor(0.0361)\n",
      "13103 Training Loss: tensor(0.0393)\n",
      "13104 Training Loss: tensor(0.0361)\n",
      "13105 Training Loss: tensor(0.0374)\n",
      "13106 Training Loss: tensor(0.0401)\n",
      "13107 Training Loss: tensor(0.0418)\n",
      "13108 Training Loss: tensor(0.0404)\n",
      "13109 Training Loss: tensor(0.0398)\n",
      "13110 Training Loss: tensor(0.0410)\n",
      "13111 Training Loss: tensor(0.0365)\n",
      "13112 Training Loss: tensor(0.0425)\n",
      "13113 Training Loss: tensor(0.0384)\n",
      "13114 Training Loss: tensor(0.0391)\n",
      "13115 Training Loss: tensor(0.0389)\n",
      "13116 Training Loss: tensor(0.0373)\n",
      "13117 Training Loss: tensor(0.0405)\n",
      "13118 Training Loss: tensor(0.0443)\n",
      "13119 Training Loss: tensor(0.0419)\n",
      "13120 Training Loss: tensor(0.0438)\n",
      "13121 Training Loss: tensor(0.0342)\n",
      "13122 Training Loss: tensor(0.0425)\n",
      "13123 Training Loss: tensor(0.0381)\n",
      "13124 Training Loss: tensor(0.0389)\n",
      "13125 Training Loss: tensor(0.0369)\n",
      "13126 Training Loss: tensor(0.0468)\n",
      "13127 Training Loss: tensor(0.0379)\n",
      "13128 Training Loss: tensor(0.0460)\n",
      "13129 Training Loss: tensor(0.0371)\n",
      "13130 Training Loss: tensor(0.0404)\n",
      "13131 Training Loss: tensor(0.0443)\n",
      "13132 Training Loss: tensor(0.0430)\n",
      "13133 Training Loss: tensor(0.0389)\n",
      "13134 Training Loss: tensor(0.0353)\n",
      "13135 Training Loss: tensor(0.0411)\n",
      "13136 Training Loss: tensor(0.0404)\n",
      "13137 Training Loss: tensor(0.0423)\n",
      "13138 Training Loss: tensor(0.0418)\n",
      "13139 Training Loss: tensor(0.0432)\n",
      "13140 Training Loss: tensor(0.0347)\n",
      "13141 Training Loss: tensor(0.0409)\n",
      "13142 Training Loss: tensor(0.0397)\n",
      "13143 Training Loss: tensor(0.0436)\n",
      "13144 Training Loss: tensor(0.0432)\n",
      "13145 Training Loss: tensor(0.0395)\n",
      "13146 Training Loss: tensor(0.0387)\n",
      "13147 Training Loss: tensor(0.0463)\n",
      "13148 Training Loss: tensor(0.0350)\n",
      "13149 Training Loss: tensor(0.0439)\n",
      "13150 Training Loss: tensor(0.0374)\n",
      "13151 Training Loss: tensor(0.0398)\n",
      "13152 Training Loss: tensor(0.0351)\n",
      "13153 Training Loss: tensor(0.0392)\n",
      "13154 Training Loss: tensor(0.0400)\n",
      "13155 Training Loss: tensor(0.0411)\n",
      "13156 Training Loss: tensor(0.0341)\n",
      "13157 Training Loss: tensor(0.0395)\n",
      "13158 Training Loss: tensor(0.0382)\n",
      "13159 Training Loss: tensor(0.0368)\n",
      "13160 Training Loss: tensor(0.0397)\n",
      "13161 Training Loss: tensor(0.0416)\n",
      "13162 Training Loss: tensor(0.0421)\n",
      "13163 Training Loss: tensor(0.0419)\n",
      "13164 Training Loss: tensor(0.0432)\n",
      "13165 Training Loss: tensor(0.0462)\n",
      "13166 Training Loss: tensor(0.0416)\n",
      "13167 Training Loss: tensor(0.0411)\n",
      "13168 Training Loss: tensor(0.0386)\n",
      "13169 Training Loss: tensor(0.0416)\n",
      "13170 Training Loss: tensor(0.0410)\n",
      "13171 Training Loss: tensor(0.0415)\n",
      "13172 Training Loss: tensor(0.0452)\n",
      "13173 Training Loss: tensor(0.0417)\n",
      "13174 Training Loss: tensor(0.0406)\n",
      "13175 Training Loss: tensor(0.0416)\n",
      "13176 Training Loss: tensor(0.0448)\n",
      "13177 Training Loss: tensor(0.0376)\n",
      "13178 Training Loss: tensor(0.0385)\n",
      "13179 Training Loss: tensor(0.0368)\n",
      "13180 Training Loss: tensor(0.0421)\n",
      "13181 Training Loss: tensor(0.0367)\n",
      "13182 Training Loss: tensor(0.0369)\n",
      "13183 Training Loss: tensor(0.0385)\n",
      "13184 Training Loss: tensor(0.0427)\n",
      "13185 Training Loss: tensor(0.0375)\n",
      "13186 Training Loss: tensor(0.0413)\n",
      "13187 Training Loss: tensor(0.0419)\n",
      "13188 Training Loss: tensor(0.0354)\n",
      "13189 Training Loss: tensor(0.0380)\n",
      "13190 Training Loss: tensor(0.0401)\n",
      "13191 Training Loss: tensor(0.0331)\n",
      "13192 Training Loss: tensor(0.0444)\n",
      "13193 Training Loss: tensor(0.0354)\n",
      "13194 Training Loss: tensor(0.0416)\n",
      "13195 Training Loss: tensor(0.0364)\n",
      "13196 Training Loss: tensor(0.0461)\n",
      "13197 Training Loss: tensor(0.0440)\n",
      "13198 Training Loss: tensor(0.0413)\n",
      "13199 Training Loss: tensor(0.0464)\n",
      "13200 Training Loss: tensor(0.0455)\n",
      "13201 Training Loss: tensor(0.0431)\n",
      "13202 Training Loss: tensor(0.0432)\n",
      "13203 Training Loss: tensor(0.0415)\n",
      "13204 Training Loss: tensor(0.0454)\n",
      "13205 Training Loss: tensor(0.0381)\n",
      "13206 Training Loss: tensor(0.0397)\n",
      "13207 Training Loss: tensor(0.0426)\n",
      "13208 Training Loss: tensor(0.0402)\n",
      "13209 Training Loss: tensor(0.0410)\n",
      "13210 Training Loss: tensor(0.0420)\n",
      "13211 Training Loss: tensor(0.0430)\n",
      "13212 Training Loss: tensor(0.0394)\n",
      "13213 Training Loss: tensor(0.0408)\n",
      "13214 Training Loss: tensor(0.0393)\n",
      "13215 Training Loss: tensor(0.0396)\n",
      "13216 Training Loss: tensor(0.0345)\n",
      "13217 Training Loss: tensor(0.0361)\n",
      "13218 Training Loss: tensor(0.0455)\n",
      "13219 Training Loss: tensor(0.0403)\n",
      "13220 Training Loss: tensor(0.0349)\n",
      "13221 Training Loss: tensor(0.0446)\n",
      "13222 Training Loss: tensor(0.0373)\n",
      "13223 Training Loss: tensor(0.0421)\n",
      "13224 Training Loss: tensor(0.0467)\n",
      "13225 Training Loss: tensor(0.0438)\n",
      "13226 Training Loss: tensor(0.0389)\n",
      "13227 Training Loss: tensor(0.0416)\n",
      "13228 Training Loss: tensor(0.0429)\n",
      "13229 Training Loss: tensor(0.0379)\n",
      "13230 Training Loss: tensor(0.0434)\n",
      "13231 Training Loss: tensor(0.0419)\n",
      "13232 Training Loss: tensor(0.0425)\n",
      "13233 Training Loss: tensor(0.0441)\n",
      "13234 Training Loss: tensor(0.0377)\n",
      "13235 Training Loss: tensor(0.0426)\n",
      "13236 Training Loss: tensor(0.0408)\n",
      "13237 Training Loss: tensor(0.0437)\n",
      "13238 Training Loss: tensor(0.0459)\n",
      "13239 Training Loss: tensor(0.0409)\n",
      "13240 Training Loss: tensor(0.0352)\n",
      "13241 Training Loss: tensor(0.0436)\n",
      "13242 Training Loss: tensor(0.0417)\n",
      "13243 Training Loss: tensor(0.0439)\n",
      "13244 Training Loss: tensor(0.0443)\n",
      "13245 Training Loss: tensor(0.0365)\n",
      "13246 Training Loss: tensor(0.0466)\n",
      "13247 Training Loss: tensor(0.0466)\n",
      "13248 Training Loss: tensor(0.0437)\n",
      "13249 Training Loss: tensor(0.0453)\n",
      "13250 Training Loss: tensor(0.0399)\n",
      "13251 Training Loss: tensor(0.0420)\n",
      "13252 Training Loss: tensor(0.0386)\n",
      "13253 Training Loss: tensor(0.0407)\n",
      "13254 Training Loss: tensor(0.0373)\n",
      "13255 Training Loss: tensor(0.0398)\n",
      "13256 Training Loss: tensor(0.0420)\n",
      "13257 Training Loss: tensor(0.0429)\n",
      "13258 Training Loss: tensor(0.0412)\n",
      "13259 Training Loss: tensor(0.0459)\n",
      "13260 Training Loss: tensor(0.0398)\n",
      "13261 Training Loss: tensor(0.0419)\n",
      "13262 Training Loss: tensor(0.0395)\n",
      "13263 Training Loss: tensor(0.0387)\n",
      "13264 Training Loss: tensor(0.0365)\n",
      "13265 Training Loss: tensor(0.0432)\n",
      "13266 Training Loss: tensor(0.0418)\n",
      "13267 Training Loss: tensor(0.0376)\n",
      "13268 Training Loss: tensor(0.0359)\n",
      "13269 Training Loss: tensor(0.0380)\n",
      "13270 Training Loss: tensor(0.0369)\n",
      "13271 Training Loss: tensor(0.0387)\n",
      "13272 Training Loss: tensor(0.0425)\n",
      "13273 Training Loss: tensor(0.0384)\n",
      "13274 Training Loss: tensor(0.0386)\n",
      "13275 Training Loss: tensor(0.0421)\n",
      "13276 Training Loss: tensor(0.0414)\n",
      "13277 Training Loss: tensor(0.0372)\n",
      "13278 Training Loss: tensor(0.0393)\n",
      "13279 Training Loss: tensor(0.0406)\n",
      "13280 Training Loss: tensor(0.0413)\n",
      "13281 Training Loss: tensor(0.0423)\n",
      "13282 Training Loss: tensor(0.0410)\n",
      "13283 Training Loss: tensor(0.0422)\n",
      "13284 Training Loss: tensor(0.0394)\n",
      "13285 Training Loss: tensor(0.0457)\n",
      "13286 Training Loss: tensor(0.0394)\n",
      "13287 Training Loss: tensor(0.0378)\n",
      "13288 Training Loss: tensor(0.0332)\n",
      "13289 Training Loss: tensor(0.0409)\n",
      "13290 Training Loss: tensor(0.0382)\n",
      "13291 Training Loss: tensor(0.0371)\n",
      "13292 Training Loss: tensor(0.0503)\n",
      "13293 Training Loss: tensor(0.0424)\n",
      "13294 Training Loss: tensor(0.0442)\n",
      "13295 Training Loss: tensor(0.0444)\n",
      "13296 Training Loss: tensor(0.0420)\n",
      "13297 Training Loss: tensor(0.0381)\n",
      "13298 Training Loss: tensor(0.0444)\n",
      "13299 Training Loss: tensor(0.0378)\n",
      "13300 Training Loss: tensor(0.0365)\n",
      "13301 Training Loss: tensor(0.0398)\n",
      "13302 Training Loss: tensor(0.0376)\n",
      "13303 Training Loss: tensor(0.0418)\n",
      "13304 Training Loss: tensor(0.0357)\n",
      "13305 Training Loss: tensor(0.0409)\n",
      "13306 Training Loss: tensor(0.0441)\n",
      "13307 Training Loss: tensor(0.0348)\n",
      "13308 Training Loss: tensor(0.0391)\n",
      "13309 Training Loss: tensor(0.0350)\n",
      "13310 Training Loss: tensor(0.0466)\n",
      "13311 Training Loss: tensor(0.0393)\n",
      "13312 Training Loss: tensor(0.0372)\n",
      "13313 Training Loss: tensor(0.0485)\n",
      "13314 Training Loss: tensor(0.0356)\n",
      "13315 Training Loss: tensor(0.0418)\n",
      "13316 Training Loss: tensor(0.0431)\n",
      "13317 Training Loss: tensor(0.0397)\n",
      "13318 Training Loss: tensor(0.0368)\n",
      "13319 Training Loss: tensor(0.0388)\n",
      "13320 Training Loss: tensor(0.0392)\n",
      "13321 Training Loss: tensor(0.0348)\n",
      "13322 Training Loss: tensor(0.0421)\n",
      "13323 Training Loss: tensor(0.0387)\n",
      "13324 Training Loss: tensor(0.0352)\n",
      "13325 Training Loss: tensor(0.0411)\n",
      "13326 Training Loss: tensor(0.0406)\n",
      "13327 Training Loss: tensor(0.0452)\n",
      "13328 Training Loss: tensor(0.0394)\n",
      "13329 Training Loss: tensor(0.0361)\n",
      "13330 Training Loss: tensor(0.0381)\n",
      "13331 Training Loss: tensor(0.0400)\n",
      "13332 Training Loss: tensor(0.0410)\n",
      "13333 Training Loss: tensor(0.0384)\n",
      "13334 Training Loss: tensor(0.0402)\n",
      "13335 Training Loss: tensor(0.0390)\n",
      "13336 Training Loss: tensor(0.0386)\n",
      "13337 Training Loss: tensor(0.0420)\n",
      "13338 Training Loss: tensor(0.0353)\n",
      "13339 Training Loss: tensor(0.0424)\n",
      "13340 Training Loss: tensor(0.0405)\n",
      "13341 Training Loss: tensor(0.0456)\n",
      "13342 Training Loss: tensor(0.0384)\n",
      "13343 Training Loss: tensor(0.0381)\n",
      "13344 Training Loss: tensor(0.0428)\n",
      "13345 Training Loss: tensor(0.0422)\n",
      "13346 Training Loss: tensor(0.0395)\n",
      "13347 Training Loss: tensor(0.0407)\n",
      "13348 Training Loss: tensor(0.0365)\n",
      "13349 Training Loss: tensor(0.0394)\n",
      "13350 Training Loss: tensor(0.0404)\n",
      "13351 Training Loss: tensor(0.0399)\n",
      "13352 Training Loss: tensor(0.0399)\n",
      "13353 Training Loss: tensor(0.0418)\n",
      "13354 Training Loss: tensor(0.0379)\n",
      "13355 Training Loss: tensor(0.0415)\n",
      "13356 Training Loss: tensor(0.0428)\n",
      "13357 Training Loss: tensor(0.0380)\n",
      "13358 Training Loss: tensor(0.0408)\n",
      "13359 Training Loss: tensor(0.0380)\n",
      "13360 Training Loss: tensor(0.0334)\n",
      "13361 Training Loss: tensor(0.0425)\n",
      "13362 Training Loss: tensor(0.0409)\n",
      "13363 Training Loss: tensor(0.0326)\n",
      "13364 Training Loss: tensor(0.0383)\n",
      "13365 Training Loss: tensor(0.0446)\n",
      "13366 Training Loss: tensor(0.0424)\n",
      "13367 Training Loss: tensor(0.0420)\n",
      "13368 Training Loss: tensor(0.0457)\n",
      "13369 Training Loss: tensor(0.0350)\n",
      "13370 Training Loss: tensor(0.0373)\n",
      "13371 Training Loss: tensor(0.0364)\n",
      "13372 Training Loss: tensor(0.0400)\n",
      "13373 Training Loss: tensor(0.0456)\n",
      "13374 Training Loss: tensor(0.0398)\n",
      "13375 Training Loss: tensor(0.0405)\n",
      "13376 Training Loss: tensor(0.0382)\n",
      "13377 Training Loss: tensor(0.0415)\n",
      "13378 Training Loss: tensor(0.0434)\n",
      "13379 Training Loss: tensor(0.0440)\n",
      "13380 Training Loss: tensor(0.0433)\n",
      "13381 Training Loss: tensor(0.0401)\n",
      "13382 Training Loss: tensor(0.0400)\n",
      "13383 Training Loss: tensor(0.0364)\n",
      "13384 Training Loss: tensor(0.0428)\n",
      "13385 Training Loss: tensor(0.0425)\n",
      "13386 Training Loss: tensor(0.0437)\n",
      "13387 Training Loss: tensor(0.0437)\n",
      "13388 Training Loss: tensor(0.0396)\n",
      "13389 Training Loss: tensor(0.0393)\n",
      "13390 Training Loss: tensor(0.0441)\n",
      "13391 Training Loss: tensor(0.0453)\n",
      "13392 Training Loss: tensor(0.0349)\n",
      "13393 Training Loss: tensor(0.0402)\n",
      "13394 Training Loss: tensor(0.0392)\n",
      "13395 Training Loss: tensor(0.0397)\n",
      "13396 Training Loss: tensor(0.0410)\n",
      "13397 Training Loss: tensor(0.0396)\n",
      "13398 Training Loss: tensor(0.0381)\n",
      "13399 Training Loss: tensor(0.0380)\n",
      "13400 Training Loss: tensor(0.0381)\n",
      "13401 Training Loss: tensor(0.0412)\n",
      "13402 Training Loss: tensor(0.0400)\n",
      "13403 Training Loss: tensor(0.0383)\n",
      "13404 Training Loss: tensor(0.0379)\n",
      "13405 Training Loss: tensor(0.0389)\n",
      "13406 Training Loss: tensor(0.0409)\n",
      "13407 Training Loss: tensor(0.0388)\n",
      "13408 Training Loss: tensor(0.0379)\n",
      "13409 Training Loss: tensor(0.0432)\n",
      "13410 Training Loss: tensor(0.0426)\n",
      "13411 Training Loss: tensor(0.0407)\n",
      "13412 Training Loss: tensor(0.0403)\n",
      "13413 Training Loss: tensor(0.0389)\n",
      "13414 Training Loss: tensor(0.0373)\n",
      "13415 Training Loss: tensor(0.0407)\n",
      "13416 Training Loss: tensor(0.0411)\n",
      "13417 Training Loss: tensor(0.0445)\n",
      "13418 Training Loss: tensor(0.0402)\n",
      "13419 Training Loss: tensor(0.0413)\n",
      "13420 Training Loss: tensor(0.0430)\n",
      "13421 Training Loss: tensor(0.0395)\n",
      "13422 Training Loss: tensor(0.0409)\n",
      "13423 Training Loss: tensor(0.0405)\n",
      "13424 Training Loss: tensor(0.0396)\n",
      "13425 Training Loss: tensor(0.0400)\n",
      "13426 Training Loss: tensor(0.0402)\n",
      "13427 Training Loss: tensor(0.0427)\n",
      "13428 Training Loss: tensor(0.0374)\n",
      "13429 Training Loss: tensor(0.0433)\n",
      "13430 Training Loss: tensor(0.0349)\n",
      "13431 Training Loss: tensor(0.0393)\n",
      "13432 Training Loss: tensor(0.0401)\n",
      "13433 Training Loss: tensor(0.0413)\n",
      "13434 Training Loss: tensor(0.0443)\n",
      "13435 Training Loss: tensor(0.0377)\n",
      "13436 Training Loss: tensor(0.0425)\n",
      "13437 Training Loss: tensor(0.0413)\n",
      "13438 Training Loss: tensor(0.0386)\n",
      "13439 Training Loss: tensor(0.0382)\n",
      "13440 Training Loss: tensor(0.0429)\n",
      "13441 Training Loss: tensor(0.0416)\n",
      "13442 Training Loss: tensor(0.0476)\n",
      "13443 Training Loss: tensor(0.0384)\n",
      "13444 Training Loss: tensor(0.0406)\n",
      "13445 Training Loss: tensor(0.0416)\n",
      "13446 Training Loss: tensor(0.0417)\n",
      "13447 Training Loss: tensor(0.0366)\n",
      "13448 Training Loss: tensor(0.0390)\n",
      "13449 Training Loss: tensor(0.0391)\n",
      "13450 Training Loss: tensor(0.0377)\n",
      "13451 Training Loss: tensor(0.0389)\n",
      "13452 Training Loss: tensor(0.0403)\n",
      "13453 Training Loss: tensor(0.0436)\n",
      "13454 Training Loss: tensor(0.0387)\n",
      "13455 Training Loss: tensor(0.0388)\n",
      "13456 Training Loss: tensor(0.0389)\n",
      "13457 Training Loss: tensor(0.0386)\n",
      "13458 Training Loss: tensor(0.0399)\n",
      "13459 Training Loss: tensor(0.0415)\n",
      "13460 Training Loss: tensor(0.0408)\n",
      "13461 Training Loss: tensor(0.0432)\n",
      "13462 Training Loss: tensor(0.0416)\n",
      "13463 Training Loss: tensor(0.0344)\n",
      "13464 Training Loss: tensor(0.0347)\n",
      "13465 Training Loss: tensor(0.0352)\n",
      "13466 Training Loss: tensor(0.0418)\n",
      "13467 Training Loss: tensor(0.0411)\n",
      "13468 Training Loss: tensor(0.0331)\n",
      "13469 Training Loss: tensor(0.0391)\n",
      "13470 Training Loss: tensor(0.0365)\n",
      "13471 Training Loss: tensor(0.0471)\n",
      "13472 Training Loss: tensor(0.0376)\n",
      "13473 Training Loss: tensor(0.0352)\n",
      "13474 Training Loss: tensor(0.0397)\n",
      "13475 Training Loss: tensor(0.0409)\n",
      "13476 Training Loss: tensor(0.0398)\n",
      "13477 Training Loss: tensor(0.0428)\n",
      "13478 Training Loss: tensor(0.0430)\n",
      "13479 Training Loss: tensor(0.0369)\n",
      "13480 Training Loss: tensor(0.0366)\n",
      "13481 Training Loss: tensor(0.0450)\n",
      "13482 Training Loss: tensor(0.0442)\n",
      "13483 Training Loss: tensor(0.0492)\n",
      "13484 Training Loss: tensor(0.0443)\n",
      "13485 Training Loss: tensor(0.0432)\n",
      "13486 Training Loss: tensor(0.0415)\n",
      "13487 Training Loss: tensor(0.0426)\n",
      "13488 Training Loss: tensor(0.0405)\n",
      "13489 Training Loss: tensor(0.0403)\n",
      "13490 Training Loss: tensor(0.0397)\n",
      "13491 Training Loss: tensor(0.0342)\n",
      "13492 Training Loss: tensor(0.0357)\n",
      "13493 Training Loss: tensor(0.0441)\n",
      "13494 Training Loss: tensor(0.0375)\n",
      "13495 Training Loss: tensor(0.0380)\n",
      "13496 Training Loss: tensor(0.0387)\n",
      "13497 Training Loss: tensor(0.0413)\n",
      "13498 Training Loss: tensor(0.0339)\n",
      "13499 Training Loss: tensor(0.0350)\n",
      "13500 Training Loss: tensor(0.0342)\n",
      "13501 Training Loss: tensor(0.0374)\n",
      "13502 Training Loss: tensor(0.0386)\n",
      "13503 Training Loss: tensor(0.0443)\n",
      "13504 Training Loss: tensor(0.0385)\n",
      "13505 Training Loss: tensor(0.0398)\n",
      "13506 Training Loss: tensor(0.0433)\n",
      "13507 Training Loss: tensor(0.0468)\n",
      "13508 Training Loss: tensor(0.0384)\n",
      "13509 Training Loss: tensor(0.0428)\n",
      "13510 Training Loss: tensor(0.0420)\n",
      "13511 Training Loss: tensor(0.0382)\n",
      "13512 Training Loss: tensor(0.0393)\n",
      "13513 Training Loss: tensor(0.0377)\n",
      "13514 Training Loss: tensor(0.0390)\n",
      "13515 Training Loss: tensor(0.0410)\n",
      "13516 Training Loss: tensor(0.0389)\n",
      "13517 Training Loss: tensor(0.0305)\n",
      "13518 Training Loss: tensor(0.0394)\n",
      "13519 Training Loss: tensor(0.0450)\n",
      "13520 Training Loss: tensor(0.0346)\n",
      "13521 Training Loss: tensor(0.0404)\n",
      "13522 Training Loss: tensor(0.0421)\n",
      "13523 Training Loss: tensor(0.0443)\n",
      "13524 Training Loss: tensor(0.0370)\n",
      "13525 Training Loss: tensor(0.0388)\n",
      "13526 Training Loss: tensor(0.0387)\n",
      "13527 Training Loss: tensor(0.0382)\n",
      "13528 Training Loss: tensor(0.0376)\n",
      "13529 Training Loss: tensor(0.0413)\n",
      "13530 Training Loss: tensor(0.0368)\n",
      "13531 Training Loss: tensor(0.0361)\n",
      "13532 Training Loss: tensor(0.0387)\n",
      "13533 Training Loss: tensor(0.0394)\n",
      "13534 Training Loss: tensor(0.0395)\n",
      "13535 Training Loss: tensor(0.0361)\n",
      "13536 Training Loss: tensor(0.0378)\n",
      "13537 Training Loss: tensor(0.0360)\n",
      "13538 Training Loss: tensor(0.0437)\n",
      "13539 Training Loss: tensor(0.0428)\n",
      "13540 Training Loss: tensor(0.0358)\n",
      "13541 Training Loss: tensor(0.0423)\n",
      "13542 Training Loss: tensor(0.0362)\n",
      "13543 Training Loss: tensor(0.0379)\n",
      "13544 Training Loss: tensor(0.0413)\n",
      "13545 Training Loss: tensor(0.0395)\n",
      "13546 Training Loss: tensor(0.0433)\n",
      "13547 Training Loss: tensor(0.0400)\n",
      "13548 Training Loss: tensor(0.0448)\n",
      "13549 Training Loss: tensor(0.0401)\n",
      "13550 Training Loss: tensor(0.0424)\n",
      "13551 Training Loss: tensor(0.0379)\n",
      "13552 Training Loss: tensor(0.0348)\n",
      "13553 Training Loss: tensor(0.0406)\n",
      "13554 Training Loss: tensor(0.0422)\n",
      "13555 Training Loss: tensor(0.0425)\n",
      "13556 Training Loss: tensor(0.0392)\n",
      "13557 Training Loss: tensor(0.0442)\n",
      "13558 Training Loss: tensor(0.0353)\n",
      "13559 Training Loss: tensor(0.0375)\n",
      "13560 Training Loss: tensor(0.0472)\n",
      "13561 Training Loss: tensor(0.0402)\n",
      "13562 Training Loss: tensor(0.0404)\n",
      "13563 Training Loss: tensor(0.0392)\n",
      "13564 Training Loss: tensor(0.0419)\n",
      "13565 Training Loss: tensor(0.0449)\n",
      "13566 Training Loss: tensor(0.0430)\n",
      "13567 Training Loss: tensor(0.0426)\n",
      "13568 Training Loss: tensor(0.0317)\n",
      "13569 Training Loss: tensor(0.0404)\n",
      "13570 Training Loss: tensor(0.0440)\n",
      "13571 Training Loss: tensor(0.0390)\n",
      "13572 Training Loss: tensor(0.0487)\n",
      "13573 Training Loss: tensor(0.0371)\n",
      "13574 Training Loss: tensor(0.0421)\n",
      "13575 Training Loss: tensor(0.0383)\n",
      "13576 Training Loss: tensor(0.0417)\n",
      "13577 Training Loss: tensor(0.0390)\n",
      "13578 Training Loss: tensor(0.0387)\n",
      "13579 Training Loss: tensor(0.0453)\n",
      "13580 Training Loss: tensor(0.0383)\n",
      "13581 Training Loss: tensor(0.0416)\n",
      "13582 Training Loss: tensor(0.0424)\n",
      "13583 Training Loss: tensor(0.0383)\n",
      "13584 Training Loss: tensor(0.0405)\n",
      "13585 Training Loss: tensor(0.0391)\n",
      "13586 Training Loss: tensor(0.0419)\n",
      "13587 Training Loss: tensor(0.0422)\n",
      "13588 Training Loss: tensor(0.0394)\n",
      "13589 Training Loss: tensor(0.0411)\n",
      "13590 Training Loss: tensor(0.0383)\n",
      "13591 Training Loss: tensor(0.0374)\n",
      "13592 Training Loss: tensor(0.0429)\n",
      "13593 Training Loss: tensor(0.0378)\n",
      "13594 Training Loss: tensor(0.0377)\n",
      "13595 Training Loss: tensor(0.0388)\n",
      "13596 Training Loss: tensor(0.0353)\n",
      "13597 Training Loss: tensor(0.0384)\n",
      "13598 Training Loss: tensor(0.0394)\n",
      "13599 Training Loss: tensor(0.0459)\n",
      "13600 Training Loss: tensor(0.0362)\n",
      "13601 Training Loss: tensor(0.0409)\n",
      "13602 Training Loss: tensor(0.0417)\n",
      "13603 Training Loss: tensor(0.0469)\n",
      "13604 Training Loss: tensor(0.0385)\n",
      "13605 Training Loss: tensor(0.0423)\n",
      "13606 Training Loss: tensor(0.0395)\n",
      "13607 Training Loss: tensor(0.0379)\n",
      "13608 Training Loss: tensor(0.0458)\n",
      "13609 Training Loss: tensor(0.0429)\n",
      "13610 Training Loss: tensor(0.0392)\n",
      "13611 Training Loss: tensor(0.0447)\n",
      "13612 Training Loss: tensor(0.0439)\n",
      "13613 Training Loss: tensor(0.0444)\n",
      "13614 Training Loss: tensor(0.0459)\n",
      "13615 Training Loss: tensor(0.0393)\n",
      "13616 Training Loss: tensor(0.0389)\n",
      "13617 Training Loss: tensor(0.0403)\n",
      "13618 Training Loss: tensor(0.0420)\n",
      "13619 Training Loss: tensor(0.0367)\n",
      "13620 Training Loss: tensor(0.0402)\n",
      "13621 Training Loss: tensor(0.0427)\n",
      "13622 Training Loss: tensor(0.0412)\n",
      "13623 Training Loss: tensor(0.0380)\n",
      "13624 Training Loss: tensor(0.0411)\n",
      "13625 Training Loss: tensor(0.0447)\n",
      "13626 Training Loss: tensor(0.0345)\n",
      "13627 Training Loss: tensor(0.0443)\n",
      "13628 Training Loss: tensor(0.0408)\n",
      "13629 Training Loss: tensor(0.0367)\n",
      "13630 Training Loss: tensor(0.0394)\n",
      "13631 Training Loss: tensor(0.0433)\n",
      "13632 Training Loss: tensor(0.0400)\n",
      "13633 Training Loss: tensor(0.0416)\n",
      "13634 Training Loss: tensor(0.0387)\n",
      "13635 Training Loss: tensor(0.0422)\n",
      "13636 Training Loss: tensor(0.0326)\n",
      "13637 Training Loss: tensor(0.0362)\n",
      "13638 Training Loss: tensor(0.0425)\n",
      "13639 Training Loss: tensor(0.0449)\n",
      "13640 Training Loss: tensor(0.0377)\n",
      "13641 Training Loss: tensor(0.0402)\n",
      "13642 Training Loss: tensor(0.0385)\n",
      "13643 Training Loss: tensor(0.0371)\n",
      "13644 Training Loss: tensor(0.0365)\n",
      "13645 Training Loss: tensor(0.0383)\n",
      "13646 Training Loss: tensor(0.0385)\n",
      "13647 Training Loss: tensor(0.0376)\n",
      "13648 Training Loss: tensor(0.0376)\n",
      "13649 Training Loss: tensor(0.0426)\n",
      "13650 Training Loss: tensor(0.0351)\n",
      "13651 Training Loss: tensor(0.0441)\n",
      "13652 Training Loss: tensor(0.0400)\n",
      "13653 Training Loss: tensor(0.0378)\n",
      "13654 Training Loss: tensor(0.0421)\n",
      "13655 Training Loss: tensor(0.0363)\n",
      "13656 Training Loss: tensor(0.0403)\n",
      "13657 Training Loss: tensor(0.0371)\n",
      "13658 Training Loss: tensor(0.0418)\n",
      "13659 Training Loss: tensor(0.0379)\n",
      "13660 Training Loss: tensor(0.0434)\n",
      "13661 Training Loss: tensor(0.0448)\n",
      "13662 Training Loss: tensor(0.0389)\n",
      "13663 Training Loss: tensor(0.0375)\n",
      "13664 Training Loss: tensor(0.0411)\n",
      "13665 Training Loss: tensor(0.0408)\n",
      "13666 Training Loss: tensor(0.0398)\n",
      "13667 Training Loss: tensor(0.0366)\n",
      "13668 Training Loss: tensor(0.0360)\n",
      "13669 Training Loss: tensor(0.0411)\n",
      "13670 Training Loss: tensor(0.0376)\n",
      "13671 Training Loss: tensor(0.0429)\n",
      "13672 Training Loss: tensor(0.0403)\n",
      "13673 Training Loss: tensor(0.0401)\n",
      "13674 Training Loss: tensor(0.0399)\n",
      "13675 Training Loss: tensor(0.0391)\n",
      "13676 Training Loss: tensor(0.0449)\n",
      "13677 Training Loss: tensor(0.0370)\n",
      "13678 Training Loss: tensor(0.0352)\n",
      "13679 Training Loss: tensor(0.0426)\n",
      "13680 Training Loss: tensor(0.0362)\n",
      "13681 Training Loss: tensor(0.0368)\n",
      "13682 Training Loss: tensor(0.0373)\n",
      "13683 Training Loss: tensor(0.0413)\n",
      "13684 Training Loss: tensor(0.0321)\n",
      "13685 Training Loss: tensor(0.0420)\n",
      "13686 Training Loss: tensor(0.0369)\n",
      "13687 Training Loss: tensor(0.0383)\n",
      "13688 Training Loss: tensor(0.0494)\n",
      "13689 Training Loss: tensor(0.0443)\n",
      "13690 Training Loss: tensor(0.0396)\n",
      "13691 Training Loss: tensor(0.0401)\n",
      "13692 Training Loss: tensor(0.0411)\n",
      "13693 Training Loss: tensor(0.0390)\n",
      "13694 Training Loss: tensor(0.0420)\n",
      "13695 Training Loss: tensor(0.0391)\n",
      "13696 Training Loss: tensor(0.0436)\n",
      "13697 Training Loss: tensor(0.0379)\n",
      "13698 Training Loss: tensor(0.0353)\n",
      "13699 Training Loss: tensor(0.0369)\n",
      "13700 Training Loss: tensor(0.0376)\n",
      "13701 Training Loss: tensor(0.0423)\n",
      "13702 Training Loss: tensor(0.0409)\n",
      "13703 Training Loss: tensor(0.0325)\n",
      "13704 Training Loss: tensor(0.0385)\n",
      "13705 Training Loss: tensor(0.0369)\n",
      "13706 Training Loss: tensor(0.0411)\n",
      "13707 Training Loss: tensor(0.0361)\n",
      "13708 Training Loss: tensor(0.0382)\n",
      "13709 Training Loss: tensor(0.0379)\n",
      "13710 Training Loss: tensor(0.0431)\n",
      "13711 Training Loss: tensor(0.0376)\n",
      "13712 Training Loss: tensor(0.0426)\n",
      "13713 Training Loss: tensor(0.0434)\n",
      "13714 Training Loss: tensor(0.0334)\n",
      "13715 Training Loss: tensor(0.0465)\n",
      "13716 Training Loss: tensor(0.0394)\n",
      "13717 Training Loss: tensor(0.0360)\n",
      "13718 Training Loss: tensor(0.0396)\n",
      "13719 Training Loss: tensor(0.0383)\n",
      "13720 Training Loss: tensor(0.0404)\n",
      "13721 Training Loss: tensor(0.0393)\n",
      "13722 Training Loss: tensor(0.0396)\n",
      "13723 Training Loss: tensor(0.0375)\n",
      "13724 Training Loss: tensor(0.0436)\n",
      "13725 Training Loss: tensor(0.0424)\n",
      "13726 Training Loss: tensor(0.0443)\n",
      "13727 Training Loss: tensor(0.0388)\n",
      "13728 Training Loss: tensor(0.0424)\n",
      "13729 Training Loss: tensor(0.0399)\n",
      "13730 Training Loss: tensor(0.0379)\n",
      "13731 Training Loss: tensor(0.0405)\n",
      "13732 Training Loss: tensor(0.0384)\n",
      "13733 Training Loss: tensor(0.0402)\n",
      "13734 Training Loss: tensor(0.0396)\n",
      "13735 Training Loss: tensor(0.0379)\n",
      "13736 Training Loss: tensor(0.0433)\n",
      "13737 Training Loss: tensor(0.0393)\n",
      "13738 Training Loss: tensor(0.0384)\n",
      "13739 Training Loss: tensor(0.0355)\n",
      "13740 Training Loss: tensor(0.0339)\n",
      "13741 Training Loss: tensor(0.0354)\n",
      "13742 Training Loss: tensor(0.0420)\n",
      "13743 Training Loss: tensor(0.0384)\n",
      "13744 Training Loss: tensor(0.0417)\n",
      "13745 Training Loss: tensor(0.0379)\n",
      "13746 Training Loss: tensor(0.0417)\n",
      "13747 Training Loss: tensor(0.0398)\n",
      "13748 Training Loss: tensor(0.0378)\n",
      "13749 Training Loss: tensor(0.0354)\n",
      "13750 Training Loss: tensor(0.0372)\n",
      "13751 Training Loss: tensor(0.0396)\n",
      "13752 Training Loss: tensor(0.0395)\n",
      "13753 Training Loss: tensor(0.0394)\n",
      "13754 Training Loss: tensor(0.0393)\n",
      "13755 Training Loss: tensor(0.0370)\n",
      "13756 Training Loss: tensor(0.0369)\n",
      "13757 Training Loss: tensor(0.0439)\n",
      "13758 Training Loss: tensor(0.0383)\n",
      "13759 Training Loss: tensor(0.0388)\n",
      "13760 Training Loss: tensor(0.0349)\n",
      "13761 Training Loss: tensor(0.0377)\n",
      "13762 Training Loss: tensor(0.0384)\n",
      "13763 Training Loss: tensor(0.0368)\n",
      "13764 Training Loss: tensor(0.0443)\n",
      "13765 Training Loss: tensor(0.0388)\n",
      "13766 Training Loss: tensor(0.0430)\n",
      "13767 Training Loss: tensor(0.0353)\n",
      "13768 Training Loss: tensor(0.0417)\n",
      "13769 Training Loss: tensor(0.0411)\n",
      "13770 Training Loss: tensor(0.0388)\n",
      "13771 Training Loss: tensor(0.0428)\n",
      "13772 Training Loss: tensor(0.0386)\n",
      "13773 Training Loss: tensor(0.0439)\n",
      "13774 Training Loss: tensor(0.0440)\n",
      "13775 Training Loss: tensor(0.0399)\n",
      "13776 Training Loss: tensor(0.0392)\n",
      "13777 Training Loss: tensor(0.0400)\n",
      "13778 Training Loss: tensor(0.0392)\n",
      "13779 Training Loss: tensor(0.0382)\n",
      "13780 Training Loss: tensor(0.0390)\n",
      "13781 Training Loss: tensor(0.0391)\n",
      "13782 Training Loss: tensor(0.0348)\n",
      "13783 Training Loss: tensor(0.0400)\n",
      "13784 Training Loss: tensor(0.0416)\n",
      "13785 Training Loss: tensor(0.0382)\n",
      "13786 Training Loss: tensor(0.0385)\n",
      "13787 Training Loss: tensor(0.0376)\n",
      "13788 Training Loss: tensor(0.0393)\n",
      "13789 Training Loss: tensor(0.0429)\n",
      "13790 Training Loss: tensor(0.0397)\n",
      "13791 Training Loss: tensor(0.0428)\n",
      "13792 Training Loss: tensor(0.0363)\n",
      "13793 Training Loss: tensor(0.0392)\n",
      "13794 Training Loss: tensor(0.0414)\n",
      "13795 Training Loss: tensor(0.0402)\n",
      "13796 Training Loss: tensor(0.0411)\n",
      "13797 Training Loss: tensor(0.0403)\n",
      "13798 Training Loss: tensor(0.0412)\n",
      "13799 Training Loss: tensor(0.0372)\n",
      "13800 Training Loss: tensor(0.0374)\n",
      "13801 Training Loss: tensor(0.0439)\n",
      "13802 Training Loss: tensor(0.0385)\n",
      "13803 Training Loss: tensor(0.0463)\n",
      "13804 Training Loss: tensor(0.0360)\n",
      "13805 Training Loss: tensor(0.0384)\n",
      "13806 Training Loss: tensor(0.0447)\n",
      "13807 Training Loss: tensor(0.0357)\n",
      "13808 Training Loss: tensor(0.0408)\n",
      "13809 Training Loss: tensor(0.0313)\n",
      "13810 Training Loss: tensor(0.0442)\n",
      "13811 Training Loss: tensor(0.0394)\n",
      "13812 Training Loss: tensor(0.0398)\n",
      "13813 Training Loss: tensor(0.0399)\n",
      "13814 Training Loss: tensor(0.0377)\n",
      "13815 Training Loss: tensor(0.0384)\n",
      "13816 Training Loss: tensor(0.0362)\n",
      "13817 Training Loss: tensor(0.0399)\n",
      "13818 Training Loss: tensor(0.0398)\n",
      "13819 Training Loss: tensor(0.0417)\n",
      "13820 Training Loss: tensor(0.0372)\n",
      "13821 Training Loss: tensor(0.0387)\n",
      "13822 Training Loss: tensor(0.0408)\n",
      "13823 Training Loss: tensor(0.0401)\n",
      "13824 Training Loss: tensor(0.0405)\n",
      "13825 Training Loss: tensor(0.0396)\n",
      "13826 Training Loss: tensor(0.0345)\n",
      "13827 Training Loss: tensor(0.0392)\n",
      "13828 Training Loss: tensor(0.0347)\n",
      "13829 Training Loss: tensor(0.0431)\n",
      "13830 Training Loss: tensor(0.0421)\n",
      "13831 Training Loss: tensor(0.0439)\n",
      "13832 Training Loss: tensor(0.0403)\n",
      "13833 Training Loss: tensor(0.0414)\n",
      "13834 Training Loss: tensor(0.0414)\n",
      "13835 Training Loss: tensor(0.0431)\n",
      "13836 Training Loss: tensor(0.0392)\n",
      "13837 Training Loss: tensor(0.0407)\n",
      "13838 Training Loss: tensor(0.0421)\n",
      "13839 Training Loss: tensor(0.0409)\n",
      "13840 Training Loss: tensor(0.0382)\n",
      "13841 Training Loss: tensor(0.0396)\n",
      "13842 Training Loss: tensor(0.0389)\n",
      "13843 Training Loss: tensor(0.0401)\n",
      "13844 Training Loss: tensor(0.0451)\n",
      "13845 Training Loss: tensor(0.0368)\n",
      "13846 Training Loss: tensor(0.0392)\n",
      "13847 Training Loss: tensor(0.0406)\n",
      "13848 Training Loss: tensor(0.0367)\n",
      "13849 Training Loss: tensor(0.0337)\n",
      "13850 Training Loss: tensor(0.0459)\n",
      "13851 Training Loss: tensor(0.0439)\n",
      "13852 Training Loss: tensor(0.0420)\n",
      "13853 Training Loss: tensor(0.0335)\n",
      "13854 Training Loss: tensor(0.0377)\n",
      "13855 Training Loss: tensor(0.0447)\n",
      "13856 Training Loss: tensor(0.0398)\n",
      "13857 Training Loss: tensor(0.0406)\n",
      "13858 Training Loss: tensor(0.0447)\n",
      "13859 Training Loss: tensor(0.0396)\n",
      "13860 Training Loss: tensor(0.0460)\n",
      "13861 Training Loss: tensor(0.0390)\n",
      "13862 Training Loss: tensor(0.0427)\n",
      "13863 Training Loss: tensor(0.0405)\n",
      "13864 Training Loss: tensor(0.0448)\n",
      "13865 Training Loss: tensor(0.0371)\n",
      "13866 Training Loss: tensor(0.0407)\n",
      "13867 Training Loss: tensor(0.0398)\n",
      "13868 Training Loss: tensor(0.0348)\n",
      "13869 Training Loss: tensor(0.0382)\n",
      "13870 Training Loss: tensor(0.0371)\n",
      "13871 Training Loss: tensor(0.0386)\n",
      "13872 Training Loss: tensor(0.0388)\n",
      "13873 Training Loss: tensor(0.0414)\n",
      "13874 Training Loss: tensor(0.0380)\n",
      "13875 Training Loss: tensor(0.0430)\n",
      "13876 Training Loss: tensor(0.0402)\n",
      "13877 Training Loss: tensor(0.0375)\n",
      "13878 Training Loss: tensor(0.0398)\n",
      "13879 Training Loss: tensor(0.0418)\n",
      "13880 Training Loss: tensor(0.0422)\n",
      "13881 Training Loss: tensor(0.0385)\n",
      "13882 Training Loss: tensor(0.0396)\n",
      "13883 Training Loss: tensor(0.0368)\n",
      "13884 Training Loss: tensor(0.0373)\n",
      "13885 Training Loss: tensor(0.0360)\n",
      "13886 Training Loss: tensor(0.0424)\n",
      "13887 Training Loss: tensor(0.0383)\n",
      "13888 Training Loss: tensor(0.0401)\n",
      "13889 Training Loss: tensor(0.0413)\n",
      "13890 Training Loss: tensor(0.0422)\n",
      "13891 Training Loss: tensor(0.0445)\n",
      "13892 Training Loss: tensor(0.0391)\n",
      "13893 Training Loss: tensor(0.0437)\n",
      "13894 Training Loss: tensor(0.0400)\n",
      "13895 Training Loss: tensor(0.0384)\n",
      "13896 Training Loss: tensor(0.0404)\n",
      "13897 Training Loss: tensor(0.0415)\n",
      "13898 Training Loss: tensor(0.0407)\n",
      "13899 Training Loss: tensor(0.0378)\n",
      "13900 Training Loss: tensor(0.0409)\n",
      "13901 Training Loss: tensor(0.0394)\n",
      "13902 Training Loss: tensor(0.0435)\n",
      "13903 Training Loss: tensor(0.0382)\n",
      "13904 Training Loss: tensor(0.0353)\n",
      "13905 Training Loss: tensor(0.0444)\n",
      "13906 Training Loss: tensor(0.0420)\n",
      "13907 Training Loss: tensor(0.0412)\n",
      "13908 Training Loss: tensor(0.0371)\n",
      "13909 Training Loss: tensor(0.0411)\n",
      "13910 Training Loss: tensor(0.0388)\n",
      "13911 Training Loss: tensor(0.0368)\n",
      "13912 Training Loss: tensor(0.0422)\n",
      "13913 Training Loss: tensor(0.0349)\n",
      "13914 Training Loss: tensor(0.0412)\n",
      "13915 Training Loss: tensor(0.0403)\n",
      "13916 Training Loss: tensor(0.0451)\n",
      "13917 Training Loss: tensor(0.0386)\n",
      "13918 Training Loss: tensor(0.0452)\n",
      "13919 Training Loss: tensor(0.0398)\n",
      "13920 Training Loss: tensor(0.0387)\n",
      "13921 Training Loss: tensor(0.0403)\n",
      "13922 Training Loss: tensor(0.0425)\n",
      "13923 Training Loss: tensor(0.0413)\n",
      "13924 Training Loss: tensor(0.0423)\n",
      "13925 Training Loss: tensor(0.0403)\n",
      "13926 Training Loss: tensor(0.0422)\n",
      "13927 Training Loss: tensor(0.0442)\n",
      "13928 Training Loss: tensor(0.0372)\n",
      "13929 Training Loss: tensor(0.0371)\n",
      "13930 Training Loss: tensor(0.0426)\n",
      "13931 Training Loss: tensor(0.0385)\n",
      "13932 Training Loss: tensor(0.0408)\n",
      "13933 Training Loss: tensor(0.0415)\n",
      "13934 Training Loss: tensor(0.0351)\n",
      "13935 Training Loss: tensor(0.0433)\n",
      "13936 Training Loss: tensor(0.0389)\n",
      "13937 Training Loss: tensor(0.0392)\n",
      "13938 Training Loss: tensor(0.0338)\n",
      "13939 Training Loss: tensor(0.0422)\n",
      "13940 Training Loss: tensor(0.0387)\n",
      "13941 Training Loss: tensor(0.0413)\n",
      "13942 Training Loss: tensor(0.0358)\n",
      "13943 Training Loss: tensor(0.0400)\n",
      "13944 Training Loss: tensor(0.0387)\n",
      "13945 Training Loss: tensor(0.0413)\n",
      "13946 Training Loss: tensor(0.0433)\n",
      "13947 Training Loss: tensor(0.0438)\n",
      "13948 Training Loss: tensor(0.0340)\n",
      "13949 Training Loss: tensor(0.0435)\n",
      "13950 Training Loss: tensor(0.0402)\n",
      "13951 Training Loss: tensor(0.0405)\n",
      "13952 Training Loss: tensor(0.0466)\n",
      "13953 Training Loss: tensor(0.0341)\n",
      "13954 Training Loss: tensor(0.0451)\n",
      "13955 Training Loss: tensor(0.0337)\n",
      "13956 Training Loss: tensor(0.0350)\n",
      "13957 Training Loss: tensor(0.0449)\n",
      "13958 Training Loss: tensor(0.0404)\n",
      "13959 Training Loss: tensor(0.0401)\n",
      "13960 Training Loss: tensor(0.0466)\n",
      "13961 Training Loss: tensor(0.0383)\n",
      "13962 Training Loss: tensor(0.0405)\n",
      "13963 Training Loss: tensor(0.0408)\n",
      "13964 Training Loss: tensor(0.0428)\n",
      "13965 Training Loss: tensor(0.0407)\n",
      "13966 Training Loss: tensor(0.0363)\n",
      "13967 Training Loss: tensor(0.0399)\n",
      "13968 Training Loss: tensor(0.0384)\n",
      "13969 Training Loss: tensor(0.0356)\n",
      "13970 Training Loss: tensor(0.0437)\n",
      "13971 Training Loss: tensor(0.0378)\n",
      "13972 Training Loss: tensor(0.0464)\n",
      "13973 Training Loss: tensor(0.0432)\n",
      "13974 Training Loss: tensor(0.0399)\n",
      "13975 Training Loss: tensor(0.0408)\n",
      "13976 Training Loss: tensor(0.0435)\n",
      "13977 Training Loss: tensor(0.0365)\n",
      "13978 Training Loss: tensor(0.0367)\n",
      "13979 Training Loss: tensor(0.0400)\n",
      "13980 Training Loss: tensor(0.0403)\n",
      "13981 Training Loss: tensor(0.0340)\n",
      "13982 Training Loss: tensor(0.0336)\n",
      "13983 Training Loss: tensor(0.0373)\n",
      "13984 Training Loss: tensor(0.0405)\n",
      "13985 Training Loss: tensor(0.0386)\n",
      "13986 Training Loss: tensor(0.0441)\n",
      "13987 Training Loss: tensor(0.0438)\n",
      "13988 Training Loss: tensor(0.0431)\n",
      "13989 Training Loss: tensor(0.0391)\n",
      "13990 Training Loss: tensor(0.0353)\n",
      "13991 Training Loss: tensor(0.0477)\n",
      "13992 Training Loss: tensor(0.0426)\n",
      "13993 Training Loss: tensor(0.0423)\n",
      "13994 Training Loss: tensor(0.0442)\n",
      "13995 Training Loss: tensor(0.0410)\n",
      "13996 Training Loss: tensor(0.0356)\n",
      "13997 Training Loss: tensor(0.0414)\n",
      "13998 Training Loss: tensor(0.0357)\n",
      "13999 Training Loss: tensor(0.0457)\n",
      "14000 Training Loss: tensor(0.0421)\n",
      "14001 Training Loss: tensor(0.0387)\n",
      "14002 Training Loss: tensor(0.0400)\n",
      "14003 Training Loss: tensor(0.0431)\n",
      "14004 Training Loss: tensor(0.0416)\n",
      "14005 Training Loss: tensor(0.0405)\n",
      "14006 Training Loss: tensor(0.0378)\n",
      "14007 Training Loss: tensor(0.0399)\n",
      "14008 Training Loss: tensor(0.0383)\n",
      "14009 Training Loss: tensor(0.0417)\n",
      "14010 Training Loss: tensor(0.0415)\n",
      "14011 Training Loss: tensor(0.0350)\n",
      "14012 Training Loss: tensor(0.0395)\n",
      "14013 Training Loss: tensor(0.0412)\n",
      "14014 Training Loss: tensor(0.0410)\n",
      "14015 Training Loss: tensor(0.0418)\n",
      "14016 Training Loss: tensor(0.0385)\n",
      "14017 Training Loss: tensor(0.0431)\n",
      "14018 Training Loss: tensor(0.0408)\n",
      "14019 Training Loss: tensor(0.0387)\n",
      "14020 Training Loss: tensor(0.0389)\n",
      "14021 Training Loss: tensor(0.0339)\n",
      "14022 Training Loss: tensor(0.0381)\n",
      "14023 Training Loss: tensor(0.0416)\n",
      "14024 Training Loss: tensor(0.0371)\n",
      "14025 Training Loss: tensor(0.0353)\n",
      "14026 Training Loss: tensor(0.0360)\n",
      "14027 Training Loss: tensor(0.0366)\n",
      "14028 Training Loss: tensor(0.0377)\n",
      "14029 Training Loss: tensor(0.0416)\n",
      "14030 Training Loss: tensor(0.0390)\n",
      "14031 Training Loss: tensor(0.0396)\n",
      "14032 Training Loss: tensor(0.0410)\n",
      "14033 Training Loss: tensor(0.0382)\n",
      "14034 Training Loss: tensor(0.0371)\n",
      "14035 Training Loss: tensor(0.0381)\n",
      "14036 Training Loss: tensor(0.0433)\n",
      "14037 Training Loss: tensor(0.0447)\n",
      "14038 Training Loss: tensor(0.0390)\n",
      "14039 Training Loss: tensor(0.0347)\n",
      "14040 Training Loss: tensor(0.0453)\n",
      "14041 Training Loss: tensor(0.0346)\n",
      "14042 Training Loss: tensor(0.0378)\n",
      "14043 Training Loss: tensor(0.0362)\n",
      "14044 Training Loss: tensor(0.0362)\n",
      "14045 Training Loss: tensor(0.0403)\n",
      "14046 Training Loss: tensor(0.0390)\n",
      "14047 Training Loss: tensor(0.0344)\n",
      "14048 Training Loss: tensor(0.0406)\n",
      "14049 Training Loss: tensor(0.0381)\n",
      "14050 Training Loss: tensor(0.0503)\n",
      "14051 Training Loss: tensor(0.0368)\n",
      "14052 Training Loss: tensor(0.0403)\n",
      "14053 Training Loss: tensor(0.0443)\n",
      "14054 Training Loss: tensor(0.0428)\n",
      "14055 Training Loss: tensor(0.0403)\n",
      "14056 Training Loss: tensor(0.0431)\n",
      "14057 Training Loss: tensor(0.0400)\n",
      "14058 Training Loss: tensor(0.0393)\n",
      "14059 Training Loss: tensor(0.0441)\n",
      "14060 Training Loss: tensor(0.0376)\n",
      "14061 Training Loss: tensor(0.0429)\n",
      "14062 Training Loss: tensor(0.0458)\n",
      "14063 Training Loss: tensor(0.0381)\n",
      "14064 Training Loss: tensor(0.0415)\n",
      "14065 Training Loss: tensor(0.0449)\n",
      "14066 Training Loss: tensor(0.0405)\n",
      "14067 Training Loss: tensor(0.0417)\n",
      "14068 Training Loss: tensor(0.0359)\n",
      "14069 Training Loss: tensor(0.0446)\n",
      "14070 Training Loss: tensor(0.0392)\n",
      "14071 Training Loss: tensor(0.0442)\n",
      "14072 Training Loss: tensor(0.0429)\n",
      "14073 Training Loss: tensor(0.0347)\n",
      "14074 Training Loss: tensor(0.0410)\n",
      "14075 Training Loss: tensor(0.0405)\n",
      "14076 Training Loss: tensor(0.0380)\n",
      "14077 Training Loss: tensor(0.0426)\n",
      "14078 Training Loss: tensor(0.0397)\n",
      "14079 Training Loss: tensor(0.0394)\n",
      "14080 Training Loss: tensor(0.0385)\n",
      "14081 Training Loss: tensor(0.0404)\n",
      "14082 Training Loss: tensor(0.0408)\n",
      "14083 Training Loss: tensor(0.0329)\n",
      "14084 Training Loss: tensor(0.0445)\n",
      "14085 Training Loss: tensor(0.0389)\n",
      "14086 Training Loss: tensor(0.0399)\n",
      "14087 Training Loss: tensor(0.0384)\n",
      "14088 Training Loss: tensor(0.0398)\n",
      "14089 Training Loss: tensor(0.0335)\n",
      "14090 Training Loss: tensor(0.0382)\n",
      "14091 Training Loss: tensor(0.0350)\n",
      "14092 Training Loss: tensor(0.0362)\n",
      "14093 Training Loss: tensor(0.0407)\n",
      "14094 Training Loss: tensor(0.0343)\n",
      "14095 Training Loss: tensor(0.0362)\n",
      "14096 Training Loss: tensor(0.0413)\n",
      "14097 Training Loss: tensor(0.0372)\n",
      "14098 Training Loss: tensor(0.0382)\n",
      "14099 Training Loss: tensor(0.0342)\n",
      "14100 Training Loss: tensor(0.0470)\n",
      "14101 Training Loss: tensor(0.0434)\n",
      "14102 Training Loss: tensor(0.0357)\n",
      "14103 Training Loss: tensor(0.0354)\n",
      "14104 Training Loss: tensor(0.0391)\n",
      "14105 Training Loss: tensor(0.0412)\n",
      "14106 Training Loss: tensor(0.0398)\n",
      "14107 Training Loss: tensor(0.0469)\n",
      "14108 Training Loss: tensor(0.0390)\n",
      "14109 Training Loss: tensor(0.0372)\n",
      "14110 Training Loss: tensor(0.0462)\n",
      "14111 Training Loss: tensor(0.0434)\n",
      "14112 Training Loss: tensor(0.0436)\n",
      "14113 Training Loss: tensor(0.0411)\n",
      "14114 Training Loss: tensor(0.0388)\n",
      "14115 Training Loss: tensor(0.0429)\n",
      "14116 Training Loss: tensor(0.0441)\n",
      "14117 Training Loss: tensor(0.0371)\n",
      "14118 Training Loss: tensor(0.0380)\n",
      "14119 Training Loss: tensor(0.0372)\n",
      "14120 Training Loss: tensor(0.0436)\n",
      "14121 Training Loss: tensor(0.0385)\n",
      "14122 Training Loss: tensor(0.0424)\n",
      "14123 Training Loss: tensor(0.0402)\n",
      "14124 Training Loss: tensor(0.0392)\n",
      "14125 Training Loss: tensor(0.0382)\n",
      "14126 Training Loss: tensor(0.0397)\n",
      "14127 Training Loss: tensor(0.0379)\n",
      "14128 Training Loss: tensor(0.0422)\n",
      "14129 Training Loss: tensor(0.0391)\n",
      "14130 Training Loss: tensor(0.0427)\n",
      "14131 Training Loss: tensor(0.0382)\n",
      "14132 Training Loss: tensor(0.0374)\n",
      "14133 Training Loss: tensor(0.0418)\n",
      "14134 Training Loss: tensor(0.0440)\n",
      "14135 Training Loss: tensor(0.0388)\n",
      "14136 Training Loss: tensor(0.0390)\n",
      "14137 Training Loss: tensor(0.0376)\n",
      "14138 Training Loss: tensor(0.0369)\n",
      "14139 Training Loss: tensor(0.0393)\n",
      "14140 Training Loss: tensor(0.0395)\n",
      "14141 Training Loss: tensor(0.0342)\n",
      "14142 Training Loss: tensor(0.0365)\n",
      "14143 Training Loss: tensor(0.0404)\n",
      "14144 Training Loss: tensor(0.0412)\n",
      "14145 Training Loss: tensor(0.0439)\n",
      "14146 Training Loss: tensor(0.0395)\n",
      "14147 Training Loss: tensor(0.0368)\n",
      "14148 Training Loss: tensor(0.0352)\n",
      "14149 Training Loss: tensor(0.0382)\n",
      "14150 Training Loss: tensor(0.0393)\n",
      "14151 Training Loss: tensor(0.0337)\n",
      "14152 Training Loss: tensor(0.0396)\n",
      "14153 Training Loss: tensor(0.0397)\n",
      "14154 Training Loss: tensor(0.0440)\n",
      "14155 Training Loss: tensor(0.0435)\n",
      "14156 Training Loss: tensor(0.0401)\n",
      "14157 Training Loss: tensor(0.0409)\n",
      "14158 Training Loss: tensor(0.0383)\n",
      "14159 Training Loss: tensor(0.0397)\n",
      "14160 Training Loss: tensor(0.0388)\n",
      "14161 Training Loss: tensor(0.0393)\n",
      "14162 Training Loss: tensor(0.0462)\n",
      "14163 Training Loss: tensor(0.0382)\n",
      "14164 Training Loss: tensor(0.0355)\n",
      "14165 Training Loss: tensor(0.0367)\n",
      "14166 Training Loss: tensor(0.0395)\n",
      "14167 Training Loss: tensor(0.0392)\n",
      "14168 Training Loss: tensor(0.0451)\n",
      "14169 Training Loss: tensor(0.0378)\n",
      "14170 Training Loss: tensor(0.0453)\n",
      "14171 Training Loss: tensor(0.0373)\n",
      "14172 Training Loss: tensor(0.0352)\n",
      "14173 Training Loss: tensor(0.0411)\n",
      "14174 Training Loss: tensor(0.0367)\n",
      "14175 Training Loss: tensor(0.0438)\n",
      "14176 Training Loss: tensor(0.0386)\n",
      "14177 Training Loss: tensor(0.0401)\n",
      "14178 Training Loss: tensor(0.0433)\n",
      "14179 Training Loss: tensor(0.0397)\n",
      "14180 Training Loss: tensor(0.0456)\n",
      "14181 Training Loss: tensor(0.0398)\n",
      "14182 Training Loss: tensor(0.0380)\n",
      "14183 Training Loss: tensor(0.0389)\n",
      "14184 Training Loss: tensor(0.0348)\n",
      "14185 Training Loss: tensor(0.0405)\n",
      "14186 Training Loss: tensor(0.0421)\n",
      "14187 Training Loss: tensor(0.0397)\n",
      "14188 Training Loss: tensor(0.0413)\n",
      "14189 Training Loss: tensor(0.0374)\n",
      "14190 Training Loss: tensor(0.0370)\n",
      "14191 Training Loss: tensor(0.0449)\n",
      "14192 Training Loss: tensor(0.0360)\n",
      "14193 Training Loss: tensor(0.0368)\n",
      "14194 Training Loss: tensor(0.0428)\n",
      "14195 Training Loss: tensor(0.0374)\n",
      "14196 Training Loss: tensor(0.0402)\n",
      "14197 Training Loss: tensor(0.0411)\n",
      "14198 Training Loss: tensor(0.0374)\n",
      "14199 Training Loss: tensor(0.0444)\n",
      "14200 Training Loss: tensor(0.0397)\n",
      "14201 Training Loss: tensor(0.0422)\n",
      "14202 Training Loss: tensor(0.0350)\n",
      "14203 Training Loss: tensor(0.0473)\n",
      "14204 Training Loss: tensor(0.0368)\n",
      "14205 Training Loss: tensor(0.0406)\n",
      "14206 Training Loss: tensor(0.0416)\n",
      "14207 Training Loss: tensor(0.0410)\n",
      "14208 Training Loss: tensor(0.0420)\n",
      "14209 Training Loss: tensor(0.0378)\n",
      "14210 Training Loss: tensor(0.0381)\n",
      "14211 Training Loss: tensor(0.0406)\n",
      "14212 Training Loss: tensor(0.0393)\n",
      "14213 Training Loss: tensor(0.0395)\n",
      "14214 Training Loss: tensor(0.0417)\n",
      "14215 Training Loss: tensor(0.0432)\n",
      "14216 Training Loss: tensor(0.0372)\n",
      "14217 Training Loss: tensor(0.0390)\n",
      "14218 Training Loss: tensor(0.0459)\n",
      "14219 Training Loss: tensor(0.0413)\n",
      "14220 Training Loss: tensor(0.0419)\n",
      "14221 Training Loss: tensor(0.0432)\n",
      "14222 Training Loss: tensor(0.0403)\n",
      "14223 Training Loss: tensor(0.0396)\n",
      "14224 Training Loss: tensor(0.0440)\n",
      "14225 Training Loss: tensor(0.0372)\n",
      "14226 Training Loss: tensor(0.0423)\n",
      "14227 Training Loss: tensor(0.0328)\n",
      "14228 Training Loss: tensor(0.0449)\n",
      "14229 Training Loss: tensor(0.0422)\n",
      "14230 Training Loss: tensor(0.0385)\n",
      "14231 Training Loss: tensor(0.0403)\n",
      "14232 Training Loss: tensor(0.0369)\n",
      "14233 Training Loss: tensor(0.0407)\n",
      "14234 Training Loss: tensor(0.0450)\n",
      "14235 Training Loss: tensor(0.0367)\n",
      "14236 Training Loss: tensor(0.0409)\n",
      "14237 Training Loss: tensor(0.0454)\n",
      "14238 Training Loss: tensor(0.0387)\n",
      "14239 Training Loss: tensor(0.0410)\n",
      "14240 Training Loss: tensor(0.0394)\n",
      "14241 Training Loss: tensor(0.0391)\n",
      "14242 Training Loss: tensor(0.0380)\n",
      "14243 Training Loss: tensor(0.0339)\n",
      "14244 Training Loss: tensor(0.0425)\n",
      "14245 Training Loss: tensor(0.0383)\n",
      "14246 Training Loss: tensor(0.0391)\n",
      "14247 Training Loss: tensor(0.0391)\n",
      "14248 Training Loss: tensor(0.0397)\n",
      "14249 Training Loss: tensor(0.0326)\n",
      "14250 Training Loss: tensor(0.0391)\n",
      "14251 Training Loss: tensor(0.0449)\n",
      "14252 Training Loss: tensor(0.0367)\n",
      "14253 Training Loss: tensor(0.0357)\n",
      "14254 Training Loss: tensor(0.0392)\n",
      "14255 Training Loss: tensor(0.0408)\n",
      "14256 Training Loss: tensor(0.0356)\n",
      "14257 Training Loss: tensor(0.0353)\n",
      "14258 Training Loss: tensor(0.0340)\n",
      "14259 Training Loss: tensor(0.0424)\n",
      "14260 Training Loss: tensor(0.0382)\n",
      "14261 Training Loss: tensor(0.0357)\n",
      "14262 Training Loss: tensor(0.0431)\n",
      "14263 Training Loss: tensor(0.0432)\n",
      "14264 Training Loss: tensor(0.0371)\n",
      "14265 Training Loss: tensor(0.0413)\n",
      "14266 Training Loss: tensor(0.0376)\n",
      "14267 Training Loss: tensor(0.0384)\n",
      "14268 Training Loss: tensor(0.0377)\n",
      "14269 Training Loss: tensor(0.0405)\n",
      "14270 Training Loss: tensor(0.0376)\n",
      "14271 Training Loss: tensor(0.0380)\n",
      "14272 Training Loss: tensor(0.0379)\n",
      "14273 Training Loss: tensor(0.0400)\n",
      "14274 Training Loss: tensor(0.0431)\n",
      "14275 Training Loss: tensor(0.0469)\n",
      "14276 Training Loss: tensor(0.0380)\n",
      "14277 Training Loss: tensor(0.0309)\n",
      "14278 Training Loss: tensor(0.0391)\n",
      "14279 Training Loss: tensor(0.0387)\n",
      "14280 Training Loss: tensor(0.0390)\n",
      "14281 Training Loss: tensor(0.0361)\n",
      "14282 Training Loss: tensor(0.0355)\n",
      "14283 Training Loss: tensor(0.0375)\n",
      "14284 Training Loss: tensor(0.0382)\n",
      "14285 Training Loss: tensor(0.0414)\n",
      "14286 Training Loss: tensor(0.0386)\n",
      "14287 Training Loss: tensor(0.0430)\n",
      "14288 Training Loss: tensor(0.0383)\n",
      "14289 Training Loss: tensor(0.0400)\n",
      "14290 Training Loss: tensor(0.0408)\n",
      "14291 Training Loss: tensor(0.0410)\n",
      "14292 Training Loss: tensor(0.0416)\n",
      "14293 Training Loss: tensor(0.0405)\n",
      "14294 Training Loss: tensor(0.0373)\n",
      "14295 Training Loss: tensor(0.0390)\n",
      "14296 Training Loss: tensor(0.0415)\n",
      "14297 Training Loss: tensor(0.0494)\n",
      "14298 Training Loss: tensor(0.0371)\n",
      "14299 Training Loss: tensor(0.0413)\n",
      "14300 Training Loss: tensor(0.0429)\n",
      "14301 Training Loss: tensor(0.0426)\n",
      "14302 Training Loss: tensor(0.0372)\n",
      "14303 Training Loss: tensor(0.0352)\n",
      "14304 Training Loss: tensor(0.0350)\n",
      "14305 Training Loss: tensor(0.0374)\n",
      "14306 Training Loss: tensor(0.0434)\n",
      "14307 Training Loss: tensor(0.0425)\n",
      "14308 Training Loss: tensor(0.0427)\n",
      "14309 Training Loss: tensor(0.0353)\n",
      "14310 Training Loss: tensor(0.0406)\n",
      "14311 Training Loss: tensor(0.0474)\n",
      "14312 Training Loss: tensor(0.0381)\n",
      "14313 Training Loss: tensor(0.0408)\n",
      "14314 Training Loss: tensor(0.0376)\n",
      "14315 Training Loss: tensor(0.0453)\n",
      "14316 Training Loss: tensor(0.0400)\n",
      "14317 Training Loss: tensor(0.0359)\n",
      "14318 Training Loss: tensor(0.0393)\n",
      "14319 Training Loss: tensor(0.0391)\n",
      "14320 Training Loss: tensor(0.0395)\n",
      "14321 Training Loss: tensor(0.0402)\n",
      "14322 Training Loss: tensor(0.0414)\n",
      "14323 Training Loss: tensor(0.0380)\n",
      "14324 Training Loss: tensor(0.0441)\n",
      "14325 Training Loss: tensor(0.0400)\n",
      "14326 Training Loss: tensor(0.0332)\n",
      "14327 Training Loss: tensor(0.0422)\n",
      "14328 Training Loss: tensor(0.0414)\n",
      "14329 Training Loss: tensor(0.0380)\n",
      "14330 Training Loss: tensor(0.0356)\n",
      "14331 Training Loss: tensor(0.0406)\n",
      "14332 Training Loss: tensor(0.0431)\n",
      "14333 Training Loss: tensor(0.0418)\n",
      "14334 Training Loss: tensor(0.0381)\n",
      "14335 Training Loss: tensor(0.0390)\n",
      "14336 Training Loss: tensor(0.0395)\n",
      "14337 Training Loss: tensor(0.0370)\n",
      "14338 Training Loss: tensor(0.0420)\n",
      "14339 Training Loss: tensor(0.0432)\n",
      "14340 Training Loss: tensor(0.0399)\n",
      "14341 Training Loss: tensor(0.0419)\n",
      "14342 Training Loss: tensor(0.0451)\n",
      "14343 Training Loss: tensor(0.0377)\n",
      "14344 Training Loss: tensor(0.0386)\n",
      "14345 Training Loss: tensor(0.0430)\n",
      "14346 Training Loss: tensor(0.0450)\n",
      "14347 Training Loss: tensor(0.0403)\n",
      "14348 Training Loss: tensor(0.0377)\n",
      "14349 Training Loss: tensor(0.0450)\n",
      "14350 Training Loss: tensor(0.0399)\n",
      "14351 Training Loss: tensor(0.0454)\n",
      "14352 Training Loss: tensor(0.0366)\n",
      "14353 Training Loss: tensor(0.0362)\n",
      "14354 Training Loss: tensor(0.0431)\n",
      "14355 Training Loss: tensor(0.0405)\n",
      "14356 Training Loss: tensor(0.0428)\n",
      "14357 Training Loss: tensor(0.0420)\n",
      "14358 Training Loss: tensor(0.0423)\n",
      "14359 Training Loss: tensor(0.0397)\n",
      "14360 Training Loss: tensor(0.0371)\n",
      "14361 Training Loss: tensor(0.0356)\n",
      "14362 Training Loss: tensor(0.0416)\n",
      "14363 Training Loss: tensor(0.0421)\n",
      "14364 Training Loss: tensor(0.0422)\n",
      "14365 Training Loss: tensor(0.0406)\n",
      "14366 Training Loss: tensor(0.0417)\n",
      "14367 Training Loss: tensor(0.0416)\n",
      "14368 Training Loss: tensor(0.0377)\n",
      "14369 Training Loss: tensor(0.0347)\n",
      "14370 Training Loss: tensor(0.0425)\n",
      "14371 Training Loss: tensor(0.0391)\n",
      "14372 Training Loss: tensor(0.0376)\n",
      "14373 Training Loss: tensor(0.0405)\n",
      "14374 Training Loss: tensor(0.0407)\n",
      "14375 Training Loss: tensor(0.0377)\n",
      "14376 Training Loss: tensor(0.0389)\n",
      "14377 Training Loss: tensor(0.0435)\n",
      "14378 Training Loss: tensor(0.0362)\n",
      "14379 Training Loss: tensor(0.0368)\n",
      "14380 Training Loss: tensor(0.0423)\n",
      "14381 Training Loss: tensor(0.0454)\n",
      "14382 Training Loss: tensor(0.0400)\n",
      "14383 Training Loss: tensor(0.0450)\n",
      "14384 Training Loss: tensor(0.0457)\n",
      "14385 Training Loss: tensor(0.0372)\n",
      "14386 Training Loss: tensor(0.0385)\n",
      "14387 Training Loss: tensor(0.0352)\n",
      "14388 Training Loss: tensor(0.0434)\n",
      "14389 Training Loss: tensor(0.0396)\n",
      "14390 Training Loss: tensor(0.0387)\n",
      "14391 Training Loss: tensor(0.0494)\n",
      "14392 Training Loss: tensor(0.0446)\n",
      "14393 Training Loss: tensor(0.0321)\n",
      "14394 Training Loss: tensor(0.0393)\n",
      "14395 Training Loss: tensor(0.0409)\n",
      "14396 Training Loss: tensor(0.0346)\n",
      "14397 Training Loss: tensor(0.0387)\n",
      "14398 Training Loss: tensor(0.0388)\n",
      "14399 Training Loss: tensor(0.0448)\n",
      "14400 Training Loss: tensor(0.0433)\n",
      "14401 Training Loss: tensor(0.0418)\n",
      "14402 Training Loss: tensor(0.0436)\n",
      "14403 Training Loss: tensor(0.0340)\n",
      "14404 Training Loss: tensor(0.0446)\n",
      "14405 Training Loss: tensor(0.0380)\n",
      "14406 Training Loss: tensor(0.0391)\n",
      "14407 Training Loss: tensor(0.0439)\n",
      "14408 Training Loss: tensor(0.0412)\n",
      "14409 Training Loss: tensor(0.0357)\n",
      "14410 Training Loss: tensor(0.0401)\n",
      "14411 Training Loss: tensor(0.0402)\n",
      "14412 Training Loss: tensor(0.0394)\n",
      "14413 Training Loss: tensor(0.0381)\n",
      "14414 Training Loss: tensor(0.0344)\n",
      "14415 Training Loss: tensor(0.0461)\n",
      "14416 Training Loss: tensor(0.0428)\n",
      "14417 Training Loss: tensor(0.0421)\n",
      "14418 Training Loss: tensor(0.0447)\n",
      "14419 Training Loss: tensor(0.0342)\n",
      "14420 Training Loss: tensor(0.0369)\n",
      "14421 Training Loss: tensor(0.0373)\n",
      "14422 Training Loss: tensor(0.0374)\n",
      "14423 Training Loss: tensor(0.0430)\n",
      "14424 Training Loss: tensor(0.0432)\n",
      "14425 Training Loss: tensor(0.0404)\n",
      "14426 Training Loss: tensor(0.0386)\n",
      "14427 Training Loss: tensor(0.0405)\n",
      "14428 Training Loss: tensor(0.0319)\n",
      "14429 Training Loss: tensor(0.0386)\n",
      "14430 Training Loss: tensor(0.0399)\n",
      "14431 Training Loss: tensor(0.0398)\n",
      "14432 Training Loss: tensor(0.0415)\n",
      "14433 Training Loss: tensor(0.0370)\n",
      "14434 Training Loss: tensor(0.0462)\n",
      "14435 Training Loss: tensor(0.0385)\n",
      "14436 Training Loss: tensor(0.0392)\n",
      "14437 Training Loss: tensor(0.0447)\n",
      "14438 Training Loss: tensor(0.0391)\n",
      "14439 Training Loss: tensor(0.0374)\n",
      "14440 Training Loss: tensor(0.0404)\n",
      "14441 Training Loss: tensor(0.0413)\n",
      "14442 Training Loss: tensor(0.0378)\n",
      "14443 Training Loss: tensor(0.0351)\n",
      "14444 Training Loss: tensor(0.0390)\n",
      "14445 Training Loss: tensor(0.0309)\n",
      "14446 Training Loss: tensor(0.0439)\n",
      "14447 Training Loss: tensor(0.0363)\n",
      "14448 Training Loss: tensor(0.0442)\n",
      "14449 Training Loss: tensor(0.0363)\n",
      "14450 Training Loss: tensor(0.0410)\n",
      "14451 Training Loss: tensor(0.0469)\n",
      "14452 Training Loss: tensor(0.0376)\n",
      "14453 Training Loss: tensor(0.0391)\n",
      "14454 Training Loss: tensor(0.0369)\n",
      "14455 Training Loss: tensor(0.0358)\n",
      "14456 Training Loss: tensor(0.0455)\n",
      "14457 Training Loss: tensor(0.0335)\n",
      "14458 Training Loss: tensor(0.0392)\n",
      "14459 Training Loss: tensor(0.0387)\n",
      "14460 Training Loss: tensor(0.0393)\n",
      "14461 Training Loss: tensor(0.0387)\n",
      "14462 Training Loss: tensor(0.0462)\n",
      "14463 Training Loss: tensor(0.0380)\n",
      "14464 Training Loss: tensor(0.0410)\n",
      "14465 Training Loss: tensor(0.0368)\n",
      "14466 Training Loss: tensor(0.0355)\n",
      "14467 Training Loss: tensor(0.0454)\n",
      "14468 Training Loss: tensor(0.0380)\n",
      "14469 Training Loss: tensor(0.0380)\n",
      "14470 Training Loss: tensor(0.0441)\n",
      "14471 Training Loss: tensor(0.0347)\n",
      "14472 Training Loss: tensor(0.0414)\n",
      "14473 Training Loss: tensor(0.0472)\n",
      "14474 Training Loss: tensor(0.0440)\n",
      "14475 Training Loss: tensor(0.0393)\n",
      "14476 Training Loss: tensor(0.0427)\n",
      "14477 Training Loss: tensor(0.0399)\n",
      "14478 Training Loss: tensor(0.0330)\n",
      "14479 Training Loss: tensor(0.0447)\n",
      "14480 Training Loss: tensor(0.0377)\n",
      "14481 Training Loss: tensor(0.0428)\n",
      "14482 Training Loss: tensor(0.0363)\n",
      "14483 Training Loss: tensor(0.0366)\n",
      "14484 Training Loss: tensor(0.0427)\n",
      "14485 Training Loss: tensor(0.0390)\n",
      "14486 Training Loss: tensor(0.0428)\n",
      "14487 Training Loss: tensor(0.0371)\n",
      "14488 Training Loss: tensor(0.0422)\n",
      "14489 Training Loss: tensor(0.0484)\n",
      "14490 Training Loss: tensor(0.0445)\n",
      "14491 Training Loss: tensor(0.0403)\n",
      "14492 Training Loss: tensor(0.0457)\n",
      "14493 Training Loss: tensor(0.0399)\n",
      "14494 Training Loss: tensor(0.0434)\n",
      "14495 Training Loss: tensor(0.0430)\n",
      "14496 Training Loss: tensor(0.0406)\n",
      "14497 Training Loss: tensor(0.0535)\n",
      "14498 Training Loss: tensor(0.0349)\n",
      "14499 Training Loss: tensor(0.0429)\n",
      "14500 Training Loss: tensor(0.0385)\n",
      "14501 Training Loss: tensor(0.0398)\n",
      "14502 Training Loss: tensor(0.0461)\n",
      "14503 Training Loss: tensor(0.0421)\n",
      "14504 Training Loss: tensor(0.0391)\n",
      "14505 Training Loss: tensor(0.0360)\n",
      "14506 Training Loss: tensor(0.0392)\n",
      "14507 Training Loss: tensor(0.0443)\n",
      "14508 Training Loss: tensor(0.0394)\n",
      "14509 Training Loss: tensor(0.0381)\n",
      "14510 Training Loss: tensor(0.0388)\n",
      "14511 Training Loss: tensor(0.0399)\n",
      "14512 Training Loss: tensor(0.0383)\n",
      "14513 Training Loss: tensor(0.0391)\n",
      "14514 Training Loss: tensor(0.0384)\n",
      "14515 Training Loss: tensor(0.0383)\n",
      "14516 Training Loss: tensor(0.0426)\n",
      "14517 Training Loss: tensor(0.0395)\n",
      "14518 Training Loss: tensor(0.0410)\n",
      "14519 Training Loss: tensor(0.0370)\n",
      "14520 Training Loss: tensor(0.0362)\n",
      "14521 Training Loss: tensor(0.0388)\n",
      "14522 Training Loss: tensor(0.0444)\n",
      "14523 Training Loss: tensor(0.0379)\n",
      "14524 Training Loss: tensor(0.0388)\n",
      "14525 Training Loss: tensor(0.0359)\n",
      "14526 Training Loss: tensor(0.0377)\n",
      "14527 Training Loss: tensor(0.0405)\n",
      "14528 Training Loss: tensor(0.0414)\n",
      "14529 Training Loss: tensor(0.0353)\n",
      "14530 Training Loss: tensor(0.0337)\n",
      "14531 Training Loss: tensor(0.0414)\n",
      "14532 Training Loss: tensor(0.0370)\n",
      "14533 Training Loss: tensor(0.0395)\n",
      "14534 Training Loss: tensor(0.0380)\n",
      "14535 Training Loss: tensor(0.0379)\n",
      "14536 Training Loss: tensor(0.0371)\n",
      "14537 Training Loss: tensor(0.0388)\n",
      "14538 Training Loss: tensor(0.0451)\n",
      "14539 Training Loss: tensor(0.0385)\n",
      "14540 Training Loss: tensor(0.0350)\n",
      "14541 Training Loss: tensor(0.0453)\n",
      "14542 Training Loss: tensor(0.0373)\n",
      "14543 Training Loss: tensor(0.0368)\n",
      "14544 Training Loss: tensor(0.0379)\n",
      "14545 Training Loss: tensor(0.0394)\n",
      "14546 Training Loss: tensor(0.0431)\n",
      "14547 Training Loss: tensor(0.0331)\n",
      "14548 Training Loss: tensor(0.0365)\n",
      "14549 Training Loss: tensor(0.0363)\n",
      "14550 Training Loss: tensor(0.0417)\n",
      "14551 Training Loss: tensor(0.0401)\n",
      "14552 Training Loss: tensor(0.0384)\n",
      "14553 Training Loss: tensor(0.0355)\n",
      "14554 Training Loss: tensor(0.0386)\n",
      "14555 Training Loss: tensor(0.0417)\n",
      "14556 Training Loss: tensor(0.0413)\n",
      "14557 Training Loss: tensor(0.0371)\n",
      "14558 Training Loss: tensor(0.0420)\n",
      "14559 Training Loss: tensor(0.0391)\n",
      "14560 Training Loss: tensor(0.0390)\n",
      "14561 Training Loss: tensor(0.0378)\n",
      "14562 Training Loss: tensor(0.0379)\n",
      "14563 Training Loss: tensor(0.0445)\n",
      "14564 Training Loss: tensor(0.0397)\n",
      "14565 Training Loss: tensor(0.0416)\n",
      "14566 Training Loss: tensor(0.0346)\n",
      "14567 Training Loss: tensor(0.0427)\n",
      "14568 Training Loss: tensor(0.0366)\n",
      "14569 Training Loss: tensor(0.0399)\n",
      "14570 Training Loss: tensor(0.0366)\n",
      "14571 Training Loss: tensor(0.0410)\n",
      "14572 Training Loss: tensor(0.0367)\n",
      "14573 Training Loss: tensor(0.0403)\n",
      "14574 Training Loss: tensor(0.0362)\n",
      "14575 Training Loss: tensor(0.0402)\n",
      "14576 Training Loss: tensor(0.0349)\n",
      "14577 Training Loss: tensor(0.0397)\n",
      "14578 Training Loss: tensor(0.0328)\n",
      "14579 Training Loss: tensor(0.0414)\n",
      "14580 Training Loss: tensor(0.0408)\n",
      "14581 Training Loss: tensor(0.0355)\n",
      "14582 Training Loss: tensor(0.0376)\n",
      "14583 Training Loss: tensor(0.0374)\n",
      "14584 Training Loss: tensor(0.0454)\n",
      "14585 Training Loss: tensor(0.0393)\n",
      "14586 Training Loss: tensor(0.0419)\n",
      "14587 Training Loss: tensor(0.0412)\n",
      "14588 Training Loss: tensor(0.0374)\n",
      "14589 Training Loss: tensor(0.0447)\n",
      "14590 Training Loss: tensor(0.0389)\n",
      "14591 Training Loss: tensor(0.0408)\n",
      "14592 Training Loss: tensor(0.0391)\n",
      "14593 Training Loss: tensor(0.0407)\n",
      "14594 Training Loss: tensor(0.0392)\n",
      "14595 Training Loss: tensor(0.0406)\n",
      "14596 Training Loss: tensor(0.0430)\n",
      "14597 Training Loss: tensor(0.0385)\n",
      "14598 Training Loss: tensor(0.0368)\n",
      "14599 Training Loss: tensor(0.0392)\n",
      "14600 Training Loss: tensor(0.0365)\n",
      "14601 Training Loss: tensor(0.0348)\n",
      "14602 Training Loss: tensor(0.0350)\n",
      "14603 Training Loss: tensor(0.0386)\n",
      "14604 Training Loss: tensor(0.0386)\n",
      "14605 Training Loss: tensor(0.0418)\n",
      "14606 Training Loss: tensor(0.0409)\n",
      "14607 Training Loss: tensor(0.0429)\n",
      "14608 Training Loss: tensor(0.0425)\n",
      "14609 Training Loss: tensor(0.0365)\n",
      "14610 Training Loss: tensor(0.0392)\n",
      "14611 Training Loss: tensor(0.0375)\n",
      "14612 Training Loss: tensor(0.0381)\n",
      "14613 Training Loss: tensor(0.0390)\n",
      "14614 Training Loss: tensor(0.0404)\n",
      "14615 Training Loss: tensor(0.0374)\n",
      "14616 Training Loss: tensor(0.0408)\n",
      "14617 Training Loss: tensor(0.0406)\n",
      "14618 Training Loss: tensor(0.0383)\n",
      "14619 Training Loss: tensor(0.0426)\n",
      "14620 Training Loss: tensor(0.0377)\n",
      "14621 Training Loss: tensor(0.0421)\n",
      "14622 Training Loss: tensor(0.0338)\n",
      "14623 Training Loss: tensor(0.0374)\n",
      "14624 Training Loss: tensor(0.0439)\n",
      "14625 Training Loss: tensor(0.0412)\n",
      "14626 Training Loss: tensor(0.0380)\n",
      "14627 Training Loss: tensor(0.0359)\n",
      "14628 Training Loss: tensor(0.0418)\n",
      "14629 Training Loss: tensor(0.0422)\n",
      "14630 Training Loss: tensor(0.0419)\n",
      "14631 Training Loss: tensor(0.0377)\n",
      "14632 Training Loss: tensor(0.0428)\n",
      "14633 Training Loss: tensor(0.0430)\n",
      "14634 Training Loss: tensor(0.0328)\n",
      "14635 Training Loss: tensor(0.0375)\n",
      "14636 Training Loss: tensor(0.0380)\n",
      "14637 Training Loss: tensor(0.0397)\n",
      "14638 Training Loss: tensor(0.0399)\n",
      "14639 Training Loss: tensor(0.0436)\n",
      "14640 Training Loss: tensor(0.0397)\n",
      "14641 Training Loss: tensor(0.0417)\n",
      "14642 Training Loss: tensor(0.0389)\n",
      "14643 Training Loss: tensor(0.0375)\n",
      "14644 Training Loss: tensor(0.0424)\n",
      "14645 Training Loss: tensor(0.0432)\n",
      "14646 Training Loss: tensor(0.0489)\n",
      "14647 Training Loss: tensor(0.0375)\n",
      "14648 Training Loss: tensor(0.0380)\n",
      "14649 Training Loss: tensor(0.0367)\n",
      "14650 Training Loss: tensor(0.0363)\n",
      "14651 Training Loss: tensor(0.0445)\n",
      "14652 Training Loss: tensor(0.0442)\n",
      "14653 Training Loss: tensor(0.0412)\n",
      "14654 Training Loss: tensor(0.0428)\n",
      "14655 Training Loss: tensor(0.0377)\n",
      "14656 Training Loss: tensor(0.0413)\n",
      "14657 Training Loss: tensor(0.0431)\n",
      "14658 Training Loss: tensor(0.0330)\n",
      "14659 Training Loss: tensor(0.0411)\n",
      "14660 Training Loss: tensor(0.0439)\n",
      "14661 Training Loss: tensor(0.0482)\n",
      "14662 Training Loss: tensor(0.0430)\n",
      "14663 Training Loss: tensor(0.0417)\n",
      "14664 Training Loss: tensor(0.0444)\n",
      "14665 Training Loss: tensor(0.0366)\n",
      "14666 Training Loss: tensor(0.0364)\n",
      "14667 Training Loss: tensor(0.0428)\n",
      "14668 Training Loss: tensor(0.0381)\n",
      "14669 Training Loss: tensor(0.0417)\n",
      "14670 Training Loss: tensor(0.0354)\n",
      "14671 Training Loss: tensor(0.0350)\n",
      "14672 Training Loss: tensor(0.0377)\n",
      "14673 Training Loss: tensor(0.0378)\n",
      "14674 Training Loss: tensor(0.0368)\n",
      "14675 Training Loss: tensor(0.0483)\n",
      "14676 Training Loss: tensor(0.0422)\n",
      "14677 Training Loss: tensor(0.0387)\n",
      "14678 Training Loss: tensor(0.0422)\n",
      "14679 Training Loss: tensor(0.0419)\n",
      "14680 Training Loss: tensor(0.0348)\n",
      "14681 Training Loss: tensor(0.0407)\n",
      "14682 Training Loss: tensor(0.0335)\n",
      "14683 Training Loss: tensor(0.0431)\n",
      "14684 Training Loss: tensor(0.0373)\n",
      "14685 Training Loss: tensor(0.0436)\n",
      "14686 Training Loss: tensor(0.0433)\n",
      "14687 Training Loss: tensor(0.0372)\n",
      "14688 Training Loss: tensor(0.0440)\n",
      "14689 Training Loss: tensor(0.0396)\n",
      "14690 Training Loss: tensor(0.0432)\n",
      "14691 Training Loss: tensor(0.0381)\n",
      "14692 Training Loss: tensor(0.0442)\n",
      "14693 Training Loss: tensor(0.0344)\n",
      "14694 Training Loss: tensor(0.0415)\n",
      "14695 Training Loss: tensor(0.0374)\n",
      "14696 Training Loss: tensor(0.0352)\n",
      "14697 Training Loss: tensor(0.0431)\n",
      "14698 Training Loss: tensor(0.0411)\n",
      "14699 Training Loss: tensor(0.0393)\n",
      "14700 Training Loss: tensor(0.0411)\n",
      "14701 Training Loss: tensor(0.0406)\n",
      "14702 Training Loss: tensor(0.0399)\n",
      "14703 Training Loss: tensor(0.0341)\n",
      "14704 Training Loss: tensor(0.0405)\n",
      "14705 Training Loss: tensor(0.0421)\n",
      "14706 Training Loss: tensor(0.0402)\n",
      "14707 Training Loss: tensor(0.0447)\n",
      "14708 Training Loss: tensor(0.0355)\n",
      "14709 Training Loss: tensor(0.0431)\n",
      "14710 Training Loss: tensor(0.0432)\n",
      "14711 Training Loss: tensor(0.0379)\n",
      "14712 Training Loss: tensor(0.0391)\n",
      "14713 Training Loss: tensor(0.0403)\n",
      "14714 Training Loss: tensor(0.0375)\n",
      "14715 Training Loss: tensor(0.0335)\n",
      "14716 Training Loss: tensor(0.0396)\n",
      "14717 Training Loss: tensor(0.0434)\n",
      "14718 Training Loss: tensor(0.0349)\n",
      "14719 Training Loss: tensor(0.0392)\n",
      "14720 Training Loss: tensor(0.0435)\n",
      "14721 Training Loss: tensor(0.0388)\n",
      "14722 Training Loss: tensor(0.0366)\n",
      "14723 Training Loss: tensor(0.0383)\n",
      "14724 Training Loss: tensor(0.0410)\n",
      "14725 Training Loss: tensor(0.0360)\n",
      "14726 Training Loss: tensor(0.0365)\n",
      "14727 Training Loss: tensor(0.0425)\n",
      "14728 Training Loss: tensor(0.0394)\n",
      "14729 Training Loss: tensor(0.0414)\n",
      "14730 Training Loss: tensor(0.0374)\n",
      "14731 Training Loss: tensor(0.0387)\n",
      "14732 Training Loss: tensor(0.0370)\n",
      "14733 Training Loss: tensor(0.0393)\n",
      "14734 Training Loss: tensor(0.0423)\n",
      "14735 Training Loss: tensor(0.0395)\n",
      "14736 Training Loss: tensor(0.0376)\n",
      "14737 Training Loss: tensor(0.0399)\n",
      "14738 Training Loss: tensor(0.0392)\n",
      "14739 Training Loss: tensor(0.0392)\n",
      "14740 Training Loss: tensor(0.0417)\n",
      "14741 Training Loss: tensor(0.0449)\n",
      "14742 Training Loss: tensor(0.0424)\n",
      "14743 Training Loss: tensor(0.0391)\n",
      "14744 Training Loss: tensor(0.0402)\n",
      "14745 Training Loss: tensor(0.0363)\n",
      "14746 Training Loss: tensor(0.0411)\n",
      "14747 Training Loss: tensor(0.0386)\n",
      "14748 Training Loss: tensor(0.0391)\n",
      "14749 Training Loss: tensor(0.0425)\n",
      "14750 Training Loss: tensor(0.0364)\n",
      "14751 Training Loss: tensor(0.0449)\n",
      "14752 Training Loss: tensor(0.0407)\n",
      "14753 Training Loss: tensor(0.0396)\n",
      "14754 Training Loss: tensor(0.0376)\n",
      "14755 Training Loss: tensor(0.0325)\n",
      "14756 Training Loss: tensor(0.0389)\n",
      "14757 Training Loss: tensor(0.0356)\n",
      "14758 Training Loss: tensor(0.0361)\n",
      "14759 Training Loss: tensor(0.0335)\n",
      "14760 Training Loss: tensor(0.0369)\n",
      "14761 Training Loss: tensor(0.0391)\n",
      "14762 Training Loss: tensor(0.0372)\n",
      "14763 Training Loss: tensor(0.0427)\n",
      "14764 Training Loss: tensor(0.0401)\n",
      "14765 Training Loss: tensor(0.0382)\n",
      "14766 Training Loss: tensor(0.0451)\n",
      "14767 Training Loss: tensor(0.0331)\n",
      "14768 Training Loss: tensor(0.0392)\n",
      "14769 Training Loss: tensor(0.0384)\n",
      "14770 Training Loss: tensor(0.0402)\n",
      "14771 Training Loss: tensor(0.0393)\n",
      "14772 Training Loss: tensor(0.0381)\n",
      "14773 Training Loss: tensor(0.0413)\n",
      "14774 Training Loss: tensor(0.0350)\n",
      "14775 Training Loss: tensor(0.0387)\n",
      "14776 Training Loss: tensor(0.0408)\n",
      "14777 Training Loss: tensor(0.0406)\n",
      "14778 Training Loss: tensor(0.0344)\n",
      "14779 Training Loss: tensor(0.0427)\n",
      "14780 Training Loss: tensor(0.0425)\n",
      "14781 Training Loss: tensor(0.0376)\n",
      "14782 Training Loss: tensor(0.0465)\n",
      "14783 Training Loss: tensor(0.0419)\n",
      "14784 Training Loss: tensor(0.0380)\n",
      "14785 Training Loss: tensor(0.0439)\n",
      "14786 Training Loss: tensor(0.0333)\n",
      "14787 Training Loss: tensor(0.0354)\n",
      "14788 Training Loss: tensor(0.0410)\n",
      "14789 Training Loss: tensor(0.0325)\n",
      "14790 Training Loss: tensor(0.0375)\n",
      "14791 Training Loss: tensor(0.0388)\n",
      "14792 Training Loss: tensor(0.0368)\n",
      "14793 Training Loss: tensor(0.0415)\n",
      "14794 Training Loss: tensor(0.0446)\n",
      "14795 Training Loss: tensor(0.0392)\n",
      "14796 Training Loss: tensor(0.0380)\n",
      "14797 Training Loss: tensor(0.0460)\n",
      "14798 Training Loss: tensor(0.0364)\n",
      "14799 Training Loss: tensor(0.0404)\n",
      "14800 Training Loss: tensor(0.0451)\n",
      "14801 Training Loss: tensor(0.0396)\n",
      "14802 Training Loss: tensor(0.0451)\n",
      "14803 Training Loss: tensor(0.0458)\n",
      "14804 Training Loss: tensor(0.0442)\n",
      "14805 Training Loss: tensor(0.0381)\n",
      "14806 Training Loss: tensor(0.0402)\n",
      "14807 Training Loss: tensor(0.0412)\n",
      "14808 Training Loss: tensor(0.0397)\n",
      "14809 Training Loss: tensor(0.0404)\n",
      "14810 Training Loss: tensor(0.0413)\n",
      "14811 Training Loss: tensor(0.0426)\n",
      "14812 Training Loss: tensor(0.0471)\n",
      "14813 Training Loss: tensor(0.0370)\n",
      "14814 Training Loss: tensor(0.0414)\n",
      "14815 Training Loss: tensor(0.0388)\n",
      "14816 Training Loss: tensor(0.0406)\n",
      "14817 Training Loss: tensor(0.0345)\n",
      "14818 Training Loss: tensor(0.0381)\n",
      "14819 Training Loss: tensor(0.0352)\n",
      "14820 Training Loss: tensor(0.0364)\n",
      "14821 Training Loss: tensor(0.0371)\n",
      "14822 Training Loss: tensor(0.0407)\n",
      "14823 Training Loss: tensor(0.0407)\n",
      "14824 Training Loss: tensor(0.0425)\n",
      "14825 Training Loss: tensor(0.0372)\n",
      "14826 Training Loss: tensor(0.0402)\n",
      "14827 Training Loss: tensor(0.0348)\n",
      "14828 Training Loss: tensor(0.0422)\n",
      "14829 Training Loss: tensor(0.0393)\n",
      "14830 Training Loss: tensor(0.0382)\n",
      "14831 Training Loss: tensor(0.0396)\n",
      "14832 Training Loss: tensor(0.0439)\n",
      "14833 Training Loss: tensor(0.0435)\n",
      "14834 Training Loss: tensor(0.0411)\n",
      "14835 Training Loss: tensor(0.0366)\n",
      "14836 Training Loss: tensor(0.0359)\n",
      "14837 Training Loss: tensor(0.0420)\n",
      "14838 Training Loss: tensor(0.0381)\n",
      "14839 Training Loss: tensor(0.0387)\n",
      "14840 Training Loss: tensor(0.0396)\n",
      "14841 Training Loss: tensor(0.0395)\n",
      "14842 Training Loss: tensor(0.0387)\n",
      "14843 Training Loss: tensor(0.0402)\n",
      "14844 Training Loss: tensor(0.0450)\n",
      "14845 Training Loss: tensor(0.0392)\n",
      "14846 Training Loss: tensor(0.0445)\n",
      "14847 Training Loss: tensor(0.0435)\n",
      "14848 Training Loss: tensor(0.0429)\n",
      "14849 Training Loss: tensor(0.0419)\n",
      "14850 Training Loss: tensor(0.0383)\n",
      "14851 Training Loss: tensor(0.0411)\n",
      "14852 Training Loss: tensor(0.0394)\n",
      "14853 Training Loss: tensor(0.0411)\n",
      "14854 Training Loss: tensor(0.0372)\n",
      "14855 Training Loss: tensor(0.0363)\n",
      "14856 Training Loss: tensor(0.0369)\n",
      "14857 Training Loss: tensor(0.0402)\n",
      "14858 Training Loss: tensor(0.0359)\n",
      "14859 Training Loss: tensor(0.0399)\n",
      "14860 Training Loss: tensor(0.0364)\n",
      "14861 Training Loss: tensor(0.0367)\n",
      "14862 Training Loss: tensor(0.0345)\n",
      "14863 Training Loss: tensor(0.0386)\n",
      "14864 Training Loss: tensor(0.0412)\n",
      "14865 Training Loss: tensor(0.0368)\n",
      "14866 Training Loss: tensor(0.0425)\n",
      "14867 Training Loss: tensor(0.0440)\n",
      "14868 Training Loss: tensor(0.0441)\n",
      "14869 Training Loss: tensor(0.0379)\n",
      "14870 Training Loss: tensor(0.0421)\n",
      "14871 Training Loss: tensor(0.0444)\n",
      "14872 Training Loss: tensor(0.0388)\n",
      "14873 Training Loss: tensor(0.0390)\n",
      "14874 Training Loss: tensor(0.0423)\n",
      "14875 Training Loss: tensor(0.0378)\n",
      "14876 Training Loss: tensor(0.0418)\n",
      "14877 Training Loss: tensor(0.0401)\n",
      "14878 Training Loss: tensor(0.0396)\n",
      "14879 Training Loss: tensor(0.0388)\n",
      "14880 Training Loss: tensor(0.0375)\n",
      "14881 Training Loss: tensor(0.0421)\n",
      "14882 Training Loss: tensor(0.0456)\n",
      "14883 Training Loss: tensor(0.0360)\n",
      "14884 Training Loss: tensor(0.0399)\n",
      "14885 Training Loss: tensor(0.0428)\n",
      "14886 Training Loss: tensor(0.0395)\n",
      "14887 Training Loss: tensor(0.0423)\n",
      "14888 Training Loss: tensor(0.0392)\n",
      "14889 Training Loss: tensor(0.0374)\n",
      "14890 Training Loss: tensor(0.0394)\n",
      "14891 Training Loss: tensor(0.0368)\n",
      "14892 Training Loss: tensor(0.0375)\n",
      "14893 Training Loss: tensor(0.0429)\n",
      "14894 Training Loss: tensor(0.0449)\n",
      "14895 Training Loss: tensor(0.0446)\n",
      "14896 Training Loss: tensor(0.0351)\n",
      "14897 Training Loss: tensor(0.0418)\n",
      "14898 Training Loss: tensor(0.0449)\n",
      "14899 Training Loss: tensor(0.0358)\n",
      "14900 Training Loss: tensor(0.0400)\n",
      "14901 Training Loss: tensor(0.0363)\n",
      "14902 Training Loss: tensor(0.0377)\n",
      "14903 Training Loss: tensor(0.0420)\n",
      "14904 Training Loss: tensor(0.0368)\n",
      "14905 Training Loss: tensor(0.0408)\n",
      "14906 Training Loss: tensor(0.0387)\n",
      "14907 Training Loss: tensor(0.0381)\n",
      "14908 Training Loss: tensor(0.0412)\n",
      "14909 Training Loss: tensor(0.0398)\n",
      "14910 Training Loss: tensor(0.0419)\n",
      "14911 Training Loss: tensor(0.0391)\n",
      "14912 Training Loss: tensor(0.0395)\n",
      "14913 Training Loss: tensor(0.0403)\n",
      "14914 Training Loss: tensor(0.0398)\n",
      "14915 Training Loss: tensor(0.0403)\n",
      "14916 Training Loss: tensor(0.0375)\n",
      "14917 Training Loss: tensor(0.0379)\n",
      "14918 Training Loss: tensor(0.0362)\n",
      "14919 Training Loss: tensor(0.0420)\n",
      "14920 Training Loss: tensor(0.0418)\n",
      "14921 Training Loss: tensor(0.0415)\n",
      "14922 Training Loss: tensor(0.0395)\n",
      "14923 Training Loss: tensor(0.0350)\n",
      "14924 Training Loss: tensor(0.0423)\n",
      "14925 Training Loss: tensor(0.0453)\n",
      "14926 Training Loss: tensor(0.0400)\n",
      "14927 Training Loss: tensor(0.0379)\n",
      "14928 Training Loss: tensor(0.0392)\n",
      "14929 Training Loss: tensor(0.0381)\n",
      "14930 Training Loss: tensor(0.0397)\n",
      "14931 Training Loss: tensor(0.0442)\n",
      "14932 Training Loss: tensor(0.0373)\n",
      "14933 Training Loss: tensor(0.0379)\n",
      "14934 Training Loss: tensor(0.0367)\n",
      "14935 Training Loss: tensor(0.0316)\n",
      "14936 Training Loss: tensor(0.0388)\n",
      "14937 Training Loss: tensor(0.0425)\n",
      "14938 Training Loss: tensor(0.0341)\n",
      "14939 Training Loss: tensor(0.0414)\n",
      "14940 Training Loss: tensor(0.0381)\n",
      "14941 Training Loss: tensor(0.0427)\n",
      "14942 Training Loss: tensor(0.0444)\n",
      "14943 Training Loss: tensor(0.0395)\n",
      "14944 Training Loss: tensor(0.0378)\n",
      "14945 Training Loss: tensor(0.0388)\n",
      "14946 Training Loss: tensor(0.0384)\n",
      "14947 Training Loss: tensor(0.0425)\n",
      "14948 Training Loss: tensor(0.0402)\n",
      "14949 Training Loss: tensor(0.0414)\n",
      "14950 Training Loss: tensor(0.0349)\n",
      "14951 Training Loss: tensor(0.0405)\n",
      "14952 Training Loss: tensor(0.0417)\n",
      "14953 Training Loss: tensor(0.0355)\n",
      "14954 Training Loss: tensor(0.0387)\n",
      "14955 Training Loss: tensor(0.0366)\n",
      "14956 Training Loss: tensor(0.0310)\n",
      "14957 Training Loss: tensor(0.0354)\n",
      "14958 Training Loss: tensor(0.0363)\n",
      "14959 Training Loss: tensor(0.0396)\n",
      "14960 Training Loss: tensor(0.0431)\n",
      "14961 Training Loss: tensor(0.0359)\n",
      "14962 Training Loss: tensor(0.0427)\n",
      "14963 Training Loss: tensor(0.0435)\n",
      "14964 Training Loss: tensor(0.0346)\n",
      "14965 Training Loss: tensor(0.0379)\n",
      "14966 Training Loss: tensor(0.0483)\n",
      "14967 Training Loss: tensor(0.0385)\n",
      "14968 Training Loss: tensor(0.0500)\n",
      "14969 Training Loss: tensor(0.0423)\n",
      "14970 Training Loss: tensor(0.0388)\n",
      "14971 Training Loss: tensor(0.0421)\n",
      "14972 Training Loss: tensor(0.0416)\n",
      "14973 Training Loss: tensor(0.0409)\n",
      "14974 Training Loss: tensor(0.0464)\n",
      "14975 Training Loss: tensor(0.0432)\n",
      "14976 Training Loss: tensor(0.0363)\n",
      "14977 Training Loss: tensor(0.0484)\n",
      "14978 Training Loss: tensor(0.0394)\n",
      "14979 Training Loss: tensor(0.0371)\n",
      "14980 Training Loss: tensor(0.0377)\n",
      "14981 Training Loss: tensor(0.0405)\n",
      "14982 Training Loss: tensor(0.0368)\n",
      "14983 Training Loss: tensor(0.0399)\n",
      "14984 Training Loss: tensor(0.0400)\n",
      "14985 Training Loss: tensor(0.0426)\n",
      "14986 Training Loss: tensor(0.0378)\n",
      "14987 Training Loss: tensor(0.0428)\n",
      "14988 Training Loss: tensor(0.0371)\n",
      "14989 Training Loss: tensor(0.0407)\n",
      "14990 Training Loss: tensor(0.0423)\n",
      "14991 Training Loss: tensor(0.0420)\n",
      "14992 Training Loss: tensor(0.0389)\n",
      "14993 Training Loss: tensor(0.0378)\n",
      "14994 Training Loss: tensor(0.0403)\n",
      "14995 Training Loss: tensor(0.0405)\n",
      "14996 Training Loss: tensor(0.0387)\n",
      "14997 Training Loss: tensor(0.0374)\n",
      "14998 Training Loss: tensor(0.0401)\n",
      "14999 Training Loss: tensor(0.0376)\n",
      "15000 Training Loss: tensor(0.0376)\n",
      "15001 Training Loss: tensor(0.0405)\n",
      "15002 Training Loss: tensor(0.0389)\n",
      "15003 Training Loss: tensor(0.0420)\n",
      "15004 Training Loss: tensor(0.0408)\n",
      "15005 Training Loss: tensor(0.0356)\n",
      "15006 Training Loss: tensor(0.0419)\n",
      "15007 Training Loss: tensor(0.0460)\n",
      "15008 Training Loss: tensor(0.0398)\n",
      "15009 Training Loss: tensor(0.0397)\n",
      "15010 Training Loss: tensor(0.0401)\n",
      "15011 Training Loss: tensor(0.0375)\n",
      "15012 Training Loss: tensor(0.0408)\n",
      "15013 Training Loss: tensor(0.0348)\n",
      "15014 Training Loss: tensor(0.0423)\n",
      "15015 Training Loss: tensor(0.0374)\n",
      "15016 Training Loss: tensor(0.0444)\n",
      "15017 Training Loss: tensor(0.0438)\n",
      "15018 Training Loss: tensor(0.0411)\n",
      "15019 Training Loss: tensor(0.0406)\n",
      "15020 Training Loss: tensor(0.0396)\n",
      "15021 Training Loss: tensor(0.0430)\n",
      "15022 Training Loss: tensor(0.0372)\n",
      "15023 Training Loss: tensor(0.0392)\n",
      "15024 Training Loss: tensor(0.0383)\n",
      "15025 Training Loss: tensor(0.0399)\n",
      "15026 Training Loss: tensor(0.0401)\n",
      "15027 Training Loss: tensor(0.0367)\n",
      "15028 Training Loss: tensor(0.0398)\n",
      "15029 Training Loss: tensor(0.0312)\n",
      "15030 Training Loss: tensor(0.0413)\n",
      "15031 Training Loss: tensor(0.0403)\n",
      "15032 Training Loss: tensor(0.0344)\n",
      "15033 Training Loss: tensor(0.0336)\n",
      "15034 Training Loss: tensor(0.0422)\n",
      "15035 Training Loss: tensor(0.0439)\n",
      "15036 Training Loss: tensor(0.0429)\n",
      "15037 Training Loss: tensor(0.0397)\n",
      "15038 Training Loss: tensor(0.0371)\n",
      "15039 Training Loss: tensor(0.0413)\n",
      "15040 Training Loss: tensor(0.0410)\n",
      "15041 Training Loss: tensor(0.0408)\n",
      "15042 Training Loss: tensor(0.0471)\n",
      "15043 Training Loss: tensor(0.0394)\n",
      "15044 Training Loss: tensor(0.0382)\n",
      "15045 Training Loss: tensor(0.0400)\n",
      "15046 Training Loss: tensor(0.0374)\n",
      "15047 Training Loss: tensor(0.0346)\n",
      "15048 Training Loss: tensor(0.0359)\n",
      "15049 Training Loss: tensor(0.0373)\n",
      "15050 Training Loss: tensor(0.0388)\n",
      "15051 Training Loss: tensor(0.0458)\n",
      "15052 Training Loss: tensor(0.0429)\n",
      "15053 Training Loss: tensor(0.0420)\n",
      "15054 Training Loss: tensor(0.0401)\n",
      "15055 Training Loss: tensor(0.0397)\n",
      "15056 Training Loss: tensor(0.0428)\n",
      "15057 Training Loss: tensor(0.0364)\n",
      "15058 Training Loss: tensor(0.0376)\n",
      "15059 Training Loss: tensor(0.0359)\n",
      "15060 Training Loss: tensor(0.0386)\n",
      "15061 Training Loss: tensor(0.0403)\n",
      "15062 Training Loss: tensor(0.0340)\n",
      "15063 Training Loss: tensor(0.0408)\n",
      "15064 Training Loss: tensor(0.0386)\n",
      "15065 Training Loss: tensor(0.0357)\n",
      "15066 Training Loss: tensor(0.0365)\n",
      "15067 Training Loss: tensor(0.0401)\n",
      "15068 Training Loss: tensor(0.0400)\n",
      "15069 Training Loss: tensor(0.0413)\n",
      "15070 Training Loss: tensor(0.0418)\n",
      "15071 Training Loss: tensor(0.0364)\n",
      "15072 Training Loss: tensor(0.0396)\n",
      "15073 Training Loss: tensor(0.0346)\n",
      "15074 Training Loss: tensor(0.0367)\n",
      "15075 Training Loss: tensor(0.0368)\n",
      "15076 Training Loss: tensor(0.0360)\n",
      "15077 Training Loss: tensor(0.0452)\n",
      "15078 Training Loss: tensor(0.0386)\n",
      "15079 Training Loss: tensor(0.0364)\n",
      "15080 Training Loss: tensor(0.0465)\n",
      "15081 Training Loss: tensor(0.0407)\n",
      "15082 Training Loss: tensor(0.0356)\n",
      "15083 Training Loss: tensor(0.0431)\n",
      "15084 Training Loss: tensor(0.0408)\n",
      "15085 Training Loss: tensor(0.0407)\n",
      "15086 Training Loss: tensor(0.0384)\n",
      "15087 Training Loss: tensor(0.0364)\n",
      "15088 Training Loss: tensor(0.0414)\n",
      "15089 Training Loss: tensor(0.0415)\n",
      "15090 Training Loss: tensor(0.0381)\n",
      "15091 Training Loss: tensor(0.0392)\n",
      "15092 Training Loss: tensor(0.0354)\n",
      "15093 Training Loss: tensor(0.0381)\n",
      "15094 Training Loss: tensor(0.0403)\n",
      "15095 Training Loss: tensor(0.0342)\n",
      "15096 Training Loss: tensor(0.0397)\n",
      "15097 Training Loss: tensor(0.0380)\n",
      "15098 Training Loss: tensor(0.0367)\n",
      "15099 Training Loss: tensor(0.0384)\n",
      "15100 Training Loss: tensor(0.0410)\n",
      "15101 Training Loss: tensor(0.0343)\n",
      "15102 Training Loss: tensor(0.0396)\n",
      "15103 Training Loss: tensor(0.0373)\n",
      "15104 Training Loss: tensor(0.0415)\n",
      "15105 Training Loss: tensor(0.0461)\n",
      "15106 Training Loss: tensor(0.0418)\n",
      "15107 Training Loss: tensor(0.0410)\n",
      "15108 Training Loss: tensor(0.0401)\n",
      "15109 Training Loss: tensor(0.0386)\n",
      "15110 Training Loss: tensor(0.0375)\n",
      "15111 Training Loss: tensor(0.0385)\n",
      "15112 Training Loss: tensor(0.0402)\n",
      "15113 Training Loss: tensor(0.0354)\n",
      "15114 Training Loss: tensor(0.0409)\n",
      "15115 Training Loss: tensor(0.0374)\n",
      "15116 Training Loss: tensor(0.0355)\n",
      "15117 Training Loss: tensor(0.0386)\n",
      "15118 Training Loss: tensor(0.0437)\n",
      "15119 Training Loss: tensor(0.0397)\n",
      "15120 Training Loss: tensor(0.0340)\n",
      "15121 Training Loss: tensor(0.0373)\n",
      "15122 Training Loss: tensor(0.0383)\n",
      "15123 Training Loss: tensor(0.0371)\n",
      "15124 Training Loss: tensor(0.0384)\n",
      "15125 Training Loss: tensor(0.0383)\n",
      "15126 Training Loss: tensor(0.0417)\n",
      "15127 Training Loss: tensor(0.0396)\n",
      "15128 Training Loss: tensor(0.0374)\n",
      "15129 Training Loss: tensor(0.0431)\n",
      "15130 Training Loss: tensor(0.0424)\n",
      "15131 Training Loss: tensor(0.0390)\n",
      "15132 Training Loss: tensor(0.0449)\n",
      "15133 Training Loss: tensor(0.0372)\n",
      "15134 Training Loss: tensor(0.0367)\n",
      "15135 Training Loss: tensor(0.0474)\n",
      "15136 Training Loss: tensor(0.0371)\n",
      "15137 Training Loss: tensor(0.0378)\n",
      "15138 Training Loss: tensor(0.0421)\n",
      "15139 Training Loss: tensor(0.0398)\n",
      "15140 Training Loss: tensor(0.0385)\n",
      "15141 Training Loss: tensor(0.0402)\n",
      "15142 Training Loss: tensor(0.0357)\n",
      "15143 Training Loss: tensor(0.0383)\n",
      "15144 Training Loss: tensor(0.0410)\n",
      "15145 Training Loss: tensor(0.0385)\n",
      "15146 Training Loss: tensor(0.0414)\n",
      "15147 Training Loss: tensor(0.0361)\n",
      "15148 Training Loss: tensor(0.0408)\n",
      "15149 Training Loss: tensor(0.0389)\n",
      "15150 Training Loss: tensor(0.0397)\n",
      "15151 Training Loss: tensor(0.0389)\n",
      "15152 Training Loss: tensor(0.0367)\n",
      "15153 Training Loss: tensor(0.0422)\n",
      "15154 Training Loss: tensor(0.0396)\n",
      "15155 Training Loss: tensor(0.0405)\n",
      "15156 Training Loss: tensor(0.0405)\n",
      "15157 Training Loss: tensor(0.0389)\n",
      "15158 Training Loss: tensor(0.0370)\n",
      "15159 Training Loss: tensor(0.0329)\n",
      "15160 Training Loss: tensor(0.0387)\n",
      "15161 Training Loss: tensor(0.0404)\n",
      "15162 Training Loss: tensor(0.0399)\n",
      "15163 Training Loss: tensor(0.0402)\n",
      "15164 Training Loss: tensor(0.0352)\n",
      "15165 Training Loss: tensor(0.0391)\n",
      "15166 Training Loss: tensor(0.0404)\n",
      "15167 Training Loss: tensor(0.0475)\n",
      "15168 Training Loss: tensor(0.0413)\n",
      "15169 Training Loss: tensor(0.0373)\n",
      "15170 Training Loss: tensor(0.0422)\n",
      "15171 Training Loss: tensor(0.0410)\n",
      "15172 Training Loss: tensor(0.0347)\n",
      "15173 Training Loss: tensor(0.0476)\n",
      "15174 Training Loss: tensor(0.0398)\n",
      "15175 Training Loss: tensor(0.0451)\n",
      "15176 Training Loss: tensor(0.0425)\n",
      "15177 Training Loss: tensor(0.0388)\n",
      "15178 Training Loss: tensor(0.0388)\n",
      "15179 Training Loss: tensor(0.0372)\n",
      "15180 Training Loss: tensor(0.0432)\n",
      "15181 Training Loss: tensor(0.0426)\n",
      "15182 Training Loss: tensor(0.0425)\n",
      "15183 Training Loss: tensor(0.0319)\n",
      "15184 Training Loss: tensor(0.0445)\n",
      "15185 Training Loss: tensor(0.0403)\n",
      "15186 Training Loss: tensor(0.0323)\n",
      "15187 Training Loss: tensor(0.0402)\n",
      "15188 Training Loss: tensor(0.0384)\n",
      "15189 Training Loss: tensor(0.0436)\n",
      "15190 Training Loss: tensor(0.0419)\n",
      "15191 Training Loss: tensor(0.0386)\n",
      "15192 Training Loss: tensor(0.0422)\n",
      "15193 Training Loss: tensor(0.0398)\n",
      "15194 Training Loss: tensor(0.0403)\n",
      "15195 Training Loss: tensor(0.0403)\n",
      "15196 Training Loss: tensor(0.0375)\n",
      "15197 Training Loss: tensor(0.0357)\n",
      "15198 Training Loss: tensor(0.0339)\n",
      "15199 Training Loss: tensor(0.0473)\n",
      "15200 Training Loss: tensor(0.0380)\n",
      "15201 Training Loss: tensor(0.0343)\n",
      "15202 Training Loss: tensor(0.0408)\n",
      "15203 Training Loss: tensor(0.0395)\n",
      "15204 Training Loss: tensor(0.0454)\n",
      "15205 Training Loss: tensor(0.0404)\n",
      "15206 Training Loss: tensor(0.0455)\n",
      "15207 Training Loss: tensor(0.0394)\n",
      "15208 Training Loss: tensor(0.0423)\n",
      "15209 Training Loss: tensor(0.0434)\n",
      "15210 Training Loss: tensor(0.0426)\n",
      "15211 Training Loss: tensor(0.0367)\n",
      "15212 Training Loss: tensor(0.0347)\n",
      "15213 Training Loss: tensor(0.0442)\n",
      "15214 Training Loss: tensor(0.0453)\n",
      "15215 Training Loss: tensor(0.0355)\n",
      "15216 Training Loss: tensor(0.0374)\n",
      "15217 Training Loss: tensor(0.0310)\n",
      "15218 Training Loss: tensor(0.0387)\n",
      "15219 Training Loss: tensor(0.0386)\n",
      "15220 Training Loss: tensor(0.0444)\n",
      "15221 Training Loss: tensor(0.0400)\n",
      "15222 Training Loss: tensor(0.0417)\n",
      "15223 Training Loss: tensor(0.0366)\n",
      "15224 Training Loss: tensor(0.0389)\n",
      "15225 Training Loss: tensor(0.0435)\n",
      "15226 Training Loss: tensor(0.0341)\n",
      "15227 Training Loss: tensor(0.0386)\n",
      "15228 Training Loss: tensor(0.0421)\n",
      "15229 Training Loss: tensor(0.0350)\n",
      "15230 Training Loss: tensor(0.0371)\n",
      "15231 Training Loss: tensor(0.0386)\n",
      "15232 Training Loss: tensor(0.0425)\n",
      "15233 Training Loss: tensor(0.0363)\n",
      "15234 Training Loss: tensor(0.0356)\n",
      "15235 Training Loss: tensor(0.0392)\n",
      "15236 Training Loss: tensor(0.0390)\n",
      "15237 Training Loss: tensor(0.0368)\n",
      "15238 Training Loss: tensor(0.0393)\n",
      "15239 Training Loss: tensor(0.0416)\n",
      "15240 Training Loss: tensor(0.0377)\n",
      "15241 Training Loss: tensor(0.0372)\n",
      "15242 Training Loss: tensor(0.0446)\n",
      "15243 Training Loss: tensor(0.0378)\n",
      "15244 Training Loss: tensor(0.0405)\n",
      "15245 Training Loss: tensor(0.0382)\n",
      "15246 Training Loss: tensor(0.0459)\n",
      "15247 Training Loss: tensor(0.0434)\n",
      "15248 Training Loss: tensor(0.0386)\n",
      "15249 Training Loss: tensor(0.0414)\n",
      "15250 Training Loss: tensor(0.0389)\n",
      "15251 Training Loss: tensor(0.0389)\n",
      "15252 Training Loss: tensor(0.0414)\n",
      "15253 Training Loss: tensor(0.0426)\n",
      "15254 Training Loss: tensor(0.0404)\n",
      "15255 Training Loss: tensor(0.0351)\n",
      "15256 Training Loss: tensor(0.0448)\n",
      "15257 Training Loss: tensor(0.0466)\n",
      "15258 Training Loss: tensor(0.0376)\n",
      "15259 Training Loss: tensor(0.0332)\n",
      "15260 Training Loss: tensor(0.0406)\n",
      "15261 Training Loss: tensor(0.0400)\n",
      "15262 Training Loss: tensor(0.0429)\n",
      "15263 Training Loss: tensor(0.0387)\n",
      "15264 Training Loss: tensor(0.0360)\n",
      "15265 Training Loss: tensor(0.0454)\n",
      "15266 Training Loss: tensor(0.0398)\n",
      "15267 Training Loss: tensor(0.0368)\n",
      "15268 Training Loss: tensor(0.0373)\n",
      "15269 Training Loss: tensor(0.0402)\n",
      "15270 Training Loss: tensor(0.0438)\n",
      "15271 Training Loss: tensor(0.0360)\n",
      "15272 Training Loss: tensor(0.0357)\n",
      "15273 Training Loss: tensor(0.0428)\n",
      "15274 Training Loss: tensor(0.0397)\n",
      "15275 Training Loss: tensor(0.0399)\n",
      "15276 Training Loss: tensor(0.0436)\n",
      "15277 Training Loss: tensor(0.0415)\n",
      "15278 Training Loss: tensor(0.0402)\n",
      "15279 Training Loss: tensor(0.0388)\n",
      "15280 Training Loss: tensor(0.0392)\n",
      "15281 Training Loss: tensor(0.0411)\n",
      "15282 Training Loss: tensor(0.0420)\n",
      "15283 Training Loss: tensor(0.0394)\n",
      "15284 Training Loss: tensor(0.0362)\n",
      "15285 Training Loss: tensor(0.0346)\n",
      "15286 Training Loss: tensor(0.0396)\n",
      "15287 Training Loss: tensor(0.0387)\n",
      "15288 Training Loss: tensor(0.0355)\n",
      "15289 Training Loss: tensor(0.0371)\n",
      "15290 Training Loss: tensor(0.0396)\n",
      "15291 Training Loss: tensor(0.0423)\n",
      "15292 Training Loss: tensor(0.0380)\n",
      "15293 Training Loss: tensor(0.0398)\n",
      "15294 Training Loss: tensor(0.0359)\n",
      "15295 Training Loss: tensor(0.0413)\n",
      "15296 Training Loss: tensor(0.0388)\n",
      "15297 Training Loss: tensor(0.0411)\n",
      "15298 Training Loss: tensor(0.0349)\n",
      "15299 Training Loss: tensor(0.0396)\n",
      "15300 Training Loss: tensor(0.0403)\n",
      "15301 Training Loss: tensor(0.0408)\n",
      "15302 Training Loss: tensor(0.0414)\n",
      "15303 Training Loss: tensor(0.0395)\n",
      "15304 Training Loss: tensor(0.0406)\n",
      "15305 Training Loss: tensor(0.0389)\n",
      "15306 Training Loss: tensor(0.0390)\n",
      "15307 Training Loss: tensor(0.0342)\n",
      "15308 Training Loss: tensor(0.0365)\n",
      "15309 Training Loss: tensor(0.0406)\n",
      "15310 Training Loss: tensor(0.0392)\n",
      "15311 Training Loss: tensor(0.0386)\n",
      "15312 Training Loss: tensor(0.0415)\n",
      "15313 Training Loss: tensor(0.0356)\n",
      "15314 Training Loss: tensor(0.0375)\n",
      "15315 Training Loss: tensor(0.0403)\n",
      "15316 Training Loss: tensor(0.0422)\n",
      "15317 Training Loss: tensor(0.0376)\n",
      "15318 Training Loss: tensor(0.0384)\n",
      "15319 Training Loss: tensor(0.0365)\n",
      "15320 Training Loss: tensor(0.0402)\n",
      "15321 Training Loss: tensor(0.0376)\n",
      "15322 Training Loss: tensor(0.0362)\n",
      "15323 Training Loss: tensor(0.0458)\n",
      "15324 Training Loss: tensor(0.0316)\n",
      "15325 Training Loss: tensor(0.0398)\n",
      "15326 Training Loss: tensor(0.0455)\n",
      "15327 Training Loss: tensor(0.0386)\n",
      "15328 Training Loss: tensor(0.0372)\n",
      "15329 Training Loss: tensor(0.0405)\n",
      "15330 Training Loss: tensor(0.0402)\n",
      "15331 Training Loss: tensor(0.0353)\n",
      "15332 Training Loss: tensor(0.0345)\n",
      "15333 Training Loss: tensor(0.0414)\n",
      "15334 Training Loss: tensor(0.0361)\n",
      "15335 Training Loss: tensor(0.0417)\n",
      "15336 Training Loss: tensor(0.0377)\n",
      "15337 Training Loss: tensor(0.0350)\n",
      "15338 Training Loss: tensor(0.0370)\n",
      "15339 Training Loss: tensor(0.0370)\n",
      "15340 Training Loss: tensor(0.0420)\n",
      "15341 Training Loss: tensor(0.0346)\n",
      "15342 Training Loss: tensor(0.0400)\n",
      "15343 Training Loss: tensor(0.0391)\n",
      "15344 Training Loss: tensor(0.0379)\n",
      "15345 Training Loss: tensor(0.0382)\n",
      "15346 Training Loss: tensor(0.0421)\n",
      "15347 Training Loss: tensor(0.0377)\n",
      "15348 Training Loss: tensor(0.0381)\n",
      "15349 Training Loss: tensor(0.0336)\n",
      "15350 Training Loss: tensor(0.0419)\n",
      "15351 Training Loss: tensor(0.0361)\n",
      "15352 Training Loss: tensor(0.0419)\n",
      "15353 Training Loss: tensor(0.0354)\n",
      "15354 Training Loss: tensor(0.0382)\n",
      "15355 Training Loss: tensor(0.0358)\n",
      "15356 Training Loss: tensor(0.0387)\n",
      "15357 Training Loss: tensor(0.0457)\n",
      "15358 Training Loss: tensor(0.0424)\n",
      "15359 Training Loss: tensor(0.0419)\n",
      "15360 Training Loss: tensor(0.0392)\n",
      "15361 Training Loss: tensor(0.0381)\n",
      "15362 Training Loss: tensor(0.0413)\n",
      "15363 Training Loss: tensor(0.0400)\n",
      "15364 Training Loss: tensor(0.0425)\n",
      "15365 Training Loss: tensor(0.0431)\n",
      "15366 Training Loss: tensor(0.0393)\n",
      "15367 Training Loss: tensor(0.0416)\n",
      "15368 Training Loss: tensor(0.0414)\n",
      "15369 Training Loss: tensor(0.0348)\n",
      "15370 Training Loss: tensor(0.0352)\n",
      "15371 Training Loss: tensor(0.0370)\n",
      "15372 Training Loss: tensor(0.0431)\n",
      "15373 Training Loss: tensor(0.0418)\n",
      "15374 Training Loss: tensor(0.0383)\n",
      "15375 Training Loss: tensor(0.0393)\n",
      "15376 Training Loss: tensor(0.0452)\n",
      "15377 Training Loss: tensor(0.0392)\n",
      "15378 Training Loss: tensor(0.0430)\n",
      "15379 Training Loss: tensor(0.0376)\n",
      "15380 Training Loss: tensor(0.0376)\n",
      "15381 Training Loss: tensor(0.0372)\n",
      "15382 Training Loss: tensor(0.0411)\n",
      "15383 Training Loss: tensor(0.0415)\n",
      "15384 Training Loss: tensor(0.0422)\n",
      "15385 Training Loss: tensor(0.0489)\n",
      "15386 Training Loss: tensor(0.0373)\n",
      "15387 Training Loss: tensor(0.0388)\n",
      "15388 Training Loss: tensor(0.0431)\n",
      "15389 Training Loss: tensor(0.0414)\n",
      "15390 Training Loss: tensor(0.0409)\n",
      "15391 Training Loss: tensor(0.0457)\n",
      "15392 Training Loss: tensor(0.0407)\n",
      "15393 Training Loss: tensor(0.0366)\n",
      "15394 Training Loss: tensor(0.0413)\n",
      "15395 Training Loss: tensor(0.0386)\n",
      "15396 Training Loss: tensor(0.0461)\n",
      "15397 Training Loss: tensor(0.0410)\n",
      "15398 Training Loss: tensor(0.0411)\n",
      "15399 Training Loss: tensor(0.0445)\n",
      "15400 Training Loss: tensor(0.0329)\n",
      "15401 Training Loss: tensor(0.0398)\n",
      "15402 Training Loss: tensor(0.0408)\n",
      "15403 Training Loss: tensor(0.0385)\n",
      "15404 Training Loss: tensor(0.0418)\n",
      "15405 Training Loss: tensor(0.0416)\n",
      "15406 Training Loss: tensor(0.0356)\n",
      "15407 Training Loss: tensor(0.0399)\n",
      "15408 Training Loss: tensor(0.0391)\n",
      "15409 Training Loss: tensor(0.0333)\n",
      "15410 Training Loss: tensor(0.0379)\n",
      "15411 Training Loss: tensor(0.0408)\n",
      "15412 Training Loss: tensor(0.0375)\n",
      "15413 Training Loss: tensor(0.0390)\n",
      "15414 Training Loss: tensor(0.0398)\n",
      "15415 Training Loss: tensor(0.0396)\n",
      "15416 Training Loss: tensor(0.0388)\n",
      "15417 Training Loss: tensor(0.0436)\n",
      "15418 Training Loss: tensor(0.0414)\n",
      "15419 Training Loss: tensor(0.0356)\n",
      "15420 Training Loss: tensor(0.0486)\n",
      "15421 Training Loss: tensor(0.0461)\n",
      "15422 Training Loss: tensor(0.0374)\n",
      "15423 Training Loss: tensor(0.0408)\n",
      "15424 Training Loss: tensor(0.0396)\n",
      "15425 Training Loss: tensor(0.0393)\n",
      "15426 Training Loss: tensor(0.0401)\n",
      "15427 Training Loss: tensor(0.0371)\n",
      "15428 Training Loss: tensor(0.0404)\n",
      "15429 Training Loss: tensor(0.0398)\n",
      "15430 Training Loss: tensor(0.0351)\n",
      "15431 Training Loss: tensor(0.0391)\n",
      "15432 Training Loss: tensor(0.0412)\n",
      "15433 Training Loss: tensor(0.0359)\n",
      "15434 Training Loss: tensor(0.0421)\n",
      "15435 Training Loss: tensor(0.0390)\n",
      "15436 Training Loss: tensor(0.0376)\n",
      "15437 Training Loss: tensor(0.0493)\n",
      "15438 Training Loss: tensor(0.0424)\n",
      "15439 Training Loss: tensor(0.0380)\n",
      "15440 Training Loss: tensor(0.0420)\n",
      "15441 Training Loss: tensor(0.0406)\n",
      "15442 Training Loss: tensor(0.0366)\n",
      "15443 Training Loss: tensor(0.0476)\n",
      "15444 Training Loss: tensor(0.0391)\n",
      "15445 Training Loss: tensor(0.0442)\n",
      "15446 Training Loss: tensor(0.0397)\n",
      "15447 Training Loss: tensor(0.0376)\n",
      "15448 Training Loss: tensor(0.0419)\n",
      "15449 Training Loss: tensor(0.0405)\n",
      "15450 Training Loss: tensor(0.0358)\n",
      "15451 Training Loss: tensor(0.0426)\n",
      "15452 Training Loss: tensor(0.0413)\n",
      "15453 Training Loss: tensor(0.0388)\n",
      "15454 Training Loss: tensor(0.0356)\n",
      "15455 Training Loss: tensor(0.0371)\n",
      "15456 Training Loss: tensor(0.0391)\n",
      "15457 Training Loss: tensor(0.0375)\n",
      "15458 Training Loss: tensor(0.0391)\n",
      "15459 Training Loss: tensor(0.0399)\n",
      "15460 Training Loss: tensor(0.0405)\n",
      "15461 Training Loss: tensor(0.0417)\n",
      "15462 Training Loss: tensor(0.0375)\n",
      "15463 Training Loss: tensor(0.0383)\n",
      "15464 Training Loss: tensor(0.0420)\n",
      "15465 Training Loss: tensor(0.0385)\n",
      "15466 Training Loss: tensor(0.0403)\n",
      "15467 Training Loss: tensor(0.0402)\n",
      "15468 Training Loss: tensor(0.0415)\n",
      "15469 Training Loss: tensor(0.0396)\n",
      "15470 Training Loss: tensor(0.0353)\n",
      "15471 Training Loss: tensor(0.0444)\n",
      "15472 Training Loss: tensor(0.0342)\n",
      "15473 Training Loss: tensor(0.0310)\n",
      "15474 Training Loss: tensor(0.0415)\n",
      "15475 Training Loss: tensor(0.0351)\n",
      "15476 Training Loss: tensor(0.0369)\n",
      "15477 Training Loss: tensor(0.0402)\n",
      "15478 Training Loss: tensor(0.0394)\n",
      "15479 Training Loss: tensor(0.0346)\n",
      "15480 Training Loss: tensor(0.0339)\n",
      "15481 Training Loss: tensor(0.0372)\n",
      "15482 Training Loss: tensor(0.0338)\n",
      "15483 Training Loss: tensor(0.0366)\n",
      "15484 Training Loss: tensor(0.0391)\n",
      "15485 Training Loss: tensor(0.0391)\n",
      "15486 Training Loss: tensor(0.0494)\n",
      "15487 Training Loss: tensor(0.0406)\n",
      "15488 Training Loss: tensor(0.0389)\n",
      "15489 Training Loss: tensor(0.0388)\n",
      "15490 Training Loss: tensor(0.0392)\n",
      "15491 Training Loss: tensor(0.0401)\n",
      "15492 Training Loss: tensor(0.0371)\n",
      "15493 Training Loss: tensor(0.0379)\n",
      "15494 Training Loss: tensor(0.0432)\n",
      "15495 Training Loss: tensor(0.0391)\n",
      "15496 Training Loss: tensor(0.0450)\n",
      "15497 Training Loss: tensor(0.0388)\n",
      "15498 Training Loss: tensor(0.0358)\n",
      "15499 Training Loss: tensor(0.0408)\n",
      "15500 Training Loss: tensor(0.0411)\n",
      "15501 Training Loss: tensor(0.0373)\n",
      "15502 Training Loss: tensor(0.0365)\n",
      "15503 Training Loss: tensor(0.0352)\n",
      "15504 Training Loss: tensor(0.0384)\n",
      "15505 Training Loss: tensor(0.0382)\n",
      "15506 Training Loss: tensor(0.0370)\n",
      "15507 Training Loss: tensor(0.0438)\n",
      "15508 Training Loss: tensor(0.0413)\n",
      "15509 Training Loss: tensor(0.0372)\n",
      "15510 Training Loss: tensor(0.0396)\n",
      "15511 Training Loss: tensor(0.0389)\n",
      "15512 Training Loss: tensor(0.0324)\n",
      "15513 Training Loss: tensor(0.0340)\n",
      "15514 Training Loss: tensor(0.0343)\n",
      "15515 Training Loss: tensor(0.0381)\n",
      "15516 Training Loss: tensor(0.0337)\n",
      "15517 Training Loss: tensor(0.0352)\n",
      "15518 Training Loss: tensor(0.0381)\n",
      "15519 Training Loss: tensor(0.0390)\n",
      "15520 Training Loss: tensor(0.0384)\n",
      "15521 Training Loss: tensor(0.0374)\n",
      "15522 Training Loss: tensor(0.0364)\n",
      "15523 Training Loss: tensor(0.0435)\n",
      "15524 Training Loss: tensor(0.0338)\n",
      "15525 Training Loss: tensor(0.0365)\n",
      "15526 Training Loss: tensor(0.0355)\n",
      "15527 Training Loss: tensor(0.0392)\n",
      "15528 Training Loss: tensor(0.0442)\n",
      "15529 Training Loss: tensor(0.0402)\n",
      "15530 Training Loss: tensor(0.0386)\n",
      "15531 Training Loss: tensor(0.0414)\n",
      "15532 Training Loss: tensor(0.0373)\n",
      "15533 Training Loss: tensor(0.0425)\n",
      "15534 Training Loss: tensor(0.0380)\n",
      "15535 Training Loss: tensor(0.0376)\n",
      "15536 Training Loss: tensor(0.0406)\n",
      "15537 Training Loss: tensor(0.0402)\n",
      "15538 Training Loss: tensor(0.0352)\n",
      "15539 Training Loss: tensor(0.0351)\n",
      "15540 Training Loss: tensor(0.0395)\n",
      "15541 Training Loss: tensor(0.0376)\n",
      "15542 Training Loss: tensor(0.0412)\n",
      "15543 Training Loss: tensor(0.0427)\n",
      "15544 Training Loss: tensor(0.0387)\n",
      "15545 Training Loss: tensor(0.0450)\n",
      "15546 Training Loss: tensor(0.0430)\n",
      "15547 Training Loss: tensor(0.0390)\n",
      "15548 Training Loss: tensor(0.0383)\n",
      "15549 Training Loss: tensor(0.0369)\n",
      "15550 Training Loss: tensor(0.0379)\n",
      "15551 Training Loss: tensor(0.0415)\n",
      "15552 Training Loss: tensor(0.0372)\n",
      "15553 Training Loss: tensor(0.0451)\n",
      "15554 Training Loss: tensor(0.0400)\n",
      "15555 Training Loss: tensor(0.0406)\n",
      "15556 Training Loss: tensor(0.0424)\n",
      "15557 Training Loss: tensor(0.0398)\n",
      "15558 Training Loss: tensor(0.0374)\n",
      "15559 Training Loss: tensor(0.0414)\n",
      "15560 Training Loss: tensor(0.0379)\n",
      "15561 Training Loss: tensor(0.0356)\n",
      "15562 Training Loss: tensor(0.0443)\n",
      "15563 Training Loss: tensor(0.0360)\n",
      "15564 Training Loss: tensor(0.0368)\n",
      "15565 Training Loss: tensor(0.0401)\n",
      "15566 Training Loss: tensor(0.0332)\n",
      "15567 Training Loss: tensor(0.0407)\n",
      "15568 Training Loss: tensor(0.0412)\n",
      "15569 Training Loss: tensor(0.0365)\n",
      "15570 Training Loss: tensor(0.0424)\n",
      "15571 Training Loss: tensor(0.0409)\n",
      "15572 Training Loss: tensor(0.0394)\n",
      "15573 Training Loss: tensor(0.0408)\n",
      "15574 Training Loss: tensor(0.0440)\n",
      "15575 Training Loss: tensor(0.0382)\n",
      "15576 Training Loss: tensor(0.0438)\n",
      "15577 Training Loss: tensor(0.0468)\n",
      "15578 Training Loss: tensor(0.0384)\n",
      "15579 Training Loss: tensor(0.0373)\n",
      "15580 Training Loss: tensor(0.0415)\n",
      "15581 Training Loss: tensor(0.0423)\n",
      "15582 Training Loss: tensor(0.0392)\n",
      "15583 Training Loss: tensor(0.0444)\n",
      "15584 Training Loss: tensor(0.0390)\n",
      "15585 Training Loss: tensor(0.0421)\n",
      "15586 Training Loss: tensor(0.0409)\n",
      "15587 Training Loss: tensor(0.0442)\n",
      "15588 Training Loss: tensor(0.0374)\n",
      "15589 Training Loss: tensor(0.0377)\n",
      "15590 Training Loss: tensor(0.0387)\n",
      "15591 Training Loss: tensor(0.0370)\n",
      "15592 Training Loss: tensor(0.0379)\n",
      "15593 Training Loss: tensor(0.0379)\n",
      "15594 Training Loss: tensor(0.0384)\n",
      "15595 Training Loss: tensor(0.0328)\n",
      "15596 Training Loss: tensor(0.0324)\n",
      "15597 Training Loss: tensor(0.0375)\n",
      "15598 Training Loss: tensor(0.0411)\n",
      "15599 Training Loss: tensor(0.0368)\n",
      "15600 Training Loss: tensor(0.0360)\n",
      "15601 Training Loss: tensor(0.0335)\n",
      "15602 Training Loss: tensor(0.0407)\n",
      "15603 Training Loss: tensor(0.0432)\n",
      "15604 Training Loss: tensor(0.0391)\n",
      "15605 Training Loss: tensor(0.0418)\n",
      "15606 Training Loss: tensor(0.0360)\n",
      "15607 Training Loss: tensor(0.0371)\n",
      "15608 Training Loss: tensor(0.0415)\n",
      "15609 Training Loss: tensor(0.0388)\n",
      "15610 Training Loss: tensor(0.0433)\n",
      "15611 Training Loss: tensor(0.0408)\n",
      "15612 Training Loss: tensor(0.0380)\n",
      "15613 Training Loss: tensor(0.0346)\n",
      "15614 Training Loss: tensor(0.0385)\n",
      "15615 Training Loss: tensor(0.0414)\n",
      "15616 Training Loss: tensor(0.0413)\n",
      "15617 Training Loss: tensor(0.0403)\n",
      "15618 Training Loss: tensor(0.0401)\n",
      "15619 Training Loss: tensor(0.0445)\n",
      "15620 Training Loss: tensor(0.0439)\n",
      "15621 Training Loss: tensor(0.0364)\n",
      "15622 Training Loss: tensor(0.0347)\n",
      "15623 Training Loss: tensor(0.0370)\n",
      "15624 Training Loss: tensor(0.0410)\n",
      "15625 Training Loss: tensor(0.0411)\n",
      "15626 Training Loss: tensor(0.0379)\n",
      "15627 Training Loss: tensor(0.0375)\n",
      "15628 Training Loss: tensor(0.0367)\n",
      "15629 Training Loss: tensor(0.0437)\n",
      "15630 Training Loss: tensor(0.0409)\n",
      "15631 Training Loss: tensor(0.0418)\n",
      "15632 Training Loss: tensor(0.0399)\n",
      "15633 Training Loss: tensor(0.0373)\n",
      "15634 Training Loss: tensor(0.0399)\n",
      "15635 Training Loss: tensor(0.0384)\n",
      "15636 Training Loss: tensor(0.0388)\n",
      "15637 Training Loss: tensor(0.0411)\n",
      "15638 Training Loss: tensor(0.0419)\n",
      "15639 Training Loss: tensor(0.0400)\n",
      "15640 Training Loss: tensor(0.0367)\n",
      "15641 Training Loss: tensor(0.0379)\n",
      "15642 Training Loss: tensor(0.0379)\n",
      "15643 Training Loss: tensor(0.0411)\n",
      "15644 Training Loss: tensor(0.0343)\n",
      "15645 Training Loss: tensor(0.0358)\n",
      "15646 Training Loss: tensor(0.0388)\n",
      "15647 Training Loss: tensor(0.0367)\n",
      "15648 Training Loss: tensor(0.0398)\n",
      "15649 Training Loss: tensor(0.0393)\n",
      "15650 Training Loss: tensor(0.0389)\n",
      "15651 Training Loss: tensor(0.0415)\n",
      "15652 Training Loss: tensor(0.0353)\n",
      "15653 Training Loss: tensor(0.0372)\n",
      "15654 Training Loss: tensor(0.0417)\n",
      "15655 Training Loss: tensor(0.0347)\n",
      "15656 Training Loss: tensor(0.0433)\n",
      "15657 Training Loss: tensor(0.0364)\n",
      "15658 Training Loss: tensor(0.0392)\n",
      "15659 Training Loss: tensor(0.0344)\n",
      "15660 Training Loss: tensor(0.0379)\n",
      "15661 Training Loss: tensor(0.0363)\n",
      "15662 Training Loss: tensor(0.0357)\n",
      "15663 Training Loss: tensor(0.0357)\n",
      "15664 Training Loss: tensor(0.0383)\n",
      "15665 Training Loss: tensor(0.0344)\n",
      "15666 Training Loss: tensor(0.0437)\n",
      "15667 Training Loss: tensor(0.0419)\n",
      "15668 Training Loss: tensor(0.0391)\n",
      "15669 Training Loss: tensor(0.0468)\n",
      "15670 Training Loss: tensor(0.0414)\n",
      "15671 Training Loss: tensor(0.0416)\n",
      "15672 Training Loss: tensor(0.0353)\n",
      "15673 Training Loss: tensor(0.0440)\n",
      "15674 Training Loss: tensor(0.0380)\n",
      "15675 Training Loss: tensor(0.0406)\n",
      "15676 Training Loss: tensor(0.0403)\n",
      "15677 Training Loss: tensor(0.0346)\n",
      "15678 Training Loss: tensor(0.0424)\n",
      "15679 Training Loss: tensor(0.0386)\n",
      "15680 Training Loss: tensor(0.0379)\n",
      "15681 Training Loss: tensor(0.0410)\n",
      "15682 Training Loss: tensor(0.0370)\n",
      "15683 Training Loss: tensor(0.0412)\n",
      "15684 Training Loss: tensor(0.0423)\n",
      "15685 Training Loss: tensor(0.0387)\n",
      "15686 Training Loss: tensor(0.0345)\n",
      "15687 Training Loss: tensor(0.0366)\n",
      "15688 Training Loss: tensor(0.0405)\n",
      "15689 Training Loss: tensor(0.0395)\n",
      "15690 Training Loss: tensor(0.0434)\n",
      "15691 Training Loss: tensor(0.0370)\n",
      "15692 Training Loss: tensor(0.0368)\n",
      "15693 Training Loss: tensor(0.0447)\n",
      "15694 Training Loss: tensor(0.0437)\n",
      "15695 Training Loss: tensor(0.0385)\n",
      "15696 Training Loss: tensor(0.0377)\n",
      "15697 Training Loss: tensor(0.0342)\n",
      "15698 Training Loss: tensor(0.0392)\n",
      "15699 Training Loss: tensor(0.0368)\n",
      "15700 Training Loss: tensor(0.0387)\n",
      "15701 Training Loss: tensor(0.0380)\n",
      "15702 Training Loss: tensor(0.0393)\n",
      "15703 Training Loss: tensor(0.0416)\n",
      "15704 Training Loss: tensor(0.0393)\n",
      "15705 Training Loss: tensor(0.0385)\n",
      "15706 Training Loss: tensor(0.0416)\n",
      "15707 Training Loss: tensor(0.0372)\n",
      "15708 Training Loss: tensor(0.0429)\n",
      "15709 Training Loss: tensor(0.0368)\n",
      "15710 Training Loss: tensor(0.0398)\n",
      "15711 Training Loss: tensor(0.0325)\n",
      "15712 Training Loss: tensor(0.0401)\n",
      "15713 Training Loss: tensor(0.0372)\n",
      "15714 Training Loss: tensor(0.0380)\n",
      "15715 Training Loss: tensor(0.0374)\n",
      "15716 Training Loss: tensor(0.0371)\n",
      "15717 Training Loss: tensor(0.0388)\n",
      "15718 Training Loss: tensor(0.0373)\n",
      "15719 Training Loss: tensor(0.0415)\n",
      "15720 Training Loss: tensor(0.0440)\n",
      "15721 Training Loss: tensor(0.0387)\n",
      "15722 Training Loss: tensor(0.0392)\n",
      "15723 Training Loss: tensor(0.0326)\n",
      "15724 Training Loss: tensor(0.0402)\n",
      "15725 Training Loss: tensor(0.0419)\n",
      "15726 Training Loss: tensor(0.0386)\n",
      "15727 Training Loss: tensor(0.0370)\n",
      "15728 Training Loss: tensor(0.0421)\n",
      "15729 Training Loss: tensor(0.0403)\n",
      "15730 Training Loss: tensor(0.0406)\n",
      "15731 Training Loss: tensor(0.0367)\n",
      "15732 Training Loss: tensor(0.0346)\n",
      "15733 Training Loss: tensor(0.0429)\n",
      "15734 Training Loss: tensor(0.0382)\n",
      "15735 Training Loss: tensor(0.0417)\n",
      "15736 Training Loss: tensor(0.0333)\n",
      "15737 Training Loss: tensor(0.0437)\n",
      "15738 Training Loss: tensor(0.0394)\n",
      "15739 Training Loss: tensor(0.0413)\n",
      "15740 Training Loss: tensor(0.0360)\n",
      "15741 Training Loss: tensor(0.0374)\n",
      "15742 Training Loss: tensor(0.0388)\n",
      "15743 Training Loss: tensor(0.0366)\n",
      "15744 Training Loss: tensor(0.0460)\n",
      "15745 Training Loss: tensor(0.0336)\n",
      "15746 Training Loss: tensor(0.0392)\n",
      "15747 Training Loss: tensor(0.0445)\n",
      "15748 Training Loss: tensor(0.0353)\n",
      "15749 Training Loss: tensor(0.0391)\n",
      "15750 Training Loss: tensor(0.0451)\n",
      "15751 Training Loss: tensor(0.0386)\n",
      "15752 Training Loss: tensor(0.0371)\n",
      "15753 Training Loss: tensor(0.0397)\n",
      "15754 Training Loss: tensor(0.0411)\n",
      "15755 Training Loss: tensor(0.0336)\n",
      "15756 Training Loss: tensor(0.0390)\n",
      "15757 Training Loss: tensor(0.0373)\n",
      "15758 Training Loss: tensor(0.0382)\n",
      "15759 Training Loss: tensor(0.0391)\n",
      "15760 Training Loss: tensor(0.0421)\n",
      "15761 Training Loss: tensor(0.0439)\n",
      "15762 Training Loss: tensor(0.0404)\n",
      "15763 Training Loss: tensor(0.0353)\n",
      "15764 Training Loss: tensor(0.0418)\n",
      "15765 Training Loss: tensor(0.0425)\n",
      "15766 Training Loss: tensor(0.0461)\n",
      "15767 Training Loss: tensor(0.0447)\n",
      "15768 Training Loss: tensor(0.0416)\n",
      "15769 Training Loss: tensor(0.0418)\n",
      "15770 Training Loss: tensor(0.0397)\n",
      "15771 Training Loss: tensor(0.0441)\n",
      "15772 Training Loss: tensor(0.0421)\n",
      "15773 Training Loss: tensor(0.0383)\n",
      "15774 Training Loss: tensor(0.0442)\n",
      "15775 Training Loss: tensor(0.0385)\n",
      "15776 Training Loss: tensor(0.0387)\n",
      "15777 Training Loss: tensor(0.0397)\n",
      "15778 Training Loss: tensor(0.0381)\n",
      "15779 Training Loss: tensor(0.0432)\n",
      "15780 Training Loss: tensor(0.0396)\n",
      "15781 Training Loss: tensor(0.0344)\n",
      "15782 Training Loss: tensor(0.0369)\n",
      "15783 Training Loss: tensor(0.0374)\n",
      "15784 Training Loss: tensor(0.0322)\n",
      "15785 Training Loss: tensor(0.0365)\n",
      "15786 Training Loss: tensor(0.0394)\n",
      "15787 Training Loss: tensor(0.0344)\n",
      "15788 Training Loss: tensor(0.0397)\n",
      "15789 Training Loss: tensor(0.0417)\n",
      "15790 Training Loss: tensor(0.0361)\n",
      "15791 Training Loss: tensor(0.0392)\n",
      "15792 Training Loss: tensor(0.0408)\n",
      "15793 Training Loss: tensor(0.0391)\n",
      "15794 Training Loss: tensor(0.0411)\n",
      "15795 Training Loss: tensor(0.0398)\n",
      "15796 Training Loss: tensor(0.0411)\n",
      "15797 Training Loss: tensor(0.0360)\n",
      "15798 Training Loss: tensor(0.0403)\n",
      "15799 Training Loss: tensor(0.0362)\n",
      "15800 Training Loss: tensor(0.0384)\n",
      "15801 Training Loss: tensor(0.0390)\n",
      "15802 Training Loss: tensor(0.0403)\n",
      "15803 Training Loss: tensor(0.0405)\n",
      "15804 Training Loss: tensor(0.0422)\n",
      "15805 Training Loss: tensor(0.0409)\n",
      "15806 Training Loss: tensor(0.0408)\n",
      "15807 Training Loss: tensor(0.0390)\n",
      "15808 Training Loss: tensor(0.0435)\n",
      "15809 Training Loss: tensor(0.0392)\n",
      "15810 Training Loss: tensor(0.0398)\n",
      "15811 Training Loss: tensor(0.0417)\n",
      "15812 Training Loss: tensor(0.0396)\n",
      "15813 Training Loss: tensor(0.0389)\n",
      "15814 Training Loss: tensor(0.0484)\n",
      "15815 Training Loss: tensor(0.0361)\n",
      "15816 Training Loss: tensor(0.0384)\n",
      "15817 Training Loss: tensor(0.0396)\n",
      "15818 Training Loss: tensor(0.0382)\n",
      "15819 Training Loss: tensor(0.0371)\n",
      "15820 Training Loss: tensor(0.0345)\n",
      "15821 Training Loss: tensor(0.0406)\n",
      "15822 Training Loss: tensor(0.0386)\n",
      "15823 Training Loss: tensor(0.0360)\n",
      "15824 Training Loss: tensor(0.0412)\n",
      "15825 Training Loss: tensor(0.0384)\n",
      "15826 Training Loss: tensor(0.0410)\n",
      "15827 Training Loss: tensor(0.0406)\n",
      "15828 Training Loss: tensor(0.0356)\n",
      "15829 Training Loss: tensor(0.0456)\n",
      "15830 Training Loss: tensor(0.0415)\n",
      "15831 Training Loss: tensor(0.0436)\n",
      "15832 Training Loss: tensor(0.0317)\n",
      "15833 Training Loss: tensor(0.0392)\n",
      "15834 Training Loss: tensor(0.0401)\n",
      "15835 Training Loss: tensor(0.0357)\n",
      "15836 Training Loss: tensor(0.0419)\n",
      "15837 Training Loss: tensor(0.0388)\n",
      "15838 Training Loss: tensor(0.0415)\n",
      "15839 Training Loss: tensor(0.0456)\n",
      "15840 Training Loss: tensor(0.0384)\n",
      "15841 Training Loss: tensor(0.0376)\n",
      "15842 Training Loss: tensor(0.0379)\n",
      "15843 Training Loss: tensor(0.0372)\n",
      "15844 Training Loss: tensor(0.0413)\n",
      "15845 Training Loss: tensor(0.0391)\n",
      "15846 Training Loss: tensor(0.0386)\n",
      "15847 Training Loss: tensor(0.0388)\n",
      "15848 Training Loss: tensor(0.0411)\n",
      "15849 Training Loss: tensor(0.0411)\n",
      "15850 Training Loss: tensor(0.0364)\n",
      "15851 Training Loss: tensor(0.0369)\n",
      "15852 Training Loss: tensor(0.0422)\n",
      "15853 Training Loss: tensor(0.0380)\n",
      "15854 Training Loss: tensor(0.0401)\n",
      "15855 Training Loss: tensor(0.0408)\n",
      "15856 Training Loss: tensor(0.0411)\n",
      "15857 Training Loss: tensor(0.0390)\n",
      "15858 Training Loss: tensor(0.0343)\n",
      "15859 Training Loss: tensor(0.0381)\n",
      "15860 Training Loss: tensor(0.0441)\n",
      "15861 Training Loss: tensor(0.0399)\n",
      "15862 Training Loss: tensor(0.0458)\n",
      "15863 Training Loss: tensor(0.0481)\n",
      "15864 Training Loss: tensor(0.0387)\n",
      "15865 Training Loss: tensor(0.0340)\n",
      "15866 Training Loss: tensor(0.0399)\n",
      "15867 Training Loss: tensor(0.0409)\n",
      "15868 Training Loss: tensor(0.0399)\n",
      "15869 Training Loss: tensor(0.0360)\n",
      "15870 Training Loss: tensor(0.0371)\n",
      "15871 Training Loss: tensor(0.0416)\n",
      "15872 Training Loss: tensor(0.0407)\n",
      "15873 Training Loss: tensor(0.0401)\n",
      "15874 Training Loss: tensor(0.0431)\n",
      "15875 Training Loss: tensor(0.0383)\n",
      "15876 Training Loss: tensor(0.0377)\n",
      "15877 Training Loss: tensor(0.0439)\n",
      "15878 Training Loss: tensor(0.0377)\n",
      "15879 Training Loss: tensor(0.0425)\n",
      "15880 Training Loss: tensor(0.0390)\n",
      "15881 Training Loss: tensor(0.0374)\n",
      "15882 Training Loss: tensor(0.0358)\n",
      "15883 Training Loss: tensor(0.0444)\n",
      "15884 Training Loss: tensor(0.0398)\n",
      "15885 Training Loss: tensor(0.0398)\n",
      "15886 Training Loss: tensor(0.0375)\n",
      "15887 Training Loss: tensor(0.0384)\n",
      "15888 Training Loss: tensor(0.0392)\n",
      "15889 Training Loss: tensor(0.0377)\n",
      "15890 Training Loss: tensor(0.0392)\n",
      "15891 Training Loss: tensor(0.0375)\n",
      "15892 Training Loss: tensor(0.0352)\n",
      "15893 Training Loss: tensor(0.0382)\n",
      "15894 Training Loss: tensor(0.0355)\n",
      "15895 Training Loss: tensor(0.0459)\n",
      "15896 Training Loss: tensor(0.0403)\n",
      "15897 Training Loss: tensor(0.0446)\n",
      "15898 Training Loss: tensor(0.0434)\n",
      "15899 Training Loss: tensor(0.0317)\n",
      "15900 Training Loss: tensor(0.0363)\n",
      "15901 Training Loss: tensor(0.0399)\n",
      "15902 Training Loss: tensor(0.0416)\n",
      "15903 Training Loss: tensor(0.0427)\n",
      "15904 Training Loss: tensor(0.0413)\n",
      "15905 Training Loss: tensor(0.0419)\n",
      "15906 Training Loss: tensor(0.0339)\n",
      "15907 Training Loss: tensor(0.0376)\n",
      "15908 Training Loss: tensor(0.0412)\n",
      "15909 Training Loss: tensor(0.0426)\n",
      "15910 Training Loss: tensor(0.0334)\n",
      "15911 Training Loss: tensor(0.0431)\n",
      "15912 Training Loss: tensor(0.0370)\n",
      "15913 Training Loss: tensor(0.0397)\n",
      "15914 Training Loss: tensor(0.0386)\n",
      "15915 Training Loss: tensor(0.0377)\n",
      "15916 Training Loss: tensor(0.0379)\n",
      "15917 Training Loss: tensor(0.0415)\n",
      "15918 Training Loss: tensor(0.0407)\n",
      "15919 Training Loss: tensor(0.0342)\n",
      "15920 Training Loss: tensor(0.0363)\n",
      "15921 Training Loss: tensor(0.0343)\n",
      "15922 Training Loss: tensor(0.0471)\n",
      "15923 Training Loss: tensor(0.0422)\n",
      "15924 Training Loss: tensor(0.0472)\n",
      "15925 Training Loss: tensor(0.0358)\n",
      "15926 Training Loss: tensor(0.0405)\n",
      "15927 Training Loss: tensor(0.0441)\n",
      "15928 Training Loss: tensor(0.0392)\n",
      "15929 Training Loss: tensor(0.0405)\n",
      "15930 Training Loss: tensor(0.0455)\n",
      "15931 Training Loss: tensor(0.0372)\n",
      "15932 Training Loss: tensor(0.0349)\n",
      "15933 Training Loss: tensor(0.0421)\n",
      "15934 Training Loss: tensor(0.0356)\n",
      "15935 Training Loss: tensor(0.0425)\n",
      "15936 Training Loss: tensor(0.0368)\n",
      "15937 Training Loss: tensor(0.0398)\n",
      "15938 Training Loss: tensor(0.0437)\n",
      "15939 Training Loss: tensor(0.0448)\n",
      "15940 Training Loss: tensor(0.0381)\n",
      "15941 Training Loss: tensor(0.0372)\n",
      "15942 Training Loss: tensor(0.0390)\n",
      "15943 Training Loss: tensor(0.0403)\n",
      "15944 Training Loss: tensor(0.0411)\n",
      "15945 Training Loss: tensor(0.0379)\n",
      "15946 Training Loss: tensor(0.0437)\n",
      "15947 Training Loss: tensor(0.0394)\n",
      "15948 Training Loss: tensor(0.0408)\n",
      "15949 Training Loss: tensor(0.0348)\n",
      "15950 Training Loss: tensor(0.0337)\n",
      "15951 Training Loss: tensor(0.0346)\n",
      "15952 Training Loss: tensor(0.0403)\n",
      "15953 Training Loss: tensor(0.0360)\n",
      "15954 Training Loss: tensor(0.0364)\n",
      "15955 Training Loss: tensor(0.0403)\n",
      "15956 Training Loss: tensor(0.0353)\n",
      "15957 Training Loss: tensor(0.0394)\n",
      "15958 Training Loss: tensor(0.0466)\n",
      "15959 Training Loss: tensor(0.0338)\n",
      "15960 Training Loss: tensor(0.0432)\n",
      "15961 Training Loss: tensor(0.0339)\n",
      "15962 Training Loss: tensor(0.0412)\n",
      "15963 Training Loss: tensor(0.0424)\n",
      "15964 Training Loss: tensor(0.0403)\n",
      "15965 Training Loss: tensor(0.0386)\n",
      "15966 Training Loss: tensor(0.0400)\n",
      "15967 Training Loss: tensor(0.0372)\n",
      "15968 Training Loss: tensor(0.0379)\n",
      "15969 Training Loss: tensor(0.0412)\n",
      "15970 Training Loss: tensor(0.0400)\n",
      "15971 Training Loss: tensor(0.0341)\n",
      "15972 Training Loss: tensor(0.0374)\n",
      "15973 Training Loss: tensor(0.0375)\n",
      "15974 Training Loss: tensor(0.0354)\n",
      "15975 Training Loss: tensor(0.0413)\n",
      "15976 Training Loss: tensor(0.0388)\n",
      "15977 Training Loss: tensor(0.0463)\n",
      "15978 Training Loss: tensor(0.0400)\n",
      "15979 Training Loss: tensor(0.0373)\n",
      "15980 Training Loss: tensor(0.0445)\n",
      "15981 Training Loss: tensor(0.0398)\n",
      "15982 Training Loss: tensor(0.0460)\n",
      "15983 Training Loss: tensor(0.0422)\n",
      "15984 Training Loss: tensor(0.0372)\n",
      "15985 Training Loss: tensor(0.0415)\n",
      "15986 Training Loss: tensor(0.0397)\n",
      "15987 Training Loss: tensor(0.0386)\n",
      "15988 Training Loss: tensor(0.0440)\n",
      "15989 Training Loss: tensor(0.0366)\n",
      "15990 Training Loss: tensor(0.0381)\n",
      "15991 Training Loss: tensor(0.0394)\n",
      "15992 Training Loss: tensor(0.0406)\n",
      "15993 Training Loss: tensor(0.0347)\n",
      "15994 Training Loss: tensor(0.0439)\n",
      "15995 Training Loss: tensor(0.0449)\n",
      "15996 Training Loss: tensor(0.0443)\n",
      "15997 Training Loss: tensor(0.0405)\n",
      "15998 Training Loss: tensor(0.0400)\n",
      "15999 Training Loss: tensor(0.0410)\n",
      "16000 Training Loss: tensor(0.0395)\n",
      "16001 Training Loss: tensor(0.0461)\n",
      "16002 Training Loss: tensor(0.0327)\n",
      "16003 Training Loss: tensor(0.0467)\n",
      "16004 Training Loss: tensor(0.0390)\n",
      "16005 Training Loss: tensor(0.0433)\n",
      "16006 Training Loss: tensor(0.0450)\n",
      "16007 Training Loss: tensor(0.0392)\n",
      "16008 Training Loss: tensor(0.0411)\n",
      "16009 Training Loss: tensor(0.0468)\n",
      "16010 Training Loss: tensor(0.0458)\n",
      "16011 Training Loss: tensor(0.0419)\n",
      "16012 Training Loss: tensor(0.0437)\n",
      "16013 Training Loss: tensor(0.0424)\n",
      "16014 Training Loss: tensor(0.0434)\n",
      "16015 Training Loss: tensor(0.0373)\n",
      "16016 Training Loss: tensor(0.0425)\n",
      "16017 Training Loss: tensor(0.0434)\n",
      "16018 Training Loss: tensor(0.0400)\n",
      "16019 Training Loss: tensor(0.0349)\n",
      "16020 Training Loss: tensor(0.0362)\n",
      "16021 Training Loss: tensor(0.0325)\n",
      "16022 Training Loss: tensor(0.0383)\n",
      "16023 Training Loss: tensor(0.0411)\n",
      "16024 Training Loss: tensor(0.0323)\n",
      "16025 Training Loss: tensor(0.0415)\n",
      "16026 Training Loss: tensor(0.0436)\n",
      "16027 Training Loss: tensor(0.0395)\n",
      "16028 Training Loss: tensor(0.0394)\n",
      "16029 Training Loss: tensor(0.0419)\n",
      "16030 Training Loss: tensor(0.0349)\n",
      "16031 Training Loss: tensor(0.0424)\n",
      "16032 Training Loss: tensor(0.0338)\n",
      "16033 Training Loss: tensor(0.0437)\n",
      "16034 Training Loss: tensor(0.0432)\n",
      "16035 Training Loss: tensor(0.0349)\n",
      "16036 Training Loss: tensor(0.0409)\n",
      "16037 Training Loss: tensor(0.0356)\n",
      "16038 Training Loss: tensor(0.0419)\n",
      "16039 Training Loss: tensor(0.0414)\n",
      "16040 Training Loss: tensor(0.0364)\n",
      "16041 Training Loss: tensor(0.0437)\n",
      "16042 Training Loss: tensor(0.0385)\n",
      "16043 Training Loss: tensor(0.0329)\n",
      "16044 Training Loss: tensor(0.0371)\n",
      "16045 Training Loss: tensor(0.0357)\n",
      "16046 Training Loss: tensor(0.0390)\n",
      "16047 Training Loss: tensor(0.0380)\n",
      "16048 Training Loss: tensor(0.0350)\n",
      "16049 Training Loss: tensor(0.0385)\n",
      "16050 Training Loss: tensor(0.0374)\n",
      "16051 Training Loss: tensor(0.0379)\n",
      "16052 Training Loss: tensor(0.0393)\n",
      "16053 Training Loss: tensor(0.0355)\n",
      "16054 Training Loss: tensor(0.0401)\n",
      "16055 Training Loss: tensor(0.0364)\n",
      "16056 Training Loss: tensor(0.0410)\n",
      "16057 Training Loss: tensor(0.0397)\n",
      "16058 Training Loss: tensor(0.0407)\n",
      "16059 Training Loss: tensor(0.0373)\n",
      "16060 Training Loss: tensor(0.0401)\n",
      "16061 Training Loss: tensor(0.0385)\n",
      "16062 Training Loss: tensor(0.0359)\n",
      "16063 Training Loss: tensor(0.0384)\n",
      "16064 Training Loss: tensor(0.0381)\n",
      "16065 Training Loss: tensor(0.0367)\n",
      "16066 Training Loss: tensor(0.0369)\n",
      "16067 Training Loss: tensor(0.0403)\n",
      "16068 Training Loss: tensor(0.0379)\n",
      "16069 Training Loss: tensor(0.0383)\n",
      "16070 Training Loss: tensor(0.0353)\n",
      "16071 Training Loss: tensor(0.0410)\n",
      "16072 Training Loss: tensor(0.0416)\n",
      "16073 Training Loss: tensor(0.0401)\n",
      "16074 Training Loss: tensor(0.0371)\n",
      "16075 Training Loss: tensor(0.0375)\n",
      "16076 Training Loss: tensor(0.0366)\n",
      "16077 Training Loss: tensor(0.0351)\n",
      "16078 Training Loss: tensor(0.0354)\n",
      "16079 Training Loss: tensor(0.0400)\n",
      "16080 Training Loss: tensor(0.0349)\n",
      "16081 Training Loss: tensor(0.0346)\n",
      "16082 Training Loss: tensor(0.0460)\n",
      "16083 Training Loss: tensor(0.0396)\n",
      "16084 Training Loss: tensor(0.0376)\n",
      "16085 Training Loss: tensor(0.0386)\n",
      "16086 Training Loss: tensor(0.0346)\n",
      "16087 Training Loss: tensor(0.0393)\n",
      "16088 Training Loss: tensor(0.0412)\n",
      "16089 Training Loss: tensor(0.0383)\n",
      "16090 Training Loss: tensor(0.0424)\n",
      "16091 Training Loss: tensor(0.0379)\n",
      "16092 Training Loss: tensor(0.0402)\n",
      "16093 Training Loss: tensor(0.0380)\n",
      "16094 Training Loss: tensor(0.0370)\n",
      "16095 Training Loss: tensor(0.0398)\n",
      "16096 Training Loss: tensor(0.0423)\n",
      "16097 Training Loss: tensor(0.0368)\n",
      "16098 Training Loss: tensor(0.0381)\n",
      "16099 Training Loss: tensor(0.0345)\n",
      "16100 Training Loss: tensor(0.0400)\n",
      "16101 Training Loss: tensor(0.0399)\n",
      "16102 Training Loss: tensor(0.0394)\n",
      "16103 Training Loss: tensor(0.0385)\n",
      "16104 Training Loss: tensor(0.0406)\n",
      "16105 Training Loss: tensor(0.0358)\n",
      "16106 Training Loss: tensor(0.0396)\n",
      "16107 Training Loss: tensor(0.0353)\n",
      "16108 Training Loss: tensor(0.0402)\n",
      "16109 Training Loss: tensor(0.0431)\n",
      "16110 Training Loss: tensor(0.0373)\n",
      "16111 Training Loss: tensor(0.0367)\n",
      "16112 Training Loss: tensor(0.0385)\n",
      "16113 Training Loss: tensor(0.0407)\n",
      "16114 Training Loss: tensor(0.0399)\n",
      "16115 Training Loss: tensor(0.0400)\n",
      "16116 Training Loss: tensor(0.0388)\n",
      "16117 Training Loss: tensor(0.0348)\n",
      "16118 Training Loss: tensor(0.0424)\n",
      "16119 Training Loss: tensor(0.0377)\n",
      "16120 Training Loss: tensor(0.0343)\n",
      "16121 Training Loss: tensor(0.0412)\n",
      "16122 Training Loss: tensor(0.0392)\n",
      "16123 Training Loss: tensor(0.0425)\n",
      "16124 Training Loss: tensor(0.0395)\n",
      "16125 Training Loss: tensor(0.0367)\n",
      "16126 Training Loss: tensor(0.0366)\n",
      "16127 Training Loss: tensor(0.0386)\n",
      "16128 Training Loss: tensor(0.0390)\n",
      "16129 Training Loss: tensor(0.0411)\n",
      "16130 Training Loss: tensor(0.0370)\n",
      "16131 Training Loss: tensor(0.0360)\n",
      "16132 Training Loss: tensor(0.0444)\n",
      "16133 Training Loss: tensor(0.0405)\n",
      "16134 Training Loss: tensor(0.0363)\n",
      "16135 Training Loss: tensor(0.0359)\n",
      "16136 Training Loss: tensor(0.0401)\n",
      "16137 Training Loss: tensor(0.0377)\n",
      "16138 Training Loss: tensor(0.0421)\n",
      "16139 Training Loss: tensor(0.0420)\n",
      "16140 Training Loss: tensor(0.0366)\n",
      "16141 Training Loss: tensor(0.0407)\n",
      "16142 Training Loss: tensor(0.0323)\n",
      "16143 Training Loss: tensor(0.0451)\n",
      "16144 Training Loss: tensor(0.0372)\n",
      "16145 Training Loss: tensor(0.0410)\n",
      "16146 Training Loss: tensor(0.0362)\n",
      "16147 Training Loss: tensor(0.0399)\n",
      "16148 Training Loss: tensor(0.0354)\n",
      "16149 Training Loss: tensor(0.0442)\n",
      "16150 Training Loss: tensor(0.0358)\n",
      "16151 Training Loss: tensor(0.0386)\n",
      "16152 Training Loss: tensor(0.0368)\n",
      "16153 Training Loss: tensor(0.0393)\n",
      "16154 Training Loss: tensor(0.0348)\n",
      "16155 Training Loss: tensor(0.0364)\n",
      "16156 Training Loss: tensor(0.0382)\n",
      "16157 Training Loss: tensor(0.0409)\n",
      "16158 Training Loss: tensor(0.0397)\n",
      "16159 Training Loss: tensor(0.0394)\n",
      "16160 Training Loss: tensor(0.0426)\n",
      "16161 Training Loss: tensor(0.0376)\n",
      "16162 Training Loss: tensor(0.0369)\n",
      "16163 Training Loss: tensor(0.0371)\n",
      "16164 Training Loss: tensor(0.0399)\n",
      "16165 Training Loss: tensor(0.0385)\n",
      "16166 Training Loss: tensor(0.0401)\n",
      "16167 Training Loss: tensor(0.0371)\n",
      "16168 Training Loss: tensor(0.0328)\n",
      "16169 Training Loss: tensor(0.0402)\n",
      "16170 Training Loss: tensor(0.0359)\n",
      "16171 Training Loss: tensor(0.0353)\n",
      "16172 Training Loss: tensor(0.0393)\n",
      "16173 Training Loss: tensor(0.0391)\n",
      "16174 Training Loss: tensor(0.0407)\n",
      "16175 Training Loss: tensor(0.0354)\n",
      "16176 Training Loss: tensor(0.0364)\n",
      "16177 Training Loss: tensor(0.0360)\n",
      "16178 Training Loss: tensor(0.0348)\n",
      "16179 Training Loss: tensor(0.0418)\n",
      "16180 Training Loss: tensor(0.0426)\n",
      "16181 Training Loss: tensor(0.0407)\n",
      "16182 Training Loss: tensor(0.0384)\n",
      "16183 Training Loss: tensor(0.0326)\n",
      "16184 Training Loss: tensor(0.0372)\n",
      "16185 Training Loss: tensor(0.0406)\n",
      "16186 Training Loss: tensor(0.0414)\n",
      "16187 Training Loss: tensor(0.0380)\n",
      "16188 Training Loss: tensor(0.0390)\n",
      "16189 Training Loss: tensor(0.0382)\n",
      "16190 Training Loss: tensor(0.0349)\n",
      "16191 Training Loss: tensor(0.0376)\n",
      "16192 Training Loss: tensor(0.0368)\n",
      "16193 Training Loss: tensor(0.0347)\n",
      "16194 Training Loss: tensor(0.0382)\n",
      "16195 Training Loss: tensor(0.0401)\n",
      "16196 Training Loss: tensor(0.0374)\n",
      "16197 Training Loss: tensor(0.0373)\n",
      "16198 Training Loss: tensor(0.0344)\n",
      "16199 Training Loss: tensor(0.0398)\n",
      "16200 Training Loss: tensor(0.0453)\n",
      "16201 Training Loss: tensor(0.0383)\n",
      "16202 Training Loss: tensor(0.0356)\n",
      "16203 Training Loss: tensor(0.0349)\n",
      "16204 Training Loss: tensor(0.0378)\n",
      "16205 Training Loss: tensor(0.0404)\n",
      "16206 Training Loss: tensor(0.0395)\n",
      "16207 Training Loss: tensor(0.0382)\n",
      "16208 Training Loss: tensor(0.0365)\n",
      "16209 Training Loss: tensor(0.0374)\n",
      "16210 Training Loss: tensor(0.0398)\n",
      "16211 Training Loss: tensor(0.0436)\n",
      "16212 Training Loss: tensor(0.0435)\n",
      "16213 Training Loss: tensor(0.0431)\n",
      "16214 Training Loss: tensor(0.0361)\n",
      "16215 Training Loss: tensor(0.0354)\n",
      "16216 Training Loss: tensor(0.0356)\n",
      "16217 Training Loss: tensor(0.0389)\n",
      "16218 Training Loss: tensor(0.0408)\n",
      "16219 Training Loss: tensor(0.0325)\n",
      "16220 Training Loss: tensor(0.0381)\n",
      "16221 Training Loss: tensor(0.0429)\n",
      "16222 Training Loss: tensor(0.0351)\n",
      "16223 Training Loss: tensor(0.0368)\n",
      "16224 Training Loss: tensor(0.0392)\n",
      "16225 Training Loss: tensor(0.0359)\n",
      "16226 Training Loss: tensor(0.0411)\n",
      "16227 Training Loss: tensor(0.0340)\n",
      "16228 Training Loss: tensor(0.0368)\n",
      "16229 Training Loss: tensor(0.0377)\n",
      "16230 Training Loss: tensor(0.0402)\n",
      "16231 Training Loss: tensor(0.0301)\n",
      "16232 Training Loss: tensor(0.0358)\n",
      "16233 Training Loss: tensor(0.0383)\n",
      "16234 Training Loss: tensor(0.0387)\n",
      "16235 Training Loss: tensor(0.0367)\n",
      "16236 Training Loss: tensor(0.0357)\n",
      "16237 Training Loss: tensor(0.0384)\n",
      "16238 Training Loss: tensor(0.0357)\n",
      "16239 Training Loss: tensor(0.0399)\n",
      "16240 Training Loss: tensor(0.0395)\n",
      "16241 Training Loss: tensor(0.0377)\n",
      "16242 Training Loss: tensor(0.0355)\n",
      "16243 Training Loss: tensor(0.0350)\n",
      "16244 Training Loss: tensor(0.0431)\n",
      "16245 Training Loss: tensor(0.0393)\n",
      "16246 Training Loss: tensor(0.0381)\n",
      "16247 Training Loss: tensor(0.0399)\n",
      "16248 Training Loss: tensor(0.0370)\n",
      "16249 Training Loss: tensor(0.0415)\n",
      "16250 Training Loss: tensor(0.0394)\n",
      "16251 Training Loss: tensor(0.0371)\n",
      "16252 Training Loss: tensor(0.0417)\n",
      "16253 Training Loss: tensor(0.0359)\n",
      "16254 Training Loss: tensor(0.0346)\n",
      "16255 Training Loss: tensor(0.0434)\n",
      "16256 Training Loss: tensor(0.0393)\n",
      "16257 Training Loss: tensor(0.0389)\n",
      "16258 Training Loss: tensor(0.0407)\n",
      "16259 Training Loss: tensor(0.0352)\n",
      "16260 Training Loss: tensor(0.0402)\n",
      "16261 Training Loss: tensor(0.0390)\n",
      "16262 Training Loss: tensor(0.0416)\n",
      "16263 Training Loss: tensor(0.0356)\n",
      "16264 Training Loss: tensor(0.0337)\n",
      "16265 Training Loss: tensor(0.0363)\n",
      "16266 Training Loss: tensor(0.0403)\n",
      "16267 Training Loss: tensor(0.0449)\n",
      "16268 Training Loss: tensor(0.0350)\n",
      "16269 Training Loss: tensor(0.0409)\n",
      "16270 Training Loss: tensor(0.0422)\n",
      "16271 Training Loss: tensor(0.0397)\n",
      "16272 Training Loss: tensor(0.0440)\n",
      "16273 Training Loss: tensor(0.0399)\n",
      "16274 Training Loss: tensor(0.0388)\n",
      "16275 Training Loss: tensor(0.0354)\n",
      "16276 Training Loss: tensor(0.0362)\n",
      "16277 Training Loss: tensor(0.0412)\n",
      "16278 Training Loss: tensor(0.0424)\n",
      "16279 Training Loss: tensor(0.0404)\n",
      "16280 Training Loss: tensor(0.0409)\n",
      "16281 Training Loss: tensor(0.0361)\n",
      "16282 Training Loss: tensor(0.0392)\n",
      "16283 Training Loss: tensor(0.0380)\n",
      "16284 Training Loss: tensor(0.0395)\n",
      "16285 Training Loss: tensor(0.0382)\n",
      "16286 Training Loss: tensor(0.0359)\n",
      "16287 Training Loss: tensor(0.0387)\n",
      "16288 Training Loss: tensor(0.0355)\n",
      "16289 Training Loss: tensor(0.0349)\n",
      "16290 Training Loss: tensor(0.0422)\n",
      "16291 Training Loss: tensor(0.0346)\n",
      "16292 Training Loss: tensor(0.0389)\n",
      "16293 Training Loss: tensor(0.0418)\n",
      "16294 Training Loss: tensor(0.0376)\n",
      "16295 Training Loss: tensor(0.0390)\n",
      "16296 Training Loss: tensor(0.0369)\n",
      "16297 Training Loss: tensor(0.0379)\n",
      "16298 Training Loss: tensor(0.0342)\n",
      "16299 Training Loss: tensor(0.0394)\n",
      "16300 Training Loss: tensor(0.0405)\n",
      "16301 Training Loss: tensor(0.0441)\n",
      "16302 Training Loss: tensor(0.0409)\n",
      "16303 Training Loss: tensor(0.0367)\n",
      "16304 Training Loss: tensor(0.0391)\n",
      "16305 Training Loss: tensor(0.0330)\n",
      "16306 Training Loss: tensor(0.0337)\n",
      "16307 Training Loss: tensor(0.0387)\n",
      "16308 Training Loss: tensor(0.0418)\n",
      "16309 Training Loss: tensor(0.0396)\n",
      "16310 Training Loss: tensor(0.0359)\n",
      "16311 Training Loss: tensor(0.0459)\n",
      "16312 Training Loss: tensor(0.0371)\n",
      "16313 Training Loss: tensor(0.0400)\n",
      "16314 Training Loss: tensor(0.0375)\n",
      "16315 Training Loss: tensor(0.0401)\n",
      "16316 Training Loss: tensor(0.0478)\n",
      "16317 Training Loss: tensor(0.0387)\n",
      "16318 Training Loss: tensor(0.0452)\n",
      "16319 Training Loss: tensor(0.0492)\n",
      "16320 Training Loss: tensor(0.0427)\n",
      "16321 Training Loss: tensor(0.0397)\n",
      "16322 Training Loss: tensor(0.0366)\n",
      "16323 Training Loss: tensor(0.0409)\n",
      "16324 Training Loss: tensor(0.0399)\n",
      "16325 Training Loss: tensor(0.0362)\n",
      "16326 Training Loss: tensor(0.0402)\n",
      "16327 Training Loss: tensor(0.0347)\n",
      "16328 Training Loss: tensor(0.0410)\n",
      "16329 Training Loss: tensor(0.0408)\n",
      "16330 Training Loss: tensor(0.0343)\n",
      "16331 Training Loss: tensor(0.0346)\n",
      "16332 Training Loss: tensor(0.0349)\n",
      "16333 Training Loss: tensor(0.0378)\n",
      "16334 Training Loss: tensor(0.0331)\n",
      "16335 Training Loss: tensor(0.0401)\n",
      "16336 Training Loss: tensor(0.0368)\n",
      "16337 Training Loss: tensor(0.0427)\n",
      "16338 Training Loss: tensor(0.0344)\n",
      "16339 Training Loss: tensor(0.0381)\n",
      "16340 Training Loss: tensor(0.0397)\n",
      "16341 Training Loss: tensor(0.0360)\n",
      "16342 Training Loss: tensor(0.0353)\n",
      "16343 Training Loss: tensor(0.0399)\n",
      "16344 Training Loss: tensor(0.0381)\n",
      "16345 Training Loss: tensor(0.0416)\n",
      "16346 Training Loss: tensor(0.0344)\n",
      "16347 Training Loss: tensor(0.0394)\n",
      "16348 Training Loss: tensor(0.0375)\n",
      "16349 Training Loss: tensor(0.0363)\n",
      "16350 Training Loss: tensor(0.0344)\n",
      "16351 Training Loss: tensor(0.0399)\n",
      "16352 Training Loss: tensor(0.0403)\n",
      "16353 Training Loss: tensor(0.0345)\n",
      "16354 Training Loss: tensor(0.0393)\n",
      "16355 Training Loss: tensor(0.0378)\n",
      "16356 Training Loss: tensor(0.0356)\n",
      "16357 Training Loss: tensor(0.0374)\n",
      "16358 Training Loss: tensor(0.0418)\n",
      "16359 Training Loss: tensor(0.0415)\n",
      "16360 Training Loss: tensor(0.0422)\n",
      "16361 Training Loss: tensor(0.0359)\n",
      "16362 Training Loss: tensor(0.0360)\n",
      "16363 Training Loss: tensor(0.0381)\n",
      "16364 Training Loss: tensor(0.0352)\n",
      "16365 Training Loss: tensor(0.0337)\n",
      "16366 Training Loss: tensor(0.0431)\n",
      "16367 Training Loss: tensor(0.0439)\n",
      "16368 Training Loss: tensor(0.0372)\n",
      "16369 Training Loss: tensor(0.0383)\n",
      "16370 Training Loss: tensor(0.0364)\n",
      "16371 Training Loss: tensor(0.0360)\n",
      "16372 Training Loss: tensor(0.0391)\n",
      "16373 Training Loss: tensor(0.0349)\n",
      "16374 Training Loss: tensor(0.0402)\n",
      "16375 Training Loss: tensor(0.0333)\n",
      "16376 Training Loss: tensor(0.0399)\n",
      "16377 Training Loss: tensor(0.0369)\n",
      "16378 Training Loss: tensor(0.0404)\n",
      "16379 Training Loss: tensor(0.0386)\n",
      "16380 Training Loss: tensor(0.0423)\n",
      "16381 Training Loss: tensor(0.0444)\n",
      "16382 Training Loss: tensor(0.0373)\n",
      "16383 Training Loss: tensor(0.0371)\n",
      "16384 Training Loss: tensor(0.0436)\n",
      "16385 Training Loss: tensor(0.0324)\n",
      "16386 Training Loss: tensor(0.0405)\n",
      "16387 Training Loss: tensor(0.0404)\n",
      "16388 Training Loss: tensor(0.0377)\n",
      "16389 Training Loss: tensor(0.0331)\n",
      "16390 Training Loss: tensor(0.0394)\n",
      "16391 Training Loss: tensor(0.0387)\n",
      "16392 Training Loss: tensor(0.0384)\n",
      "16393 Training Loss: tensor(0.0348)\n",
      "16394 Training Loss: tensor(0.0389)\n",
      "16395 Training Loss: tensor(0.0418)\n",
      "16396 Training Loss: tensor(0.0411)\n",
      "16397 Training Loss: tensor(0.0398)\n",
      "16398 Training Loss: tensor(0.0397)\n",
      "16399 Training Loss: tensor(0.0377)\n",
      "16400 Training Loss: tensor(0.0387)\n",
      "16401 Training Loss: tensor(0.0399)\n",
      "16402 Training Loss: tensor(0.0407)\n",
      "16403 Training Loss: tensor(0.0401)\n",
      "16404 Training Loss: tensor(0.0365)\n",
      "16405 Training Loss: tensor(0.0378)\n",
      "16406 Training Loss: tensor(0.0369)\n",
      "16407 Training Loss: tensor(0.0434)\n",
      "16408 Training Loss: tensor(0.0409)\n",
      "16409 Training Loss: tensor(0.0398)\n",
      "16410 Training Loss: tensor(0.0337)\n",
      "16411 Training Loss: tensor(0.0382)\n",
      "16412 Training Loss: tensor(0.0380)\n",
      "16413 Training Loss: tensor(0.0412)\n",
      "16414 Training Loss: tensor(0.0366)\n",
      "16415 Training Loss: tensor(0.0396)\n",
      "16416 Training Loss: tensor(0.0335)\n",
      "16417 Training Loss: tensor(0.0422)\n",
      "16418 Training Loss: tensor(0.0387)\n",
      "16419 Training Loss: tensor(0.0412)\n",
      "16420 Training Loss: tensor(0.0367)\n",
      "16421 Training Loss: tensor(0.0372)\n",
      "16422 Training Loss: tensor(0.0391)\n",
      "16423 Training Loss: tensor(0.0331)\n",
      "16424 Training Loss: tensor(0.0379)\n",
      "16425 Training Loss: tensor(0.0384)\n",
      "16426 Training Loss: tensor(0.0398)\n",
      "16427 Training Loss: tensor(0.0364)\n",
      "16428 Training Loss: tensor(0.0389)\n",
      "16429 Training Loss: tensor(0.0387)\n",
      "16430 Training Loss: tensor(0.0409)\n",
      "16431 Training Loss: tensor(0.0430)\n",
      "16432 Training Loss: tensor(0.0392)\n",
      "16433 Training Loss: tensor(0.0364)\n",
      "16434 Training Loss: tensor(0.0384)\n",
      "16435 Training Loss: tensor(0.0347)\n",
      "16436 Training Loss: tensor(0.0414)\n",
      "16437 Training Loss: tensor(0.0371)\n",
      "16438 Training Loss: tensor(0.0410)\n",
      "16439 Training Loss: tensor(0.0421)\n",
      "16440 Training Loss: tensor(0.0413)\n",
      "16441 Training Loss: tensor(0.0382)\n",
      "16442 Training Loss: tensor(0.0354)\n",
      "16443 Training Loss: tensor(0.0400)\n",
      "16444 Training Loss: tensor(0.0416)\n",
      "16445 Training Loss: tensor(0.0366)\n",
      "16446 Training Loss: tensor(0.0461)\n",
      "16447 Training Loss: tensor(0.0340)\n",
      "16448 Training Loss: tensor(0.0352)\n",
      "16449 Training Loss: tensor(0.0413)\n",
      "16450 Training Loss: tensor(0.0393)\n",
      "16451 Training Loss: tensor(0.0346)\n",
      "16452 Training Loss: tensor(0.0349)\n",
      "16453 Training Loss: tensor(0.0392)\n",
      "16454 Training Loss: tensor(0.0391)\n",
      "16455 Training Loss: tensor(0.0416)\n",
      "16456 Training Loss: tensor(0.0354)\n",
      "16457 Training Loss: tensor(0.0385)\n",
      "16458 Training Loss: tensor(0.0364)\n",
      "16459 Training Loss: tensor(0.0375)\n",
      "16460 Training Loss: tensor(0.0426)\n",
      "16461 Training Loss: tensor(0.0378)\n",
      "16462 Training Loss: tensor(0.0398)\n",
      "16463 Training Loss: tensor(0.0422)\n",
      "16464 Training Loss: tensor(0.0400)\n",
      "16465 Training Loss: tensor(0.0381)\n",
      "16466 Training Loss: tensor(0.0384)\n",
      "16467 Training Loss: tensor(0.0408)\n",
      "16468 Training Loss: tensor(0.0347)\n",
      "16469 Training Loss: tensor(0.0390)\n",
      "16470 Training Loss: tensor(0.0396)\n",
      "16471 Training Loss: tensor(0.0389)\n",
      "16472 Training Loss: tensor(0.0335)\n",
      "16473 Training Loss: tensor(0.0383)\n",
      "16474 Training Loss: tensor(0.0344)\n",
      "16475 Training Loss: tensor(0.0347)\n",
      "16476 Training Loss: tensor(0.0405)\n",
      "16477 Training Loss: tensor(0.0427)\n",
      "16478 Training Loss: tensor(0.0446)\n",
      "16479 Training Loss: tensor(0.0363)\n",
      "16480 Training Loss: tensor(0.0396)\n",
      "16481 Training Loss: tensor(0.0389)\n",
      "16482 Training Loss: tensor(0.0412)\n",
      "16483 Training Loss: tensor(0.0452)\n",
      "16484 Training Loss: tensor(0.0422)\n",
      "16485 Training Loss: tensor(0.0365)\n",
      "16486 Training Loss: tensor(0.0386)\n",
      "16487 Training Loss: tensor(0.0371)\n",
      "16488 Training Loss: tensor(0.0372)\n",
      "16489 Training Loss: tensor(0.0397)\n",
      "16490 Training Loss: tensor(0.0387)\n",
      "16491 Training Loss: tensor(0.0414)\n",
      "16492 Training Loss: tensor(0.0389)\n",
      "16493 Training Loss: tensor(0.0339)\n",
      "16494 Training Loss: tensor(0.0409)\n",
      "16495 Training Loss: tensor(0.0378)\n",
      "16496 Training Loss: tensor(0.0365)\n",
      "16497 Training Loss: tensor(0.0499)\n",
      "16498 Training Loss: tensor(0.0373)\n",
      "16499 Training Loss: tensor(0.0387)\n",
      "16500 Training Loss: tensor(0.0395)\n",
      "16501 Training Loss: tensor(0.0373)\n",
      "16502 Training Loss: tensor(0.0376)\n",
      "16503 Training Loss: tensor(0.0378)\n",
      "16504 Training Loss: tensor(0.0364)\n",
      "16505 Training Loss: tensor(0.0411)\n",
      "16506 Training Loss: tensor(0.0425)\n",
      "16507 Training Loss: tensor(0.0371)\n",
      "16508 Training Loss: tensor(0.0372)\n",
      "16509 Training Loss: tensor(0.0342)\n",
      "16510 Training Loss: tensor(0.0386)\n",
      "16511 Training Loss: tensor(0.0396)\n",
      "16512 Training Loss: tensor(0.0383)\n",
      "16513 Training Loss: tensor(0.0445)\n",
      "16514 Training Loss: tensor(0.0418)\n",
      "16515 Training Loss: tensor(0.0378)\n",
      "16516 Training Loss: tensor(0.0429)\n",
      "16517 Training Loss: tensor(0.0353)\n",
      "16518 Training Loss: tensor(0.0347)\n",
      "16519 Training Loss: tensor(0.0347)\n",
      "16520 Training Loss: tensor(0.0367)\n",
      "16521 Training Loss: tensor(0.0375)\n",
      "16522 Training Loss: tensor(0.0355)\n",
      "16523 Training Loss: tensor(0.0382)\n",
      "16524 Training Loss: tensor(0.0405)\n",
      "16525 Training Loss: tensor(0.0373)\n",
      "16526 Training Loss: tensor(0.0423)\n",
      "16527 Training Loss: tensor(0.0402)\n",
      "16528 Training Loss: tensor(0.0322)\n",
      "16529 Training Loss: tensor(0.0390)\n",
      "16530 Training Loss: tensor(0.0336)\n",
      "16531 Training Loss: tensor(0.0412)\n",
      "16532 Training Loss: tensor(0.0388)\n",
      "16533 Training Loss: tensor(0.0434)\n",
      "16534 Training Loss: tensor(0.0378)\n",
      "16535 Training Loss: tensor(0.0346)\n",
      "16536 Training Loss: tensor(0.0397)\n",
      "16537 Training Loss: tensor(0.0399)\n",
      "16538 Training Loss: tensor(0.0382)\n",
      "16539 Training Loss: tensor(0.0402)\n",
      "16540 Training Loss: tensor(0.0392)\n",
      "16541 Training Loss: tensor(0.0414)\n",
      "16542 Training Loss: tensor(0.0410)\n",
      "16543 Training Loss: tensor(0.0378)\n",
      "16544 Training Loss: tensor(0.0405)\n",
      "16545 Training Loss: tensor(0.0353)\n",
      "16546 Training Loss: tensor(0.0382)\n",
      "16547 Training Loss: tensor(0.0368)\n",
      "16548 Training Loss: tensor(0.0419)\n",
      "16549 Training Loss: tensor(0.0376)\n",
      "16550 Training Loss: tensor(0.0349)\n",
      "16551 Training Loss: tensor(0.0395)\n",
      "16552 Training Loss: tensor(0.0423)\n",
      "16553 Training Loss: tensor(0.0352)\n",
      "16554 Training Loss: tensor(0.0396)\n",
      "16555 Training Loss: tensor(0.0432)\n",
      "16556 Training Loss: tensor(0.0347)\n",
      "16557 Training Loss: tensor(0.0403)\n",
      "16558 Training Loss: tensor(0.0351)\n",
      "16559 Training Loss: tensor(0.0404)\n",
      "16560 Training Loss: tensor(0.0416)\n",
      "16561 Training Loss: tensor(0.0364)\n",
      "16562 Training Loss: tensor(0.0388)\n",
      "16563 Training Loss: tensor(0.0452)\n",
      "16564 Training Loss: tensor(0.0412)\n",
      "16565 Training Loss: tensor(0.0388)\n",
      "16566 Training Loss: tensor(0.0351)\n",
      "16567 Training Loss: tensor(0.0391)\n",
      "16568 Training Loss: tensor(0.0361)\n",
      "16569 Training Loss: tensor(0.0377)\n",
      "16570 Training Loss: tensor(0.0346)\n",
      "16571 Training Loss: tensor(0.0371)\n",
      "16572 Training Loss: tensor(0.0364)\n",
      "16573 Training Loss: tensor(0.0344)\n",
      "16574 Training Loss: tensor(0.0385)\n",
      "16575 Training Loss: tensor(0.0349)\n",
      "16576 Training Loss: tensor(0.0396)\n",
      "16577 Training Loss: tensor(0.0372)\n",
      "16578 Training Loss: tensor(0.0389)\n",
      "16579 Training Loss: tensor(0.0454)\n",
      "16580 Training Loss: tensor(0.0348)\n",
      "16581 Training Loss: tensor(0.0380)\n",
      "16582 Training Loss: tensor(0.0375)\n",
      "16583 Training Loss: tensor(0.0416)\n",
      "16584 Training Loss: tensor(0.0384)\n",
      "16585 Training Loss: tensor(0.0392)\n",
      "16586 Training Loss: tensor(0.0389)\n",
      "16587 Training Loss: tensor(0.0384)\n",
      "16588 Training Loss: tensor(0.0395)\n",
      "16589 Training Loss: tensor(0.0370)\n",
      "16590 Training Loss: tensor(0.0340)\n",
      "16591 Training Loss: tensor(0.0343)\n",
      "16592 Training Loss: tensor(0.0375)\n",
      "16593 Training Loss: tensor(0.0383)\n",
      "16594 Training Loss: tensor(0.0401)\n",
      "16595 Training Loss: tensor(0.0416)\n",
      "16596 Training Loss: tensor(0.0392)\n",
      "16597 Training Loss: tensor(0.0394)\n",
      "16598 Training Loss: tensor(0.0392)\n",
      "16599 Training Loss: tensor(0.0370)\n",
      "16600 Training Loss: tensor(0.0410)\n",
      "16601 Training Loss: tensor(0.0379)\n",
      "16602 Training Loss: tensor(0.0368)\n",
      "16603 Training Loss: tensor(0.0385)\n",
      "16604 Training Loss: tensor(0.0386)\n",
      "16605 Training Loss: tensor(0.0370)\n",
      "16606 Training Loss: tensor(0.0351)\n",
      "16607 Training Loss: tensor(0.0332)\n",
      "16608 Training Loss: tensor(0.0406)\n",
      "16609 Training Loss: tensor(0.0447)\n",
      "16610 Training Loss: tensor(0.0385)\n",
      "16611 Training Loss: tensor(0.0383)\n",
      "16612 Training Loss: tensor(0.0402)\n",
      "16613 Training Loss: tensor(0.0441)\n",
      "16614 Training Loss: tensor(0.0376)\n",
      "16615 Training Loss: tensor(0.0389)\n",
      "16616 Training Loss: tensor(0.0487)\n",
      "16617 Training Loss: tensor(0.0415)\n",
      "16618 Training Loss: tensor(0.0365)\n",
      "16619 Training Loss: tensor(0.0406)\n",
      "16620 Training Loss: tensor(0.0355)\n",
      "16621 Training Loss: tensor(0.0361)\n",
      "16622 Training Loss: tensor(0.0371)\n",
      "16623 Training Loss: tensor(0.0350)\n",
      "16624 Training Loss: tensor(0.0366)\n",
      "16625 Training Loss: tensor(0.0405)\n",
      "16626 Training Loss: tensor(0.0406)\n",
      "16627 Training Loss: tensor(0.0419)\n",
      "16628 Training Loss: tensor(0.0338)\n",
      "16629 Training Loss: tensor(0.0368)\n",
      "16630 Training Loss: tensor(0.0351)\n",
      "16631 Training Loss: tensor(0.0422)\n",
      "16632 Training Loss: tensor(0.0336)\n",
      "16633 Training Loss: tensor(0.0354)\n",
      "16634 Training Loss: tensor(0.0411)\n",
      "16635 Training Loss: tensor(0.0360)\n",
      "16636 Training Loss: tensor(0.0368)\n",
      "16637 Training Loss: tensor(0.0418)\n",
      "16638 Training Loss: tensor(0.0391)\n",
      "16639 Training Loss: tensor(0.0364)\n",
      "16640 Training Loss: tensor(0.0350)\n",
      "16641 Training Loss: tensor(0.0418)\n",
      "16642 Training Loss: tensor(0.0416)\n",
      "16643 Training Loss: tensor(0.0376)\n",
      "16644 Training Loss: tensor(0.0413)\n",
      "16645 Training Loss: tensor(0.0432)\n",
      "16646 Training Loss: tensor(0.0340)\n",
      "16647 Training Loss: tensor(0.0362)\n",
      "16648 Training Loss: tensor(0.0425)\n",
      "16649 Training Loss: tensor(0.0408)\n",
      "16650 Training Loss: tensor(0.0426)\n",
      "16651 Training Loss: tensor(0.0358)\n",
      "16652 Training Loss: tensor(0.0332)\n",
      "16653 Training Loss: tensor(0.0395)\n",
      "16654 Training Loss: tensor(0.0359)\n",
      "16655 Training Loss: tensor(0.0347)\n",
      "16656 Training Loss: tensor(0.0360)\n",
      "16657 Training Loss: tensor(0.0452)\n",
      "16658 Training Loss: tensor(0.0390)\n",
      "16659 Training Loss: tensor(0.0373)\n",
      "16660 Training Loss: tensor(0.0385)\n",
      "16661 Training Loss: tensor(0.0372)\n",
      "16662 Training Loss: tensor(0.0397)\n",
      "16663 Training Loss: tensor(0.0404)\n",
      "16664 Training Loss: tensor(0.0408)\n",
      "16665 Training Loss: tensor(0.0375)\n",
      "16666 Training Loss: tensor(0.0366)\n",
      "16667 Training Loss: tensor(0.0396)\n",
      "16668 Training Loss: tensor(0.0351)\n",
      "16669 Training Loss: tensor(0.0409)\n",
      "16670 Training Loss: tensor(0.0340)\n",
      "16671 Training Loss: tensor(0.0365)\n",
      "16672 Training Loss: tensor(0.0424)\n",
      "16673 Training Loss: tensor(0.0310)\n",
      "16674 Training Loss: tensor(0.0369)\n",
      "16675 Training Loss: tensor(0.0376)\n",
      "16676 Training Loss: tensor(0.0396)\n",
      "16677 Training Loss: tensor(0.0381)\n",
      "16678 Training Loss: tensor(0.0404)\n",
      "16679 Training Loss: tensor(0.0404)\n",
      "16680 Training Loss: tensor(0.0398)\n",
      "16681 Training Loss: tensor(0.0379)\n",
      "16682 Training Loss: tensor(0.0328)\n",
      "16683 Training Loss: tensor(0.0394)\n",
      "16684 Training Loss: tensor(0.0427)\n",
      "16685 Training Loss: tensor(0.0380)\n",
      "16686 Training Loss: tensor(0.0335)\n",
      "16687 Training Loss: tensor(0.0404)\n",
      "16688 Training Loss: tensor(0.0386)\n",
      "16689 Training Loss: tensor(0.0422)\n",
      "16690 Training Loss: tensor(0.0379)\n",
      "16691 Training Loss: tensor(0.0339)\n",
      "16692 Training Loss: tensor(0.0385)\n",
      "16693 Training Loss: tensor(0.0379)\n",
      "16694 Training Loss: tensor(0.0372)\n",
      "16695 Training Loss: tensor(0.0385)\n",
      "16696 Training Loss: tensor(0.0359)\n",
      "16697 Training Loss: tensor(0.0391)\n",
      "16698 Training Loss: tensor(0.0382)\n",
      "16699 Training Loss: tensor(0.0395)\n",
      "16700 Training Loss: tensor(0.0343)\n",
      "16701 Training Loss: tensor(0.0332)\n",
      "16702 Training Loss: tensor(0.0405)\n",
      "16703 Training Loss: tensor(0.0356)\n",
      "16704 Training Loss: tensor(0.0384)\n",
      "16705 Training Loss: tensor(0.0364)\n",
      "16706 Training Loss: tensor(0.0344)\n",
      "16707 Training Loss: tensor(0.0375)\n",
      "16708 Training Loss: tensor(0.0402)\n",
      "16709 Training Loss: tensor(0.0373)\n",
      "16710 Training Loss: tensor(0.0389)\n",
      "16711 Training Loss: tensor(0.0380)\n",
      "16712 Training Loss: tensor(0.0343)\n",
      "16713 Training Loss: tensor(0.0406)\n",
      "16714 Training Loss: tensor(0.0378)\n",
      "16715 Training Loss: tensor(0.0417)\n",
      "16716 Training Loss: tensor(0.0368)\n",
      "16717 Training Loss: tensor(0.0403)\n",
      "16718 Training Loss: tensor(0.0440)\n",
      "16719 Training Loss: tensor(0.0420)\n",
      "16720 Training Loss: tensor(0.0387)\n",
      "16721 Training Loss: tensor(0.0406)\n",
      "16722 Training Loss: tensor(0.0391)\n",
      "16723 Training Loss: tensor(0.0352)\n",
      "16724 Training Loss: tensor(0.0407)\n",
      "16725 Training Loss: tensor(0.0335)\n",
      "16726 Training Loss: tensor(0.0351)\n",
      "16727 Training Loss: tensor(0.0350)\n",
      "16728 Training Loss: tensor(0.0371)\n",
      "16729 Training Loss: tensor(0.0420)\n",
      "16730 Training Loss: tensor(0.0334)\n",
      "16731 Training Loss: tensor(0.0386)\n",
      "16732 Training Loss: tensor(0.0370)\n",
      "16733 Training Loss: tensor(0.0451)\n",
      "16734 Training Loss: tensor(0.0439)\n",
      "16735 Training Loss: tensor(0.0405)\n",
      "16736 Training Loss: tensor(0.0416)\n",
      "16737 Training Loss: tensor(0.0374)\n",
      "16738 Training Loss: tensor(0.0381)\n",
      "16739 Training Loss: tensor(0.0380)\n",
      "16740 Training Loss: tensor(0.0386)\n",
      "16741 Training Loss: tensor(0.0390)\n",
      "16742 Training Loss: tensor(0.0397)\n",
      "16743 Training Loss: tensor(0.0403)\n",
      "16744 Training Loss: tensor(0.0391)\n",
      "16745 Training Loss: tensor(0.0370)\n",
      "16746 Training Loss: tensor(0.0388)\n",
      "16747 Training Loss: tensor(0.0419)\n",
      "16748 Training Loss: tensor(0.0390)\n",
      "16749 Training Loss: tensor(0.0400)\n",
      "16750 Training Loss: tensor(0.0361)\n",
      "16751 Training Loss: tensor(0.0409)\n",
      "16752 Training Loss: tensor(0.0391)\n",
      "16753 Training Loss: tensor(0.0404)\n",
      "16754 Training Loss: tensor(0.0313)\n",
      "16755 Training Loss: tensor(0.0412)\n",
      "16756 Training Loss: tensor(0.0373)\n",
      "16757 Training Loss: tensor(0.0396)\n",
      "16758 Training Loss: tensor(0.0363)\n",
      "16759 Training Loss: tensor(0.0336)\n",
      "16760 Training Loss: tensor(0.0445)\n",
      "16761 Training Loss: tensor(0.0385)\n",
      "16762 Training Loss: tensor(0.0445)\n",
      "16763 Training Loss: tensor(0.0388)\n",
      "16764 Training Loss: tensor(0.0357)\n",
      "16765 Training Loss: tensor(0.0372)\n",
      "16766 Training Loss: tensor(0.0391)\n",
      "16767 Training Loss: tensor(0.0395)\n",
      "16768 Training Loss: tensor(0.0458)\n",
      "16769 Training Loss: tensor(0.0379)\n",
      "16770 Training Loss: tensor(0.0366)\n",
      "16771 Training Loss: tensor(0.0410)\n",
      "16772 Training Loss: tensor(0.0431)\n",
      "16773 Training Loss: tensor(0.0470)\n",
      "16774 Training Loss: tensor(0.0390)\n",
      "16775 Training Loss: tensor(0.0413)\n",
      "16776 Training Loss: tensor(0.0369)\n",
      "16777 Training Loss: tensor(0.0356)\n",
      "16778 Training Loss: tensor(0.0362)\n",
      "16779 Training Loss: tensor(0.0412)\n",
      "16780 Training Loss: tensor(0.0382)\n",
      "16781 Training Loss: tensor(0.0403)\n",
      "16782 Training Loss: tensor(0.0386)\n",
      "16783 Training Loss: tensor(0.0393)\n",
      "16784 Training Loss: tensor(0.0383)\n",
      "16785 Training Loss: tensor(0.0377)\n",
      "16786 Training Loss: tensor(0.0334)\n",
      "16787 Training Loss: tensor(0.0363)\n",
      "16788 Training Loss: tensor(0.0434)\n",
      "16789 Training Loss: tensor(0.0367)\n",
      "16790 Training Loss: tensor(0.0403)\n",
      "16791 Training Loss: tensor(0.0380)\n",
      "16792 Training Loss: tensor(0.0342)\n",
      "16793 Training Loss: tensor(0.0363)\n",
      "16794 Training Loss: tensor(0.0385)\n",
      "16795 Training Loss: tensor(0.0403)\n",
      "16796 Training Loss: tensor(0.0384)\n",
      "16797 Training Loss: tensor(0.0364)\n",
      "16798 Training Loss: tensor(0.0412)\n",
      "16799 Training Loss: tensor(0.0412)\n",
      "16800 Training Loss: tensor(0.0402)\n",
      "16801 Training Loss: tensor(0.0371)\n",
      "16802 Training Loss: tensor(0.0389)\n",
      "16803 Training Loss: tensor(0.0372)\n",
      "16804 Training Loss: tensor(0.0364)\n",
      "16805 Training Loss: tensor(0.0335)\n",
      "16806 Training Loss: tensor(0.0393)\n",
      "16807 Training Loss: tensor(0.0383)\n",
      "16808 Training Loss: tensor(0.0355)\n",
      "16809 Training Loss: tensor(0.0391)\n",
      "16810 Training Loss: tensor(0.0378)\n",
      "16811 Training Loss: tensor(0.0423)\n",
      "16812 Training Loss: tensor(0.0322)\n",
      "16813 Training Loss: tensor(0.0351)\n",
      "16814 Training Loss: tensor(0.0394)\n",
      "16815 Training Loss: tensor(0.0425)\n",
      "16816 Training Loss: tensor(0.0431)\n",
      "16817 Training Loss: tensor(0.0445)\n",
      "16818 Training Loss: tensor(0.0396)\n",
      "16819 Training Loss: tensor(0.0406)\n",
      "16820 Training Loss: tensor(0.0455)\n",
      "16821 Training Loss: tensor(0.0389)\n",
      "16822 Training Loss: tensor(0.0346)\n",
      "16823 Training Loss: tensor(0.0431)\n",
      "16824 Training Loss: tensor(0.0420)\n",
      "16825 Training Loss: tensor(0.0399)\n",
      "16826 Training Loss: tensor(0.0438)\n",
      "16827 Training Loss: tensor(0.0357)\n",
      "16828 Training Loss: tensor(0.0362)\n",
      "16829 Training Loss: tensor(0.0414)\n",
      "16830 Training Loss: tensor(0.0386)\n",
      "16831 Training Loss: tensor(0.0410)\n",
      "16832 Training Loss: tensor(0.0396)\n",
      "16833 Training Loss: tensor(0.0391)\n",
      "16834 Training Loss: tensor(0.0411)\n",
      "16835 Training Loss: tensor(0.0366)\n",
      "16836 Training Loss: tensor(0.0370)\n",
      "16837 Training Loss: tensor(0.0430)\n",
      "16838 Training Loss: tensor(0.0393)\n",
      "16839 Training Loss: tensor(0.0376)\n",
      "16840 Training Loss: tensor(0.0440)\n",
      "16841 Training Loss: tensor(0.0401)\n",
      "16842 Training Loss: tensor(0.0377)\n",
      "16843 Training Loss: tensor(0.0355)\n",
      "16844 Training Loss: tensor(0.0388)\n",
      "16845 Training Loss: tensor(0.0418)\n",
      "16846 Training Loss: tensor(0.0408)\n",
      "16847 Training Loss: tensor(0.0385)\n",
      "16848 Training Loss: tensor(0.0362)\n",
      "16849 Training Loss: tensor(0.0390)\n",
      "16850 Training Loss: tensor(0.0360)\n",
      "16851 Training Loss: tensor(0.0408)\n",
      "16852 Training Loss: tensor(0.0379)\n",
      "16853 Training Loss: tensor(0.0386)\n",
      "16854 Training Loss: tensor(0.0464)\n",
      "16855 Training Loss: tensor(0.0400)\n",
      "16856 Training Loss: tensor(0.0424)\n",
      "16857 Training Loss: tensor(0.0336)\n",
      "16858 Training Loss: tensor(0.0359)\n",
      "16859 Training Loss: tensor(0.0423)\n",
      "16860 Training Loss: tensor(0.0405)\n",
      "16861 Training Loss: tensor(0.0428)\n",
      "16862 Training Loss: tensor(0.0408)\n",
      "16863 Training Loss: tensor(0.0371)\n",
      "16864 Training Loss: tensor(0.0393)\n",
      "16865 Training Loss: tensor(0.0363)\n",
      "16866 Training Loss: tensor(0.0359)\n",
      "16867 Training Loss: tensor(0.0409)\n",
      "16868 Training Loss: tensor(0.0475)\n",
      "16869 Training Loss: tensor(0.0389)\n",
      "16870 Training Loss: tensor(0.0376)\n",
      "16871 Training Loss: tensor(0.0357)\n",
      "16872 Training Loss: tensor(0.0389)\n",
      "16873 Training Loss: tensor(0.0389)\n",
      "16874 Training Loss: tensor(0.0370)\n",
      "16875 Training Loss: tensor(0.0339)\n",
      "16876 Training Loss: tensor(0.0390)\n",
      "16877 Training Loss: tensor(0.0347)\n",
      "16878 Training Loss: tensor(0.0392)\n",
      "16879 Training Loss: tensor(0.0375)\n",
      "16880 Training Loss: tensor(0.0371)\n",
      "16881 Training Loss: tensor(0.0386)\n",
      "16882 Training Loss: tensor(0.0336)\n",
      "16883 Training Loss: tensor(0.0383)\n",
      "16884 Training Loss: tensor(0.0372)\n",
      "16885 Training Loss: tensor(0.0376)\n",
      "16886 Training Loss: tensor(0.0353)\n",
      "16887 Training Loss: tensor(0.0399)\n",
      "16888 Training Loss: tensor(0.0368)\n",
      "16889 Training Loss: tensor(0.0366)\n",
      "16890 Training Loss: tensor(0.0349)\n",
      "16891 Training Loss: tensor(0.0372)\n",
      "16892 Training Loss: tensor(0.0416)\n",
      "16893 Training Loss: tensor(0.0369)\n",
      "16894 Training Loss: tensor(0.0362)\n",
      "16895 Training Loss: tensor(0.0350)\n",
      "16896 Training Loss: tensor(0.0386)\n",
      "16897 Training Loss: tensor(0.0351)\n",
      "16898 Training Loss: tensor(0.0355)\n",
      "16899 Training Loss: tensor(0.0401)\n",
      "16900 Training Loss: tensor(0.0394)\n",
      "16901 Training Loss: tensor(0.0384)\n",
      "16902 Training Loss: tensor(0.0372)\n",
      "16903 Training Loss: tensor(0.0354)\n",
      "16904 Training Loss: tensor(0.0342)\n",
      "16905 Training Loss: tensor(0.0324)\n",
      "16906 Training Loss: tensor(0.0354)\n",
      "16907 Training Loss: tensor(0.0450)\n",
      "16908 Training Loss: tensor(0.0402)\n",
      "16909 Training Loss: tensor(0.0355)\n",
      "16910 Training Loss: tensor(0.0349)\n",
      "16911 Training Loss: tensor(0.0422)\n",
      "16912 Training Loss: tensor(0.0363)\n",
      "16913 Training Loss: tensor(0.0447)\n",
      "16914 Training Loss: tensor(0.0418)\n",
      "16915 Training Loss: tensor(0.0391)\n",
      "16916 Training Loss: tensor(0.0385)\n",
      "16917 Training Loss: tensor(0.0376)\n",
      "16918 Training Loss: tensor(0.0423)\n",
      "16919 Training Loss: tensor(0.0342)\n",
      "16920 Training Loss: tensor(0.0402)\n",
      "16921 Training Loss: tensor(0.0407)\n",
      "16922 Training Loss: tensor(0.0411)\n",
      "16923 Training Loss: tensor(0.0364)\n",
      "16924 Training Loss: tensor(0.0363)\n",
      "16925 Training Loss: tensor(0.0398)\n",
      "16926 Training Loss: tensor(0.0382)\n",
      "16927 Training Loss: tensor(0.0384)\n",
      "16928 Training Loss: tensor(0.0362)\n",
      "16929 Training Loss: tensor(0.0431)\n",
      "16930 Training Loss: tensor(0.0403)\n",
      "16931 Training Loss: tensor(0.0388)\n",
      "16932 Training Loss: tensor(0.0336)\n",
      "16933 Training Loss: tensor(0.0366)\n",
      "16934 Training Loss: tensor(0.0443)\n",
      "16935 Training Loss: tensor(0.0350)\n",
      "16936 Training Loss: tensor(0.0388)\n",
      "16937 Training Loss: tensor(0.0361)\n",
      "16938 Training Loss: tensor(0.0367)\n",
      "16939 Training Loss: tensor(0.0337)\n",
      "16940 Training Loss: tensor(0.0468)\n",
      "16941 Training Loss: tensor(0.0357)\n",
      "16942 Training Loss: tensor(0.0398)\n",
      "16943 Training Loss: tensor(0.0359)\n",
      "16944 Training Loss: tensor(0.0355)\n",
      "16945 Training Loss: tensor(0.0396)\n",
      "16946 Training Loss: tensor(0.0359)\n",
      "16947 Training Loss: tensor(0.0412)\n",
      "16948 Training Loss: tensor(0.0385)\n",
      "16949 Training Loss: tensor(0.0373)\n",
      "16950 Training Loss: tensor(0.0407)\n",
      "16951 Training Loss: tensor(0.0368)\n",
      "16952 Training Loss: tensor(0.0361)\n",
      "16953 Training Loss: tensor(0.0329)\n",
      "16954 Training Loss: tensor(0.0407)\n",
      "16955 Training Loss: tensor(0.0429)\n",
      "16956 Training Loss: tensor(0.0387)\n",
      "16957 Training Loss: tensor(0.0411)\n",
      "16958 Training Loss: tensor(0.0413)\n",
      "16959 Training Loss: tensor(0.0373)\n",
      "16960 Training Loss: tensor(0.0363)\n",
      "16961 Training Loss: tensor(0.0450)\n",
      "16962 Training Loss: tensor(0.0420)\n",
      "16963 Training Loss: tensor(0.0334)\n",
      "16964 Training Loss: tensor(0.0320)\n",
      "16965 Training Loss: tensor(0.0344)\n",
      "16966 Training Loss: tensor(0.0350)\n",
      "16967 Training Loss: tensor(0.0416)\n",
      "16968 Training Loss: tensor(0.0367)\n",
      "16969 Training Loss: tensor(0.0386)\n",
      "16970 Training Loss: tensor(0.0399)\n",
      "16971 Training Loss: tensor(0.0322)\n",
      "16972 Training Loss: tensor(0.0375)\n",
      "16973 Training Loss: tensor(0.0449)\n",
      "16974 Training Loss: tensor(0.0435)\n",
      "16975 Training Loss: tensor(0.0409)\n",
      "16976 Training Loss: tensor(0.0415)\n",
      "16977 Training Loss: tensor(0.0424)\n",
      "16978 Training Loss: tensor(0.0426)\n",
      "16979 Training Loss: tensor(0.0383)\n",
      "16980 Training Loss: tensor(0.0412)\n",
      "16981 Training Loss: tensor(0.0493)\n",
      "16982 Training Loss: tensor(0.0364)\n",
      "16983 Training Loss: tensor(0.0377)\n",
      "16984 Training Loss: tensor(0.0443)\n",
      "16985 Training Loss: tensor(0.0361)\n",
      "16986 Training Loss: tensor(0.0397)\n",
      "16987 Training Loss: tensor(0.0440)\n",
      "16988 Training Loss: tensor(0.0355)\n",
      "16989 Training Loss: tensor(0.0437)\n",
      "16990 Training Loss: tensor(0.0463)\n",
      "16991 Training Loss: tensor(0.0370)\n",
      "16992 Training Loss: tensor(0.0437)\n",
      "16993 Training Loss: tensor(0.0399)\n",
      "16994 Training Loss: tensor(0.0356)\n",
      "16995 Training Loss: tensor(0.0433)\n",
      "16996 Training Loss: tensor(0.0388)\n",
      "16997 Training Loss: tensor(0.0361)\n",
      "16998 Training Loss: tensor(0.0359)\n",
      "16999 Training Loss: tensor(0.0374)\n",
      "17000 Training Loss: tensor(0.0413)\n",
      "17001 Training Loss: tensor(0.0372)\n",
      "17002 Training Loss: tensor(0.0404)\n",
      "17003 Training Loss: tensor(0.0334)\n",
      "17004 Training Loss: tensor(0.0366)\n",
      "17005 Training Loss: tensor(0.0428)\n",
      "17006 Training Loss: tensor(0.0374)\n",
      "17007 Training Loss: tensor(0.0413)\n",
      "17008 Training Loss: tensor(0.0350)\n",
      "17009 Training Loss: tensor(0.0397)\n",
      "17010 Training Loss: tensor(0.0361)\n",
      "17011 Training Loss: tensor(0.0364)\n",
      "17012 Training Loss: tensor(0.0394)\n",
      "17013 Training Loss: tensor(0.0375)\n",
      "17014 Training Loss: tensor(0.0402)\n",
      "17015 Training Loss: tensor(0.0377)\n",
      "17016 Training Loss: tensor(0.0395)\n",
      "17017 Training Loss: tensor(0.0409)\n",
      "17018 Training Loss: tensor(0.0457)\n",
      "17019 Training Loss: tensor(0.0349)\n",
      "17020 Training Loss: tensor(0.0335)\n",
      "17021 Training Loss: tensor(0.0420)\n",
      "17022 Training Loss: tensor(0.0377)\n",
      "17023 Training Loss: tensor(0.0389)\n",
      "17024 Training Loss: tensor(0.0362)\n",
      "17025 Training Loss: tensor(0.0315)\n",
      "17026 Training Loss: tensor(0.0354)\n",
      "17027 Training Loss: tensor(0.0395)\n",
      "17028 Training Loss: tensor(0.0383)\n",
      "17029 Training Loss: tensor(0.0361)\n",
      "17030 Training Loss: tensor(0.0451)\n",
      "17031 Training Loss: tensor(0.0404)\n",
      "17032 Training Loss: tensor(0.0410)\n",
      "17033 Training Loss: tensor(0.0388)\n",
      "17034 Training Loss: tensor(0.0458)\n",
      "17035 Training Loss: tensor(0.0395)\n",
      "17036 Training Loss: tensor(0.0340)\n",
      "17037 Training Loss: tensor(0.0388)\n",
      "17038 Training Loss: tensor(0.0381)\n",
      "17039 Training Loss: tensor(0.0358)\n",
      "17040 Training Loss: tensor(0.0392)\n",
      "17041 Training Loss: tensor(0.0386)\n",
      "17042 Training Loss: tensor(0.0385)\n",
      "17043 Training Loss: tensor(0.0394)\n",
      "17044 Training Loss: tensor(0.0355)\n",
      "17045 Training Loss: tensor(0.0383)\n",
      "17046 Training Loss: tensor(0.0393)\n",
      "17047 Training Loss: tensor(0.0379)\n",
      "17048 Training Loss: tensor(0.0372)\n",
      "17049 Training Loss: tensor(0.0389)\n",
      "17050 Training Loss: tensor(0.0365)\n",
      "17051 Training Loss: tensor(0.0418)\n",
      "17052 Training Loss: tensor(0.0389)\n",
      "17053 Training Loss: tensor(0.0382)\n",
      "17054 Training Loss: tensor(0.0424)\n",
      "17055 Training Loss: tensor(0.0380)\n",
      "17056 Training Loss: tensor(0.0437)\n",
      "17057 Training Loss: tensor(0.0374)\n",
      "17058 Training Loss: tensor(0.0361)\n",
      "17059 Training Loss: tensor(0.0350)\n",
      "17060 Training Loss: tensor(0.0392)\n",
      "17061 Training Loss: tensor(0.0389)\n",
      "17062 Training Loss: tensor(0.0352)\n",
      "17063 Training Loss: tensor(0.0414)\n",
      "17064 Training Loss: tensor(0.0381)\n",
      "17065 Training Loss: tensor(0.0368)\n",
      "17066 Training Loss: tensor(0.0388)\n",
      "17067 Training Loss: tensor(0.0414)\n",
      "17068 Training Loss: tensor(0.0366)\n",
      "17069 Training Loss: tensor(0.0444)\n",
      "17070 Training Loss: tensor(0.0407)\n",
      "17071 Training Loss: tensor(0.0375)\n",
      "17072 Training Loss: tensor(0.0410)\n",
      "17073 Training Loss: tensor(0.0358)\n",
      "17074 Training Loss: tensor(0.0404)\n",
      "17075 Training Loss: tensor(0.0394)\n",
      "17076 Training Loss: tensor(0.0342)\n",
      "17077 Training Loss: tensor(0.0349)\n",
      "17078 Training Loss: tensor(0.0386)\n",
      "17079 Training Loss: tensor(0.0361)\n",
      "17080 Training Loss: tensor(0.0353)\n",
      "17081 Training Loss: tensor(0.0413)\n",
      "17082 Training Loss: tensor(0.0402)\n",
      "17083 Training Loss: tensor(0.0355)\n",
      "17084 Training Loss: tensor(0.0385)\n",
      "17085 Training Loss: tensor(0.0361)\n",
      "17086 Training Loss: tensor(0.0375)\n",
      "17087 Training Loss: tensor(0.0422)\n",
      "17088 Training Loss: tensor(0.0373)\n",
      "17089 Training Loss: tensor(0.0405)\n",
      "17090 Training Loss: tensor(0.0392)\n",
      "17091 Training Loss: tensor(0.0386)\n",
      "17092 Training Loss: tensor(0.0397)\n",
      "17093 Training Loss: tensor(0.0370)\n",
      "17094 Training Loss: tensor(0.0372)\n",
      "17095 Training Loss: tensor(0.0417)\n",
      "17096 Training Loss: tensor(0.0369)\n",
      "17097 Training Loss: tensor(0.0430)\n",
      "17098 Training Loss: tensor(0.0373)\n",
      "17099 Training Loss: tensor(0.0393)\n",
      "17100 Training Loss: tensor(0.0390)\n",
      "17101 Training Loss: tensor(0.0366)\n",
      "17102 Training Loss: tensor(0.0353)\n",
      "17103 Training Loss: tensor(0.0359)\n",
      "17104 Training Loss: tensor(0.0342)\n",
      "17105 Training Loss: tensor(0.0445)\n",
      "17106 Training Loss: tensor(0.0398)\n",
      "17107 Training Loss: tensor(0.0373)\n",
      "17108 Training Loss: tensor(0.0331)\n",
      "17109 Training Loss: tensor(0.0407)\n",
      "17110 Training Loss: tensor(0.0414)\n",
      "17111 Training Loss: tensor(0.0442)\n",
      "17112 Training Loss: tensor(0.0420)\n",
      "17113 Training Loss: tensor(0.0371)\n",
      "17114 Training Loss: tensor(0.0365)\n",
      "17115 Training Loss: tensor(0.0325)\n",
      "17116 Training Loss: tensor(0.0421)\n",
      "17117 Training Loss: tensor(0.0383)\n",
      "17118 Training Loss: tensor(0.0388)\n",
      "17119 Training Loss: tensor(0.0362)\n",
      "17120 Training Loss: tensor(0.0383)\n",
      "17121 Training Loss: tensor(0.0380)\n",
      "17122 Training Loss: tensor(0.0399)\n",
      "17123 Training Loss: tensor(0.0347)\n",
      "17124 Training Loss: tensor(0.0412)\n",
      "17125 Training Loss: tensor(0.0400)\n",
      "17126 Training Loss: tensor(0.0381)\n",
      "17127 Training Loss: tensor(0.0379)\n",
      "17128 Training Loss: tensor(0.0364)\n",
      "17129 Training Loss: tensor(0.0397)\n",
      "17130 Training Loss: tensor(0.0394)\n",
      "17131 Training Loss: tensor(0.0397)\n",
      "17132 Training Loss: tensor(0.0371)\n",
      "17133 Training Loss: tensor(0.0376)\n",
      "17134 Training Loss: tensor(0.0346)\n",
      "17135 Training Loss: tensor(0.0386)\n",
      "17136 Training Loss: tensor(0.0425)\n",
      "17137 Training Loss: tensor(0.0348)\n",
      "17138 Training Loss: tensor(0.0322)\n",
      "17139 Training Loss: tensor(0.0390)\n",
      "17140 Training Loss: tensor(0.0398)\n",
      "17141 Training Loss: tensor(0.0342)\n",
      "17142 Training Loss: tensor(0.0366)\n",
      "17143 Training Loss: tensor(0.0417)\n",
      "17144 Training Loss: tensor(0.0367)\n",
      "17145 Training Loss: tensor(0.0374)\n",
      "17146 Training Loss: tensor(0.0340)\n",
      "17147 Training Loss: tensor(0.0360)\n",
      "17148 Training Loss: tensor(0.0387)\n",
      "17149 Training Loss: tensor(0.0321)\n",
      "17150 Training Loss: tensor(0.0374)\n",
      "17151 Training Loss: tensor(0.0383)\n",
      "17152 Training Loss: tensor(0.0315)\n",
      "17153 Training Loss: tensor(0.0339)\n",
      "17154 Training Loss: tensor(0.0390)\n",
      "17155 Training Loss: tensor(0.0337)\n",
      "17156 Training Loss: tensor(0.0424)\n",
      "17157 Training Loss: tensor(0.0385)\n",
      "17158 Training Loss: tensor(0.0354)\n",
      "17159 Training Loss: tensor(0.0387)\n",
      "17160 Training Loss: tensor(0.0369)\n",
      "17161 Training Loss: tensor(0.0396)\n",
      "17162 Training Loss: tensor(0.0389)\n",
      "17163 Training Loss: tensor(0.0339)\n",
      "17164 Training Loss: tensor(0.0373)\n",
      "17165 Training Loss: tensor(0.0394)\n",
      "17166 Training Loss: tensor(0.0448)\n",
      "17167 Training Loss: tensor(0.0349)\n",
      "17168 Training Loss: tensor(0.0405)\n",
      "17169 Training Loss: tensor(0.0334)\n",
      "17170 Training Loss: tensor(0.0348)\n",
      "17171 Training Loss: tensor(0.0367)\n",
      "17172 Training Loss: tensor(0.0341)\n",
      "17173 Training Loss: tensor(0.0422)\n",
      "17174 Training Loss: tensor(0.0388)\n",
      "17175 Training Loss: tensor(0.0367)\n",
      "17176 Training Loss: tensor(0.0408)\n",
      "17177 Training Loss: tensor(0.0376)\n",
      "17178 Training Loss: tensor(0.0375)\n",
      "17179 Training Loss: tensor(0.0425)\n",
      "17180 Training Loss: tensor(0.0369)\n",
      "17181 Training Loss: tensor(0.0361)\n",
      "17182 Training Loss: tensor(0.0343)\n",
      "17183 Training Loss: tensor(0.0386)\n",
      "17184 Training Loss: tensor(0.0387)\n",
      "17185 Training Loss: tensor(0.0366)\n",
      "17186 Training Loss: tensor(0.0382)\n",
      "17187 Training Loss: tensor(0.0430)\n",
      "17188 Training Loss: tensor(0.0405)\n",
      "17189 Training Loss: tensor(0.0371)\n",
      "17190 Training Loss: tensor(0.0399)\n",
      "17191 Training Loss: tensor(0.0375)\n",
      "17192 Training Loss: tensor(0.0391)\n",
      "17193 Training Loss: tensor(0.0318)\n",
      "17194 Training Loss: tensor(0.0367)\n",
      "17195 Training Loss: tensor(0.0391)\n",
      "17196 Training Loss: tensor(0.0347)\n",
      "17197 Training Loss: tensor(0.0374)\n",
      "17198 Training Loss: tensor(0.0371)\n",
      "17199 Training Loss: tensor(0.0382)\n",
      "17200 Training Loss: tensor(0.0348)\n",
      "17201 Training Loss: tensor(0.0411)\n",
      "17202 Training Loss: tensor(0.0376)\n",
      "17203 Training Loss: tensor(0.0358)\n",
      "17204 Training Loss: tensor(0.0295)\n",
      "17205 Training Loss: tensor(0.0374)\n",
      "17206 Training Loss: tensor(0.0433)\n",
      "17207 Training Loss: tensor(0.0351)\n",
      "17208 Training Loss: tensor(0.0402)\n",
      "17209 Training Loss: tensor(0.0395)\n",
      "17210 Training Loss: tensor(0.0371)\n",
      "17211 Training Loss: tensor(0.0379)\n",
      "17212 Training Loss: tensor(0.0340)\n",
      "17213 Training Loss: tensor(0.0363)\n",
      "17214 Training Loss: tensor(0.0388)\n",
      "17215 Training Loss: tensor(0.0396)\n",
      "17216 Training Loss: tensor(0.0379)\n",
      "17217 Training Loss: tensor(0.0347)\n",
      "17218 Training Loss: tensor(0.0368)\n",
      "17219 Training Loss: tensor(0.0406)\n",
      "17220 Training Loss: tensor(0.0376)\n",
      "17221 Training Loss: tensor(0.0398)\n",
      "17222 Training Loss: tensor(0.0439)\n",
      "17223 Training Loss: tensor(0.0329)\n",
      "17224 Training Loss: tensor(0.0357)\n",
      "17225 Training Loss: tensor(0.0379)\n",
      "17226 Training Loss: tensor(0.0391)\n",
      "17227 Training Loss: tensor(0.0372)\n",
      "17228 Training Loss: tensor(0.0434)\n",
      "17229 Training Loss: tensor(0.0380)\n",
      "17230 Training Loss: tensor(0.0384)\n",
      "17231 Training Loss: tensor(0.0409)\n",
      "17232 Training Loss: tensor(0.0423)\n",
      "17233 Training Loss: tensor(0.0387)\n",
      "17234 Training Loss: tensor(0.0407)\n",
      "17235 Training Loss: tensor(0.0332)\n",
      "17236 Training Loss: tensor(0.0402)\n",
      "17237 Training Loss: tensor(0.0391)\n",
      "17238 Training Loss: tensor(0.0394)\n",
      "17239 Training Loss: tensor(0.0363)\n",
      "17240 Training Loss: tensor(0.0435)\n",
      "17241 Training Loss: tensor(0.0347)\n",
      "17242 Training Loss: tensor(0.0410)\n",
      "17243 Training Loss: tensor(0.0400)\n",
      "17244 Training Loss: tensor(0.0408)\n",
      "17245 Training Loss: tensor(0.0362)\n",
      "17246 Training Loss: tensor(0.0332)\n",
      "17247 Training Loss: tensor(0.0362)\n",
      "17248 Training Loss: tensor(0.0386)\n",
      "17249 Training Loss: tensor(0.0391)\n",
      "17250 Training Loss: tensor(0.0403)\n",
      "17251 Training Loss: tensor(0.0337)\n",
      "17252 Training Loss: tensor(0.0387)\n",
      "17253 Training Loss: tensor(0.0375)\n",
      "17254 Training Loss: tensor(0.0365)\n",
      "17255 Training Loss: tensor(0.0399)\n",
      "17256 Training Loss: tensor(0.0391)\n",
      "17257 Training Loss: tensor(0.0324)\n",
      "17258 Training Loss: tensor(0.0395)\n",
      "17259 Training Loss: tensor(0.0415)\n",
      "17260 Training Loss: tensor(0.0391)\n",
      "17261 Training Loss: tensor(0.0373)\n",
      "17262 Training Loss: tensor(0.0394)\n",
      "17263 Training Loss: tensor(0.0406)\n",
      "17264 Training Loss: tensor(0.0413)\n",
      "17265 Training Loss: tensor(0.0381)\n",
      "17266 Training Loss: tensor(0.0367)\n",
      "17267 Training Loss: tensor(0.0383)\n",
      "17268 Training Loss: tensor(0.0358)\n",
      "17269 Training Loss: tensor(0.0344)\n",
      "17270 Training Loss: tensor(0.0384)\n",
      "17271 Training Loss: tensor(0.0387)\n",
      "17272 Training Loss: tensor(0.0375)\n",
      "17273 Training Loss: tensor(0.0329)\n",
      "17274 Training Loss: tensor(0.0344)\n",
      "17275 Training Loss: tensor(0.0357)\n",
      "17276 Training Loss: tensor(0.0399)\n",
      "17277 Training Loss: tensor(0.0410)\n",
      "17278 Training Loss: tensor(0.0362)\n",
      "17279 Training Loss: tensor(0.0370)\n",
      "17280 Training Loss: tensor(0.0393)\n",
      "17281 Training Loss: tensor(0.0394)\n",
      "17282 Training Loss: tensor(0.0363)\n",
      "17283 Training Loss: tensor(0.0415)\n",
      "17284 Training Loss: tensor(0.0344)\n",
      "17285 Training Loss: tensor(0.0368)\n",
      "17286 Training Loss: tensor(0.0379)\n",
      "17287 Training Loss: tensor(0.0343)\n",
      "17288 Training Loss: tensor(0.0382)\n",
      "17289 Training Loss: tensor(0.0376)\n",
      "17290 Training Loss: tensor(0.0415)\n",
      "17291 Training Loss: tensor(0.0411)\n",
      "17292 Training Loss: tensor(0.0373)\n",
      "17293 Training Loss: tensor(0.0386)\n",
      "17294 Training Loss: tensor(0.0367)\n",
      "17295 Training Loss: tensor(0.0365)\n",
      "17296 Training Loss: tensor(0.0348)\n",
      "17297 Training Loss: tensor(0.0387)\n",
      "17298 Training Loss: tensor(0.0375)\n",
      "17299 Training Loss: tensor(0.0372)\n",
      "17300 Training Loss: tensor(0.0372)\n",
      "17301 Training Loss: tensor(0.0363)\n",
      "17302 Training Loss: tensor(0.0397)\n",
      "17303 Training Loss: tensor(0.0390)\n",
      "17304 Training Loss: tensor(0.0390)\n",
      "17305 Training Loss: tensor(0.0372)\n",
      "17306 Training Loss: tensor(0.0388)\n",
      "17307 Training Loss: tensor(0.0396)\n",
      "17308 Training Loss: tensor(0.0374)\n",
      "17309 Training Loss: tensor(0.0372)\n",
      "17310 Training Loss: tensor(0.0374)\n",
      "17311 Training Loss: tensor(0.0350)\n",
      "17312 Training Loss: tensor(0.0367)\n",
      "17313 Training Loss: tensor(0.0412)\n",
      "17314 Training Loss: tensor(0.0339)\n",
      "17315 Training Loss: tensor(0.0356)\n",
      "17316 Training Loss: tensor(0.0384)\n",
      "17317 Training Loss: tensor(0.0392)\n",
      "17318 Training Loss: tensor(0.0384)\n",
      "17319 Training Loss: tensor(0.0374)\n",
      "17320 Training Loss: tensor(0.0369)\n",
      "17321 Training Loss: tensor(0.0427)\n",
      "17322 Training Loss: tensor(0.0398)\n",
      "17323 Training Loss: tensor(0.0422)\n",
      "17324 Training Loss: tensor(0.0345)\n",
      "17325 Training Loss: tensor(0.0357)\n",
      "17326 Training Loss: tensor(0.0374)\n",
      "17327 Training Loss: tensor(0.0401)\n",
      "17328 Training Loss: tensor(0.0394)\n",
      "17329 Training Loss: tensor(0.0390)\n",
      "17330 Training Loss: tensor(0.0403)\n",
      "17331 Training Loss: tensor(0.0397)\n",
      "17332 Training Loss: tensor(0.0414)\n",
      "17333 Training Loss: tensor(0.0401)\n",
      "17334 Training Loss: tensor(0.0372)\n",
      "17335 Training Loss: tensor(0.0361)\n",
      "17336 Training Loss: tensor(0.0365)\n",
      "17337 Training Loss: tensor(0.0391)\n",
      "17338 Training Loss: tensor(0.0390)\n",
      "17339 Training Loss: tensor(0.0364)\n",
      "17340 Training Loss: tensor(0.0308)\n",
      "17341 Training Loss: tensor(0.0373)\n",
      "17342 Training Loss: tensor(0.0389)\n",
      "17343 Training Loss: tensor(0.0380)\n",
      "17344 Training Loss: tensor(0.0409)\n",
      "17345 Training Loss: tensor(0.0421)\n",
      "17346 Training Loss: tensor(0.0405)\n",
      "17347 Training Loss: tensor(0.0324)\n",
      "17348 Training Loss: tensor(0.0393)\n",
      "17349 Training Loss: tensor(0.0362)\n",
      "17350 Training Loss: tensor(0.0390)\n",
      "17351 Training Loss: tensor(0.0400)\n",
      "17352 Training Loss: tensor(0.0413)\n",
      "17353 Training Loss: tensor(0.0365)\n",
      "17354 Training Loss: tensor(0.0422)\n",
      "17355 Training Loss: tensor(0.0378)\n",
      "17356 Training Loss: tensor(0.0346)\n",
      "17357 Training Loss: tensor(0.0401)\n",
      "17358 Training Loss: tensor(0.0387)\n",
      "17359 Training Loss: tensor(0.0386)\n",
      "17360 Training Loss: tensor(0.0424)\n",
      "17361 Training Loss: tensor(0.0380)\n",
      "17362 Training Loss: tensor(0.0322)\n",
      "17363 Training Loss: tensor(0.0395)\n",
      "17364 Training Loss: tensor(0.0398)\n",
      "17365 Training Loss: tensor(0.0426)\n",
      "17366 Training Loss: tensor(0.0333)\n",
      "17367 Training Loss: tensor(0.0387)\n",
      "17368 Training Loss: tensor(0.0361)\n",
      "17369 Training Loss: tensor(0.0402)\n",
      "17370 Training Loss: tensor(0.0398)\n",
      "17371 Training Loss: tensor(0.0347)\n",
      "17372 Training Loss: tensor(0.0374)\n",
      "17373 Training Loss: tensor(0.0423)\n",
      "17374 Training Loss: tensor(0.0397)\n",
      "17375 Training Loss: tensor(0.0341)\n",
      "17376 Training Loss: tensor(0.0350)\n",
      "17377 Training Loss: tensor(0.0373)\n",
      "17378 Training Loss: tensor(0.0538)\n",
      "17379 Training Loss: tensor(0.0394)\n",
      "17380 Training Loss: tensor(0.0413)\n",
      "17381 Training Loss: tensor(0.0403)\n",
      "17382 Training Loss: tensor(0.0374)\n",
      "17383 Training Loss: tensor(0.0414)\n",
      "17384 Training Loss: tensor(0.0412)\n",
      "17385 Training Loss: tensor(0.0373)\n",
      "17386 Training Loss: tensor(0.0369)\n",
      "17387 Training Loss: tensor(0.0417)\n",
      "17388 Training Loss: tensor(0.0429)\n",
      "17389 Training Loss: tensor(0.0427)\n",
      "17390 Training Loss: tensor(0.0391)\n",
      "17391 Training Loss: tensor(0.0350)\n",
      "17392 Training Loss: tensor(0.0398)\n",
      "17393 Training Loss: tensor(0.0420)\n",
      "17394 Training Loss: tensor(0.0366)\n",
      "17395 Training Loss: tensor(0.0448)\n",
      "17396 Training Loss: tensor(0.0436)\n",
      "17397 Training Loss: tensor(0.0341)\n",
      "17398 Training Loss: tensor(0.0444)\n",
      "17399 Training Loss: tensor(0.0403)\n",
      "17400 Training Loss: tensor(0.0393)\n",
      "17401 Training Loss: tensor(0.0403)\n",
      "17402 Training Loss: tensor(0.0352)\n",
      "17403 Training Loss: tensor(0.0322)\n",
      "17404 Training Loss: tensor(0.0349)\n",
      "17405 Training Loss: tensor(0.0414)\n",
      "17406 Training Loss: tensor(0.0387)\n",
      "17407 Training Loss: tensor(0.0393)\n",
      "17408 Training Loss: tensor(0.0352)\n",
      "17409 Training Loss: tensor(0.0420)\n",
      "17410 Training Loss: tensor(0.0389)\n",
      "17411 Training Loss: tensor(0.0396)\n",
      "17412 Training Loss: tensor(0.0395)\n",
      "17413 Training Loss: tensor(0.0381)\n",
      "17414 Training Loss: tensor(0.0355)\n",
      "17415 Training Loss: tensor(0.0437)\n",
      "17416 Training Loss: tensor(0.0438)\n",
      "17417 Training Loss: tensor(0.0368)\n",
      "17418 Training Loss: tensor(0.0388)\n",
      "17419 Training Loss: tensor(0.0364)\n",
      "17420 Training Loss: tensor(0.0407)\n",
      "17421 Training Loss: tensor(0.0399)\n",
      "17422 Training Loss: tensor(0.0407)\n",
      "17423 Training Loss: tensor(0.0373)\n",
      "17424 Training Loss: tensor(0.0320)\n",
      "17425 Training Loss: tensor(0.0428)\n",
      "17426 Training Loss: tensor(0.0406)\n",
      "17427 Training Loss: tensor(0.0366)\n",
      "17428 Training Loss: tensor(0.0433)\n",
      "17429 Training Loss: tensor(0.0432)\n",
      "17430 Training Loss: tensor(0.0360)\n",
      "17431 Training Loss: tensor(0.0329)\n",
      "17432 Training Loss: tensor(0.0403)\n",
      "17433 Training Loss: tensor(0.0437)\n",
      "17434 Training Loss: tensor(0.0375)\n",
      "17435 Training Loss: tensor(0.0387)\n",
      "17436 Training Loss: tensor(0.0365)\n",
      "17437 Training Loss: tensor(0.0438)\n",
      "17438 Training Loss: tensor(0.0365)\n",
      "17439 Training Loss: tensor(0.0404)\n",
      "17440 Training Loss: tensor(0.0367)\n",
      "17441 Training Loss: tensor(0.0369)\n",
      "17442 Training Loss: tensor(0.0392)\n",
      "17443 Training Loss: tensor(0.0384)\n",
      "17444 Training Loss: tensor(0.0328)\n",
      "17445 Training Loss: tensor(0.0459)\n",
      "17446 Training Loss: tensor(0.0386)\n",
      "17447 Training Loss: tensor(0.0425)\n",
      "17448 Training Loss: tensor(0.0371)\n",
      "17449 Training Loss: tensor(0.0424)\n",
      "17450 Training Loss: tensor(0.0386)\n",
      "17451 Training Loss: tensor(0.0376)\n",
      "17452 Training Loss: tensor(0.0370)\n",
      "17453 Training Loss: tensor(0.0392)\n",
      "17454 Training Loss: tensor(0.0468)\n",
      "17455 Training Loss: tensor(0.0327)\n",
      "17456 Training Loss: tensor(0.0354)\n",
      "17457 Training Loss: tensor(0.0387)\n",
      "17458 Training Loss: tensor(0.0384)\n",
      "17459 Training Loss: tensor(0.0368)\n",
      "17460 Training Loss: tensor(0.0353)\n",
      "17461 Training Loss: tensor(0.0369)\n",
      "17462 Training Loss: tensor(0.0363)\n",
      "17463 Training Loss: tensor(0.0380)\n",
      "17464 Training Loss: tensor(0.0380)\n",
      "17465 Training Loss: tensor(0.0357)\n",
      "17466 Training Loss: tensor(0.0374)\n",
      "17467 Training Loss: tensor(0.0382)\n",
      "17468 Training Loss: tensor(0.0348)\n",
      "17469 Training Loss: tensor(0.0418)\n",
      "17470 Training Loss: tensor(0.0407)\n",
      "17471 Training Loss: tensor(0.0404)\n",
      "17472 Training Loss: tensor(0.0399)\n",
      "17473 Training Loss: tensor(0.0398)\n",
      "17474 Training Loss: tensor(0.0398)\n",
      "17475 Training Loss: tensor(0.0336)\n",
      "17476 Training Loss: tensor(0.0421)\n",
      "17477 Training Loss: tensor(0.0325)\n",
      "17478 Training Loss: tensor(0.0425)\n",
      "17479 Training Loss: tensor(0.0355)\n",
      "17480 Training Loss: tensor(0.0382)\n",
      "17481 Training Loss: tensor(0.0366)\n",
      "17482 Training Loss: tensor(0.0373)\n",
      "17483 Training Loss: tensor(0.0377)\n",
      "17484 Training Loss: tensor(0.0329)\n",
      "17485 Training Loss: tensor(0.0357)\n",
      "17486 Training Loss: tensor(0.0422)\n",
      "17487 Training Loss: tensor(0.0351)\n",
      "17488 Training Loss: tensor(0.0362)\n",
      "17489 Training Loss: tensor(0.0359)\n",
      "17490 Training Loss: tensor(0.0384)\n",
      "17491 Training Loss: tensor(0.0380)\n",
      "17492 Training Loss: tensor(0.0385)\n",
      "17493 Training Loss: tensor(0.0470)\n",
      "17494 Training Loss: tensor(0.0353)\n",
      "17495 Training Loss: tensor(0.0392)\n",
      "17496 Training Loss: tensor(0.0414)\n",
      "17497 Training Loss: tensor(0.0405)\n",
      "17498 Training Loss: tensor(0.0358)\n",
      "17499 Training Loss: tensor(0.0347)\n",
      "17500 Training Loss: tensor(0.0458)\n",
      "17501 Training Loss: tensor(0.0361)\n",
      "17502 Training Loss: tensor(0.0362)\n",
      "17503 Training Loss: tensor(0.0367)\n",
      "17504 Training Loss: tensor(0.0353)\n",
      "17505 Training Loss: tensor(0.0380)\n",
      "17506 Training Loss: tensor(0.0401)\n",
      "17507 Training Loss: tensor(0.0393)\n",
      "17508 Training Loss: tensor(0.0446)\n",
      "17509 Training Loss: tensor(0.0392)\n",
      "17510 Training Loss: tensor(0.0396)\n",
      "17511 Training Loss: tensor(0.0396)\n",
      "17512 Training Loss: tensor(0.0383)\n",
      "17513 Training Loss: tensor(0.0340)\n",
      "17514 Training Loss: tensor(0.0400)\n",
      "17515 Training Loss: tensor(0.0383)\n",
      "17516 Training Loss: tensor(0.0368)\n",
      "17517 Training Loss: tensor(0.0377)\n",
      "17518 Training Loss: tensor(0.0363)\n",
      "17519 Training Loss: tensor(0.0384)\n",
      "17520 Training Loss: tensor(0.0351)\n",
      "17521 Training Loss: tensor(0.0418)\n",
      "17522 Training Loss: tensor(0.0326)\n",
      "17523 Training Loss: tensor(0.0320)\n",
      "17524 Training Loss: tensor(0.0397)\n",
      "17525 Training Loss: tensor(0.0371)\n",
      "17526 Training Loss: tensor(0.0414)\n",
      "17527 Training Loss: tensor(0.0383)\n",
      "17528 Training Loss: tensor(0.0409)\n",
      "17529 Training Loss: tensor(0.0437)\n",
      "17530 Training Loss: tensor(0.0445)\n",
      "17531 Training Loss: tensor(0.0388)\n",
      "17532 Training Loss: tensor(0.0406)\n",
      "17533 Training Loss: tensor(0.0410)\n",
      "17534 Training Loss: tensor(0.0389)\n",
      "17535 Training Loss: tensor(0.0447)\n",
      "17536 Training Loss: tensor(0.0384)\n",
      "17537 Training Loss: tensor(0.0348)\n",
      "17538 Training Loss: tensor(0.0382)\n",
      "17539 Training Loss: tensor(0.0429)\n",
      "17540 Training Loss: tensor(0.0368)\n",
      "17541 Training Loss: tensor(0.0373)\n",
      "17542 Training Loss: tensor(0.0404)\n",
      "17543 Training Loss: tensor(0.0371)\n",
      "17544 Training Loss: tensor(0.0389)\n",
      "17545 Training Loss: tensor(0.0338)\n",
      "17546 Training Loss: tensor(0.0362)\n",
      "17547 Training Loss: tensor(0.0329)\n",
      "17548 Training Loss: tensor(0.0401)\n",
      "17549 Training Loss: tensor(0.0381)\n",
      "17550 Training Loss: tensor(0.0421)\n",
      "17551 Training Loss: tensor(0.0365)\n",
      "17552 Training Loss: tensor(0.0324)\n",
      "17553 Training Loss: tensor(0.0407)\n",
      "17554 Training Loss: tensor(0.0383)\n",
      "17555 Training Loss: tensor(0.0373)\n",
      "17556 Training Loss: tensor(0.0347)\n",
      "17557 Training Loss: tensor(0.0426)\n",
      "17558 Training Loss: tensor(0.0430)\n",
      "17559 Training Loss: tensor(0.0366)\n",
      "17560 Training Loss: tensor(0.0408)\n",
      "17561 Training Loss: tensor(0.0354)\n",
      "17562 Training Loss: tensor(0.0339)\n",
      "17563 Training Loss: tensor(0.0363)\n",
      "17564 Training Loss: tensor(0.0389)\n",
      "17565 Training Loss: tensor(0.0369)\n",
      "17566 Training Loss: tensor(0.0338)\n",
      "17567 Training Loss: tensor(0.0381)\n",
      "17568 Training Loss: tensor(0.0358)\n",
      "17569 Training Loss: tensor(0.0416)\n",
      "17570 Training Loss: tensor(0.0371)\n",
      "17571 Training Loss: tensor(0.0423)\n",
      "17572 Training Loss: tensor(0.0373)\n",
      "17573 Training Loss: tensor(0.0357)\n",
      "17574 Training Loss: tensor(0.0368)\n",
      "17575 Training Loss: tensor(0.0378)\n",
      "17576 Training Loss: tensor(0.0387)\n",
      "17577 Training Loss: tensor(0.0377)\n",
      "17578 Training Loss: tensor(0.0378)\n",
      "17579 Training Loss: tensor(0.0385)\n",
      "17580 Training Loss: tensor(0.0370)\n",
      "17581 Training Loss: tensor(0.0410)\n",
      "17582 Training Loss: tensor(0.0406)\n",
      "17583 Training Loss: tensor(0.0373)\n",
      "17584 Training Loss: tensor(0.0366)\n",
      "17585 Training Loss: tensor(0.0368)\n",
      "17586 Training Loss: tensor(0.0410)\n",
      "17587 Training Loss: tensor(0.0372)\n",
      "17588 Training Loss: tensor(0.0400)\n",
      "17589 Training Loss: tensor(0.0339)\n",
      "17590 Training Loss: tensor(0.0330)\n",
      "17591 Training Loss: tensor(0.0339)\n",
      "17592 Training Loss: tensor(0.0392)\n",
      "17593 Training Loss: tensor(0.0428)\n",
      "17594 Training Loss: tensor(0.0366)\n",
      "17595 Training Loss: tensor(0.0416)\n",
      "17596 Training Loss: tensor(0.0414)\n",
      "17597 Training Loss: tensor(0.0386)\n",
      "17598 Training Loss: tensor(0.0331)\n",
      "17599 Training Loss: tensor(0.0373)\n",
      "17600 Training Loss: tensor(0.0414)\n",
      "17601 Training Loss: tensor(0.0391)\n",
      "17602 Training Loss: tensor(0.0349)\n",
      "17603 Training Loss: tensor(0.0352)\n",
      "17604 Training Loss: tensor(0.0409)\n",
      "17605 Training Loss: tensor(0.0381)\n",
      "17606 Training Loss: tensor(0.0332)\n",
      "17607 Training Loss: tensor(0.0334)\n",
      "17608 Training Loss: tensor(0.0395)\n",
      "17609 Training Loss: tensor(0.0388)\n",
      "17610 Training Loss: tensor(0.0378)\n",
      "17611 Training Loss: tensor(0.0387)\n",
      "17612 Training Loss: tensor(0.0378)\n",
      "17613 Training Loss: tensor(0.0392)\n",
      "17614 Training Loss: tensor(0.0424)\n",
      "17615 Training Loss: tensor(0.0345)\n",
      "17616 Training Loss: tensor(0.0363)\n",
      "17617 Training Loss: tensor(0.0400)\n",
      "17618 Training Loss: tensor(0.0326)\n",
      "17619 Training Loss: tensor(0.0360)\n",
      "17620 Training Loss: tensor(0.0375)\n",
      "17621 Training Loss: tensor(0.0379)\n",
      "17622 Training Loss: tensor(0.0389)\n",
      "17623 Training Loss: tensor(0.0390)\n",
      "17624 Training Loss: tensor(0.0436)\n",
      "17625 Training Loss: tensor(0.0363)\n",
      "17626 Training Loss: tensor(0.0407)\n",
      "17627 Training Loss: tensor(0.0354)\n",
      "17628 Training Loss: tensor(0.0394)\n",
      "17629 Training Loss: tensor(0.0347)\n",
      "17630 Training Loss: tensor(0.0347)\n",
      "17631 Training Loss: tensor(0.0406)\n",
      "17632 Training Loss: tensor(0.0402)\n",
      "17633 Training Loss: tensor(0.0403)\n",
      "17634 Training Loss: tensor(0.0387)\n",
      "17635 Training Loss: tensor(0.0367)\n",
      "17636 Training Loss: tensor(0.0368)\n",
      "17637 Training Loss: tensor(0.0380)\n",
      "17638 Training Loss: tensor(0.0408)\n",
      "17639 Training Loss: tensor(0.0363)\n",
      "17640 Training Loss: tensor(0.0377)\n",
      "17641 Training Loss: tensor(0.0381)\n",
      "17642 Training Loss: tensor(0.0437)\n",
      "17643 Training Loss: tensor(0.0382)\n",
      "17644 Training Loss: tensor(0.0391)\n",
      "17645 Training Loss: tensor(0.0359)\n",
      "17646 Training Loss: tensor(0.0339)\n",
      "17647 Training Loss: tensor(0.0405)\n",
      "17648 Training Loss: tensor(0.0329)\n",
      "17649 Training Loss: tensor(0.0366)\n",
      "17650 Training Loss: tensor(0.0375)\n",
      "17651 Training Loss: tensor(0.0344)\n",
      "17652 Training Loss: tensor(0.0432)\n",
      "17653 Training Loss: tensor(0.0394)\n",
      "17654 Training Loss: tensor(0.0394)\n",
      "17655 Training Loss: tensor(0.0318)\n",
      "17656 Training Loss: tensor(0.0343)\n",
      "17657 Training Loss: tensor(0.0403)\n",
      "17658 Training Loss: tensor(0.0385)\n",
      "17659 Training Loss: tensor(0.0396)\n",
      "17660 Training Loss: tensor(0.0388)\n",
      "17661 Training Loss: tensor(0.0397)\n",
      "17662 Training Loss: tensor(0.0366)\n",
      "17663 Training Loss: tensor(0.0365)\n",
      "17664 Training Loss: tensor(0.0404)\n",
      "17665 Training Loss: tensor(0.0390)\n",
      "17666 Training Loss: tensor(0.0373)\n",
      "17667 Training Loss: tensor(0.0370)\n",
      "17668 Training Loss: tensor(0.0397)\n",
      "17669 Training Loss: tensor(0.0390)\n",
      "17670 Training Loss: tensor(0.0347)\n",
      "17671 Training Loss: tensor(0.0437)\n",
      "17672 Training Loss: tensor(0.0381)\n",
      "17673 Training Loss: tensor(0.0374)\n",
      "17674 Training Loss: tensor(0.0370)\n",
      "17675 Training Loss: tensor(0.0424)\n",
      "17676 Training Loss: tensor(0.0406)\n",
      "17677 Training Loss: tensor(0.0391)\n",
      "17678 Training Loss: tensor(0.0445)\n",
      "17679 Training Loss: tensor(0.0368)\n",
      "17680 Training Loss: tensor(0.0376)\n",
      "17681 Training Loss: tensor(0.0396)\n",
      "17682 Training Loss: tensor(0.0406)\n",
      "17683 Training Loss: tensor(0.0358)\n",
      "17684 Training Loss: tensor(0.0349)\n",
      "17685 Training Loss: tensor(0.0376)\n",
      "17686 Training Loss: tensor(0.0364)\n",
      "17687 Training Loss: tensor(0.0351)\n",
      "17688 Training Loss: tensor(0.0385)\n",
      "17689 Training Loss: tensor(0.0377)\n",
      "17690 Training Loss: tensor(0.0335)\n",
      "17691 Training Loss: tensor(0.0391)\n",
      "17692 Training Loss: tensor(0.0380)\n",
      "17693 Training Loss: tensor(0.0387)\n",
      "17694 Training Loss: tensor(0.0383)\n",
      "17695 Training Loss: tensor(0.0388)\n",
      "17696 Training Loss: tensor(0.0423)\n",
      "17697 Training Loss: tensor(0.0373)\n",
      "17698 Training Loss: tensor(0.0362)\n",
      "17699 Training Loss: tensor(0.0373)\n",
      "17700 Training Loss: tensor(0.0408)\n",
      "17701 Training Loss: tensor(0.0401)\n",
      "17702 Training Loss: tensor(0.0469)\n",
      "17703 Training Loss: tensor(0.0401)\n",
      "17704 Training Loss: tensor(0.0429)\n",
      "17705 Training Loss: tensor(0.0333)\n",
      "17706 Training Loss: tensor(0.0412)\n",
      "17707 Training Loss: tensor(0.0377)\n",
      "17708 Training Loss: tensor(0.0429)\n",
      "17709 Training Loss: tensor(0.0390)\n",
      "17710 Training Loss: tensor(0.0392)\n",
      "17711 Training Loss: tensor(0.0361)\n",
      "17712 Training Loss: tensor(0.0424)\n",
      "17713 Training Loss: tensor(0.0367)\n",
      "17714 Training Loss: tensor(0.0387)\n",
      "17715 Training Loss: tensor(0.0385)\n",
      "17716 Training Loss: tensor(0.0364)\n",
      "17717 Training Loss: tensor(0.0357)\n",
      "17718 Training Loss: tensor(0.0405)\n",
      "17719 Training Loss: tensor(0.0372)\n",
      "17720 Training Loss: tensor(0.0403)\n",
      "17721 Training Loss: tensor(0.0403)\n",
      "17722 Training Loss: tensor(0.0338)\n",
      "17723 Training Loss: tensor(0.0402)\n",
      "17724 Training Loss: tensor(0.0396)\n",
      "17725 Training Loss: tensor(0.0404)\n",
      "17726 Training Loss: tensor(0.0390)\n",
      "17727 Training Loss: tensor(0.0371)\n",
      "17728 Training Loss: tensor(0.0367)\n",
      "17729 Training Loss: tensor(0.0371)\n",
      "17730 Training Loss: tensor(0.0395)\n",
      "17731 Training Loss: tensor(0.0378)\n",
      "17732 Training Loss: tensor(0.0390)\n",
      "17733 Training Loss: tensor(0.0402)\n",
      "17734 Training Loss: tensor(0.0395)\n",
      "17735 Training Loss: tensor(0.0362)\n",
      "17736 Training Loss: tensor(0.0432)\n",
      "17737 Training Loss: tensor(0.0439)\n",
      "17738 Training Loss: tensor(0.0458)\n",
      "17739 Training Loss: tensor(0.0346)\n",
      "17740 Training Loss: tensor(0.0341)\n",
      "17741 Training Loss: tensor(0.0369)\n",
      "17742 Training Loss: tensor(0.0419)\n",
      "17743 Training Loss: tensor(0.0383)\n",
      "17744 Training Loss: tensor(0.0462)\n",
      "17745 Training Loss: tensor(0.0432)\n",
      "17746 Training Loss: tensor(0.0372)\n",
      "17747 Training Loss: tensor(0.0446)\n",
      "17748 Training Loss: tensor(0.0393)\n",
      "17749 Training Loss: tensor(0.0380)\n",
      "17750 Training Loss: tensor(0.0393)\n",
      "17751 Training Loss: tensor(0.0386)\n",
      "17752 Training Loss: tensor(0.0381)\n",
      "17753 Training Loss: tensor(0.0398)\n",
      "17754 Training Loss: tensor(0.0452)\n",
      "17755 Training Loss: tensor(0.0359)\n",
      "17756 Training Loss: tensor(0.0444)\n",
      "17757 Training Loss: tensor(0.0327)\n",
      "17758 Training Loss: tensor(0.0358)\n",
      "17759 Training Loss: tensor(0.0400)\n",
      "17760 Training Loss: tensor(0.0419)\n",
      "17761 Training Loss: tensor(0.0362)\n",
      "17762 Training Loss: tensor(0.0386)\n",
      "17763 Training Loss: tensor(0.0359)\n",
      "17764 Training Loss: tensor(0.0363)\n",
      "17765 Training Loss: tensor(0.0381)\n",
      "17766 Training Loss: tensor(0.0385)\n",
      "17767 Training Loss: tensor(0.0388)\n",
      "17768 Training Loss: tensor(0.0399)\n",
      "17769 Training Loss: tensor(0.0377)\n",
      "17770 Training Loss: tensor(0.0331)\n",
      "17771 Training Loss: tensor(0.0391)\n",
      "17772 Training Loss: tensor(0.0387)\n",
      "17773 Training Loss: tensor(0.0394)\n",
      "17774 Training Loss: tensor(0.0380)\n",
      "17775 Training Loss: tensor(0.0349)\n",
      "17776 Training Loss: tensor(0.0370)\n",
      "17777 Training Loss: tensor(0.0400)\n",
      "17778 Training Loss: tensor(0.0338)\n",
      "17779 Training Loss: tensor(0.0375)\n",
      "17780 Training Loss: tensor(0.0373)\n",
      "17781 Training Loss: tensor(0.0342)\n",
      "17782 Training Loss: tensor(0.0407)\n",
      "17783 Training Loss: tensor(0.0342)\n",
      "17784 Training Loss: tensor(0.0377)\n",
      "17785 Training Loss: tensor(0.0327)\n",
      "17786 Training Loss: tensor(0.0347)\n",
      "17787 Training Loss: tensor(0.0398)\n",
      "17788 Training Loss: tensor(0.0361)\n",
      "17789 Training Loss: tensor(0.0388)\n",
      "17790 Training Loss: tensor(0.0346)\n",
      "17791 Training Loss: tensor(0.0401)\n",
      "17792 Training Loss: tensor(0.0364)\n",
      "17793 Training Loss: tensor(0.0389)\n",
      "17794 Training Loss: tensor(0.0386)\n",
      "17795 Training Loss: tensor(0.0409)\n",
      "17796 Training Loss: tensor(0.0338)\n",
      "17797 Training Loss: tensor(0.0408)\n",
      "17798 Training Loss: tensor(0.0332)\n",
      "17799 Training Loss: tensor(0.0341)\n",
      "17800 Training Loss: tensor(0.0346)\n",
      "17801 Training Loss: tensor(0.0356)\n",
      "17802 Training Loss: tensor(0.0344)\n",
      "17803 Training Loss: tensor(0.0349)\n",
      "17804 Training Loss: tensor(0.0427)\n",
      "17805 Training Loss: tensor(0.0372)\n",
      "17806 Training Loss: tensor(0.0388)\n",
      "17807 Training Loss: tensor(0.0425)\n",
      "17808 Training Loss: tensor(0.0413)\n",
      "17809 Training Loss: tensor(0.0338)\n",
      "17810 Training Loss: tensor(0.0390)\n",
      "17811 Training Loss: tensor(0.0394)\n",
      "17812 Training Loss: tensor(0.0378)\n",
      "17813 Training Loss: tensor(0.0380)\n",
      "17814 Training Loss: tensor(0.0400)\n",
      "17815 Training Loss: tensor(0.0420)\n",
      "17816 Training Loss: tensor(0.0383)\n",
      "17817 Training Loss: tensor(0.0459)\n",
      "17818 Training Loss: tensor(0.0335)\n",
      "17819 Training Loss: tensor(0.0381)\n",
      "17820 Training Loss: tensor(0.0368)\n",
      "17821 Training Loss: tensor(0.0356)\n",
      "17822 Training Loss: tensor(0.0376)\n",
      "17823 Training Loss: tensor(0.0372)\n",
      "17824 Training Loss: tensor(0.0366)\n",
      "17825 Training Loss: tensor(0.0346)\n",
      "17826 Training Loss: tensor(0.0396)\n",
      "17827 Training Loss: tensor(0.0350)\n",
      "17828 Training Loss: tensor(0.0348)\n",
      "17829 Training Loss: tensor(0.0460)\n",
      "17830 Training Loss: tensor(0.0355)\n",
      "17831 Training Loss: tensor(0.0369)\n",
      "17832 Training Loss: tensor(0.0390)\n",
      "17833 Training Loss: tensor(0.0373)\n",
      "17834 Training Loss: tensor(0.0390)\n",
      "17835 Training Loss: tensor(0.0380)\n",
      "17836 Training Loss: tensor(0.0357)\n",
      "17837 Training Loss: tensor(0.0403)\n",
      "17838 Training Loss: tensor(0.0347)\n",
      "17839 Training Loss: tensor(0.0369)\n",
      "17840 Training Loss: tensor(0.0442)\n",
      "17841 Training Loss: tensor(0.0339)\n",
      "17842 Training Loss: tensor(0.0382)\n",
      "17843 Training Loss: tensor(0.0383)\n",
      "17844 Training Loss: tensor(0.0422)\n",
      "17845 Training Loss: tensor(0.0360)\n",
      "17846 Training Loss: tensor(0.0343)\n",
      "17847 Training Loss: tensor(0.0398)\n",
      "17848 Training Loss: tensor(0.0398)\n",
      "17849 Training Loss: tensor(0.0369)\n",
      "17850 Training Loss: tensor(0.0384)\n",
      "17851 Training Loss: tensor(0.0376)\n",
      "17852 Training Loss: tensor(0.0441)\n",
      "17853 Training Loss: tensor(0.0414)\n",
      "17854 Training Loss: tensor(0.0367)\n",
      "17855 Training Loss: tensor(0.0380)\n",
      "17856 Training Loss: tensor(0.0372)\n",
      "17857 Training Loss: tensor(0.0376)\n",
      "17858 Training Loss: tensor(0.0350)\n",
      "17859 Training Loss: tensor(0.0371)\n",
      "17860 Training Loss: tensor(0.0387)\n",
      "17861 Training Loss: tensor(0.0389)\n",
      "17862 Training Loss: tensor(0.0415)\n",
      "17863 Training Loss: tensor(0.0425)\n",
      "17864 Training Loss: tensor(0.0361)\n",
      "17865 Training Loss: tensor(0.0332)\n",
      "17866 Training Loss: tensor(0.0413)\n",
      "17867 Training Loss: tensor(0.0416)\n",
      "17868 Training Loss: tensor(0.0370)\n",
      "17869 Training Loss: tensor(0.0362)\n",
      "17870 Training Loss: tensor(0.0377)\n",
      "17871 Training Loss: tensor(0.0437)\n",
      "17872 Training Loss: tensor(0.0372)\n",
      "17873 Training Loss: tensor(0.0424)\n",
      "17874 Training Loss: tensor(0.0403)\n",
      "17875 Training Loss: tensor(0.0344)\n",
      "17876 Training Loss: tensor(0.0416)\n",
      "17877 Training Loss: tensor(0.0407)\n",
      "17878 Training Loss: tensor(0.0366)\n",
      "17879 Training Loss: tensor(0.0411)\n",
      "17880 Training Loss: tensor(0.0405)\n",
      "17881 Training Loss: tensor(0.0411)\n",
      "17882 Training Loss: tensor(0.0362)\n",
      "17883 Training Loss: tensor(0.0399)\n",
      "17884 Training Loss: tensor(0.0372)\n",
      "17885 Training Loss: tensor(0.0403)\n",
      "17886 Training Loss: tensor(0.0354)\n",
      "17887 Training Loss: tensor(0.0395)\n",
      "17888 Training Loss: tensor(0.0404)\n",
      "17889 Training Loss: tensor(0.0379)\n",
      "17890 Training Loss: tensor(0.0347)\n",
      "17891 Training Loss: tensor(0.0349)\n",
      "17892 Training Loss: tensor(0.0440)\n",
      "17893 Training Loss: tensor(0.0399)\n",
      "17894 Training Loss: tensor(0.0400)\n",
      "17895 Training Loss: tensor(0.0468)\n",
      "17896 Training Loss: tensor(0.0410)\n",
      "17897 Training Loss: tensor(0.0435)\n",
      "17898 Training Loss: tensor(0.0340)\n",
      "17899 Training Loss: tensor(0.0431)\n",
      "17900 Training Loss: tensor(0.0421)\n",
      "17901 Training Loss: tensor(0.0364)\n",
      "17902 Training Loss: tensor(0.0349)\n",
      "17903 Training Loss: tensor(0.0414)\n",
      "17904 Training Loss: tensor(0.0365)\n",
      "17905 Training Loss: tensor(0.0373)\n",
      "17906 Training Loss: tensor(0.0417)\n",
      "17907 Training Loss: tensor(0.0382)\n",
      "17908 Training Loss: tensor(0.0346)\n",
      "17909 Training Loss: tensor(0.0333)\n",
      "17910 Training Loss: tensor(0.0383)\n",
      "17911 Training Loss: tensor(0.0379)\n",
      "17912 Training Loss: tensor(0.0402)\n",
      "17913 Training Loss: tensor(0.0310)\n",
      "17914 Training Loss: tensor(0.0347)\n",
      "17915 Training Loss: tensor(0.0353)\n",
      "17916 Training Loss: tensor(0.0353)\n",
      "17917 Training Loss: tensor(0.0399)\n",
      "17918 Training Loss: tensor(0.0375)\n",
      "17919 Training Loss: tensor(0.0383)\n",
      "17920 Training Loss: tensor(0.0376)\n",
      "17921 Training Loss: tensor(0.0342)\n",
      "17922 Training Loss: tensor(0.0387)\n",
      "17923 Training Loss: tensor(0.0397)\n",
      "17924 Training Loss: tensor(0.0368)\n",
      "17925 Training Loss: tensor(0.0385)\n",
      "17926 Training Loss: tensor(0.0359)\n",
      "17927 Training Loss: tensor(0.0387)\n",
      "17928 Training Loss: tensor(0.0399)\n",
      "17929 Training Loss: tensor(0.0413)\n",
      "17930 Training Loss: tensor(0.0426)\n",
      "17931 Training Loss: tensor(0.0403)\n",
      "17932 Training Loss: tensor(0.0403)\n",
      "17933 Training Loss: tensor(0.0310)\n",
      "17934 Training Loss: tensor(0.0415)\n",
      "17935 Training Loss: tensor(0.0381)\n",
      "17936 Training Loss: tensor(0.0392)\n",
      "17937 Training Loss: tensor(0.0349)\n",
      "17938 Training Loss: tensor(0.0381)\n",
      "17939 Training Loss: tensor(0.0358)\n",
      "17940 Training Loss: tensor(0.0381)\n",
      "17941 Training Loss: tensor(0.0354)\n",
      "17942 Training Loss: tensor(0.0381)\n",
      "17943 Training Loss: tensor(0.0400)\n",
      "17944 Training Loss: tensor(0.0348)\n",
      "17945 Training Loss: tensor(0.0369)\n",
      "17946 Training Loss: tensor(0.0362)\n",
      "17947 Training Loss: tensor(0.0404)\n",
      "17948 Training Loss: tensor(0.0396)\n",
      "17949 Training Loss: tensor(0.0399)\n",
      "17950 Training Loss: tensor(0.0323)\n",
      "17951 Training Loss: tensor(0.0314)\n",
      "17952 Training Loss: tensor(0.0363)\n",
      "17953 Training Loss: tensor(0.0345)\n",
      "17954 Training Loss: tensor(0.0421)\n",
      "17955 Training Loss: tensor(0.0402)\n",
      "17956 Training Loss: tensor(0.0395)\n",
      "17957 Training Loss: tensor(0.0407)\n",
      "17958 Training Loss: tensor(0.0388)\n",
      "17959 Training Loss: tensor(0.0423)\n",
      "17960 Training Loss: tensor(0.0393)\n",
      "17961 Training Loss: tensor(0.0356)\n",
      "17962 Training Loss: tensor(0.0349)\n",
      "17963 Training Loss: tensor(0.0417)\n",
      "17964 Training Loss: tensor(0.0395)\n",
      "17965 Training Loss: tensor(0.0331)\n",
      "17966 Training Loss: tensor(0.0359)\n",
      "17967 Training Loss: tensor(0.0350)\n",
      "17968 Training Loss: tensor(0.0408)\n",
      "17969 Training Loss: tensor(0.0400)\n",
      "17970 Training Loss: tensor(0.0357)\n",
      "17971 Training Loss: tensor(0.0367)\n",
      "17972 Training Loss: tensor(0.0382)\n",
      "17973 Training Loss: tensor(0.0389)\n",
      "17974 Training Loss: tensor(0.0386)\n",
      "17975 Training Loss: tensor(0.0356)\n",
      "17976 Training Loss: tensor(0.0355)\n",
      "17977 Training Loss: tensor(0.0394)\n",
      "17978 Training Loss: tensor(0.0406)\n",
      "17979 Training Loss: tensor(0.0385)\n",
      "17980 Training Loss: tensor(0.0333)\n",
      "17981 Training Loss: tensor(0.0350)\n",
      "17982 Training Loss: tensor(0.0330)\n",
      "17983 Training Loss: tensor(0.0407)\n",
      "17984 Training Loss: tensor(0.0367)\n",
      "17985 Training Loss: tensor(0.0413)\n",
      "17986 Training Loss: tensor(0.0382)\n",
      "17987 Training Loss: tensor(0.0384)\n",
      "17988 Training Loss: tensor(0.0366)\n",
      "17989 Training Loss: tensor(0.0388)\n",
      "17990 Training Loss: tensor(0.0448)\n",
      "17991 Training Loss: tensor(0.0367)\n",
      "17992 Training Loss: tensor(0.0417)\n",
      "17993 Training Loss: tensor(0.0352)\n",
      "17994 Training Loss: tensor(0.0390)\n",
      "17995 Training Loss: tensor(0.0377)\n",
      "17996 Training Loss: tensor(0.0362)\n",
      "17997 Training Loss: tensor(0.0363)\n",
      "17998 Training Loss: tensor(0.0359)\n",
      "17999 Training Loss: tensor(0.0380)\n",
      "18000 Training Loss: tensor(0.0470)\n",
      "18001 Training Loss: tensor(0.0459)\n",
      "18002 Training Loss: tensor(0.0435)\n",
      "18003 Training Loss: tensor(0.0406)\n",
      "18004 Training Loss: tensor(0.0346)\n",
      "18005 Training Loss: tensor(0.0330)\n",
      "18006 Training Loss: tensor(0.0362)\n",
      "18007 Training Loss: tensor(0.0359)\n",
      "18008 Training Loss: tensor(0.0364)\n",
      "18009 Training Loss: tensor(0.0369)\n",
      "18010 Training Loss: tensor(0.0405)\n",
      "18011 Training Loss: tensor(0.0402)\n",
      "18012 Training Loss: tensor(0.0376)\n",
      "18013 Training Loss: tensor(0.0356)\n",
      "18014 Training Loss: tensor(0.0398)\n",
      "18015 Training Loss: tensor(0.0356)\n",
      "18016 Training Loss: tensor(0.0432)\n",
      "18017 Training Loss: tensor(0.0344)\n",
      "18018 Training Loss: tensor(0.0368)\n",
      "18019 Training Loss: tensor(0.0382)\n",
      "18020 Training Loss: tensor(0.0357)\n",
      "18021 Training Loss: tensor(0.0369)\n",
      "18022 Training Loss: tensor(0.0378)\n",
      "18023 Training Loss: tensor(0.0402)\n",
      "18024 Training Loss: tensor(0.0353)\n",
      "18025 Training Loss: tensor(0.0397)\n",
      "18026 Training Loss: tensor(0.0347)\n",
      "18027 Training Loss: tensor(0.0365)\n",
      "18028 Training Loss: tensor(0.0365)\n",
      "18029 Training Loss: tensor(0.0354)\n",
      "18030 Training Loss: tensor(0.0392)\n",
      "18031 Training Loss: tensor(0.0398)\n",
      "18032 Training Loss: tensor(0.0417)\n",
      "18033 Training Loss: tensor(0.0405)\n",
      "18034 Training Loss: tensor(0.0380)\n",
      "18035 Training Loss: tensor(0.0413)\n",
      "18036 Training Loss: tensor(0.0361)\n",
      "18037 Training Loss: tensor(0.0405)\n",
      "18038 Training Loss: tensor(0.0384)\n",
      "18039 Training Loss: tensor(0.0422)\n",
      "18040 Training Loss: tensor(0.0376)\n",
      "18041 Training Loss: tensor(0.0343)\n",
      "18042 Training Loss: tensor(0.0338)\n",
      "18043 Training Loss: tensor(0.0364)\n",
      "18044 Training Loss: tensor(0.0382)\n",
      "18045 Training Loss: tensor(0.0353)\n",
      "18046 Training Loss: tensor(0.0355)\n",
      "18047 Training Loss: tensor(0.0389)\n",
      "18048 Training Loss: tensor(0.0334)\n",
      "18049 Training Loss: tensor(0.0414)\n",
      "18050 Training Loss: tensor(0.0351)\n",
      "18051 Training Loss: tensor(0.0347)\n",
      "18052 Training Loss: tensor(0.0388)\n",
      "18053 Training Loss: tensor(0.0359)\n",
      "18054 Training Loss: tensor(0.0357)\n",
      "18055 Training Loss: tensor(0.0453)\n",
      "18056 Training Loss: tensor(0.0382)\n",
      "18057 Training Loss: tensor(0.0427)\n",
      "18058 Training Loss: tensor(0.0422)\n",
      "18059 Training Loss: tensor(0.0352)\n",
      "18060 Training Loss: tensor(0.0406)\n",
      "18061 Training Loss: tensor(0.0439)\n",
      "18062 Training Loss: tensor(0.0344)\n",
      "18063 Training Loss: tensor(0.0329)\n",
      "18064 Training Loss: tensor(0.0357)\n",
      "18065 Training Loss: tensor(0.0377)\n",
      "18066 Training Loss: tensor(0.0458)\n",
      "18067 Training Loss: tensor(0.0381)\n",
      "18068 Training Loss: tensor(0.0333)\n",
      "18069 Training Loss: tensor(0.0363)\n",
      "18070 Training Loss: tensor(0.0422)\n",
      "18071 Training Loss: tensor(0.0366)\n",
      "18072 Training Loss: tensor(0.0410)\n",
      "18073 Training Loss: tensor(0.0382)\n",
      "18074 Training Loss: tensor(0.0407)\n",
      "18075 Training Loss: tensor(0.0394)\n",
      "18076 Training Loss: tensor(0.0382)\n",
      "18077 Training Loss: tensor(0.0425)\n",
      "18078 Training Loss: tensor(0.0376)\n",
      "18079 Training Loss: tensor(0.0395)\n",
      "18080 Training Loss: tensor(0.0325)\n",
      "18081 Training Loss: tensor(0.0370)\n",
      "18082 Training Loss: tensor(0.0362)\n",
      "18083 Training Loss: tensor(0.0425)\n",
      "18084 Training Loss: tensor(0.0374)\n",
      "18085 Training Loss: tensor(0.0395)\n",
      "18086 Training Loss: tensor(0.0377)\n",
      "18087 Training Loss: tensor(0.0410)\n",
      "18088 Training Loss: tensor(0.0414)\n",
      "18089 Training Loss: tensor(0.0427)\n",
      "18090 Training Loss: tensor(0.0324)\n",
      "18091 Training Loss: tensor(0.0435)\n",
      "18092 Training Loss: tensor(0.0372)\n",
      "18093 Training Loss: tensor(0.0415)\n",
      "18094 Training Loss: tensor(0.0363)\n",
      "18095 Training Loss: tensor(0.0371)\n",
      "18096 Training Loss: tensor(0.0393)\n",
      "18097 Training Loss: tensor(0.0351)\n",
      "18098 Training Loss: tensor(0.0393)\n",
      "18099 Training Loss: tensor(0.0374)\n",
      "18100 Training Loss: tensor(0.0386)\n",
      "18101 Training Loss: tensor(0.0334)\n",
      "18102 Training Loss: tensor(0.0391)\n",
      "18103 Training Loss: tensor(0.0365)\n",
      "18104 Training Loss: tensor(0.0362)\n",
      "18105 Training Loss: tensor(0.0351)\n",
      "18106 Training Loss: tensor(0.0332)\n",
      "18107 Training Loss: tensor(0.0433)\n",
      "18108 Training Loss: tensor(0.0401)\n",
      "18109 Training Loss: tensor(0.0339)\n",
      "18110 Training Loss: tensor(0.0393)\n",
      "18111 Training Loss: tensor(0.0358)\n",
      "18112 Training Loss: tensor(0.0372)\n",
      "18113 Training Loss: tensor(0.0424)\n",
      "18114 Training Loss: tensor(0.0343)\n",
      "18115 Training Loss: tensor(0.0392)\n",
      "18116 Training Loss: tensor(0.0413)\n",
      "18117 Training Loss: tensor(0.0383)\n",
      "18118 Training Loss: tensor(0.0386)\n",
      "18119 Training Loss: tensor(0.0420)\n",
      "18120 Training Loss: tensor(0.0411)\n",
      "18121 Training Loss: tensor(0.0434)\n",
      "18122 Training Loss: tensor(0.0359)\n",
      "18123 Training Loss: tensor(0.0341)\n",
      "18124 Training Loss: tensor(0.0422)\n",
      "18125 Training Loss: tensor(0.0384)\n",
      "18126 Training Loss: tensor(0.0375)\n",
      "18127 Training Loss: tensor(0.0405)\n",
      "18128 Training Loss: tensor(0.0391)\n",
      "18129 Training Loss: tensor(0.0362)\n",
      "18130 Training Loss: tensor(0.0399)\n",
      "18131 Training Loss: tensor(0.0354)\n",
      "18132 Training Loss: tensor(0.0367)\n",
      "18133 Training Loss: tensor(0.0418)\n",
      "18134 Training Loss: tensor(0.0402)\n",
      "18135 Training Loss: tensor(0.0335)\n",
      "18136 Training Loss: tensor(0.0369)\n",
      "18137 Training Loss: tensor(0.0364)\n",
      "18138 Training Loss: tensor(0.0382)\n",
      "18139 Training Loss: tensor(0.0401)\n",
      "18140 Training Loss: tensor(0.0386)\n",
      "18141 Training Loss: tensor(0.0360)\n",
      "18142 Training Loss: tensor(0.0400)\n",
      "18143 Training Loss: tensor(0.0372)\n",
      "18144 Training Loss: tensor(0.0408)\n",
      "18145 Training Loss: tensor(0.0415)\n",
      "18146 Training Loss: tensor(0.0392)\n",
      "18147 Training Loss: tensor(0.0363)\n",
      "18148 Training Loss: tensor(0.0310)\n",
      "18149 Training Loss: tensor(0.0337)\n",
      "18150 Training Loss: tensor(0.0395)\n",
      "18151 Training Loss: tensor(0.0353)\n",
      "18152 Training Loss: tensor(0.0426)\n",
      "18153 Training Loss: tensor(0.0394)\n",
      "18154 Training Loss: tensor(0.0371)\n",
      "18155 Training Loss: tensor(0.0386)\n",
      "18156 Training Loss: tensor(0.0412)\n",
      "18157 Training Loss: tensor(0.0311)\n",
      "18158 Training Loss: tensor(0.0356)\n",
      "18159 Training Loss: tensor(0.0331)\n",
      "18160 Training Loss: tensor(0.0331)\n",
      "18161 Training Loss: tensor(0.0421)\n",
      "18162 Training Loss: tensor(0.0337)\n",
      "18163 Training Loss: tensor(0.0415)\n",
      "18164 Training Loss: tensor(0.0353)\n",
      "18165 Training Loss: tensor(0.0408)\n",
      "18166 Training Loss: tensor(0.0411)\n",
      "18167 Training Loss: tensor(0.0397)\n",
      "18168 Training Loss: tensor(0.0364)\n",
      "18169 Training Loss: tensor(0.0299)\n",
      "18170 Training Loss: tensor(0.0394)\n",
      "18171 Training Loss: tensor(0.0403)\n",
      "18172 Training Loss: tensor(0.0346)\n",
      "18173 Training Loss: tensor(0.0361)\n",
      "18174 Training Loss: tensor(0.0316)\n",
      "18175 Training Loss: tensor(0.0421)\n",
      "18176 Training Loss: tensor(0.0387)\n",
      "18177 Training Loss: tensor(0.0347)\n",
      "18178 Training Loss: tensor(0.0432)\n",
      "18179 Training Loss: tensor(0.0383)\n",
      "18180 Training Loss: tensor(0.0342)\n",
      "18181 Training Loss: tensor(0.0355)\n",
      "18182 Training Loss: tensor(0.0392)\n",
      "18183 Training Loss: tensor(0.0405)\n",
      "18184 Training Loss: tensor(0.0459)\n",
      "18185 Training Loss: tensor(0.0352)\n",
      "18186 Training Loss: tensor(0.0423)\n",
      "18187 Training Loss: tensor(0.0347)\n",
      "18188 Training Loss: tensor(0.0436)\n",
      "18189 Training Loss: tensor(0.0449)\n",
      "18190 Training Loss: tensor(0.0415)\n",
      "18191 Training Loss: tensor(0.0392)\n",
      "18192 Training Loss: tensor(0.0420)\n",
      "18193 Training Loss: tensor(0.0406)\n",
      "18194 Training Loss: tensor(0.0394)\n",
      "18195 Training Loss: tensor(0.0389)\n",
      "18196 Training Loss: tensor(0.0378)\n",
      "18197 Training Loss: tensor(0.0420)\n",
      "18198 Training Loss: tensor(0.0447)\n",
      "18199 Training Loss: tensor(0.0387)\n",
      "18200 Training Loss: tensor(0.0343)\n",
      "18201 Training Loss: tensor(0.0415)\n",
      "18202 Training Loss: tensor(0.0404)\n",
      "18203 Training Loss: tensor(0.0350)\n",
      "18204 Training Loss: tensor(0.0373)\n",
      "18205 Training Loss: tensor(0.0369)\n",
      "18206 Training Loss: tensor(0.0360)\n",
      "18207 Training Loss: tensor(0.0448)\n",
      "18208 Training Loss: tensor(0.0402)\n",
      "18209 Training Loss: tensor(0.0403)\n",
      "18210 Training Loss: tensor(0.0379)\n",
      "18211 Training Loss: tensor(0.0378)\n",
      "18212 Training Loss: tensor(0.0423)\n",
      "18213 Training Loss: tensor(0.0393)\n",
      "18214 Training Loss: tensor(0.0417)\n",
      "18215 Training Loss: tensor(0.0417)\n",
      "18216 Training Loss: tensor(0.0392)\n",
      "18217 Training Loss: tensor(0.0375)\n",
      "18218 Training Loss: tensor(0.0336)\n",
      "18219 Training Loss: tensor(0.0336)\n",
      "18220 Training Loss: tensor(0.0344)\n",
      "18221 Training Loss: tensor(0.0345)\n",
      "18222 Training Loss: tensor(0.0381)\n",
      "18223 Training Loss: tensor(0.0351)\n",
      "18224 Training Loss: tensor(0.0361)\n",
      "18225 Training Loss: tensor(0.0335)\n",
      "18226 Training Loss: tensor(0.0362)\n",
      "18227 Training Loss: tensor(0.0321)\n",
      "18228 Training Loss: tensor(0.0361)\n",
      "18229 Training Loss: tensor(0.0402)\n",
      "18230 Training Loss: tensor(0.0379)\n",
      "18231 Training Loss: tensor(0.0421)\n",
      "18232 Training Loss: tensor(0.0342)\n",
      "18233 Training Loss: tensor(0.0404)\n",
      "18234 Training Loss: tensor(0.0368)\n",
      "18235 Training Loss: tensor(0.0366)\n",
      "18236 Training Loss: tensor(0.0365)\n",
      "18237 Training Loss: tensor(0.0337)\n",
      "18238 Training Loss: tensor(0.0344)\n",
      "18239 Training Loss: tensor(0.0377)\n",
      "18240 Training Loss: tensor(0.0373)\n",
      "18241 Training Loss: tensor(0.0467)\n",
      "18242 Training Loss: tensor(0.0419)\n",
      "18243 Training Loss: tensor(0.0373)\n",
      "18244 Training Loss: tensor(0.0313)\n",
      "18245 Training Loss: tensor(0.0414)\n",
      "18246 Training Loss: tensor(0.0359)\n",
      "18247 Training Loss: tensor(0.0366)\n",
      "18248 Training Loss: tensor(0.0352)\n",
      "18249 Training Loss: tensor(0.0329)\n",
      "18250 Training Loss: tensor(0.0327)\n",
      "18251 Training Loss: tensor(0.0336)\n",
      "18252 Training Loss: tensor(0.0323)\n",
      "18253 Training Loss: tensor(0.0326)\n",
      "18254 Training Loss: tensor(0.0398)\n",
      "18255 Training Loss: tensor(0.0350)\n",
      "18256 Training Loss: tensor(0.0357)\n",
      "18257 Training Loss: tensor(0.0387)\n",
      "18258 Training Loss: tensor(0.0400)\n",
      "18259 Training Loss: tensor(0.0373)\n",
      "18260 Training Loss: tensor(0.0430)\n",
      "18261 Training Loss: tensor(0.0344)\n",
      "18262 Training Loss: tensor(0.0453)\n",
      "18263 Training Loss: tensor(0.0386)\n",
      "18264 Training Loss: tensor(0.0345)\n",
      "18265 Training Loss: tensor(0.0333)\n",
      "18266 Training Loss: tensor(0.0375)\n",
      "18267 Training Loss: tensor(0.0333)\n",
      "18268 Training Loss: tensor(0.0430)\n",
      "18269 Training Loss: tensor(0.0365)\n",
      "18270 Training Loss: tensor(0.0339)\n",
      "18271 Training Loss: tensor(0.0387)\n",
      "18272 Training Loss: tensor(0.0400)\n",
      "18273 Training Loss: tensor(0.0368)\n",
      "18274 Training Loss: tensor(0.0397)\n",
      "18275 Training Loss: tensor(0.0341)\n",
      "18276 Training Loss: tensor(0.0364)\n",
      "18277 Training Loss: tensor(0.0384)\n",
      "18278 Training Loss: tensor(0.0393)\n",
      "18279 Training Loss: tensor(0.0354)\n",
      "18280 Training Loss: tensor(0.0332)\n",
      "18281 Training Loss: tensor(0.0395)\n",
      "18282 Training Loss: tensor(0.0339)\n",
      "18283 Training Loss: tensor(0.0360)\n",
      "18284 Training Loss: tensor(0.0322)\n",
      "18285 Training Loss: tensor(0.0350)\n",
      "18286 Training Loss: tensor(0.0366)\n",
      "18287 Training Loss: tensor(0.0416)\n",
      "18288 Training Loss: tensor(0.0312)\n",
      "18289 Training Loss: tensor(0.0353)\n",
      "18290 Training Loss: tensor(0.0381)\n",
      "18291 Training Loss: tensor(0.0389)\n",
      "18292 Training Loss: tensor(0.0373)\n",
      "18293 Training Loss: tensor(0.0420)\n",
      "18294 Training Loss: tensor(0.0322)\n",
      "18295 Training Loss: tensor(0.0379)\n",
      "18296 Training Loss: tensor(0.0323)\n",
      "18297 Training Loss: tensor(0.0423)\n",
      "18298 Training Loss: tensor(0.0387)\n",
      "18299 Training Loss: tensor(0.0371)\n",
      "18300 Training Loss: tensor(0.0409)\n",
      "18301 Training Loss: tensor(0.0353)\n",
      "18302 Training Loss: tensor(0.0332)\n",
      "18303 Training Loss: tensor(0.0352)\n",
      "18304 Training Loss: tensor(0.0349)\n",
      "18305 Training Loss: tensor(0.0334)\n",
      "18306 Training Loss: tensor(0.0422)\n",
      "18307 Training Loss: tensor(0.0421)\n",
      "18308 Training Loss: tensor(0.0409)\n",
      "18309 Training Loss: tensor(0.0401)\n",
      "18310 Training Loss: tensor(0.0397)\n",
      "18311 Training Loss: tensor(0.0373)\n",
      "18312 Training Loss: tensor(0.0358)\n",
      "18313 Training Loss: tensor(0.0420)\n",
      "18314 Training Loss: tensor(0.0437)\n",
      "18315 Training Loss: tensor(0.0406)\n",
      "18316 Training Loss: tensor(0.0388)\n",
      "18317 Training Loss: tensor(0.0314)\n",
      "18318 Training Loss: tensor(0.0365)\n",
      "18319 Training Loss: tensor(0.0371)\n",
      "18320 Training Loss: tensor(0.0379)\n",
      "18321 Training Loss: tensor(0.0373)\n",
      "18322 Training Loss: tensor(0.0402)\n",
      "18323 Training Loss: tensor(0.0411)\n",
      "18324 Training Loss: tensor(0.0384)\n",
      "18325 Training Loss: tensor(0.0388)\n",
      "18326 Training Loss: tensor(0.0382)\n",
      "18327 Training Loss: tensor(0.0350)\n",
      "18328 Training Loss: tensor(0.0419)\n",
      "18329 Training Loss: tensor(0.0381)\n",
      "18330 Training Loss: tensor(0.0375)\n",
      "18331 Training Loss: tensor(0.0360)\n",
      "18332 Training Loss: tensor(0.0389)\n",
      "18333 Training Loss: tensor(0.0375)\n",
      "18334 Training Loss: tensor(0.0399)\n",
      "18335 Training Loss: tensor(0.0364)\n",
      "18336 Training Loss: tensor(0.0414)\n",
      "18337 Training Loss: tensor(0.0351)\n",
      "18338 Training Loss: tensor(0.0379)\n",
      "18339 Training Loss: tensor(0.0349)\n",
      "18340 Training Loss: tensor(0.0379)\n",
      "18341 Training Loss: tensor(0.0391)\n",
      "18342 Training Loss: tensor(0.0449)\n",
      "18343 Training Loss: tensor(0.0365)\n",
      "18344 Training Loss: tensor(0.0394)\n",
      "18345 Training Loss: tensor(0.0421)\n",
      "18346 Training Loss: tensor(0.0404)\n",
      "18347 Training Loss: tensor(0.0344)\n",
      "18348 Training Loss: tensor(0.0392)\n",
      "18349 Training Loss: tensor(0.0390)\n",
      "18350 Training Loss: tensor(0.0413)\n",
      "18351 Training Loss: tensor(0.0373)\n",
      "18352 Training Loss: tensor(0.0412)\n",
      "18353 Training Loss: tensor(0.0362)\n",
      "18354 Training Loss: tensor(0.0401)\n",
      "18355 Training Loss: tensor(0.0419)\n",
      "18356 Training Loss: tensor(0.0404)\n",
      "18357 Training Loss: tensor(0.0394)\n",
      "18358 Training Loss: tensor(0.0373)\n",
      "18359 Training Loss: tensor(0.0417)\n",
      "18360 Training Loss: tensor(0.0359)\n",
      "18361 Training Loss: tensor(0.0374)\n",
      "18362 Training Loss: tensor(0.0360)\n",
      "18363 Training Loss: tensor(0.0329)\n",
      "18364 Training Loss: tensor(0.0383)\n",
      "18365 Training Loss: tensor(0.0450)\n",
      "18366 Training Loss: tensor(0.0377)\n",
      "18367 Training Loss: tensor(0.0461)\n",
      "18368 Training Loss: tensor(0.0389)\n",
      "18369 Training Loss: tensor(0.0350)\n",
      "18370 Training Loss: tensor(0.0403)\n",
      "18371 Training Loss: tensor(0.0386)\n",
      "18372 Training Loss: tensor(0.0369)\n",
      "18373 Training Loss: tensor(0.0410)\n",
      "18374 Training Loss: tensor(0.0403)\n",
      "18375 Training Loss: tensor(0.0384)\n",
      "18376 Training Loss: tensor(0.0338)\n",
      "18377 Training Loss: tensor(0.0351)\n",
      "18378 Training Loss: tensor(0.0380)\n",
      "18379 Training Loss: tensor(0.0377)\n",
      "18380 Training Loss: tensor(0.0419)\n",
      "18381 Training Loss: tensor(0.0361)\n",
      "18382 Training Loss: tensor(0.0460)\n",
      "18383 Training Loss: tensor(0.0417)\n",
      "18384 Training Loss: tensor(0.0380)\n",
      "18385 Training Loss: tensor(0.0362)\n",
      "18386 Training Loss: tensor(0.0388)\n",
      "18387 Training Loss: tensor(0.0334)\n",
      "18388 Training Loss: tensor(0.0441)\n",
      "18389 Training Loss: tensor(0.0381)\n",
      "18390 Training Loss: tensor(0.0358)\n",
      "18391 Training Loss: tensor(0.0369)\n",
      "18392 Training Loss: tensor(0.0369)\n",
      "18393 Training Loss: tensor(0.0390)\n",
      "18394 Training Loss: tensor(0.0354)\n",
      "18395 Training Loss: tensor(0.0352)\n",
      "18396 Training Loss: tensor(0.0335)\n",
      "18397 Training Loss: tensor(0.0399)\n",
      "18398 Training Loss: tensor(0.0323)\n",
      "18399 Training Loss: tensor(0.0334)\n",
      "18400 Training Loss: tensor(0.0372)\n",
      "18401 Training Loss: tensor(0.0391)\n",
      "18402 Training Loss: tensor(0.0323)\n",
      "18403 Training Loss: tensor(0.0331)\n",
      "18404 Training Loss: tensor(0.0391)\n",
      "18405 Training Loss: tensor(0.0384)\n",
      "18406 Training Loss: tensor(0.0399)\n",
      "18407 Training Loss: tensor(0.0380)\n",
      "18408 Training Loss: tensor(0.0374)\n",
      "18409 Training Loss: tensor(0.0394)\n",
      "18410 Training Loss: tensor(0.0342)\n",
      "18411 Training Loss: tensor(0.0428)\n",
      "18412 Training Loss: tensor(0.0385)\n",
      "18413 Training Loss: tensor(0.0359)\n",
      "18414 Training Loss: tensor(0.0407)\n",
      "18415 Training Loss: tensor(0.0417)\n",
      "18416 Training Loss: tensor(0.0407)\n",
      "18417 Training Loss: tensor(0.0333)\n",
      "18418 Training Loss: tensor(0.0373)\n",
      "18419 Training Loss: tensor(0.0336)\n",
      "18420 Training Loss: tensor(0.0420)\n",
      "18421 Training Loss: tensor(0.0388)\n",
      "18422 Training Loss: tensor(0.0401)\n",
      "18423 Training Loss: tensor(0.0406)\n",
      "18424 Training Loss: tensor(0.0363)\n",
      "18425 Training Loss: tensor(0.0378)\n",
      "18426 Training Loss: tensor(0.0432)\n",
      "18427 Training Loss: tensor(0.0382)\n",
      "18428 Training Loss: tensor(0.0362)\n",
      "18429 Training Loss: tensor(0.0396)\n",
      "18430 Training Loss: tensor(0.0330)\n",
      "18431 Training Loss: tensor(0.0363)\n",
      "18432 Training Loss: tensor(0.0351)\n",
      "18433 Training Loss: tensor(0.0384)\n",
      "18434 Training Loss: tensor(0.0402)\n",
      "18435 Training Loss: tensor(0.0383)\n",
      "18436 Training Loss: tensor(0.0365)\n",
      "18437 Training Loss: tensor(0.0389)\n",
      "18438 Training Loss: tensor(0.0377)\n",
      "18439 Training Loss: tensor(0.0417)\n",
      "18440 Training Loss: tensor(0.0412)\n",
      "18441 Training Loss: tensor(0.0366)\n",
      "18442 Training Loss: tensor(0.0362)\n",
      "18443 Training Loss: tensor(0.0412)\n",
      "18444 Training Loss: tensor(0.0371)\n",
      "18445 Training Loss: tensor(0.0351)\n",
      "18446 Training Loss: tensor(0.0393)\n",
      "18447 Training Loss: tensor(0.0346)\n",
      "18448 Training Loss: tensor(0.0366)\n",
      "18449 Training Loss: tensor(0.0418)\n",
      "18450 Training Loss: tensor(0.0379)\n",
      "18451 Training Loss: tensor(0.0364)\n",
      "18452 Training Loss: tensor(0.0427)\n",
      "18453 Training Loss: tensor(0.0343)\n",
      "18454 Training Loss: tensor(0.0445)\n",
      "18455 Training Loss: tensor(0.0361)\n",
      "18456 Training Loss: tensor(0.0333)\n",
      "18457 Training Loss: tensor(0.0369)\n",
      "18458 Training Loss: tensor(0.0367)\n",
      "18459 Training Loss: tensor(0.0372)\n",
      "18460 Training Loss: tensor(0.0355)\n",
      "18461 Training Loss: tensor(0.0432)\n",
      "18462 Training Loss: tensor(0.0400)\n",
      "18463 Training Loss: tensor(0.0358)\n",
      "18464 Training Loss: tensor(0.0325)\n",
      "18465 Training Loss: tensor(0.0364)\n",
      "18466 Training Loss: tensor(0.0384)\n",
      "18467 Training Loss: tensor(0.0329)\n",
      "18468 Training Loss: tensor(0.0373)\n",
      "18469 Training Loss: tensor(0.0341)\n",
      "18470 Training Loss: tensor(0.0390)\n",
      "18471 Training Loss: tensor(0.0419)\n",
      "18472 Training Loss: tensor(0.0363)\n",
      "18473 Training Loss: tensor(0.0422)\n",
      "18474 Training Loss: tensor(0.0369)\n",
      "18475 Training Loss: tensor(0.0413)\n",
      "18476 Training Loss: tensor(0.0400)\n",
      "18477 Training Loss: tensor(0.0352)\n",
      "18478 Training Loss: tensor(0.0427)\n",
      "18479 Training Loss: tensor(0.0374)\n",
      "18480 Training Loss: tensor(0.0397)\n",
      "18481 Training Loss: tensor(0.0380)\n",
      "18482 Training Loss: tensor(0.0364)\n",
      "18483 Training Loss: tensor(0.0393)\n",
      "18484 Training Loss: tensor(0.0416)\n",
      "18485 Training Loss: tensor(0.0370)\n",
      "18486 Training Loss: tensor(0.0427)\n",
      "18487 Training Loss: tensor(0.0396)\n",
      "18488 Training Loss: tensor(0.0371)\n",
      "18489 Training Loss: tensor(0.0352)\n",
      "18490 Training Loss: tensor(0.0444)\n",
      "18491 Training Loss: tensor(0.0393)\n",
      "18492 Training Loss: tensor(0.0391)\n",
      "18493 Training Loss: tensor(0.0386)\n",
      "18494 Training Loss: tensor(0.0331)\n",
      "18495 Training Loss: tensor(0.0380)\n",
      "18496 Training Loss: tensor(0.0427)\n",
      "18497 Training Loss: tensor(0.0391)\n",
      "18498 Training Loss: tensor(0.0352)\n",
      "18499 Training Loss: tensor(0.0376)\n",
      "18500 Training Loss: tensor(0.0382)\n",
      "18501 Training Loss: tensor(0.0373)\n",
      "18502 Training Loss: tensor(0.0385)\n",
      "18503 Training Loss: tensor(0.0368)\n",
      "18504 Training Loss: tensor(0.0399)\n",
      "18505 Training Loss: tensor(0.0383)\n",
      "18506 Training Loss: tensor(0.0378)\n",
      "18507 Training Loss: tensor(0.0322)\n",
      "18508 Training Loss: tensor(0.0441)\n",
      "18509 Training Loss: tensor(0.0346)\n",
      "18510 Training Loss: tensor(0.0389)\n",
      "18511 Training Loss: tensor(0.0388)\n",
      "18512 Training Loss: tensor(0.0349)\n",
      "18513 Training Loss: tensor(0.0378)\n",
      "18514 Training Loss: tensor(0.0336)\n",
      "18515 Training Loss: tensor(0.0326)\n",
      "18516 Training Loss: tensor(0.0387)\n",
      "18517 Training Loss: tensor(0.0437)\n",
      "18518 Training Loss: tensor(0.0350)\n",
      "18519 Training Loss: tensor(0.0366)\n",
      "18520 Training Loss: tensor(0.0386)\n",
      "18521 Training Loss: tensor(0.0373)\n",
      "18522 Training Loss: tensor(0.0333)\n",
      "18523 Training Loss: tensor(0.0365)\n",
      "18524 Training Loss: tensor(0.0339)\n",
      "18525 Training Loss: tensor(0.0346)\n",
      "18526 Training Loss: tensor(0.0423)\n",
      "18527 Training Loss: tensor(0.0383)\n",
      "18528 Training Loss: tensor(0.0394)\n",
      "18529 Training Loss: tensor(0.0403)\n",
      "18530 Training Loss: tensor(0.0362)\n",
      "18531 Training Loss: tensor(0.0368)\n",
      "18532 Training Loss: tensor(0.0373)\n",
      "18533 Training Loss: tensor(0.0394)\n",
      "18534 Training Loss: tensor(0.0363)\n",
      "18535 Training Loss: tensor(0.0336)\n",
      "18536 Training Loss: tensor(0.0368)\n",
      "18537 Training Loss: tensor(0.0359)\n",
      "18538 Training Loss: tensor(0.0350)\n",
      "18539 Training Loss: tensor(0.0387)\n",
      "18540 Training Loss: tensor(0.0407)\n",
      "18541 Training Loss: tensor(0.0364)\n",
      "18542 Training Loss: tensor(0.0370)\n",
      "18543 Training Loss: tensor(0.0333)\n",
      "18544 Training Loss: tensor(0.0402)\n",
      "18545 Training Loss: tensor(0.0368)\n",
      "18546 Training Loss: tensor(0.0425)\n",
      "18547 Training Loss: tensor(0.0404)\n",
      "18548 Training Loss: tensor(0.0372)\n",
      "18549 Training Loss: tensor(0.0382)\n",
      "18550 Training Loss: tensor(0.0326)\n",
      "18551 Training Loss: tensor(0.0419)\n",
      "18552 Training Loss: tensor(0.0401)\n",
      "18553 Training Loss: tensor(0.0351)\n",
      "18554 Training Loss: tensor(0.0381)\n",
      "18555 Training Loss: tensor(0.0391)\n",
      "18556 Training Loss: tensor(0.0373)\n",
      "18557 Training Loss: tensor(0.0354)\n",
      "18558 Training Loss: tensor(0.0455)\n",
      "18559 Training Loss: tensor(0.0379)\n",
      "18560 Training Loss: tensor(0.0422)\n",
      "18561 Training Loss: tensor(0.0389)\n",
      "18562 Training Loss: tensor(0.0364)\n",
      "18563 Training Loss: tensor(0.0375)\n",
      "18564 Training Loss: tensor(0.0350)\n",
      "18565 Training Loss: tensor(0.0371)\n",
      "18566 Training Loss: tensor(0.0368)\n",
      "18567 Training Loss: tensor(0.0357)\n",
      "18568 Training Loss: tensor(0.0378)\n",
      "18569 Training Loss: tensor(0.0369)\n",
      "18570 Training Loss: tensor(0.0383)\n",
      "18571 Training Loss: tensor(0.0353)\n",
      "18572 Training Loss: tensor(0.0362)\n",
      "18573 Training Loss: tensor(0.0398)\n",
      "18574 Training Loss: tensor(0.0364)\n",
      "18575 Training Loss: tensor(0.0377)\n",
      "18576 Training Loss: tensor(0.0329)\n",
      "18577 Training Loss: tensor(0.0363)\n",
      "18578 Training Loss: tensor(0.0349)\n",
      "18579 Training Loss: tensor(0.0422)\n",
      "18580 Training Loss: tensor(0.0395)\n",
      "18581 Training Loss: tensor(0.0392)\n",
      "18582 Training Loss: tensor(0.0305)\n",
      "18583 Training Loss: tensor(0.0428)\n",
      "18584 Training Loss: tensor(0.0358)\n",
      "18585 Training Loss: tensor(0.0337)\n",
      "18586 Training Loss: tensor(0.0370)\n",
      "18587 Training Loss: tensor(0.0392)\n",
      "18588 Training Loss: tensor(0.0369)\n",
      "18589 Training Loss: tensor(0.0379)\n",
      "18590 Training Loss: tensor(0.0352)\n",
      "18591 Training Loss: tensor(0.0368)\n",
      "18592 Training Loss: tensor(0.0368)\n",
      "18593 Training Loss: tensor(0.0380)\n",
      "18594 Training Loss: tensor(0.0390)\n",
      "18595 Training Loss: tensor(0.0408)\n",
      "18596 Training Loss: tensor(0.0381)\n",
      "18597 Training Loss: tensor(0.0354)\n",
      "18598 Training Loss: tensor(0.0370)\n",
      "18599 Training Loss: tensor(0.0359)\n",
      "18600 Training Loss: tensor(0.0394)\n",
      "18601 Training Loss: tensor(0.0344)\n",
      "18602 Training Loss: tensor(0.0372)\n",
      "18603 Training Loss: tensor(0.0350)\n",
      "18604 Training Loss: tensor(0.0375)\n",
      "18605 Training Loss: tensor(0.0422)\n",
      "18606 Training Loss: tensor(0.0339)\n",
      "18607 Training Loss: tensor(0.0396)\n",
      "18608 Training Loss: tensor(0.0359)\n",
      "18609 Training Loss: tensor(0.0442)\n",
      "18610 Training Loss: tensor(0.0424)\n",
      "18611 Training Loss: tensor(0.0464)\n",
      "18612 Training Loss: tensor(0.0380)\n",
      "18613 Training Loss: tensor(0.0360)\n",
      "18614 Training Loss: tensor(0.0402)\n",
      "18615 Training Loss: tensor(0.0386)\n",
      "18616 Training Loss: tensor(0.0368)\n",
      "18617 Training Loss: tensor(0.0376)\n",
      "18618 Training Loss: tensor(0.0392)\n",
      "18619 Training Loss: tensor(0.0445)\n",
      "18620 Training Loss: tensor(0.0389)\n",
      "18621 Training Loss: tensor(0.0368)\n",
      "18622 Training Loss: tensor(0.0370)\n",
      "18623 Training Loss: tensor(0.0411)\n",
      "18624 Training Loss: tensor(0.0368)\n",
      "18625 Training Loss: tensor(0.0337)\n",
      "18626 Training Loss: tensor(0.0374)\n",
      "18627 Training Loss: tensor(0.0393)\n",
      "18628 Training Loss: tensor(0.0365)\n",
      "18629 Training Loss: tensor(0.0383)\n",
      "18630 Training Loss: tensor(0.0335)\n",
      "18631 Training Loss: tensor(0.0345)\n",
      "18632 Training Loss: tensor(0.0359)\n",
      "18633 Training Loss: tensor(0.0492)\n",
      "18634 Training Loss: tensor(0.0401)\n",
      "18635 Training Loss: tensor(0.0376)\n",
      "18636 Training Loss: tensor(0.0429)\n",
      "18637 Training Loss: tensor(0.0342)\n",
      "18638 Training Loss: tensor(0.0396)\n",
      "18639 Training Loss: tensor(0.0420)\n",
      "18640 Training Loss: tensor(0.0329)\n",
      "18641 Training Loss: tensor(0.0422)\n",
      "18642 Training Loss: tensor(0.0414)\n",
      "18643 Training Loss: tensor(0.0411)\n",
      "18644 Training Loss: tensor(0.0373)\n",
      "18645 Training Loss: tensor(0.0362)\n",
      "18646 Training Loss: tensor(0.0409)\n",
      "18647 Training Loss: tensor(0.0365)\n",
      "18648 Training Loss: tensor(0.0399)\n",
      "18649 Training Loss: tensor(0.0363)\n",
      "18650 Training Loss: tensor(0.0360)\n",
      "18651 Training Loss: tensor(0.0326)\n",
      "18652 Training Loss: tensor(0.0414)\n",
      "18653 Training Loss: tensor(0.0359)\n",
      "18654 Training Loss: tensor(0.0438)\n",
      "18655 Training Loss: tensor(0.0354)\n",
      "18656 Training Loss: tensor(0.0335)\n",
      "18657 Training Loss: tensor(0.0452)\n",
      "18658 Training Loss: tensor(0.0360)\n",
      "18659 Training Loss: tensor(0.0377)\n",
      "18660 Training Loss: tensor(0.0410)\n",
      "18661 Training Loss: tensor(0.0365)\n",
      "18662 Training Loss: tensor(0.0377)\n",
      "18663 Training Loss: tensor(0.0382)\n",
      "18664 Training Loss: tensor(0.0397)\n",
      "18665 Training Loss: tensor(0.0370)\n",
      "18666 Training Loss: tensor(0.0470)\n",
      "18667 Training Loss: tensor(0.0420)\n",
      "18668 Training Loss: tensor(0.0389)\n",
      "18669 Training Loss: tensor(0.0412)\n",
      "18670 Training Loss: tensor(0.0391)\n",
      "18671 Training Loss: tensor(0.0358)\n",
      "18672 Training Loss: tensor(0.0389)\n",
      "18673 Training Loss: tensor(0.0324)\n",
      "18674 Training Loss: tensor(0.0367)\n",
      "18675 Training Loss: tensor(0.0374)\n",
      "18676 Training Loss: tensor(0.0386)\n",
      "18677 Training Loss: tensor(0.0388)\n",
      "18678 Training Loss: tensor(0.0376)\n",
      "18679 Training Loss: tensor(0.0358)\n",
      "18680 Training Loss: tensor(0.0388)\n",
      "18681 Training Loss: tensor(0.0385)\n",
      "18682 Training Loss: tensor(0.0339)\n",
      "18683 Training Loss: tensor(0.0330)\n",
      "18684 Training Loss: tensor(0.0386)\n",
      "18685 Training Loss: tensor(0.0364)\n",
      "18686 Training Loss: tensor(0.0323)\n",
      "18687 Training Loss: tensor(0.0407)\n",
      "18688 Training Loss: tensor(0.0365)\n",
      "18689 Training Loss: tensor(0.0364)\n",
      "18690 Training Loss: tensor(0.0382)\n",
      "18691 Training Loss: tensor(0.0376)\n",
      "18692 Training Loss: tensor(0.0365)\n",
      "18693 Training Loss: tensor(0.0420)\n",
      "18694 Training Loss: tensor(0.0381)\n",
      "18695 Training Loss: tensor(0.0368)\n",
      "18696 Training Loss: tensor(0.0401)\n",
      "18697 Training Loss: tensor(0.0394)\n",
      "18698 Training Loss: tensor(0.0355)\n",
      "18699 Training Loss: tensor(0.0377)\n",
      "18700 Training Loss: tensor(0.0401)\n",
      "18701 Training Loss: tensor(0.0395)\n",
      "18702 Training Loss: tensor(0.0347)\n",
      "18703 Training Loss: tensor(0.0394)\n",
      "18704 Training Loss: tensor(0.0386)\n",
      "18705 Training Loss: tensor(0.0398)\n",
      "18706 Training Loss: tensor(0.0388)\n",
      "18707 Training Loss: tensor(0.0357)\n",
      "18708 Training Loss: tensor(0.0410)\n",
      "18709 Training Loss: tensor(0.0375)\n",
      "18710 Training Loss: tensor(0.0398)\n",
      "18711 Training Loss: tensor(0.0352)\n",
      "18712 Training Loss: tensor(0.0357)\n",
      "18713 Training Loss: tensor(0.0369)\n",
      "18714 Training Loss: tensor(0.0399)\n",
      "18715 Training Loss: tensor(0.0366)\n",
      "18716 Training Loss: tensor(0.0418)\n",
      "18717 Training Loss: tensor(0.0351)\n",
      "18718 Training Loss: tensor(0.0390)\n",
      "18719 Training Loss: tensor(0.0376)\n",
      "18720 Training Loss: tensor(0.0360)\n",
      "18721 Training Loss: tensor(0.0317)\n",
      "18722 Training Loss: tensor(0.0361)\n",
      "18723 Training Loss: tensor(0.0381)\n",
      "18724 Training Loss: tensor(0.0369)\n",
      "18725 Training Loss: tensor(0.0366)\n",
      "18726 Training Loss: tensor(0.0407)\n",
      "18727 Training Loss: tensor(0.0384)\n",
      "18728 Training Loss: tensor(0.0417)\n",
      "18729 Training Loss: tensor(0.0393)\n",
      "18730 Training Loss: tensor(0.0391)\n",
      "18731 Training Loss: tensor(0.0384)\n",
      "18732 Training Loss: tensor(0.0393)\n",
      "18733 Training Loss: tensor(0.0401)\n",
      "18734 Training Loss: tensor(0.0411)\n",
      "18735 Training Loss: tensor(0.0404)\n",
      "18736 Training Loss: tensor(0.0388)\n",
      "18737 Training Loss: tensor(0.0434)\n",
      "18738 Training Loss: tensor(0.0342)\n",
      "18739 Training Loss: tensor(0.0377)\n",
      "18740 Training Loss: tensor(0.0399)\n",
      "18741 Training Loss: tensor(0.0382)\n",
      "18742 Training Loss: tensor(0.0390)\n",
      "18743 Training Loss: tensor(0.0368)\n",
      "18744 Training Loss: tensor(0.0426)\n",
      "18745 Training Loss: tensor(0.0389)\n",
      "18746 Training Loss: tensor(0.0326)\n",
      "18747 Training Loss: tensor(0.0366)\n",
      "18748 Training Loss: tensor(0.0330)\n",
      "18749 Training Loss: tensor(0.0328)\n",
      "18750 Training Loss: tensor(0.0362)\n",
      "18751 Training Loss: tensor(0.0379)\n",
      "18752 Training Loss: tensor(0.0343)\n",
      "18753 Training Loss: tensor(0.0383)\n",
      "18754 Training Loss: tensor(0.0380)\n",
      "18755 Training Loss: tensor(0.0371)\n",
      "18756 Training Loss: tensor(0.0400)\n",
      "18757 Training Loss: tensor(0.0361)\n",
      "18758 Training Loss: tensor(0.0392)\n",
      "18759 Training Loss: tensor(0.0364)\n",
      "18760 Training Loss: tensor(0.0369)\n",
      "18761 Training Loss: tensor(0.0377)\n",
      "18762 Training Loss: tensor(0.0410)\n",
      "18763 Training Loss: tensor(0.0363)\n",
      "18764 Training Loss: tensor(0.0386)\n",
      "18765 Training Loss: tensor(0.0358)\n",
      "18766 Training Loss: tensor(0.0392)\n",
      "18767 Training Loss: tensor(0.0428)\n",
      "18768 Training Loss: tensor(0.0378)\n",
      "18769 Training Loss: tensor(0.0336)\n",
      "18770 Training Loss: tensor(0.0317)\n",
      "18771 Training Loss: tensor(0.0359)\n",
      "18772 Training Loss: tensor(0.0338)\n",
      "18773 Training Loss: tensor(0.0400)\n",
      "18774 Training Loss: tensor(0.0374)\n",
      "18775 Training Loss: tensor(0.0360)\n",
      "18776 Training Loss: tensor(0.0362)\n",
      "18777 Training Loss: tensor(0.0379)\n",
      "18778 Training Loss: tensor(0.0427)\n",
      "18779 Training Loss: tensor(0.0412)\n",
      "18780 Training Loss: tensor(0.0382)\n",
      "18781 Training Loss: tensor(0.0347)\n",
      "18782 Training Loss: tensor(0.0323)\n",
      "18783 Training Loss: tensor(0.0351)\n",
      "18784 Training Loss: tensor(0.0351)\n",
      "18785 Training Loss: tensor(0.0370)\n",
      "18786 Training Loss: tensor(0.0313)\n",
      "18787 Training Loss: tensor(0.0316)\n",
      "18788 Training Loss: tensor(0.0397)\n",
      "18789 Training Loss: tensor(0.0393)\n",
      "18790 Training Loss: tensor(0.0373)\n",
      "18791 Training Loss: tensor(0.0398)\n",
      "18792 Training Loss: tensor(0.0339)\n",
      "18793 Training Loss: tensor(0.0428)\n",
      "18794 Training Loss: tensor(0.0475)\n",
      "18795 Training Loss: tensor(0.0345)\n",
      "18796 Training Loss: tensor(0.0352)\n",
      "18797 Training Loss: tensor(0.0370)\n",
      "18798 Training Loss: tensor(0.0391)\n",
      "18799 Training Loss: tensor(0.0337)\n",
      "18800 Training Loss: tensor(0.0345)\n",
      "18801 Training Loss: tensor(0.0371)\n",
      "18802 Training Loss: tensor(0.0355)\n",
      "18803 Training Loss: tensor(0.0363)\n",
      "18804 Training Loss: tensor(0.0404)\n",
      "18805 Training Loss: tensor(0.0348)\n",
      "18806 Training Loss: tensor(0.0370)\n",
      "18807 Training Loss: tensor(0.0433)\n",
      "18808 Training Loss: tensor(0.0362)\n",
      "18809 Training Loss: tensor(0.0408)\n",
      "18810 Training Loss: tensor(0.0384)\n",
      "18811 Training Loss: tensor(0.0407)\n",
      "18812 Training Loss: tensor(0.0348)\n",
      "18813 Training Loss: tensor(0.0356)\n",
      "18814 Training Loss: tensor(0.0299)\n",
      "18815 Training Loss: tensor(0.0414)\n",
      "18816 Training Loss: tensor(0.0416)\n",
      "18817 Training Loss: tensor(0.0328)\n",
      "18818 Training Loss: tensor(0.0356)\n",
      "18819 Training Loss: tensor(0.0357)\n",
      "18820 Training Loss: tensor(0.0411)\n",
      "18821 Training Loss: tensor(0.0438)\n",
      "18822 Training Loss: tensor(0.0359)\n",
      "18823 Training Loss: tensor(0.0316)\n",
      "18824 Training Loss: tensor(0.0364)\n",
      "18825 Training Loss: tensor(0.0405)\n",
      "18826 Training Loss: tensor(0.0395)\n",
      "18827 Training Loss: tensor(0.0351)\n",
      "18828 Training Loss: tensor(0.0371)\n",
      "18829 Training Loss: tensor(0.0392)\n",
      "18830 Training Loss: tensor(0.0383)\n",
      "18831 Training Loss: tensor(0.0386)\n",
      "18832 Training Loss: tensor(0.0357)\n",
      "18833 Training Loss: tensor(0.0389)\n",
      "18834 Training Loss: tensor(0.0409)\n",
      "18835 Training Loss: tensor(0.0345)\n",
      "18836 Training Loss: tensor(0.0374)\n",
      "18837 Training Loss: tensor(0.0369)\n",
      "18838 Training Loss: tensor(0.0417)\n",
      "18839 Training Loss: tensor(0.0402)\n",
      "18840 Training Loss: tensor(0.0450)\n",
      "18841 Training Loss: tensor(0.0373)\n",
      "18842 Training Loss: tensor(0.0388)\n",
      "18843 Training Loss: tensor(0.0395)\n",
      "18844 Training Loss: tensor(0.0365)\n",
      "18845 Training Loss: tensor(0.0355)\n",
      "18846 Training Loss: tensor(0.0349)\n",
      "18847 Training Loss: tensor(0.0347)\n",
      "18848 Training Loss: tensor(0.0393)\n",
      "18849 Training Loss: tensor(0.0351)\n",
      "18850 Training Loss: tensor(0.0353)\n",
      "18851 Training Loss: tensor(0.0352)\n",
      "18852 Training Loss: tensor(0.0362)\n",
      "18853 Training Loss: tensor(0.0345)\n",
      "18854 Training Loss: tensor(0.0377)\n",
      "18855 Training Loss: tensor(0.0346)\n",
      "18856 Training Loss: tensor(0.0392)\n",
      "18857 Training Loss: tensor(0.0399)\n",
      "18858 Training Loss: tensor(0.0390)\n",
      "18859 Training Loss: tensor(0.0343)\n",
      "18860 Training Loss: tensor(0.0419)\n",
      "18861 Training Loss: tensor(0.0404)\n",
      "18862 Training Loss: tensor(0.0381)\n",
      "18863 Training Loss: tensor(0.0351)\n",
      "18864 Training Loss: tensor(0.0360)\n",
      "18865 Training Loss: tensor(0.0353)\n",
      "18866 Training Loss: tensor(0.0390)\n",
      "18867 Training Loss: tensor(0.0392)\n",
      "18868 Training Loss: tensor(0.0377)\n",
      "18869 Training Loss: tensor(0.0339)\n",
      "18870 Training Loss: tensor(0.0387)\n",
      "18871 Training Loss: tensor(0.0379)\n",
      "18872 Training Loss: tensor(0.0387)\n",
      "18873 Training Loss: tensor(0.0393)\n",
      "18874 Training Loss: tensor(0.0368)\n",
      "18875 Training Loss: tensor(0.0358)\n",
      "18876 Training Loss: tensor(0.0338)\n",
      "18877 Training Loss: tensor(0.0413)\n",
      "18878 Training Loss: tensor(0.0350)\n",
      "18879 Training Loss: tensor(0.0343)\n",
      "18880 Training Loss: tensor(0.0373)\n",
      "18881 Training Loss: tensor(0.0386)\n",
      "18882 Training Loss: tensor(0.0381)\n",
      "18883 Training Loss: tensor(0.0330)\n",
      "18884 Training Loss: tensor(0.0359)\n",
      "18885 Training Loss: tensor(0.0367)\n",
      "18886 Training Loss: tensor(0.0373)\n",
      "18887 Training Loss: tensor(0.0394)\n",
      "18888 Training Loss: tensor(0.0314)\n",
      "18889 Training Loss: tensor(0.0393)\n",
      "18890 Training Loss: tensor(0.0360)\n",
      "18891 Training Loss: tensor(0.0386)\n",
      "18892 Training Loss: tensor(0.0405)\n",
      "18893 Training Loss: tensor(0.0367)\n",
      "18894 Training Loss: tensor(0.0381)\n",
      "18895 Training Loss: tensor(0.0356)\n",
      "18896 Training Loss: tensor(0.0404)\n",
      "18897 Training Loss: tensor(0.0396)\n",
      "18898 Training Loss: tensor(0.0369)\n",
      "18899 Training Loss: tensor(0.0309)\n",
      "18900 Training Loss: tensor(0.0413)\n",
      "18901 Training Loss: tensor(0.0423)\n",
      "18902 Training Loss: tensor(0.0344)\n",
      "18903 Training Loss: tensor(0.0380)\n",
      "18904 Training Loss: tensor(0.0359)\n",
      "18905 Training Loss: tensor(0.0394)\n",
      "18906 Training Loss: tensor(0.0377)\n",
      "18907 Training Loss: tensor(0.0374)\n",
      "18908 Training Loss: tensor(0.0340)\n",
      "18909 Training Loss: tensor(0.0349)\n",
      "18910 Training Loss: tensor(0.0404)\n",
      "18911 Training Loss: tensor(0.0366)\n",
      "18912 Training Loss: tensor(0.0400)\n",
      "18913 Training Loss: tensor(0.0420)\n",
      "18914 Training Loss: tensor(0.0417)\n",
      "18915 Training Loss: tensor(0.0435)\n",
      "18916 Training Loss: tensor(0.0369)\n",
      "18917 Training Loss: tensor(0.0389)\n",
      "18918 Training Loss: tensor(0.0378)\n",
      "18919 Training Loss: tensor(0.0345)\n",
      "18920 Training Loss: tensor(0.0363)\n",
      "18921 Training Loss: tensor(0.0401)\n",
      "18922 Training Loss: tensor(0.0328)\n",
      "18923 Training Loss: tensor(0.0367)\n",
      "18924 Training Loss: tensor(0.0363)\n",
      "18925 Training Loss: tensor(0.0429)\n",
      "18926 Training Loss: tensor(0.0392)\n",
      "18927 Training Loss: tensor(0.0350)\n",
      "18928 Training Loss: tensor(0.0418)\n",
      "18929 Training Loss: tensor(0.0354)\n",
      "18930 Training Loss: tensor(0.0352)\n",
      "18931 Training Loss: tensor(0.0447)\n",
      "18932 Training Loss: tensor(0.0416)\n",
      "18933 Training Loss: tensor(0.0332)\n",
      "18934 Training Loss: tensor(0.0350)\n",
      "18935 Training Loss: tensor(0.0386)\n",
      "18936 Training Loss: tensor(0.0345)\n",
      "18937 Training Loss: tensor(0.0430)\n",
      "18938 Training Loss: tensor(0.0344)\n",
      "18939 Training Loss: tensor(0.0402)\n",
      "18940 Training Loss: tensor(0.0346)\n",
      "18941 Training Loss: tensor(0.0378)\n",
      "18942 Training Loss: tensor(0.0392)\n",
      "18943 Training Loss: tensor(0.0363)\n",
      "18944 Training Loss: tensor(0.0355)\n",
      "18945 Training Loss: tensor(0.0342)\n",
      "18946 Training Loss: tensor(0.0381)\n",
      "18947 Training Loss: tensor(0.0393)\n",
      "18948 Training Loss: tensor(0.0352)\n",
      "18949 Training Loss: tensor(0.0365)\n",
      "18950 Training Loss: tensor(0.0421)\n",
      "18951 Training Loss: tensor(0.0325)\n",
      "18952 Training Loss: tensor(0.0327)\n",
      "18953 Training Loss: tensor(0.0366)\n",
      "18954 Training Loss: tensor(0.0386)\n",
      "18955 Training Loss: tensor(0.0397)\n",
      "18956 Training Loss: tensor(0.0387)\n",
      "18957 Training Loss: tensor(0.0358)\n",
      "18958 Training Loss: tensor(0.0413)\n",
      "18959 Training Loss: tensor(0.0399)\n",
      "18960 Training Loss: tensor(0.0322)\n",
      "18961 Training Loss: tensor(0.0364)\n",
      "18962 Training Loss: tensor(0.0359)\n",
      "18963 Training Loss: tensor(0.0368)\n",
      "18964 Training Loss: tensor(0.0375)\n",
      "18965 Training Loss: tensor(0.0361)\n",
      "18966 Training Loss: tensor(0.0368)\n",
      "18967 Training Loss: tensor(0.0342)\n",
      "18968 Training Loss: tensor(0.0361)\n",
      "18969 Training Loss: tensor(0.0390)\n",
      "18970 Training Loss: tensor(0.0403)\n",
      "18971 Training Loss: tensor(0.0419)\n",
      "18972 Training Loss: tensor(0.0355)\n",
      "18973 Training Loss: tensor(0.0436)\n",
      "18974 Training Loss: tensor(0.0383)\n",
      "18975 Training Loss: tensor(0.0373)\n",
      "18976 Training Loss: tensor(0.0394)\n",
      "18977 Training Loss: tensor(0.0410)\n",
      "18978 Training Loss: tensor(0.0361)\n",
      "18979 Training Loss: tensor(0.0384)\n",
      "18980 Training Loss: tensor(0.0373)\n",
      "18981 Training Loss: tensor(0.0342)\n",
      "18982 Training Loss: tensor(0.0391)\n",
      "18983 Training Loss: tensor(0.0383)\n",
      "18984 Training Loss: tensor(0.0368)\n",
      "18985 Training Loss: tensor(0.0374)\n",
      "18986 Training Loss: tensor(0.0335)\n",
      "18987 Training Loss: tensor(0.0380)\n",
      "18988 Training Loss: tensor(0.0378)\n",
      "18989 Training Loss: tensor(0.0369)\n",
      "18990 Training Loss: tensor(0.0363)\n",
      "18991 Training Loss: tensor(0.0427)\n",
      "18992 Training Loss: tensor(0.0362)\n",
      "18993 Training Loss: tensor(0.0353)\n",
      "18994 Training Loss: tensor(0.0402)\n",
      "18995 Training Loss: tensor(0.0332)\n",
      "18996 Training Loss: tensor(0.0388)\n",
      "18997 Training Loss: tensor(0.0361)\n",
      "18998 Training Loss: tensor(0.0335)\n",
      "18999 Training Loss: tensor(0.0377)\n",
      "19000 Training Loss: tensor(0.0439)\n",
      "19001 Training Loss: tensor(0.0353)\n",
      "19002 Training Loss: tensor(0.0411)\n",
      "19003 Training Loss: tensor(0.0369)\n",
      "19004 Training Loss: tensor(0.0386)\n",
      "19005 Training Loss: tensor(0.0367)\n",
      "19006 Training Loss: tensor(0.0416)\n",
      "19007 Training Loss: tensor(0.0367)\n",
      "19008 Training Loss: tensor(0.0368)\n",
      "19009 Training Loss: tensor(0.0371)\n",
      "19010 Training Loss: tensor(0.0424)\n",
      "19011 Training Loss: tensor(0.0393)\n",
      "19012 Training Loss: tensor(0.0387)\n",
      "19013 Training Loss: tensor(0.0330)\n",
      "19014 Training Loss: tensor(0.0360)\n",
      "19015 Training Loss: tensor(0.0356)\n",
      "19016 Training Loss: tensor(0.0437)\n",
      "19017 Training Loss: tensor(0.0417)\n",
      "19018 Training Loss: tensor(0.0376)\n",
      "19019 Training Loss: tensor(0.0408)\n",
      "19020 Training Loss: tensor(0.0371)\n",
      "19021 Training Loss: tensor(0.0410)\n",
      "19022 Training Loss: tensor(0.0373)\n",
      "19023 Training Loss: tensor(0.0384)\n",
      "19024 Training Loss: tensor(0.0353)\n",
      "19025 Training Loss: tensor(0.0341)\n",
      "19026 Training Loss: tensor(0.0357)\n",
      "19027 Training Loss: tensor(0.0387)\n",
      "19028 Training Loss: tensor(0.0424)\n",
      "19029 Training Loss: tensor(0.0436)\n",
      "19030 Training Loss: tensor(0.0335)\n",
      "19031 Training Loss: tensor(0.0428)\n",
      "19032 Training Loss: tensor(0.0399)\n",
      "19033 Training Loss: tensor(0.0373)\n",
      "19034 Training Loss: tensor(0.0389)\n",
      "19035 Training Loss: tensor(0.0394)\n",
      "19036 Training Loss: tensor(0.0314)\n",
      "19037 Training Loss: tensor(0.0381)\n",
      "19038 Training Loss: tensor(0.0369)\n",
      "19039 Training Loss: tensor(0.0363)\n",
      "19040 Training Loss: tensor(0.0461)\n",
      "19041 Training Loss: tensor(0.0376)\n",
      "19042 Training Loss: tensor(0.0361)\n",
      "19043 Training Loss: tensor(0.0360)\n",
      "19044 Training Loss: tensor(0.0350)\n",
      "19045 Training Loss: tensor(0.0387)\n",
      "19046 Training Loss: tensor(0.0398)\n",
      "19047 Training Loss: tensor(0.0406)\n",
      "19048 Training Loss: tensor(0.0425)\n",
      "19049 Training Loss: tensor(0.0357)\n",
      "19050 Training Loss: tensor(0.0391)\n",
      "19051 Training Loss: tensor(0.0386)\n",
      "19052 Training Loss: tensor(0.0385)\n",
      "19053 Training Loss: tensor(0.0371)\n",
      "19054 Training Loss: tensor(0.0392)\n",
      "19055 Training Loss: tensor(0.0379)\n",
      "19056 Training Loss: tensor(0.0359)\n",
      "19057 Training Loss: tensor(0.0422)\n",
      "19058 Training Loss: tensor(0.0327)\n",
      "19059 Training Loss: tensor(0.0360)\n",
      "19060 Training Loss: tensor(0.0408)\n",
      "19061 Training Loss: tensor(0.0358)\n",
      "19062 Training Loss: tensor(0.0368)\n",
      "19063 Training Loss: tensor(0.0380)\n",
      "19064 Training Loss: tensor(0.0353)\n",
      "19065 Training Loss: tensor(0.0370)\n",
      "19066 Training Loss: tensor(0.0356)\n",
      "19067 Training Loss: tensor(0.0353)\n",
      "19068 Training Loss: tensor(0.0392)\n",
      "19069 Training Loss: tensor(0.0400)\n",
      "19070 Training Loss: tensor(0.0431)\n",
      "19071 Training Loss: tensor(0.0330)\n",
      "19072 Training Loss: tensor(0.0365)\n",
      "19073 Training Loss: tensor(0.0366)\n",
      "19074 Training Loss: tensor(0.0368)\n",
      "19075 Training Loss: tensor(0.0366)\n",
      "19076 Training Loss: tensor(0.0381)\n",
      "19077 Training Loss: tensor(0.0392)\n",
      "19078 Training Loss: tensor(0.0454)\n",
      "19079 Training Loss: tensor(0.0326)\n",
      "19080 Training Loss: tensor(0.0381)\n",
      "19081 Training Loss: tensor(0.0334)\n",
      "19082 Training Loss: tensor(0.0370)\n",
      "19083 Training Loss: tensor(0.0343)\n",
      "19084 Training Loss: tensor(0.0387)\n",
      "19085 Training Loss: tensor(0.0443)\n",
      "19086 Training Loss: tensor(0.0361)\n",
      "19087 Training Loss: tensor(0.0346)\n",
      "19088 Training Loss: tensor(0.0385)\n",
      "19089 Training Loss: tensor(0.0413)\n",
      "19090 Training Loss: tensor(0.0371)\n",
      "19091 Training Loss: tensor(0.0380)\n",
      "19092 Training Loss: tensor(0.0349)\n",
      "19093 Training Loss: tensor(0.0411)\n",
      "19094 Training Loss: tensor(0.0329)\n",
      "19095 Training Loss: tensor(0.0389)\n",
      "19096 Training Loss: tensor(0.0388)\n",
      "19097 Training Loss: tensor(0.0357)\n",
      "19098 Training Loss: tensor(0.0364)\n",
      "19099 Training Loss: tensor(0.0357)\n",
      "19100 Training Loss: tensor(0.0377)\n",
      "19101 Training Loss: tensor(0.0384)\n",
      "19102 Training Loss: tensor(0.0384)\n",
      "19103 Training Loss: tensor(0.0378)\n",
      "19104 Training Loss: tensor(0.0369)\n",
      "19105 Training Loss: tensor(0.0397)\n",
      "19106 Training Loss: tensor(0.0350)\n",
      "19107 Training Loss: tensor(0.0433)\n",
      "19108 Training Loss: tensor(0.0388)\n",
      "19109 Training Loss: tensor(0.0391)\n",
      "19110 Training Loss: tensor(0.0348)\n",
      "19111 Training Loss: tensor(0.0367)\n",
      "19112 Training Loss: tensor(0.0341)\n",
      "19113 Training Loss: tensor(0.0384)\n",
      "19114 Training Loss: tensor(0.0380)\n",
      "19115 Training Loss: tensor(0.0362)\n",
      "19116 Training Loss: tensor(0.0369)\n",
      "19117 Training Loss: tensor(0.0340)\n",
      "19118 Training Loss: tensor(0.0370)\n",
      "19119 Training Loss: tensor(0.0389)\n",
      "19120 Training Loss: tensor(0.0386)\n",
      "19121 Training Loss: tensor(0.0370)\n",
      "19122 Training Loss: tensor(0.0385)\n",
      "19123 Training Loss: tensor(0.0367)\n",
      "19124 Training Loss: tensor(0.0395)\n",
      "19125 Training Loss: tensor(0.0365)\n",
      "19126 Training Loss: tensor(0.0385)\n",
      "19127 Training Loss: tensor(0.0322)\n",
      "19128 Training Loss: tensor(0.0347)\n",
      "19129 Training Loss: tensor(0.0365)\n",
      "19130 Training Loss: tensor(0.0398)\n",
      "19131 Training Loss: tensor(0.0453)\n",
      "19132 Training Loss: tensor(0.0378)\n",
      "19133 Training Loss: tensor(0.0373)\n",
      "19134 Training Loss: tensor(0.0361)\n",
      "19135 Training Loss: tensor(0.0366)\n",
      "19136 Training Loss: tensor(0.0377)\n",
      "19137 Training Loss: tensor(0.0364)\n",
      "19138 Training Loss: tensor(0.0381)\n",
      "19139 Training Loss: tensor(0.0402)\n",
      "19140 Training Loss: tensor(0.0392)\n",
      "19141 Training Loss: tensor(0.0382)\n",
      "19142 Training Loss: tensor(0.0373)\n",
      "19143 Training Loss: tensor(0.0350)\n",
      "19144 Training Loss: tensor(0.0409)\n",
      "19145 Training Loss: tensor(0.0326)\n",
      "19146 Training Loss: tensor(0.0342)\n",
      "19147 Training Loss: tensor(0.0365)\n",
      "19148 Training Loss: tensor(0.0327)\n",
      "19149 Training Loss: tensor(0.0406)\n",
      "19150 Training Loss: tensor(0.0392)\n",
      "19151 Training Loss: tensor(0.0351)\n",
      "19152 Training Loss: tensor(0.0379)\n",
      "19153 Training Loss: tensor(0.0384)\n",
      "19154 Training Loss: tensor(0.0347)\n",
      "19155 Training Loss: tensor(0.0405)\n",
      "19156 Training Loss: tensor(0.0452)\n",
      "19157 Training Loss: tensor(0.0344)\n",
      "19158 Training Loss: tensor(0.0391)\n",
      "19159 Training Loss: tensor(0.0357)\n",
      "19160 Training Loss: tensor(0.0375)\n",
      "19161 Training Loss: tensor(0.0379)\n",
      "19162 Training Loss: tensor(0.0368)\n",
      "19163 Training Loss: tensor(0.0353)\n",
      "19164 Training Loss: tensor(0.0331)\n",
      "19165 Training Loss: tensor(0.0385)\n",
      "19166 Training Loss: tensor(0.0369)\n",
      "19167 Training Loss: tensor(0.0448)\n",
      "19168 Training Loss: tensor(0.0382)\n",
      "19169 Training Loss: tensor(0.0304)\n",
      "19170 Training Loss: tensor(0.0348)\n",
      "19171 Training Loss: tensor(0.0341)\n",
      "19172 Training Loss: tensor(0.0351)\n",
      "19173 Training Loss: tensor(0.0410)\n",
      "19174 Training Loss: tensor(0.0381)\n",
      "19175 Training Loss: tensor(0.0367)\n",
      "19176 Training Loss: tensor(0.0362)\n",
      "19177 Training Loss: tensor(0.0334)\n",
      "19178 Training Loss: tensor(0.0332)\n",
      "19179 Training Loss: tensor(0.0437)\n",
      "19180 Training Loss: tensor(0.0357)\n",
      "19181 Training Loss: tensor(0.0407)\n",
      "19182 Training Loss: tensor(0.0403)\n",
      "19183 Training Loss: tensor(0.0372)\n",
      "19184 Training Loss: tensor(0.0362)\n",
      "19185 Training Loss: tensor(0.0356)\n",
      "19186 Training Loss: tensor(0.0346)\n",
      "19187 Training Loss: tensor(0.0356)\n",
      "19188 Training Loss: tensor(0.0383)\n",
      "19189 Training Loss: tensor(0.0378)\n",
      "19190 Training Loss: tensor(0.0376)\n",
      "19191 Training Loss: tensor(0.0348)\n",
      "19192 Training Loss: tensor(0.0368)\n",
      "19193 Training Loss: tensor(0.0374)\n",
      "19194 Training Loss: tensor(0.0353)\n",
      "19195 Training Loss: tensor(0.0410)\n",
      "19196 Training Loss: tensor(0.0342)\n",
      "19197 Training Loss: tensor(0.0386)\n",
      "19198 Training Loss: tensor(0.0384)\n",
      "19199 Training Loss: tensor(0.0362)\n",
      "19200 Training Loss: tensor(0.0372)\n",
      "19201 Training Loss: tensor(0.0441)\n",
      "19202 Training Loss: tensor(0.0387)\n",
      "19203 Training Loss: tensor(0.0375)\n",
      "19204 Training Loss: tensor(0.0406)\n",
      "19205 Training Loss: tensor(0.0406)\n",
      "19206 Training Loss: tensor(0.0347)\n",
      "19207 Training Loss: tensor(0.0404)\n",
      "19208 Training Loss: tensor(0.0410)\n",
      "19209 Training Loss: tensor(0.0349)\n",
      "19210 Training Loss: tensor(0.0374)\n",
      "19211 Training Loss: tensor(0.0363)\n",
      "19212 Training Loss: tensor(0.0326)\n",
      "19213 Training Loss: tensor(0.0398)\n",
      "19214 Training Loss: tensor(0.0330)\n",
      "19215 Training Loss: tensor(0.0384)\n",
      "19216 Training Loss: tensor(0.0353)\n",
      "19217 Training Loss: tensor(0.0356)\n",
      "19218 Training Loss: tensor(0.0350)\n",
      "19219 Training Loss: tensor(0.0393)\n",
      "19220 Training Loss: tensor(0.0435)\n",
      "19221 Training Loss: tensor(0.0372)\n",
      "19222 Training Loss: tensor(0.0370)\n",
      "19223 Training Loss: tensor(0.0401)\n",
      "19224 Training Loss: tensor(0.0383)\n",
      "19225 Training Loss: tensor(0.0407)\n",
      "19226 Training Loss: tensor(0.0423)\n",
      "19227 Training Loss: tensor(0.0363)\n",
      "19228 Training Loss: tensor(0.0344)\n",
      "19229 Training Loss: tensor(0.0409)\n",
      "19230 Training Loss: tensor(0.0392)\n",
      "19231 Training Loss: tensor(0.0395)\n",
      "19232 Training Loss: tensor(0.0438)\n",
      "19233 Training Loss: tensor(0.0387)\n",
      "19234 Training Loss: tensor(0.0410)\n",
      "19235 Training Loss: tensor(0.0431)\n",
      "19236 Training Loss: tensor(0.0356)\n",
      "19237 Training Loss: tensor(0.0317)\n",
      "19238 Training Loss: tensor(0.0404)\n",
      "19239 Training Loss: tensor(0.0373)\n",
      "19240 Training Loss: tensor(0.0377)\n",
      "19241 Training Loss: tensor(0.0365)\n",
      "19242 Training Loss: tensor(0.0368)\n",
      "19243 Training Loss: tensor(0.0386)\n",
      "19244 Training Loss: tensor(0.0375)\n",
      "19245 Training Loss: tensor(0.0377)\n",
      "19246 Training Loss: tensor(0.0385)\n",
      "19247 Training Loss: tensor(0.0421)\n",
      "19248 Training Loss: tensor(0.0382)\n",
      "19249 Training Loss: tensor(0.0370)\n",
      "19250 Training Loss: tensor(0.0376)\n",
      "19251 Training Loss: tensor(0.0313)\n",
      "19252 Training Loss: tensor(0.0357)\n",
      "19253 Training Loss: tensor(0.0333)\n",
      "19254 Training Loss: tensor(0.0375)\n",
      "19255 Training Loss: tensor(0.0358)\n",
      "19256 Training Loss: tensor(0.0335)\n",
      "19257 Training Loss: tensor(0.0348)\n",
      "19258 Training Loss: tensor(0.0373)\n",
      "19259 Training Loss: tensor(0.0350)\n",
      "19260 Training Loss: tensor(0.0394)\n",
      "19261 Training Loss: tensor(0.0332)\n",
      "19262 Training Loss: tensor(0.0401)\n",
      "19263 Training Loss: tensor(0.0436)\n",
      "19264 Training Loss: tensor(0.0351)\n",
      "19265 Training Loss: tensor(0.0387)\n",
      "19266 Training Loss: tensor(0.0373)\n",
      "19267 Training Loss: tensor(0.0399)\n",
      "19268 Training Loss: tensor(0.0369)\n",
      "19269 Training Loss: tensor(0.0412)\n",
      "19270 Training Loss: tensor(0.0385)\n",
      "19271 Training Loss: tensor(0.0373)\n",
      "19272 Training Loss: tensor(0.0369)\n",
      "19273 Training Loss: tensor(0.0328)\n",
      "19274 Training Loss: tensor(0.0370)\n",
      "19275 Training Loss: tensor(0.0404)\n",
      "19276 Training Loss: tensor(0.0378)\n",
      "19277 Training Loss: tensor(0.0379)\n",
      "19278 Training Loss: tensor(0.0334)\n",
      "19279 Training Loss: tensor(0.0402)\n",
      "19280 Training Loss: tensor(0.0425)\n",
      "19281 Training Loss: tensor(0.0353)\n",
      "19282 Training Loss: tensor(0.0382)\n",
      "19283 Training Loss: tensor(0.0329)\n",
      "19284 Training Loss: tensor(0.0387)\n",
      "19285 Training Loss: tensor(0.0349)\n",
      "19286 Training Loss: tensor(0.0376)\n",
      "19287 Training Loss: tensor(0.0421)\n",
      "19288 Training Loss: tensor(0.0383)\n",
      "19289 Training Loss: tensor(0.0397)\n",
      "19290 Training Loss: tensor(0.0383)\n",
      "19291 Training Loss: tensor(0.0396)\n",
      "19292 Training Loss: tensor(0.0404)\n",
      "19293 Training Loss: tensor(0.0365)\n",
      "19294 Training Loss: tensor(0.0408)\n",
      "19295 Training Loss: tensor(0.0384)\n",
      "19296 Training Loss: tensor(0.0343)\n",
      "19297 Training Loss: tensor(0.0392)\n",
      "19298 Training Loss: tensor(0.0361)\n",
      "19299 Training Loss: tensor(0.0383)\n",
      "19300 Training Loss: tensor(0.0378)\n",
      "19301 Training Loss: tensor(0.0379)\n",
      "19302 Training Loss: tensor(0.0366)\n",
      "19303 Training Loss: tensor(0.0374)\n",
      "19304 Training Loss: tensor(0.0362)\n",
      "19305 Training Loss: tensor(0.0368)\n",
      "19306 Training Loss: tensor(0.0320)\n",
      "19307 Training Loss: tensor(0.0349)\n",
      "19308 Training Loss: tensor(0.0368)\n",
      "19309 Training Loss: tensor(0.0389)\n",
      "19310 Training Loss: tensor(0.0348)\n",
      "19311 Training Loss: tensor(0.0376)\n",
      "19312 Training Loss: tensor(0.0355)\n",
      "19313 Training Loss: tensor(0.0367)\n",
      "19314 Training Loss: tensor(0.0348)\n",
      "19315 Training Loss: tensor(0.0435)\n",
      "19316 Training Loss: tensor(0.0372)\n",
      "19317 Training Loss: tensor(0.0342)\n",
      "19318 Training Loss: tensor(0.0385)\n",
      "19319 Training Loss: tensor(0.0380)\n",
      "19320 Training Loss: tensor(0.0369)\n",
      "19321 Training Loss: tensor(0.0372)\n",
      "19322 Training Loss: tensor(0.0342)\n",
      "19323 Training Loss: tensor(0.0369)\n",
      "19324 Training Loss: tensor(0.0441)\n",
      "19325 Training Loss: tensor(0.0317)\n",
      "19326 Training Loss: tensor(0.0386)\n",
      "19327 Training Loss: tensor(0.0392)\n",
      "19328 Training Loss: tensor(0.0390)\n",
      "19329 Training Loss: tensor(0.0306)\n",
      "19330 Training Loss: tensor(0.0411)\n",
      "19331 Training Loss: tensor(0.0391)\n",
      "19332 Training Loss: tensor(0.0329)\n",
      "19333 Training Loss: tensor(0.0367)\n",
      "19334 Training Loss: tensor(0.0412)\n",
      "19335 Training Loss: tensor(0.0384)\n",
      "19336 Training Loss: tensor(0.0378)\n",
      "19337 Training Loss: tensor(0.0391)\n",
      "19338 Training Loss: tensor(0.0372)\n",
      "19339 Training Loss: tensor(0.0318)\n",
      "19340 Training Loss: tensor(0.0360)\n",
      "19341 Training Loss: tensor(0.0429)\n",
      "19342 Training Loss: tensor(0.0327)\n",
      "19343 Training Loss: tensor(0.0384)\n",
      "19344 Training Loss: tensor(0.0349)\n",
      "19345 Training Loss: tensor(0.0339)\n",
      "19346 Training Loss: tensor(0.0394)\n",
      "19347 Training Loss: tensor(0.0385)\n",
      "19348 Training Loss: tensor(0.0361)\n",
      "19349 Training Loss: tensor(0.0404)\n",
      "19350 Training Loss: tensor(0.0379)\n",
      "19351 Training Loss: tensor(0.0395)\n",
      "19352 Training Loss: tensor(0.0385)\n",
      "19353 Training Loss: tensor(0.0314)\n",
      "19354 Training Loss: tensor(0.0376)\n",
      "19355 Training Loss: tensor(0.0369)\n",
      "19356 Training Loss: tensor(0.0382)\n",
      "19357 Training Loss: tensor(0.0340)\n",
      "19358 Training Loss: tensor(0.0362)\n",
      "19359 Training Loss: tensor(0.0360)\n",
      "19360 Training Loss: tensor(0.0360)\n",
      "19361 Training Loss: tensor(0.0346)\n",
      "19362 Training Loss: tensor(0.0367)\n",
      "19363 Training Loss: tensor(0.0366)\n",
      "19364 Training Loss: tensor(0.0339)\n",
      "19365 Training Loss: tensor(0.0349)\n",
      "19366 Training Loss: tensor(0.0398)\n",
      "19367 Training Loss: tensor(0.0371)\n",
      "19368 Training Loss: tensor(0.0329)\n",
      "19369 Training Loss: tensor(0.0415)\n",
      "19370 Training Loss: tensor(0.0349)\n",
      "19371 Training Loss: tensor(0.0370)\n",
      "19372 Training Loss: tensor(0.0328)\n",
      "19373 Training Loss: tensor(0.0399)\n",
      "19374 Training Loss: tensor(0.0396)\n",
      "19375 Training Loss: tensor(0.0381)\n",
      "19376 Training Loss: tensor(0.0375)\n",
      "19377 Training Loss: tensor(0.0337)\n",
      "19378 Training Loss: tensor(0.0344)\n",
      "19379 Training Loss: tensor(0.0376)\n",
      "19380 Training Loss: tensor(0.0387)\n",
      "19381 Training Loss: tensor(0.0374)\n",
      "19382 Training Loss: tensor(0.0439)\n",
      "19383 Training Loss: tensor(0.0408)\n",
      "19384 Training Loss: tensor(0.0371)\n",
      "19385 Training Loss: tensor(0.0430)\n",
      "19386 Training Loss: tensor(0.0365)\n",
      "19387 Training Loss: tensor(0.0336)\n",
      "19388 Training Loss: tensor(0.0354)\n",
      "19389 Training Loss: tensor(0.0338)\n",
      "19390 Training Loss: tensor(0.0361)\n",
      "19391 Training Loss: tensor(0.0366)\n",
      "19392 Training Loss: tensor(0.0379)\n",
      "19393 Training Loss: tensor(0.0358)\n",
      "19394 Training Loss: tensor(0.0366)\n",
      "19395 Training Loss: tensor(0.0349)\n",
      "19396 Training Loss: tensor(0.0347)\n",
      "19397 Training Loss: tensor(0.0310)\n",
      "19398 Training Loss: tensor(0.0409)\n",
      "19399 Training Loss: tensor(0.0368)\n",
      "19400 Training Loss: tensor(0.0367)\n",
      "19401 Training Loss: tensor(0.0302)\n",
      "19402 Training Loss: tensor(0.0324)\n",
      "19403 Training Loss: tensor(0.0384)\n",
      "19404 Training Loss: tensor(0.0364)\n",
      "19405 Training Loss: tensor(0.0451)\n",
      "19406 Training Loss: tensor(0.0334)\n",
      "19407 Training Loss: tensor(0.0324)\n",
      "19408 Training Loss: tensor(0.0313)\n",
      "19409 Training Loss: tensor(0.0399)\n",
      "19410 Training Loss: tensor(0.0363)\n",
      "19411 Training Loss: tensor(0.0401)\n",
      "19412 Training Loss: tensor(0.0380)\n",
      "19413 Training Loss: tensor(0.0372)\n",
      "19414 Training Loss: tensor(0.0338)\n",
      "19415 Training Loss: tensor(0.0380)\n",
      "19416 Training Loss: tensor(0.0358)\n",
      "19417 Training Loss: tensor(0.0362)\n",
      "19418 Training Loss: tensor(0.0373)\n",
      "19419 Training Loss: tensor(0.0393)\n",
      "19420 Training Loss: tensor(0.0335)\n",
      "19421 Training Loss: tensor(0.0404)\n",
      "19422 Training Loss: tensor(0.0371)\n",
      "19423 Training Loss: tensor(0.0394)\n",
      "19424 Training Loss: tensor(0.0357)\n",
      "19425 Training Loss: tensor(0.0382)\n",
      "19426 Training Loss: tensor(0.0317)\n",
      "19427 Training Loss: tensor(0.0461)\n",
      "19428 Training Loss: tensor(0.0333)\n",
      "19429 Training Loss: tensor(0.0377)\n",
      "19430 Training Loss: tensor(0.0384)\n",
      "19431 Training Loss: tensor(0.0320)\n",
      "19432 Training Loss: tensor(0.0372)\n",
      "19433 Training Loss: tensor(0.0352)\n",
      "19434 Training Loss: tensor(0.0345)\n",
      "19435 Training Loss: tensor(0.0391)\n",
      "19436 Training Loss: tensor(0.0381)\n",
      "19437 Training Loss: tensor(0.0339)\n",
      "19438 Training Loss: tensor(0.0403)\n",
      "19439 Training Loss: tensor(0.0327)\n",
      "19440 Training Loss: tensor(0.0384)\n",
      "19441 Training Loss: tensor(0.0374)\n",
      "19442 Training Loss: tensor(0.0344)\n",
      "19443 Training Loss: tensor(0.0357)\n",
      "19444 Training Loss: tensor(0.0385)\n",
      "19445 Training Loss: tensor(0.0389)\n",
      "19446 Training Loss: tensor(0.0304)\n",
      "19447 Training Loss: tensor(0.0379)\n",
      "19448 Training Loss: tensor(0.0358)\n",
      "19449 Training Loss: tensor(0.0357)\n",
      "19450 Training Loss: tensor(0.0381)\n",
      "19451 Training Loss: tensor(0.0421)\n",
      "19452 Training Loss: tensor(0.0403)\n",
      "19453 Training Loss: tensor(0.0341)\n",
      "19454 Training Loss: tensor(0.0412)\n",
      "19455 Training Loss: tensor(0.0406)\n",
      "19456 Training Loss: tensor(0.0358)\n",
      "19457 Training Loss: tensor(0.0419)\n",
      "19458 Training Loss: tensor(0.0329)\n",
      "19459 Training Loss: tensor(0.0395)\n",
      "19460 Training Loss: tensor(0.0369)\n",
      "19461 Training Loss: tensor(0.0351)\n",
      "19462 Training Loss: tensor(0.0379)\n",
      "19463 Training Loss: tensor(0.0349)\n",
      "19464 Training Loss: tensor(0.0428)\n",
      "19465 Training Loss: tensor(0.0350)\n",
      "19466 Training Loss: tensor(0.0329)\n",
      "19467 Training Loss: tensor(0.0377)\n",
      "19468 Training Loss: tensor(0.0326)\n",
      "19469 Training Loss: tensor(0.0380)\n",
      "19470 Training Loss: tensor(0.0428)\n",
      "19471 Training Loss: tensor(0.0339)\n",
      "19472 Training Loss: tensor(0.0367)\n",
      "19473 Training Loss: tensor(0.0382)\n",
      "19474 Training Loss: tensor(0.0419)\n",
      "19475 Training Loss: tensor(0.0383)\n",
      "19476 Training Loss: tensor(0.0400)\n",
      "19477 Training Loss: tensor(0.0417)\n",
      "19478 Training Loss: tensor(0.0341)\n",
      "19479 Training Loss: tensor(0.0396)\n",
      "19480 Training Loss: tensor(0.0362)\n",
      "19481 Training Loss: tensor(0.0351)\n",
      "19482 Training Loss: tensor(0.0400)\n",
      "19483 Training Loss: tensor(0.0353)\n",
      "19484 Training Loss: tensor(0.0376)\n",
      "19485 Training Loss: tensor(0.0374)\n",
      "19486 Training Loss: tensor(0.0373)\n",
      "19487 Training Loss: tensor(0.0419)\n",
      "19488 Training Loss: tensor(0.0338)\n",
      "19489 Training Loss: tensor(0.0448)\n",
      "19490 Training Loss: tensor(0.0385)\n",
      "19491 Training Loss: tensor(0.0352)\n",
      "19492 Training Loss: tensor(0.0406)\n",
      "19493 Training Loss: tensor(0.0354)\n",
      "19494 Training Loss: tensor(0.0375)\n",
      "19495 Training Loss: tensor(0.0355)\n",
      "19496 Training Loss: tensor(0.0328)\n",
      "19497 Training Loss: tensor(0.0386)\n",
      "19498 Training Loss: tensor(0.0315)\n",
      "19499 Training Loss: tensor(0.0289)\n",
      "19500 Training Loss: tensor(0.0431)\n",
      "19501 Training Loss: tensor(0.0403)\n",
      "19502 Training Loss: tensor(0.0365)\n",
      "19503 Training Loss: tensor(0.0424)\n",
      "19504 Training Loss: tensor(0.0333)\n",
      "19505 Training Loss: tensor(0.0425)\n",
      "19506 Training Loss: tensor(0.0362)\n",
      "19507 Training Loss: tensor(0.0397)\n",
      "19508 Training Loss: tensor(0.0357)\n",
      "19509 Training Loss: tensor(0.0385)\n",
      "19510 Training Loss: tensor(0.0348)\n",
      "19511 Training Loss: tensor(0.0397)\n",
      "19512 Training Loss: tensor(0.0468)\n",
      "19513 Training Loss: tensor(0.0322)\n",
      "19514 Training Loss: tensor(0.0355)\n",
      "19515 Training Loss: tensor(0.0386)\n",
      "19516 Training Loss: tensor(0.0401)\n",
      "19517 Training Loss: tensor(0.0366)\n",
      "19518 Training Loss: tensor(0.0417)\n",
      "19519 Training Loss: tensor(0.0325)\n",
      "19520 Training Loss: tensor(0.0434)\n",
      "19521 Training Loss: tensor(0.0388)\n",
      "19522 Training Loss: tensor(0.0430)\n",
      "19523 Training Loss: tensor(0.0362)\n",
      "19524 Training Loss: tensor(0.0395)\n",
      "19525 Training Loss: tensor(0.0397)\n",
      "19526 Training Loss: tensor(0.0363)\n",
      "19527 Training Loss: tensor(0.0426)\n",
      "19528 Training Loss: tensor(0.0354)\n",
      "19529 Training Loss: tensor(0.0390)\n",
      "19530 Training Loss: tensor(0.0419)\n",
      "19531 Training Loss: tensor(0.0333)\n",
      "19532 Training Loss: tensor(0.0394)\n",
      "19533 Training Loss: tensor(0.0371)\n",
      "19534 Training Loss: tensor(0.0363)\n",
      "19535 Training Loss: tensor(0.0399)\n",
      "19536 Training Loss: tensor(0.0360)\n",
      "19537 Training Loss: tensor(0.0399)\n",
      "19538 Training Loss: tensor(0.0402)\n",
      "19539 Training Loss: tensor(0.0353)\n",
      "19540 Training Loss: tensor(0.0358)\n",
      "19541 Training Loss: tensor(0.0344)\n",
      "19542 Training Loss: tensor(0.0367)\n",
      "19543 Training Loss: tensor(0.0380)\n",
      "19544 Training Loss: tensor(0.0347)\n",
      "19545 Training Loss: tensor(0.0357)\n",
      "19546 Training Loss: tensor(0.0378)\n",
      "19547 Training Loss: tensor(0.0328)\n",
      "19548 Training Loss: tensor(0.0354)\n",
      "19549 Training Loss: tensor(0.0362)\n",
      "19550 Training Loss: tensor(0.0344)\n",
      "19551 Training Loss: tensor(0.0370)\n",
      "19552 Training Loss: tensor(0.0348)\n",
      "19553 Training Loss: tensor(0.0367)\n",
      "19554 Training Loss: tensor(0.0363)\n",
      "19555 Training Loss: tensor(0.0357)\n",
      "19556 Training Loss: tensor(0.0417)\n",
      "19557 Training Loss: tensor(0.0398)\n",
      "19558 Training Loss: tensor(0.0414)\n",
      "19559 Training Loss: tensor(0.0399)\n",
      "19560 Training Loss: tensor(0.0356)\n",
      "19561 Training Loss: tensor(0.0440)\n",
      "19562 Training Loss: tensor(0.0382)\n",
      "19563 Training Loss: tensor(0.0352)\n",
      "19564 Training Loss: tensor(0.0422)\n",
      "19565 Training Loss: tensor(0.0391)\n",
      "19566 Training Loss: tensor(0.0390)\n",
      "19567 Training Loss: tensor(0.0408)\n",
      "19568 Training Loss: tensor(0.0351)\n",
      "19569 Training Loss: tensor(0.0337)\n",
      "19570 Training Loss: tensor(0.0415)\n",
      "19571 Training Loss: tensor(0.0364)\n",
      "19572 Training Loss: tensor(0.0453)\n",
      "19573 Training Loss: tensor(0.0406)\n",
      "19574 Training Loss: tensor(0.0393)\n",
      "19575 Training Loss: tensor(0.0380)\n",
      "19576 Training Loss: tensor(0.0341)\n",
      "19577 Training Loss: tensor(0.0392)\n",
      "19578 Training Loss: tensor(0.0388)\n",
      "19579 Training Loss: tensor(0.0337)\n",
      "19580 Training Loss: tensor(0.0378)\n",
      "19581 Training Loss: tensor(0.0323)\n",
      "19582 Training Loss: tensor(0.0370)\n",
      "19583 Training Loss: tensor(0.0388)\n",
      "19584 Training Loss: tensor(0.0364)\n",
      "19585 Training Loss: tensor(0.0351)\n",
      "19586 Training Loss: tensor(0.0380)\n",
      "19587 Training Loss: tensor(0.0371)\n",
      "19588 Training Loss: tensor(0.0414)\n",
      "19589 Training Loss: tensor(0.0307)\n",
      "19590 Training Loss: tensor(0.0419)\n",
      "19591 Training Loss: tensor(0.0323)\n",
      "19592 Training Loss: tensor(0.0355)\n",
      "19593 Training Loss: tensor(0.0429)\n",
      "19594 Training Loss: tensor(0.0373)\n",
      "19595 Training Loss: tensor(0.0372)\n",
      "19596 Training Loss: tensor(0.0385)\n",
      "19597 Training Loss: tensor(0.0317)\n",
      "19598 Training Loss: tensor(0.0307)\n",
      "19599 Training Loss: tensor(0.0334)\n",
      "19600 Training Loss: tensor(0.0349)\n",
      "19601 Training Loss: tensor(0.0401)\n",
      "19602 Training Loss: tensor(0.0437)\n",
      "19603 Training Loss: tensor(0.0384)\n",
      "19604 Training Loss: tensor(0.0388)\n",
      "19605 Training Loss: tensor(0.0355)\n",
      "19606 Training Loss: tensor(0.0368)\n",
      "19607 Training Loss: tensor(0.0369)\n",
      "19608 Training Loss: tensor(0.0373)\n",
      "19609 Training Loss: tensor(0.0392)\n",
      "19610 Training Loss: tensor(0.0368)\n",
      "19611 Training Loss: tensor(0.0338)\n",
      "19612 Training Loss: tensor(0.0381)\n",
      "19613 Training Loss: tensor(0.0358)\n",
      "19614 Training Loss: tensor(0.0352)\n",
      "19615 Training Loss: tensor(0.0346)\n",
      "19616 Training Loss: tensor(0.0355)\n",
      "19617 Training Loss: tensor(0.0367)\n",
      "19618 Training Loss: tensor(0.0403)\n",
      "19619 Training Loss: tensor(0.0362)\n",
      "19620 Training Loss: tensor(0.0393)\n",
      "19621 Training Loss: tensor(0.0417)\n",
      "19622 Training Loss: tensor(0.0335)\n",
      "19623 Training Loss: tensor(0.0409)\n",
      "19624 Training Loss: tensor(0.0359)\n",
      "19625 Training Loss: tensor(0.0426)\n",
      "19626 Training Loss: tensor(0.0402)\n",
      "19627 Training Loss: tensor(0.0335)\n",
      "19628 Training Loss: tensor(0.0360)\n",
      "19629 Training Loss: tensor(0.0380)\n",
      "19630 Training Loss: tensor(0.0398)\n",
      "19631 Training Loss: tensor(0.0425)\n",
      "19632 Training Loss: tensor(0.0376)\n",
      "19633 Training Loss: tensor(0.0399)\n",
      "19634 Training Loss: tensor(0.0335)\n",
      "19635 Training Loss: tensor(0.0366)\n",
      "19636 Training Loss: tensor(0.0392)\n",
      "19637 Training Loss: tensor(0.0355)\n",
      "19638 Training Loss: tensor(0.0346)\n",
      "19639 Training Loss: tensor(0.0341)\n",
      "19640 Training Loss: tensor(0.0377)\n",
      "19641 Training Loss: tensor(0.0345)\n",
      "19642 Training Loss: tensor(0.0326)\n",
      "19643 Training Loss: tensor(0.0396)\n",
      "19644 Training Loss: tensor(0.0413)\n",
      "19645 Training Loss: tensor(0.0380)\n",
      "19646 Training Loss: tensor(0.0361)\n",
      "19647 Training Loss: tensor(0.0367)\n",
      "19648 Training Loss: tensor(0.0319)\n",
      "19649 Training Loss: tensor(0.0378)\n",
      "19650 Training Loss: tensor(0.0391)\n",
      "19651 Training Loss: tensor(0.0376)\n",
      "19652 Training Loss: tensor(0.0373)\n",
      "19653 Training Loss: tensor(0.0325)\n",
      "19654 Training Loss: tensor(0.0378)\n",
      "19655 Training Loss: tensor(0.0361)\n",
      "19656 Training Loss: tensor(0.0387)\n",
      "19657 Training Loss: tensor(0.0348)\n",
      "19658 Training Loss: tensor(0.0340)\n",
      "19659 Training Loss: tensor(0.0368)\n",
      "19660 Training Loss: tensor(0.0351)\n",
      "19661 Training Loss: tensor(0.0412)\n",
      "19662 Training Loss: tensor(0.0422)\n",
      "19663 Training Loss: tensor(0.0365)\n",
      "19664 Training Loss: tensor(0.0390)\n",
      "19665 Training Loss: tensor(0.0406)\n",
      "19666 Training Loss: tensor(0.0410)\n",
      "19667 Training Loss: tensor(0.0373)\n",
      "19668 Training Loss: tensor(0.0357)\n",
      "19669 Training Loss: tensor(0.0367)\n",
      "19670 Training Loss: tensor(0.0346)\n",
      "19671 Training Loss: tensor(0.0405)\n",
      "19672 Training Loss: tensor(0.0413)\n",
      "19673 Training Loss: tensor(0.0347)\n",
      "19674 Training Loss: tensor(0.0365)\n",
      "19675 Training Loss: tensor(0.0374)\n",
      "19676 Training Loss: tensor(0.0369)\n",
      "19677 Training Loss: tensor(0.0352)\n",
      "19678 Training Loss: tensor(0.0430)\n",
      "19679 Training Loss: tensor(0.0355)\n",
      "19680 Training Loss: tensor(0.0305)\n",
      "19681 Training Loss: tensor(0.0344)\n",
      "19682 Training Loss: tensor(0.0391)\n",
      "19683 Training Loss: tensor(0.0356)\n",
      "19684 Training Loss: tensor(0.0384)\n",
      "19685 Training Loss: tensor(0.0355)\n",
      "19686 Training Loss: tensor(0.0307)\n",
      "19687 Training Loss: tensor(0.0358)\n",
      "19688 Training Loss: tensor(0.0346)\n",
      "19689 Training Loss: tensor(0.0366)\n",
      "19690 Training Loss: tensor(0.0388)\n",
      "19691 Training Loss: tensor(0.0363)\n",
      "19692 Training Loss: tensor(0.0372)\n",
      "19693 Training Loss: tensor(0.0379)\n",
      "19694 Training Loss: tensor(0.0414)\n",
      "19695 Training Loss: tensor(0.0349)\n",
      "19696 Training Loss: tensor(0.0352)\n",
      "19697 Training Loss: tensor(0.0370)\n",
      "19698 Training Loss: tensor(0.0375)\n",
      "19699 Training Loss: tensor(0.0377)\n",
      "19700 Training Loss: tensor(0.0353)\n",
      "19701 Training Loss: tensor(0.0349)\n",
      "19702 Training Loss: tensor(0.0383)\n",
      "19703 Training Loss: tensor(0.0460)\n",
      "19704 Training Loss: tensor(0.0389)\n",
      "19705 Training Loss: tensor(0.0380)\n",
      "19706 Training Loss: tensor(0.0417)\n",
      "19707 Training Loss: tensor(0.0376)\n",
      "19708 Training Loss: tensor(0.0407)\n",
      "19709 Training Loss: tensor(0.0371)\n",
      "19710 Training Loss: tensor(0.0360)\n",
      "19711 Training Loss: tensor(0.0434)\n",
      "19712 Training Loss: tensor(0.0395)\n",
      "19713 Training Loss: tensor(0.0344)\n",
      "19714 Training Loss: tensor(0.0362)\n",
      "19715 Training Loss: tensor(0.0398)\n",
      "19716 Training Loss: tensor(0.0330)\n",
      "19717 Training Loss: tensor(0.0366)\n",
      "19718 Training Loss: tensor(0.0366)\n",
      "19719 Training Loss: tensor(0.0371)\n",
      "19720 Training Loss: tensor(0.0326)\n",
      "19721 Training Loss: tensor(0.0347)\n",
      "19722 Training Loss: tensor(0.0384)\n",
      "19723 Training Loss: tensor(0.0387)\n",
      "19724 Training Loss: tensor(0.0422)\n",
      "19725 Training Loss: tensor(0.0355)\n",
      "19726 Training Loss: tensor(0.0363)\n",
      "19727 Training Loss: tensor(0.0347)\n",
      "19728 Training Loss: tensor(0.0381)\n",
      "19729 Training Loss: tensor(0.0331)\n",
      "19730 Training Loss: tensor(0.0373)\n",
      "19731 Training Loss: tensor(0.0355)\n",
      "19732 Training Loss: tensor(0.0354)\n",
      "19733 Training Loss: tensor(0.0354)\n",
      "19734 Training Loss: tensor(0.0373)\n",
      "19735 Training Loss: tensor(0.0389)\n",
      "19736 Training Loss: tensor(0.0363)\n",
      "19737 Training Loss: tensor(0.0351)\n",
      "19738 Training Loss: tensor(0.0318)\n",
      "19739 Training Loss: tensor(0.0374)\n",
      "19740 Training Loss: tensor(0.0380)\n",
      "19741 Training Loss: tensor(0.0311)\n",
      "19742 Training Loss: tensor(0.0322)\n",
      "19743 Training Loss: tensor(0.0409)\n",
      "19744 Training Loss: tensor(0.0364)\n",
      "19745 Training Loss: tensor(0.0371)\n",
      "19746 Training Loss: tensor(0.0369)\n",
      "19747 Training Loss: tensor(0.0367)\n",
      "19748 Training Loss: tensor(0.0336)\n",
      "19749 Training Loss: tensor(0.0392)\n",
      "19750 Training Loss: tensor(0.0365)\n",
      "19751 Training Loss: tensor(0.0403)\n",
      "19752 Training Loss: tensor(0.0323)\n",
      "19753 Training Loss: tensor(0.0416)\n",
      "19754 Training Loss: tensor(0.0355)\n",
      "19755 Training Loss: tensor(0.0368)\n",
      "19756 Training Loss: tensor(0.0380)\n",
      "19757 Training Loss: tensor(0.0324)\n",
      "19758 Training Loss: tensor(0.0382)\n",
      "19759 Training Loss: tensor(0.0341)\n",
      "19760 Training Loss: tensor(0.0424)\n",
      "19761 Training Loss: tensor(0.0342)\n",
      "19762 Training Loss: tensor(0.0357)\n",
      "19763 Training Loss: tensor(0.0340)\n",
      "19764 Training Loss: tensor(0.0334)\n",
      "19765 Training Loss: tensor(0.0339)\n",
      "19766 Training Loss: tensor(0.0378)\n",
      "19767 Training Loss: tensor(0.0366)\n",
      "19768 Training Loss: tensor(0.0409)\n",
      "19769 Training Loss: tensor(0.0299)\n",
      "19770 Training Loss: tensor(0.0405)\n",
      "19771 Training Loss: tensor(0.0331)\n",
      "19772 Training Loss: tensor(0.0398)\n",
      "19773 Training Loss: tensor(0.0367)\n",
      "19774 Training Loss: tensor(0.0365)\n",
      "19775 Training Loss: tensor(0.0394)\n",
      "19776 Training Loss: tensor(0.0394)\n",
      "19777 Training Loss: tensor(0.0353)\n",
      "19778 Training Loss: tensor(0.0380)\n",
      "19779 Training Loss: tensor(0.0363)\n",
      "19780 Training Loss: tensor(0.0377)\n",
      "19781 Training Loss: tensor(0.0317)\n",
      "19782 Training Loss: tensor(0.0336)\n",
      "19783 Training Loss: tensor(0.0351)\n",
      "19784 Training Loss: tensor(0.0325)\n",
      "19785 Training Loss: tensor(0.0363)\n",
      "19786 Training Loss: tensor(0.0331)\n",
      "19787 Training Loss: tensor(0.0309)\n",
      "19788 Training Loss: tensor(0.0376)\n",
      "19789 Training Loss: tensor(0.0381)\n",
      "19790 Training Loss: tensor(0.0350)\n",
      "19791 Training Loss: tensor(0.0332)\n",
      "19792 Training Loss: tensor(0.0350)\n",
      "19793 Training Loss: tensor(0.0406)\n",
      "19794 Training Loss: tensor(0.0313)\n",
      "19795 Training Loss: tensor(0.0402)\n",
      "19796 Training Loss: tensor(0.0364)\n",
      "19797 Training Loss: tensor(0.0326)\n",
      "19798 Training Loss: tensor(0.0358)\n",
      "19799 Training Loss: tensor(0.0374)\n",
      "19800 Training Loss: tensor(0.0409)\n",
      "19801 Training Loss: tensor(0.0405)\n",
      "19802 Training Loss: tensor(0.0364)\n",
      "19803 Training Loss: tensor(0.0405)\n",
      "19804 Training Loss: tensor(0.0344)\n",
      "19805 Training Loss: tensor(0.0417)\n",
      "19806 Training Loss: tensor(0.0394)\n",
      "19807 Training Loss: tensor(0.0392)\n",
      "19808 Training Loss: tensor(0.0337)\n",
      "19809 Training Loss: tensor(0.0349)\n",
      "19810 Training Loss: tensor(0.0327)\n",
      "19811 Training Loss: tensor(0.0329)\n",
      "19812 Training Loss: tensor(0.0356)\n",
      "19813 Training Loss: tensor(0.0336)\n",
      "19814 Training Loss: tensor(0.0344)\n",
      "19815 Training Loss: tensor(0.0365)\n",
      "19816 Training Loss: tensor(0.0347)\n",
      "19817 Training Loss: tensor(0.0387)\n",
      "19818 Training Loss: tensor(0.0353)\n",
      "19819 Training Loss: tensor(0.0333)\n",
      "19820 Training Loss: tensor(0.0353)\n",
      "19821 Training Loss: tensor(0.0424)\n",
      "19822 Training Loss: tensor(0.0374)\n",
      "19823 Training Loss: tensor(0.0327)\n",
      "19824 Training Loss: tensor(0.0391)\n",
      "19825 Training Loss: tensor(0.0327)\n",
      "19826 Training Loss: tensor(0.0464)\n",
      "19827 Training Loss: tensor(0.0393)\n",
      "19828 Training Loss: tensor(0.0374)\n",
      "19829 Training Loss: tensor(0.0403)\n",
      "19830 Training Loss: tensor(0.0441)\n",
      "19831 Training Loss: tensor(0.0397)\n",
      "19832 Training Loss: tensor(0.0334)\n",
      "19833 Training Loss: tensor(0.0339)\n",
      "19834 Training Loss: tensor(0.0371)\n",
      "19835 Training Loss: tensor(0.0334)\n",
      "19836 Training Loss: tensor(0.0445)\n",
      "19837 Training Loss: tensor(0.0387)\n",
      "19838 Training Loss: tensor(0.0369)\n",
      "19839 Training Loss: tensor(0.0320)\n",
      "19840 Training Loss: tensor(0.0376)\n",
      "19841 Training Loss: tensor(0.0346)\n",
      "19842 Training Loss: tensor(0.0386)\n",
      "19843 Training Loss: tensor(0.0352)\n",
      "19844 Training Loss: tensor(0.0352)\n",
      "19845 Training Loss: tensor(0.0349)\n",
      "19846 Training Loss: tensor(0.0364)\n",
      "19847 Training Loss: tensor(0.0386)\n",
      "19848 Training Loss: tensor(0.0364)\n",
      "19849 Training Loss: tensor(0.0391)\n",
      "19850 Training Loss: tensor(0.0371)\n",
      "19851 Training Loss: tensor(0.0345)\n",
      "19852 Training Loss: tensor(0.0312)\n",
      "19853 Training Loss: tensor(0.0355)\n",
      "19854 Training Loss: tensor(0.0325)\n",
      "19855 Training Loss: tensor(0.0383)\n",
      "19856 Training Loss: tensor(0.0356)\n",
      "19857 Training Loss: tensor(0.0362)\n",
      "19858 Training Loss: tensor(0.0405)\n",
      "19859 Training Loss: tensor(0.0356)\n",
      "19860 Training Loss: tensor(0.0417)\n",
      "19861 Training Loss: tensor(0.0364)\n",
      "19862 Training Loss: tensor(0.0358)\n",
      "19863 Training Loss: tensor(0.0354)\n",
      "19864 Training Loss: tensor(0.0340)\n",
      "19865 Training Loss: tensor(0.0349)\n",
      "19866 Training Loss: tensor(0.0390)\n",
      "19867 Training Loss: tensor(0.0372)\n",
      "19868 Training Loss: tensor(0.0428)\n",
      "19869 Training Loss: tensor(0.0382)\n",
      "19870 Training Loss: tensor(0.0427)\n",
      "19871 Training Loss: tensor(0.0375)\n",
      "19872 Training Loss: tensor(0.0395)\n",
      "19873 Training Loss: tensor(0.0375)\n",
      "19874 Training Loss: tensor(0.0400)\n",
      "19875 Training Loss: tensor(0.0354)\n",
      "19876 Training Loss: tensor(0.0428)\n",
      "19877 Training Loss: tensor(0.0391)\n",
      "19878 Training Loss: tensor(0.0380)\n",
      "19879 Training Loss: tensor(0.0430)\n",
      "19880 Training Loss: tensor(0.0357)\n",
      "19881 Training Loss: tensor(0.0337)\n",
      "19882 Training Loss: tensor(0.0373)\n",
      "19883 Training Loss: tensor(0.0410)\n",
      "19884 Training Loss: tensor(0.0378)\n",
      "19885 Training Loss: tensor(0.0356)\n",
      "19886 Training Loss: tensor(0.0334)\n",
      "19887 Training Loss: tensor(0.0368)\n",
      "19888 Training Loss: tensor(0.0410)\n",
      "19889 Training Loss: tensor(0.0384)\n",
      "19890 Training Loss: tensor(0.0377)\n",
      "19891 Training Loss: tensor(0.0394)\n",
      "19892 Training Loss: tensor(0.0352)\n",
      "19893 Training Loss: tensor(0.0394)\n",
      "19894 Training Loss: tensor(0.0372)\n",
      "19895 Training Loss: tensor(0.0393)\n",
      "19896 Training Loss: tensor(0.0400)\n",
      "19897 Training Loss: tensor(0.0380)\n",
      "19898 Training Loss: tensor(0.0334)\n",
      "19899 Training Loss: tensor(0.0409)\n",
      "19900 Training Loss: tensor(0.0354)\n",
      "19901 Training Loss: tensor(0.0361)\n",
      "19902 Training Loss: tensor(0.0414)\n",
      "19903 Training Loss: tensor(0.0343)\n",
      "19904 Training Loss: tensor(0.0358)\n",
      "19905 Training Loss: tensor(0.0377)\n",
      "19906 Training Loss: tensor(0.0369)\n",
      "19907 Training Loss: tensor(0.0365)\n",
      "19908 Training Loss: tensor(0.0391)\n",
      "19909 Training Loss: tensor(0.0385)\n",
      "19910 Training Loss: tensor(0.0346)\n",
      "19911 Training Loss: tensor(0.0377)\n",
      "19912 Training Loss: tensor(0.0304)\n",
      "19913 Training Loss: tensor(0.0350)\n",
      "19914 Training Loss: tensor(0.0358)\n",
      "19915 Training Loss: tensor(0.0347)\n",
      "19916 Training Loss: tensor(0.0350)\n",
      "19917 Training Loss: tensor(0.0323)\n",
      "19918 Training Loss: tensor(0.0401)\n",
      "19919 Training Loss: tensor(0.0366)\n",
      "19920 Training Loss: tensor(0.0378)\n",
      "19921 Training Loss: tensor(0.0397)\n",
      "19922 Training Loss: tensor(0.0376)\n",
      "19923 Training Loss: tensor(0.0373)\n",
      "19924 Training Loss: tensor(0.0402)\n",
      "19925 Training Loss: tensor(0.0384)\n",
      "19926 Training Loss: tensor(0.0372)\n",
      "19927 Training Loss: tensor(0.0387)\n",
      "19928 Training Loss: tensor(0.0379)\n",
      "19929 Training Loss: tensor(0.0393)\n",
      "19930 Training Loss: tensor(0.0370)\n",
      "19931 Training Loss: tensor(0.0385)\n",
      "19932 Training Loss: tensor(0.0373)\n",
      "19933 Training Loss: tensor(0.0341)\n",
      "19934 Training Loss: tensor(0.0343)\n",
      "19935 Training Loss: tensor(0.0395)\n",
      "19936 Training Loss: tensor(0.0329)\n",
      "19937 Training Loss: tensor(0.0393)\n",
      "19938 Training Loss: tensor(0.0335)\n",
      "19939 Training Loss: tensor(0.0359)\n",
      "19940 Training Loss: tensor(0.0408)\n",
      "19941 Training Loss: tensor(0.0385)\n",
      "19942 Training Loss: tensor(0.0329)\n",
      "19943 Training Loss: tensor(0.0318)\n",
      "19944 Training Loss: tensor(0.0406)\n",
      "19945 Training Loss: tensor(0.0363)\n",
      "19946 Training Loss: tensor(0.0386)\n",
      "19947 Training Loss: tensor(0.0341)\n",
      "19948 Training Loss: tensor(0.0326)\n",
      "19949 Training Loss: tensor(0.0376)\n",
      "19950 Training Loss: tensor(0.0392)\n",
      "19951 Training Loss: tensor(0.0325)\n",
      "19952 Training Loss: tensor(0.0381)\n",
      "19953 Training Loss: tensor(0.0322)\n",
      "19954 Training Loss: tensor(0.0393)\n",
      "19955 Training Loss: tensor(0.0379)\n",
      "19956 Training Loss: tensor(0.0398)\n",
      "19957 Training Loss: tensor(0.0396)\n",
      "19958 Training Loss: tensor(0.0359)\n",
      "19959 Training Loss: tensor(0.0345)\n",
      "19960 Training Loss: tensor(0.0357)\n",
      "19961 Training Loss: tensor(0.0381)\n",
      "19962 Training Loss: tensor(0.0373)\n",
      "19963 Training Loss: tensor(0.0373)\n",
      "19964 Training Loss: tensor(0.0356)\n",
      "19965 Training Loss: tensor(0.0369)\n",
      "19966 Training Loss: tensor(0.0356)\n",
      "19967 Training Loss: tensor(0.0351)\n",
      "19968 Training Loss: tensor(0.0334)\n",
      "19969 Training Loss: tensor(0.0369)\n",
      "19970 Training Loss: tensor(0.0377)\n",
      "19971 Training Loss: tensor(0.0358)\n",
      "19972 Training Loss: tensor(0.0405)\n",
      "19973 Training Loss: tensor(0.0387)\n",
      "19974 Training Loss: tensor(0.0372)\n",
      "19975 Training Loss: tensor(0.0390)\n",
      "19976 Training Loss: tensor(0.0430)\n",
      "19977 Training Loss: tensor(0.0364)\n",
      "19978 Training Loss: tensor(0.0361)\n",
      "19979 Training Loss: tensor(0.0408)\n",
      "19980 Training Loss: tensor(0.0353)\n",
      "19981 Training Loss: tensor(0.0314)\n",
      "19982 Training Loss: tensor(0.0360)\n",
      "19983 Training Loss: tensor(0.0375)\n",
      "19984 Training Loss: tensor(0.0401)\n",
      "19985 Training Loss: tensor(0.0385)\n",
      "19986 Training Loss: tensor(0.0333)\n",
      "19987 Training Loss: tensor(0.0379)\n",
      "19988 Training Loss: tensor(0.0369)\n",
      "19989 Training Loss: tensor(0.0324)\n",
      "19990 Training Loss: tensor(0.0346)\n",
      "19991 Training Loss: tensor(0.0381)\n",
      "19992 Training Loss: tensor(0.0389)\n",
      "19993 Training Loss: tensor(0.0271)\n",
      "19994 Training Loss: tensor(0.0397)\n",
      "19995 Training Loss: tensor(0.0376)\n",
      "19996 Training Loss: tensor(0.0383)\n",
      "19997 Training Loss: tensor(0.0309)\n",
      "19998 Training Loss: tensor(0.0345)\n",
      "19999 Training Loss: tensor(0.0389)\n"
     ]
    }
   ],
   "source": [
    "# actural training of the nn model\n",
    "iterations = 20000\n",
    "previous_validation_loss = 99999999.0\n",
    "loss_data = []\n",
    "for epoch in range(iterations):\n",
    "    optimizer.zero_grad() # to make the gradients zero\n",
    "\n",
    "    # loss based on initial boundary conditions\n",
    "    pt_x_ibc = Variable(torch.from_numpy(x_bc).float(), requires_grad=False).to(device)\n",
    "    pt_y_ibc = Variable(torch.from_numpy(y_bc).float(), requires_grad=False).to(device)\n",
    "    pt_u_ibc = Variable(torch.from_numpy(u_bc).float(), requires_grad=False).to(device)\n",
    "\n",
    "    net_bc_out = net(pt_x_ibc,pt_y_ibc)\n",
    "    mse_u = mse_cost_function(net_bc_out,pt_u_ibc)\n",
    "\n",
    "    # loss based on PDE\n",
    "    x_collocation = np.random.uniform(low=-1, high=1, size=(1800,1))\n",
    "    y_collocation = np.random.uniform(low=-1, high=1, size=(1800,1))\n",
    "    all_zeros = np.zeros((1800,1))\n",
    "\n",
    "    pt_x_collocation = Variable(torch.from_numpy(x_collocation).float(), requires_grad=True).to(device)\n",
    "    pt_y_collocation = Variable(torch.from_numpy(y_collocation).float(), requires_grad=True).to(device)\n",
    "    pt_all_zeros = Variable(torch.from_numpy(all_zeros).float(), requires_grad=False).to(device)\n",
    "    \n",
    "    f_out = f(pt_x_collocation,pt_y_collocation,net)\n",
    "    mse_f = mse_cost_function(f_out,pt_all_zeros)\n",
    "\n",
    "    # combining the loss function\n",
    "    loss = mse_u + mse_f\n",
    "\n",
    "    loss.backward() # computing gradients using backward propagation\n",
    "    optimizer.step()\n",
    "\n",
    "    with torch.autograd.no_grad():\n",
    "        print(epoch,\"Training Loss:\",loss.data)\n",
    "        loss_data.append(loss.data.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_data)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data\n",
    "with open('pinn_loss.npy', 'wb') as f:\n",
    "    np.save(f, np.array(loss_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "torch.save(net.state_dict(), \"laplace_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (hidden_layer1): Linear(in_features=2, out_features=10, bias=True)\n",
       "  (hidden_layer2): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (hidden_layer3): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (hidden_layer4): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (hidden_layer5): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (output_layer): Linear(in_features=10, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "net = Net()\n",
    "net.load_state_dict(torch.load(\"laplace_model.pt\"))\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-1,1,0.01)\n",
    "y = np.arange(-1,1,0.01)\n",
    "ms_x, ms_y = np.meshgrid(x,y)\n",
    "x = np.ravel(ms_x).reshape(-1,1)\n",
    "y = np.ravel(ms_y).reshape(-1,1)\n",
    "\n",
    "pt_x = Variable(torch.from_numpy(x).float(), requires_grad=True).to(device)\n",
    "pt_y = Variable(torch.from_numpy(y).float(), requires_grad=True).to(device)\n",
    "pt_u = net(pt_x,pt_y)\n",
    "u = pt_u.data.cpu().numpy()\n",
    "ms_uu = u.reshape(ms_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7fa698262430>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.contourf(np.arange(-1,1,0.01),np.arange(-1,1,0.01),ms_uu)\n",
    "plt.axis('scaled')\n",
    "plt.xlim([-1, 1])\n",
    "plt.ylim([-1, 1])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Warning:</span><span style=\"color: #808000; text-decoration-color: #808000\"> VTK requires 3D points, but 2D points given. Appending </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">0</span><span style=\"color: #808000; text-decoration-color: #808000\"> third component.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mWarning:\u001b[0m\u001b[33m VTK requires 3D points, but 2D points given. Appending \u001b[0m\u001b[1;33m0\u001b[0m\u001b[33m third component.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import meshio\n",
    "from scipy.spatial import Delaunay\n",
    "\n",
    "# two triangles and one quad\n",
    "points = np.concatenate((x,y),axis=1)\n",
    "points_connection = Delaunay(points)\n",
    "cells = [(\"triangle\", points_connection.simplices)]\n",
    "\n",
    "mesh = meshio.Mesh(\n",
    "    points,\n",
    "    cells,\n",
    "    point_data={\"u\": u},\n",
    ")\n",
    "mesh.write(\n",
    "    \"laplace.vtk\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "926ad5c238a4420011e83b59655d6cd21b021d9ee1aa0438d38aa969cfe55744"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch11')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
