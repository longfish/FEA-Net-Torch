{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d(X, K):\n",
    "\n",
    "    # Convolution in deep learning is a misnomer.\n",
    "    # In fact, it is cross-correlation.\n",
    "    # https://d2l.ai/chapter_convolutional-neural-networks/conv-layer.html\n",
    "    # This is equivalent as Conv2D that that input_channel == output_channel == 1 and stride == 1.\n",
    "\n",
    "    assert X.dim() == 2 and K.dim() == 2\n",
    "\n",
    "    h, w = K.shape\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()\n",
    "\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparse_kernel_matrix(K, h_X, w_X):\n",
    "\n",
    "    # Assuming no channels and stride == 1.\n",
    "    # Convert the kernel matrix to sparse matrix (dense matrix with lots of zeros in fact).\n",
    "    # This is a little bit brain-twisting.\n",
    "\n",
    "    h_K, w_K = K.shape\n",
    "\n",
    "    h_Y, w_Y = h_X - h_K + 1, w_X - w_K + 1\n",
    "\n",
    "    W = torch.zeros((h_Y * w_Y, h_X * w_X))\n",
    "    for i in range(h_Y):\n",
    "        for j in range(w_Y):\n",
    "            for ii in range(h_K):\n",
    "                for jj in range(w_K):\n",
    "                    W[i * w_Y + j, i * w_X + j + ii * w_X + jj] = K[ii, jj]\n",
    "\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_as_matrix_mul(X, K):\n",
    "\n",
    "    # Assuming no channels and stride == 1.\n",
    "    # Convert the kernel matrix to sparse matrix (dense matrix with lots of zeros in fact).\n",
    "    # This is a little bit brain-twisting.\n",
    "\n",
    "    h_K, w_K = K.shape\n",
    "    h_X, w_X = X.shape\n",
    "\n",
    "    h_Y, w_Y = h_X - h_K + 1, w_X - w_K + 1\n",
    "\n",
    "    W = get_sparse_kernel_matrix(K=K, h_X=h_X, w_X=w_X) # given dimensions of the sparse matrix\n",
    "\n",
    "    Y = torch.matmul(W, X.reshape(-1)).reshape(h_Y, w_Y)\n",
    "\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_transposed_2d_as_matrix_mul(X, K):\n",
    "\n",
    "    # Assuming no channels and stride == 1.\n",
    "    # Convert the kernel matrix to sparse matrix (dense matrix with lots of zeros in fact).\n",
    "    # This is a little bit brain-twisting.\n",
    "\n",
    "    h_K, w_K = K.shape\n",
    "    h_X, w_X = X.shape\n",
    "\n",
    "    h_Y, w_Y = h_X + h_K - 1, w_X + w_K - 1\n",
    "\n",
    "    # It's like the kernel were applied on the output tensor.\n",
    "    W = get_sparse_kernel_matrix(K=K, h_X=h_Y, w_X=w_Y)\n",
    "\n",
    "    # Weight matrix tranposed.\n",
    "    Y = torch.matmul(W.T, X.reshape(-1)).reshape(h_Y, w_Y)\n",
    "\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      "tensor([[ 0.,  1.,  2.,  3.,  4.,  5.],\n",
      "        [ 6.,  7.,  8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15., 16., 17.],\n",
      "        [18., 19., 20., 21., 22., 23.],\n",
      "        [24., 25., 26., 27., 28., 29.]])\n",
      "K:\n",
      "tensor([[0., 1., 2., 3.],\n",
      "        [4., 5., 6., 7.]])\n",
      "Cross-Correlation:\n",
      "tensor([[184., 212., 240.],\n",
      "        [352., 380., 408.],\n",
      "        [520., 548., 576.],\n",
      "        [688., 716., 744.]])\n"
     ]
    }
   ],
   "source": [
    "X = torch.arange(30).reshape(5, 6).float()\n",
    "K = torch.arange(8).reshape(2, 4).float()\n",
    "print(\"X:\")\n",
    "print(X)\n",
    "print(\"K:\")\n",
    "print(K)\n",
    "print(\"Cross-Correlation:\")\n",
    "Y = corr2d(X=X, K=K)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolution:\n",
      "tensor([[184., 212., 240.],\n",
      "        [352., 380., 408.],\n",
      "        [520., 548., 576.],\n",
      "        [688., 716., 744.]])\n"
     ]
    }
   ],
   "source": [
    "conv = nn.Conv2d(in_channels=1,\n",
    "                 out_channels=1,\n",
    "                 kernel_size=K.shape,\n",
    "                 padding=0,\n",
    "                 stride=1,\n",
    "                 bias=False)\n",
    "conv.weight.data = K.unsqueeze(0).unsqueeze(0)\n",
    "Z1 = conv(X.unsqueeze(0).unsqueeze(0)).squeeze(0).squeeze(0).detach()\n",
    "print(\"Convolution:\")\n",
    "print(Z1)\n",
    "assert torch.equal(Y, Z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolution as Matrix Multiplication:\n",
      "tensor([[184., 212., 240.],\n",
      "        [352., 380., 408.],\n",
      "        [520., 548., 576.],\n",
      "        [688., 716., 744.]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Convolution as Matrix Multiplication:\")\n",
    "Z2 = conv2d_as_matrix_mul(X=X, K=K)\n",
    "print(Z2)\n",
    "assert torch.equal(Y, Z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transposed Convolution:\n",
      "tensor([[    0.,   184.,   580.,  1216.,  1116.,   720.],\n",
      "        [  736.,  2120.,  4208.,  5984.,  4880.,  2904.],\n",
      "        [ 1408.,  3800.,  7232., 10016.,  7904.,  4584.],\n",
      "        [ 2080.,  5480., 10256., 14048., 10928.,  6264.],\n",
      "        [ 2752.,  6304., 10684., 12832.,  9476.,  5208.]])\n"
     ]
    }
   ],
   "source": [
    "conv_transposed = nn.ConvTranspose2d(in_channels=1,\n",
    "                                     out_channels=1,\n",
    "                                     kernel_size=K.shape,\n",
    "                                     padding=0,\n",
    "                                     stride=1,\n",
    "                                     bias=False)\n",
    "conv_transposed.weight.data = K.unsqueeze(0).unsqueeze(0)\n",
    "Z3 = conv_transposed(Y.unsqueeze(0).unsqueeze(0)).squeeze(0).squeeze(0).detach()\n",
    "print(\"Transposed Convolution:\")\n",
    "print(Z3)\n",
    "# The shape will \"go back\".\n",
    "assert Z3.shape == X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transposed Convolution as Matrix Multiplication:\n",
      "tensor([[    0.,   184.,   580.,  1216.,  1116.,   720.],\n",
      "        [  736.,  2120.,  4208.,  5984.,  4880.,  2904.],\n",
      "        [ 1408.,  3800.,  7232., 10016.,  7904.,  4584.],\n",
      "        [ 2080.,  5480., 10256., 14048., 10928.,  6264.],\n",
      "        [ 2752.,  6304., 10684., 12832.,  9476.,  5208.]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Transposed Convolution as Matrix Multiplication:\")\n",
    "Z4 = conv_transposed_2d_as_matrix_mul(X=Y, K=K)\n",
    "print(Z4)\n",
    "assert torch.equal(Z3, Z4)\n",
    "assert Z4.shape == X.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "adc47a0b8fb92a57ced3d982b13e77220f6a395e2f8555d9f7ba3376cde9af4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
